{"user_input": "Can you explane what AMG-RAG is and how it improves medical question answering?", "reference_contexts": ["Omar & Mohammed / Journal of Applied Science and Technology Trends Vol. 06, No. 02, pp. 342 –357 (2025) 345 record-high accuracy, while open-source models achieved impressive gains through prompt optimization. Feng et al.[22] developed the Knowledge Graph-based Thought (KGT) framework that integrated LLMs with a pan- cancer knowledge graph for biomedical question answering. KGT was designed to reason on the knowledge graph schema and identify optimal subgraphs to use for directing accurate answer generation, all without fine-tuning the LLMs. The framework is benchmarked against a new dataset (PcQA) designed specifically for pan-cancer KGQA tasks and has outperformed all existing state-of-the-art approaches by a rather large margin. KGT’s practicality in biomedical issues was highlighted through case studies for drug repositioning, drug resistance, and biomarker discovery. Their approach exhibited robust adaptability among various LLMs. Rezaei et al.[26] developed AMG-RAG, a dynamic framework that utilizes autonomous LLM agents with medical search tools in the continuous construction and real-time updating of Medical Knowledge Graphs (MKGs). Their system incorporated confidence scoring and multi-hop reasoning to improve accuracy and interpretability in medical question answering. AMG-RAG outperformed size models on both very hard MEDQA benchmarks and more accessible MedMCQA ones, proving that it could conduct efficient reasoning based on current structured medical knowledge. They also used Neo4j to manage the knowledge graphs while adding external searches to ensure the latest data. Tiwari et al.[24] presented Auto-Cypher, a recent automated pipeline for producing high-quality synthetic data for training LLMs by mapping natural language to Cypher queries for graph databases like Neo4j. The pipeline deployed the novelty of LLM- as-database-filler to synthesize Neo4j databases for the execution of generated queries to ensure their correctness. A sizable dataset called SynthCypher was created, spanning multiple domains and complex queries, leading to a 40% improvement in LLM performance on Cypher generation. The datasets were used to fine-tune open-source models such as LLaMA, Mistral, and Qwen, and the SPIDER benchmark was adapted for evaluation purposes. Mohammed et al.[29] proposed a hybridized GraphRAG framework combining Neo4j-based UMLS knowledge graphs with a vector store for medical textbooks to create an improved U.S.M.L.E.-style clinical question-answering approach. The project integrated symbolic reasoning from knowledge graphs with semantic retrieval performed on text embeddings to enhance relevance and accuracy via adaptive re-ranking and query expansion. The system had the answers produced by GPT-4o- Mini, with different prompting strategies encouraging evidence- based and traceable responses grounded in verified medical knowledge. Experiments showed that the hybrid approach improved factual accuracy and citation fidelity as compared to the L.L.M.-only approach, enhancing transparency and reliability. It is shown that binding both structured and unstructured medical knowledge sources could aid in ameliorating hallucinations and hence improve clinical trustworthiness in AI-driven medical QA. Yang et al.[30] articulated sepsis knowledge graph was crafted by combining multicenter clinical data from over 10,000 patients with the help of GPT-4 for entity recognition and relationship extraction. Real-world data were collected from three hospitals and integrated with clinical guidelines and databases from the public domain. The knowledge graph contained 1,894 nodes and 2,021 relationships pertaining to diseases, symptoms, biomarkers, treatments, and complications. GPT outperformed other models in every resolution on sepsis- specific datasets to obtain high F1-score results. The constructed graph highlighted complex interactions in sepsis for assisting clinical decision-making and was implemented on Neo4j. Guan et al.[20] proposed a novel method for constructing a local knowledge graph from retrieved biomedical documents by extracting propositional claims. They carried out layer wise summarization on this graph to capture multi-document relationships and provide comprehensive contextual information to a language model for question-answering purposes. The method resolved issues in multi-document biomedical QA, such as noise cancellation and efficient context usage. They then tested their method on several benchmarks for biomedical question answering, achieving performance at least comparable to, if not better than, existing retrieval-augmented generation (RAG) baselines. The study established enhanced reasoning and answer accuracy of the model achieved through structured graph summarization. Previous studies have improved biomedical QA using KGs and LLMs, but important gaps remain. Most systems lack transparent, graph-based justifications, rely on limited evaluation methods, or depend on cloud resources that reduce privacy and reproducibility. Our framework addresses these gaps by providing visible Cypher queries with evidence subgraphs, applying comprehensive performance metrics across difficulty levels, and ensuring fully local, privacy-preserving deployment. Table I summarizes key previous studies on biomedical knowledge graphs and question answering, outlining their methods, datasets, and main limitations."], "reference": "AMG-RAG is a dynamic framework developed by Rezaei et al. that utilizes autonomous large language model (LLM) agents combined with medical search tools to continuously construct and update Medical Knowledge Graphs (MKGs) in real time. The system incorporates confidence scoring and multi-hop reasoning to enhance accuracy and interpretability in medical question answering. AMG-RAG outperformed larger models on challenging MEDQA benchmarks as well as more accessible MedMCQA tasks, demonstrating its ability to efficiently reason based on current structured medical knowledge. It uses Neo4j to manage the knowledge graphs and integrates external searches to ensure the inclusion of the latest data.", "synthesizer_name": "single_hop_specifc_query_synthesizer"}
{"user_input": "What LLMs do in biomedical system for understand natural language?", "reference_contexts": ["III. PRELIMINARIES This section outlines the fundamental concepts required to understand the proposed system. It introduces biomedical knowledge graphs, explains how Neo4j stores data in graph form, and describes the use of Cypher for querying. It also provides a brief overview of large language models (LLMs) and their role in interpreting natural language. A. Biomedical Knowledge Graphs Biomedical Knowledge Graphs (BKGs) provide a structured representation of complex biomedical information by modeling diverse medical entities, such as diseases, drugs, symptoms, and biological pathways, as interconnected nodes within a graph structure. The edges in these graphs represent the semantic relationships between these entities, including ’treats’, ’causes’, ’interacts with’ and many others, as illustrated in Fig 1. This form of representation enables the integration of heterogeneous biomedical data from a wide range of sources, including"], "reference": "Large language models (LLMs) play a role in interpreting natural language within the proposed biomedical system.", "synthesizer_name": "single_hop_specifc_query_synthesizer"}
{"user_input": "how LLaMA 3 help in answer from biomedical data?", "reference_contexts": ["Omar & Mohammed / Journal of Applied Science and Technology Trends Vol. 06, No. 02, pp. 342 –357 (2025) 348 drugs, diseases, symptoms) and execution returns structured data (tuples) relevant to the question. Step 5. Answer Synthesis: The structured tuples flow to Answer Synthesis, which aggregates and formats them into a concise raw answer. This raw answer is sent back to LLaMA 3 to optionally refine the phrasing while preserving the retrieved facts. Step 6. Result Presentation: LLaMA 3 produces the final answer, which the interface displays together with the executed Cypher query and an optional preview of the returned rows, improving transparency and trust. The pipeline couples LLM-based language understanding (LLaMA 3) with a schema-grounded Neo4j knowledge graph. The Cypher Query Gen refines the query formulation, Query Execution retrieves evidence and Answer Synthesis converts structured results into readable outputs that produce answers that are accurate, interpretable, and easy to audit directly from the displayed query and evidence."], "reference": "LLaMA 3 is used to refine the phrasing of the raw answer while preserving the retrieved facts, and it produces the final answer that is displayed along with the executed Cypher query and an optional preview of the returned rows, improving transparency and trust in the biomedical question answering pipeline.", "synthesizer_name": "single_hop_specifc_query_synthesizer"}
{"user_input": "Can you explane in detale what the Biomedicl Knowldge Hub is and how it is constucted, including the types of entities and relationships it contains?", "reference_contexts": ["B. Dataset and Knowledge Graph Construction 1) Dataset The integrated Biomedical Knowledge Hub (iBKH), a large biomedical knowledge base, forms the first level of the system and integrates information from various curated high-quality biomedical databases. This implies that the data set includes various types of entities, such as diseases, symptoms, drugs, biological pathways, etc. This study used the representative subset of the iBKH dataset, which contained 65828 biomedical entities. These entities are semantically interconnected through a total of 3004166 relationships, thus creating a rich knowledge graph. The iBKH dataset was originally introduced in [11], and it is freely available at (https://github.com/wcm-wanglab/iBKH). This dataset is the core semantic foundation upon which this study is built. The knowledge graph is populated from multiple tabular sources (CSV files), each listing entities or relationships. The main input files and their contents are as follows: • Disease vocabulary(disease_vocab.csv): Contains columns such as primary (a unique disease ID), name, do_id (Disease Ontology ID), kegg_id, and umls_cui (UMLS Concept ID). Each row represents a disease node with external identifiers. • Drug vocabulary (drug_vocab.csv): Includes primary (unique drug ID), name, drugbank_id, kegg_id, pharmgkb_id, umls_cui, mesh_id, iDISK_id and CID (PubChem ID). Each row defines a drug node with standard database identifiers. • Symptom vocabulary (symptom_vocab.csv): Contains primary (unique symptom ID), name, mesh_id, umls_cui and iDISK_id. Each row defines a symptom node. • Side effect vocabulary (side_effect_vocab.csv): Includes primary (unique side-effect ID) and name. Each row represents a side-effect node (with UMLS ID when available). • Pathway vocabulary (pathway_vocab.csv): Contains primary (unique pathway ID), name, reactome_id, go_id, and kegg_id. Each row defines a biological pathway node. Relationship files (each row typically contains two entity IDs and one or more boolean flags or codes) include: • Disease–Symptom links (Di_Sy_res.csv): Rows include Disease and Symptom IDs, a presence flag (1 or 0) and a data source. If Present = 1, a HAS_SYMPTOM edge is created from the disease to the symptom, with properties for presence and source. • Disease–Disease links (di_di_res.csv): Rows include Disease_1 and Disease_2 IDs with binary flags for is_a and Resemble. If is_a = 1, an (IS_A) edge is created (Disease_1 → Disease_2); if Resemble = 1, a RESEMBLES edge is created. The source field is used for provenance. • Drug–Disease links (D_Di_res.csv): Includes Drug and Disease IDs with several binary flags. If a flag equals 1, a corresponding edge is created: o TREATS (Treats = 1) o PALLIATES (Palliates = 1) o ASSOCIATED_WITH (Associate = 1) o ALLEVIATES_REDUCES (alleviates = 1) o TREATMENT_THERAPY (treatment/therapy = 1) o INHIBITS_CELL_GROWTH (inhibits cell growth = 1) o HAS_BIOMARKER (biomarkers = 1) o PREVENTS_SUPPRESSES (prevents/suppresses = 1) o ROLE_IN_PATHOGENESIS (role in disease pathogenesis = 1) • Drug–SideEffect links (D_SE_res.csv): Contains Drug and SideEffect IDs with a Source column. Each row creates a CAUSES edge from the drug to the side effect, with source as an edge property. • Drug–Drug interactions (D_D_res.csv): Rows include Drug_1 and Drug_2 IDs with flags for Interaction and Resemble. If Interaction = 1, an INTERACTS_WITH edge is created (bidirectional). If Resemble = 1, a RESEMBLES edge is added. • Drug–Pathway links (D_Pwy_res.csv): Includes Drug ID and Pathway ID. Each row generates an ASSOCIATED_WITH edge from the drug to the pathway. • Disease–Pathway links (Di_Pwy_res.csv): Contains Disease ID and Pathway ID. Each row creates an ASSOCIATED_WITH edge from the disease to the pathway. 2) Data Upload Performance The time required to upload different types of entities and relationships into the Neo4j biomedical knowledge graph, measured in seconds. These measurements reflect both the size and complexity of the data being processed. As shown in Table II, the longest upload time is for Drug- Drug Relationships, which takes approximately 190 seconds due to the large number of edges (over 3 million). Following this, Disease-Disease and Drug-Disease Relationships also require considerable time for loading. On the other hand, individual"], "reference": "The integrated Biomedical Knowledge Hub (iBKH) is a large biomedical knowledge base that forms the first level of the system by integrating information from various curated high-quality biomedical databases. It includes various types of entities such as diseases, symptoms, drugs, biological pathways, and more. The representative subset used in this study contains 65,828 biomedical entities that are semantically interconnected through a total of 3,004,166 relationships, creating a rich knowledge graph. The knowledge graph is populated from multiple tabular sources (CSV files), each listing entities or relationships. Entity files include disease vocabulary, drug vocabulary, symptom vocabulary, side effect vocabulary, and pathway vocabulary, each containing unique IDs and standard database identifiers. Relationship files describe links such as Disease–Symptom, Disease–Disease, Drug–Disease, Drug–SideEffect, Drug–Drug interactions, Drug–Pathway, and Disease–Pathway, with specific edge types like HAS_SYMPTOM, IS_A, TREATS, CAUSES, INTERACTS_WITH, and ASSOCIATED_WITH. These relationships are defined by binary flags or presence indicators and include provenance information where applicable.", "synthesizer_name": "single_hop_specifc_query_synthesizer"}
{"user_input": "How do biomedical knowledge graphs utilize the Neo4j graph database and Cypher queries to improve biomedical question answering accuracy and transparency?", "reference_contexts": ["<1-hop>\n\nOmar & Mohammed / Journal of Applied Science and Technology Trends Vol. 06, No. 02, pp. 342 –357 (2025) 345 record-high accuracy, while open-source models achieved impressive gains through prompt optimization. Feng et al.[22] developed the Knowledge Graph-based Thought (KGT) framework that integrated LLMs with a pan- cancer knowledge graph for biomedical question answering. KGT was designed to reason on the knowledge graph schema and identify optimal subgraphs to use for directing accurate answer generation, all without fine-tuning the LLMs. The framework is benchmarked against a new dataset (PcQA) designed specifically for pan-cancer KGQA tasks and has outperformed all existing state-of-the-art approaches by a rather large margin. KGT’s practicality in biomedical issues was highlighted through case studies for drug repositioning, drug resistance, and biomarker discovery. Their approach exhibited robust adaptability among various LLMs. Rezaei et al.[26] developed AMG-RAG, a dynamic framework that utilizes autonomous LLM agents with medical search tools in the continuous construction and real-time updating of Medical Knowledge Graphs (MKGs). Their system incorporated confidence scoring and multi-hop reasoning to improve accuracy and interpretability in medical question answering. AMG-RAG outperformed size models on both very hard MEDQA benchmarks and more accessible MedMCQA ones, proving that it could conduct efficient reasoning based on current structured medical knowledge. They also used Neo4j to manage the knowledge graphs while adding external searches to ensure the latest data. Tiwari et al.[24] presented Auto-Cypher, a recent automated pipeline for producing high-quality synthetic data for training LLMs by mapping natural language to Cypher queries for graph databases like Neo4j. The pipeline deployed the novelty of LLM- as-database-filler to synthesize Neo4j databases for the execution of generated queries to ensure their correctness. A sizable dataset called SynthCypher was created, spanning multiple domains and complex queries, leading to a 40% improvement in LLM performance on Cypher generation. The datasets were used to fine-tune open-source models such as LLaMA, Mistral, and Qwen, and the SPIDER benchmark was adapted for evaluation purposes. Mohammed et al.[29] proposed a hybridized GraphRAG framework combining Neo4j-based UMLS knowledge graphs with a vector store for medical textbooks to create an improved U.S.M.L.E.-style clinical question-answering approach. The project integrated symbolic reasoning from knowledge graphs with semantic retrieval performed on text embeddings to enhance relevance and accuracy via adaptive re-ranking and query expansion. The system had the answers produced by GPT-4o- Mini, with different prompting strategies encouraging evidence- based and traceable responses grounded in verified medical knowledge. Experiments showed that the hybrid approach improved factual accuracy and citation fidelity as compared to the L.L.M.-only approach, enhancing transparency and reliability. It is shown that binding both structured and unstructured medical knowledge sources could aid in ameliorating hallucinations and hence improve clinical trustworthiness in AI-driven medical QA. Yang et al.[30] articulated sepsis knowledge graph was crafted by combining multicenter clinical data from over 10,000 patients with the help of GPT-4 for entity recognition and relationship extraction. Real-world data were collected from three hospitals and integrated with clinical guidelines and databases from the public domain. The knowledge graph contained 1,894 nodes and 2,021 relationships pertaining to diseases, symptoms, biomarkers, treatments, and complications. GPT outperformed other models in every resolution on sepsis- specific datasets to obtain high F1-score results. The constructed graph highlighted complex interactions in sepsis for assisting clinical decision-making and was implemented on Neo4j. Guan et al.[20] proposed a novel method for constructing a local knowledge graph from retrieved biomedical documents by extracting propositional claims. They carried out layer wise summarization on this graph to capture multi-document relationships and provide comprehensive contextual information to a language model for question-answering purposes. The method resolved issues in multi-document biomedical QA, such as noise cancellation and efficient context usage. They then tested their method on several benchmarks for biomedical question answering, achieving performance at least comparable to, if not better than, existing retrieval-augmented generation (RAG) baselines. The study established enhanced reasoning and answer accuracy of the model achieved through structured graph summarization. Previous studies have improved biomedical QA using KGs and LLMs, but important gaps remain. Most systems lack transparent, graph-based justifications, rely on limited evaluation methods, or depend on cloud resources that reduce privacy and reproducibility. Our framework addresses these gaps by providing visible Cypher queries with evidence subgraphs, applying comprehensive performance metrics across difficulty levels, and ensuring fully local, privacy-preserving deployment. Table I summarizes key previous studies on biomedical knowledge graphs and question answering, outlining their methods, datasets, and main limitations.", "<2-hop>\n\nIII. PRELIMINARIES This section outlines the fundamental concepts required to understand the proposed system. It introduces biomedical knowledge graphs, explains how Neo4j stores data in graph form, and describes the use of Cypher for querying. It also provides a brief overview of large language models (LLMs) and their role in interpreting natural language. A. Biomedical Knowledge Graphs Biomedical Knowledge Graphs (BKGs) provide a structured representation of complex biomedical information by modeling diverse medical entities, such as diseases, drugs, symptoms, and biological pathways, as interconnected nodes within a graph structure. The edges in these graphs represent the semantic relationships between these entities, including ’treats’, ’causes’, ’interacts with’ and many others, as illustrated in Fig 1. This form of representation enables the integration of heterogeneous biomedical data from a wide range of sources, including"], "reference": "Biomedical knowledge graphs (BKGs) provide a structured representation of complex biomedical information by modeling diverse medical entities such as diseases, drugs, symptoms, and biological pathways as interconnected nodes, with edges representing semantic relationships like 'treats', 'causes', and 'interacts with'. Neo4j stores this data in graph form, enabling efficient management and querying of these relationships. Cypher, Neo4j's query language, is used to retrieve and manipulate data within these graphs. Frameworks such as AMG-RAG utilize Neo4j to manage continuously updated medical knowledge graphs, incorporating confidence scoring and multi-hop reasoning to enhance accuracy and interpretability in medical question answering. Additionally, Auto-Cypher automates the generation of Cypher queries to produce high-quality synthetic data for training large language models, improving their performance in graph query generation. Hybrid approaches like GraphRAG combine Neo4j-based knowledge graphs with vector stores for semantic retrieval, enhancing factual accuracy, citation fidelity, and transparency in clinical question answering. Overall, the integration of biomedical knowledge graphs with Neo4j and Cypher queries enables accurate, transparent, and efficient biomedical question answering by combining structured graph reasoning with advanced language model capabilities.", "synthesizer_name": "multi_hop_abstract_query_synthesizer"}
{"user_input": "How does prompt engin eering, including Chain-of-Thought (CoT) prompting, contribute to the integration of knowledge graphs with LLMs in the KGT framework to improve biomedical KGQA performance?", "reference_contexts": ["<1-hop>\n\n6 | GigaScience , 2025, Vol. 14 Inference Subgraph inference Based on the relational chains and attribute data in the subgraph, determine the r ele v ance to the question text. Prune an y err oneous information, r etaining onl y the corr ect r elational c hains. Natural language output The LLM divides the subgr a ph into multiple relational chains, each of which outputs a sentence in natural language, and then the LLM generates natural language output. LLMs Inference and Output Prompt is presented in Supplementary Fig. S3 .", "<2-hop>\n\nResults Ev alua tion criteria We use e v aluators based on GPT-4 [ 38 ], BERTScore [ 39 ], and ROUGE [ 40 ] to assess the accuracy of the generated ans wers . As a scoring bot, GPT-4 e v aluates and assigns scores based on the sim- ilarity in meaning between 2 sentences. GPT-4–based Evaluation Pr ompt is pr esented in Supplementary Fig. S4 . BERTScor e e v alu- ates semantic similarity using context-sensiti ve embed dings, of- fering a compr ehensiv e e v aluation of langua ge model outputs. ROUGE, on the other hand, e v aluates the longest common subse- quence (LCS) between the generated text and the reference text, focusing on sequence-based similarity to assess the fluency and the pr eserv ation of semantic content. Baselines To assess the adv anta ges of our fr ame w ork, w e compare it with se v er al a ppr oac hes that can be dir ectl y a pplied for KGQA tasks without fine-tuning. We introduce a straightforw ar d baseline ap- pr oac h, named Base, which is similar to KG-GPT [ 32 ], curr entl y the leading method in the KGQA field, excluding the sentence segmentation step of KG-GPT. Initiall y, this involv es le v er a ging an LLM to r etrie v e r ele v ant information fr om the KG by gener at- ing a query statement. Then, another LLM is used to answer the question with the r etrie v ed information. To enhance the baseline, we incor por ate CoT pr ompting [ 19 ] and in-context learning (ICL) tec hniques [ 41 ], collectiv el y r eferr ed to as CoT&ICL. The pr ompts for these methods are illustrated in Supplementary Table S5 . Ad- ditionally, we implement KG-GPT [ 32 ] to enhance the r etrie v al and r easoning ca pabilities of the LLMs. For a fair comparison, all meth- ods are based on Code-Llama-13B [ 42 ]. To further underscore the efficacy of our fr ame w ork, w e con- duct a compar ativ e anal ysis of KGT, whic h is built upon Code- Llama-13B, a gainst 2 highl y ca pable lar ge langua ge models that ar e pr ominent in the general and biomedical domains: ChatGPT- 3.5 [ 1 ] and Taiyi [ 43 ]. ChatGPT-3.5, a leader in tasks across the general domain, has exhibited competitive performance in a wide r ange of a pplications. To compensate for its limited biomedi- cal kno wledge, w e emplo y 2 methodologies pr e viousl y described, Base and CoT&ICL, as advanced baselines to augment ChatGPT- 3.5’s capabilities. Taiyi, a cutting-edge LLM in biomedicine, pre- trained on 2 trillion tokens, le v er a ges its extensiv e biomedical knowledge base for direct question answering, bypassing the need for knowledge gr a ph r etrie v al. Due to the scarcity of KGQA datasets within the biomedical domain, all experiments are conducted on our ne wl y pr oposed benchmark, named PcQA.", "<3-hop>\n\nCompar a ti v e analysis across different KGQA methods We e v aluated the ca pabilities of v arious methods based on Code-Llama-13B, with the experimental results presented in Ta- ble 3 . The experimental results indicate that the Code-Llama- 13B model, enhanced with KGT, consistently surpasses competing methods across all metrics assessed. Notabl y, KG-GPT impr ov es the F1 score by 15.7% over previous methods CoT&ICL, while our method KGT increases the F1 score by 33% over KG-GPT. Because KG-GPT overlooks the impact of entity types and attributes on an- swers within the biomedical domain, this ac hie v ement positions our a ppr oac h as a pioneering benc hmark in biomedical KGQA, eclipsing pr e viousl y established best pr actices. Compar a ti v e analysis across di v erse LLMs We present a comparative study of KGT applied to Code-Llama- 13B against 2 highly capable LLMs in the general and biomedical domains, with experimental results displayed in Table 4 . Code- Llama-13B, enhanced by KGT, significantly outperforms its peers, ac hie ving the highest marks in e v ery assessment metric: a GPT- 4 Eval score of 92.4, a BERTScore of 97.7, and a ROUGE F1 score of 86.8. Remarkably, our approach’s F1 score surpasses that of ChatGPT-3.5 with the Base method by 52.7%, the CoT&ICL method by 36.3%, and Taiyi’s base model by 67.3%. These results highlight KGT’s substantial contribution to improving the performance of lar ge langua ge models for the pan-cancer KGQA task. Ev en when integrated with open-source general models, KGT exhibits re- markable performance, outstripping both the recognized state- of-the-art closed-source large language models and those specif- icall y tailor ed for the biomedical domain. This showcases KGT’s adeptness at parsing and le v er a ging knowledge gr a ph data, set- ting a new standard for future research and applications in the field. Assessing KGT’s effecti v eness on di v erse LLM platforms To underscore the adaptability and effectiveness of our KGT fr ame work when applied to a range of large language models, we conduct experiments on se v er al LLMs: Zephyr [ 44 ], Llama-2 [ 2 ], and Code-Llama [ 42 ]. The outcomes, illustrated in Fig. 3 , re- veal that while the CoT&ICL techniques significantly boost per- formance in terms of F1 score, our KGT methodology delivers e v en mor e substantial enhancements acr oss all e v aluated mod- els . T his demonstrates not only the effectiveness of CoT&ICL as a performance-enhancing strategy but also highlights the superior advancements and impact of KGT, establishing its dominance and efficiency in knowledge gr a ph question-answering tasks. Ablation study for dissecting the components of KGT In our effort to illuminate the individual contributions of the com- ponents that constitute our KGT fr ame work and their collective impact on enhancing the performance of LLMs, we define 4 foun- dational modules: (i) question analysis for the extraction of piv- otal information, (ii) gr a ph sc hema–based infer ence to identify the optimal relational chains in the knowledge graph, (iii) the generation of query statements to facilitate subgraph construc- tion, and (iv) the inference process coupled with the articulation of results in natural language . T his ablation study, grounded on the Code-Llama-13B model, is meticulously designed to e v alu- ate the efficacy of these components. Since gr a ph sc hema–based infer ence r equir es the pr ocess of question anal ysis, the ques- Downloaded from https://academic.oup.com/gigascience/article/doi/10.1093/gigascience/giae082/7943459 by guest on 11 January 2026"], "reference": "Prompt engineering in the KGT framework involves dividing the knowledge graph subgraph into multiple relational chains and generating natural language outputs from these chains, which helps prune erroneous information and retain only correct relational chains. Chain-of-Thought (CoT) prompting, combined with in-context learning (ICL) techniques, is used to enhance baseline methods by improving retrieval and reasoning capabilities of large language models (LLMs). The KGT framework builds upon these techniques by incorporating graph schema-based inference, query statement generation, and natural language articulation of results, which collectively improve the accuracy and transparency of clinical question answering. Experimental results show that KGT, based on Code-Llama-13B, significantly outperforms other methods including KG-GPT and CoT&ICL baselines, achieving higher scores across GPT-4 evaluation, BERTScore, and ROUGE metrics. This demonstrates that prompt engineering, especially CoT prompting, effectively facilitates the integration of knowledge graphs with LLMs, leading to substantial improvements in biomedical KGQA tasks.", "synthesizer_name": "multi_hop_abstract_query_synthesizer"}
{"user_input": "How does the pan-cancer knowledge graph framework (KGT) integrate heterogeneous biomedical data to improve biomedical question answering without fine-tuning large language models?", "reference_contexts": ["<1-hop>\n\nOmar & Mohammed / Journal of Applied Science and Technology Trends Vol. 06, No. 02, pp. 342 –357 (2025) 345 record-high accuracy, while open-source models achieved impressive gains through prompt optimization. Feng et al.[22] developed the Knowledge Graph-based Thought (KGT) framework that integrated LLMs with a pan- cancer knowledge graph for biomedical question answering. KGT was designed to reason on the knowledge graph schema and identify optimal subgraphs to use for directing accurate answer generation, all without fine-tuning the LLMs. The framework is benchmarked against a new dataset (PcQA) designed specifically for pan-cancer KGQA tasks and has outperformed all existing state-of-the-art approaches by a rather large margin. KGT’s practicality in biomedical issues was highlighted through case studies for drug repositioning, drug resistance, and biomarker discovery. Their approach exhibited robust adaptability among various LLMs. Rezaei et al.[26] developed AMG-RAG, a dynamic framework that utilizes autonomous LLM agents with medical search tools in the continuous construction and real-time updating of Medical Knowledge Graphs (MKGs). Their system incorporated confidence scoring and multi-hop reasoning to improve accuracy and interpretability in medical question answering. AMG-RAG outperformed size models on both very hard MEDQA benchmarks and more accessible MedMCQA ones, proving that it could conduct efficient reasoning based on current structured medical knowledge. They also used Neo4j to manage the knowledge graphs while adding external searches to ensure the latest data. Tiwari et al.[24] presented Auto-Cypher, a recent automated pipeline for producing high-quality synthetic data for training LLMs by mapping natural language to Cypher queries for graph databases like Neo4j. The pipeline deployed the novelty of LLM- as-database-filler to synthesize Neo4j databases for the execution of generated queries to ensure their correctness. A sizable dataset called SynthCypher was created, spanning multiple domains and complex queries, leading to a 40% improvement in LLM performance on Cypher generation. The datasets were used to fine-tune open-source models such as LLaMA, Mistral, and Qwen, and the SPIDER benchmark was adapted for evaluation purposes. Mohammed et al.[29] proposed a hybridized GraphRAG framework combining Neo4j-based UMLS knowledge graphs with a vector store for medical textbooks to create an improved U.S.M.L.E.-style clinical question-answering approach. The project integrated symbolic reasoning from knowledge graphs with semantic retrieval performed on text embeddings to enhance relevance and accuracy via adaptive re-ranking and query expansion. The system had the answers produced by GPT-4o- Mini, with different prompting strategies encouraging evidence- based and traceable responses grounded in verified medical knowledge. Experiments showed that the hybrid approach improved factual accuracy and citation fidelity as compared to the L.L.M.-only approach, enhancing transparency and reliability. It is shown that binding both structured and unstructured medical knowledge sources could aid in ameliorating hallucinations and hence improve clinical trustworthiness in AI-driven medical QA. Yang et al.[30] articulated sepsis knowledge graph was crafted by combining multicenter clinical data from over 10,000 patients with the help of GPT-4 for entity recognition and relationship extraction. Real-world data were collected from three hospitals and integrated with clinical guidelines and databases from the public domain. The knowledge graph contained 1,894 nodes and 2,021 relationships pertaining to diseases, symptoms, biomarkers, treatments, and complications. GPT outperformed other models in every resolution on sepsis- specific datasets to obtain high F1-score results. The constructed graph highlighted complex interactions in sepsis for assisting clinical decision-making and was implemented on Neo4j. Guan et al.[20] proposed a novel method for constructing a local knowledge graph from retrieved biomedical documents by extracting propositional claims. They carried out layer wise summarization on this graph to capture multi-document relationships and provide comprehensive contextual information to a language model for question-answering purposes. The method resolved issues in multi-document biomedical QA, such as noise cancellation and efficient context usage. They then tested their method on several benchmarks for biomedical question answering, achieving performance at least comparable to, if not better than, existing retrieval-augmented generation (RAG) baselines. The study established enhanced reasoning and answer accuracy of the model achieved through structured graph summarization. Previous studies have improved biomedical QA using KGs and LLMs, but important gaps remain. Most systems lack transparent, graph-based justifications, rely on limited evaluation methods, or depend on cloud resources that reduce privacy and reproducibility. Our framework addresses these gaps by providing visible Cypher queries with evidence subgraphs, applying comprehensive performance metrics across difficulty levels, and ensuring fully local, privacy-preserving deployment. Table I summarizes key previous studies on biomedical knowledge graphs and question answering, outlining their methods, datasets, and main limitations.", "<2-hop>\n\nIII. PRELIMINARIES This section outlines the fundamental concepts required to understand the proposed system. It introduces biomedical knowledge graphs, explains how Neo4j stores data in graph form, and describes the use of Cypher for querying. It also provides a brief overview of large language models (LLMs) and their role in interpreting natural language. A. Biomedical Knowledge Graphs Biomedical Knowledge Graphs (BKGs) provide a structured representation of complex biomedical information by modeling diverse medical entities, such as diseases, drugs, symptoms, and biological pathways, as interconnected nodes within a graph structure. The edges in these graphs represent the semantic relationships between these entities, including ’treats’, ’causes’, ’interacts with’ and many others, as illustrated in Fig 1. This form of representation enables the integration of heterogeneous biomedical data from a wide range of sources, including"], "reference": "The pan-cancer knowledge graph framework (KGT) integrates heterogeneous biomedical data by reasoning on the knowledge graph schema to identify optimal subgraphs for directing accurate answer generation. Biomedical Knowledge Graphs (BKGs) provide a structured representation of diverse medical entities such as diseases, drugs, symptoms, and biological pathways as interconnected nodes, with edges representing semantic relationships like 'treats' and 'causes'. This integration of heterogeneous biomedical data enables KGT to operate without fine-tuning large language models (LLMs), instead leveraging the graph structure to guide reasoning. KGT was benchmarked on a pan-cancer KGQA dataset and outperformed existing state-of-the-art approaches, demonstrating robust adaptability across various LLMs and practical utility in biomedical tasks such as drug repositioning, drug resistance, and biomarker discovery.", "synthesizer_name": "multi_hop_abstract_query_synthesizer"}
{"user_input": "How do the use of Large Language Models (LLMs) combined with structured and unstructured data, along with human and automated evaluation metrics, address the challenge of hallucination in LLMs within biomedical question answering systems?", "reference_contexts": ["<1-hop>\n\nconducting safety checks by applying 24 predefined rules to ensure ethical and factual accuracy, and summarizing the results.30 Glicksberg et al. developed an ensemble model that combined structured and unstructured data to predict hospi­ tal admission probabilities. These predicted probabilities, along with similar historical cases, were incorporated into the prompt to enhance the performance of LLM.37 Chen et al. used Chain-of-Thought (CoT) prompting to improve LLM reasoning capabilities.39 Kresevic et al. customized prompts to help the model interpret structured guidelines, combined with few-shot learning using 54 question-answer pairs.27 Jeong et al. fine-tuned LLMs to assess the relevance of retrieved evidence, ensure all statements were evidence-based, and confirm that the response effectively addressed the query.34", "<2-hop>\n\nEvaluation Nine studies used human evaluation, 8 relied on automated evaluation (eg, similarity comparisons between generated sentences and original answers), and 3 used a mix of both. Outcomes from human evaluation showed an overall OR of 1.65 (95% CI: 1.36-2.03), while automatic evaluation resulted in an OR of 1.20 (95% CI: 1.1-1.41). The differen­ ces between the 2 were statistically significant (P < .01). There were 4 human evaluators on average, with the range spanning from 1 to 10. Most human evaluators were physicians from relevant specialties according to the study focus. In one case, 3 diabetic patients were involved in evaluating the understandability of diabetes-related patient queries.30 Twelve studies used self-curated datasets focused on research tasks. Examples included the ClinicalQA bench­ mark, which comprised 314 open-ended questions about treatment guidelines and clinical calculations generated by physicians,28 and 43 diabetes-related questions sourced from the National Institute of Diabetes and Digestive and Kidney Diseases website.30 Simulated cases from medical examina­ tions were also utilized.25 Three studies used EHR data.35,37,40 Six studies used public benchmark datasets, such US board exam practice questions, MedMCQA29,34 and longform question-answering benchmarks (eg, LiveQA, Med­ icationQA).34 The self-curated datasets averaged 76 ques­ tions, ranging from 7 to 314. The length of public benchmark datasets varied significantly, from 102 questions in the LiveQA dataset28 to 194 000 questions in the MedMCQA dataset.34 Most studies reported evaluation metrics for the final response generation, while 4 (25%) also included specific metrics to evaluate the retrieval process. For instance, 1 study measured recall in context retrieval,24 another evaluated retrieval accuracy,33 and a fine-tuned LLM was developed to assess the relevance of retrieved information to the user’s query.34 Additionally, 1 study evaluated the accuracy of using LLMs to extract text from figures and tables during document preprocessing.27 The final evaluation metrics focused on the generated responses, consistent with those used in LLM-only systems. These metrics could be catego­ rized as accuracy, completeness, user perception, safety, hal­ lucination, citation, bias, and language. Accuracy was the most frequently reported metric, covering Likert scale rat­ ings, match rates, correct treatment percentages,9 AUC, AUPRC, and F1 scores, as well as text similarity metrics like ROUGE (ROUGE-1, ROUGE-2, ROUGE-L), BLEU, METEOR, and BERTScore,21 which compared LLM- generated responses to expert-provided answers. Complete­ ness metrics assessed whether responses included all neces­ sary information, typically using Likert scales. User perception captured subjective feedback from both healthcare providers and patients on understandability, helpfulness, and whether responses met user intent, usually using Likert scales. Safety metrics focused both on user-related and system- related aspects. These metrics assessed potential harm, adver­ sarial safety, and risk management,24 ensuring that outputs were free of harmful content or risks. Scientific validity and adherence to evidence were also evaluated.29 One study used adversarial prompting, defined as intentionally adding harm­ ful directives to a prompt, to evaluate the safety of the RAG system.28 Hallucinations were primarily identified through manual review, with definitions varying across studies. Some studies defined hallucinations as nonfactual information, while one study added 2 other types of hallucinations: input- conflicting (content deviating from user-provided input) and contextual-conflicting (content conflicting with previously generated information).27,41 Citation metrics measured the accuracy of provided references, with valid references consid­ ered those that pointed to established publications, guide­ lines, or research. Bias and language were evaluated for clarity and neutrality, ensuring responses were unbiased and empathetic to patient concerns.24"], "reference": "Large Language Models (LLMs) have been enhanced by integrating structured and unstructured data, as demonstrated by Glicksberg et al., who developed an ensemble model that predicted hospital admission probabilities and incorporated these predictions and similar historical cases into the prompt to improve LLM performance. Additionally, techniques such as Chain-of-Thought prompting and customized prompts have been used to improve reasoning and interpretation of structured guidelines. To address hallucination in LLMs, multiple evaluation strategies have been employed. Nine studies used human evaluation, often involving physicians or patients, while eight relied on automated metrics comparing generated responses to expert answers using measures like ROUGE, BLEU, and BERTScore. Hallucinations were primarily identified through manual review, with some studies categorizing hallucinations into nonfactual information, input-conflicting, and contextual-conflicting types. Safety metrics also assessed potential harm and adversarial risks to ensure outputs were free from harmful content. Citation accuracy was evaluated to confirm references pointed to established sources, and bias and language clarity were monitored to maintain neutrality and empathy. Together, these approaches combining advanced LLM prompting, integration of biomedical data, and rigorous multi-faceted evaluation help mitigate hallucination and improve the factual accuracy and safety of biomedical question answering systems.", "synthesizer_name": "multi_hop_abstract_query_synthesizer"}
{"user_input": "How does the integration of Disease Ontology identifiers in the iBKH dataset facilitate the construction of a comprehensive biomedical knowledge graph, and how does STRING's approach to managing multiple hypothesis testing in Disease Ontology terms enhance the statistical power of enrichment analyses?", "reference_contexts": ["<1-hop>\n\nB. Dataset and Knowledge Graph Construction 1) Dataset The integrated Biomedical Knowledge Hub (iBKH), a large biomedical knowledge base, forms the first level of the system and integrates information from various curated high-quality biomedical databases. This implies that the data set includes various types of entities, such as diseases, symptoms, drugs, biological pathways, etc. This study used the representative subset of the iBKH dataset, which contained 65828 biomedical entities. These entities are semantically interconnected through a total of 3004166 relationships, thus creating a rich knowledge graph. The iBKH dataset was originally introduced in [11], and it is freely available at (https://github.com/wcm-wanglab/iBKH). This dataset is the core semantic foundation upon which this study is built. The knowledge graph is populated from multiple tabular sources (CSV files), each listing entities or relationships. The main input files and their contents are as follows: • Disease vocabulary(disease_vocab.csv): Contains columns such as primary (a unique disease ID), name, do_id (Disease Ontology ID), kegg_id, and umls_cui (UMLS Concept ID). Each row represents a disease node with external identifiers. • Drug vocabulary (drug_vocab.csv): Includes primary (unique drug ID), name, drugbank_id, kegg_id, pharmgkb_id, umls_cui, mesh_id, iDISK_id and CID (PubChem ID). Each row defines a drug node with standard database identifiers. • Symptom vocabulary (symptom_vocab.csv): Contains primary (unique symptom ID), name, mesh_id, umls_cui and iDISK_id. Each row defines a symptom node. • Side effect vocabulary (side_effect_vocab.csv): Includes primary (unique side-effect ID) and name. Each row represents a side-effect node (with UMLS ID when available). • Pathway vocabulary (pathway_vocab.csv): Contains primary (unique pathway ID), name, reactome_id, go_id, and kegg_id. Each row defines a biological pathway node. Relationship files (each row typically contains two entity IDs and one or more boolean flags or codes) include: • Disease–Symptom links (Di_Sy_res.csv): Rows include Disease and Symptom IDs, a presence flag (1 or 0) and a data source. If Present = 1, a HAS_SYMPTOM edge is created from the disease to the symptom, with properties for presence and source. • Disease–Disease links (di_di_res.csv): Rows include Disease_1 and Disease_2 IDs with binary flags for is_a and Resemble. If is_a = 1, an (IS_A) edge is created (Disease_1 → Disease_2); if Resemble = 1, a RESEMBLES edge is created. The source field is used for provenance. • Drug–Disease links (D_Di_res.csv): Includes Drug and Disease IDs with several binary flags. If a flag equals 1, a corresponding edge is created: o TREATS (Treats = 1) o PALLIATES (Palliates = 1) o ASSOCIATED_WITH (Associate = 1) o ALLEVIATES_REDUCES (alleviates = 1) o TREATMENT_THERAPY (treatment/therapy = 1) o INHIBITS_CELL_GROWTH (inhibits cell growth = 1) o HAS_BIOMARKER (biomarkers = 1) o PREVENTS_SUPPRESSES (prevents/suppresses = 1) o ROLE_IN_PATHOGENESIS (role in disease pathogenesis = 1) • Drug–SideEffect links (D_SE_res.csv): Contains Drug and SideEffect IDs with a Source column. Each row creates a CAUSES edge from the drug to the side effect, with source as an edge property. • Drug–Drug interactions (D_D_res.csv): Rows include Drug_1 and Drug_2 IDs with flags for Interaction and Resemble. If Interaction = 1, an INTERACTS_WITH edge is created (bidirectional). If Resemble = 1, a RESEMBLES edge is added. • Drug–Pathway links (D_Pwy_res.csv): Includes Drug ID and Pathway ID. Each row generates an ASSOCIATED_WITH edge from the drug to the pathway. • Disease–Pathway links (Di_Pwy_res.csv): Contains Disease ID and Pathway ID. Each row creates an ASSOCIATED_WITH edge from the disease to the pathway. 2) Data Upload Performance The time required to upload different types of entities and relationships into the Neo4j biomedical knowledge graph, measured in seconds. These measurements reflect both the size and complexity of the data being processed. As shown in Table II, the longest upload time is for Drug- Drug Relationships, which takes approximately 190 seconds due to the large number of edges (over 3 million). Following this, Disease-Disease and Drug-Disease Relationships also require considerable time for loading. On the other hand, individual", "<2-hop>\n\nFDR correction In its analyses, STRING performs a test for each path- way (‘term’) within a given pathway collection (‘category’). Like many other enrichment tools, STRING employs the Benjamini–Hochberg correction ( 36 ) to adjust for multiple hypothesis testing. This statistical correction is essential for managing the FDR effectively. However, the larger the term count and the greater the diversity of the terms, the higher the required correction and the lower the chance of exceed- ing the alpha level (significance threshold) for a given term. One method used to address this issue involves restricting the analysis to a subset of terms, such as ‘GO slims’ that focus on broad, high-level terms with a simplified ontology struc- ture ( 16 ). However, this approach has some disadvantages: it might exclude terms particularly relevant to a tested gene set, it may not cover all functions by design and it is mainly ap- plicable to hierarchical classification systems such as the Gene Ontology. STRING utilizes several hierarchical ontologies, such as the Brenda Tissue Ontology, Disease Ontology, Human Phe- notype Ontology and the aforementioned Gene Ontology; they typically contain many more leaf-ward nodes than nodes closer to the root. The parental nodes must contain all the genes included in the child terms, creating an imbalance in the ontology with many more smaller terms and fewer larger terms. Removing smaller terms can substantially increase sta- tistical power . However , simply eliminating all small terms could negatively impact discoverability, especially for small query gene sets where the smaller terms might be the most rel- evant for biological interpretation. The ontologies are usually large, with > 10 000 terms (and therefore tests) in the Biolog- ical Process branch of the Gene Ontology tree. Such strong corrections have the effect that for larger query gene sets, or for backgrounds that cover only part of the proteome, it might not be statistically possible to detect enrichments for a subset of the terms; these can therefore be removed from considera- tion. In its new version, STRING takes a more flexible approach—by only testing terms that have a priori any statis- tical potential for enrichment. This decision is based on sev- eral parameters: the size of the term, the size of the query set, the background size and the number of tests conducted. By calculating the smallest and largest term sizes that could the- oretically be enriched given the FDR correction, STRING can determine which terms are viable for inclusion in the anal- ysis. Terms outside these bounds are excluded from testing. This methodical exclusion significantly enhances the statisti- cal power of the enrichment analysis, without omitting any terms that could be enriched. For smaller query sets, this strat- egy will maintain all terms, as even the smallest tested sets (term size = 2) might still be theoretically enriched. How- ever , for larger , less specific sets or for tests against a cus- tom smaller background, this approach markedly improves the statistical power, enabling STRING to perform more fo- cused analyses without the drawbacks of increased false posi- tive rates or omitting potentially biologically interesting terms. Although this method leverages the characteristics of ontolo- gies, it does not require a hierarchy (parent–child relationships between the terms) to function and can be applied to any pathway / term collection with an imbalance of small terms, such as PMID-derived gene sets and STRING neighborhood clusters."], "reference": "The iBKH dataset integrates Disease Ontology identifiers (do_id) within its disease vocabulary, allowing each disease node to be uniquely linked to standardized ontology terms. This integration enables the construction of a rich biomedical knowledge graph by semantically connecting diseases with symptoms, drugs, pathways, and other entities through well-defined relationships such as HAS_SYMPTOM, IS_A, and ASSOCIATED_WITH. The use of Disease Ontology IDs ensures consistent disease representation and facilitates interoperability across multiple biomedical databases. Meanwhile, STRING enhances the statistical power of enrichment analyses involving Disease Ontology terms by employing a flexible method to manage multiple hypothesis testing. Instead of testing all ontology terms, STRING calculates the smallest and largest term sizes that could theoretically be enriched after applying the Benjamini–Hochberg false discovery rate correction. Terms that fall outside these bounds are excluded from testing, which reduces the number of tests and thus the severity of the correction. This approach maintains all potentially enrichable terms, especially for smaller query sets, and improves the ability to detect significant enrichments without increasing false positives or omitting biologically relevant terms. Together, the integration of Disease Ontology in iBKH and STRING's refined statistical correction method enable more accurate and interpretable biomedical knowledge graph construction and enrichment analysis.", "synthesizer_name": "multi_hop_specific_query_synthesizer"}
{"user_input": "How do Zhang et al. and Wang et al. contribute to improving retrieval strategies in biomedical knowledge graph-augmented language models?", "reference_contexts": ["<1-hop>\n\nintegrates over 40 publicly available biomedical knowledge sources across separate domains, such as genes, proteins, drugs, compounds, and diseases, along with their known relationships.32 Two studies used textbooks, such as Harri­ son's Principles of Internal Medicine, while 3 others utilized electronic health record (EHR) data. Additionally, Zakka et al. added over 500 markdown files from MDCalc to improve clinical calculation capabilities in LLM.28 Two stud­ ies employed real-time online browsing to search academic sites, such as PubMed and UpToDate. The amount of retrieval resources varied across studies, ranging from a small dataset specific to 6 osteoarthritis guidelines to a large dataset of EHR data from 7 hospitals. Retrieval strategies Identified retrieval strategies were grouped based on the RAG stages: pre-retrieval, retrieval, and post-retrieval. Figure 3 presents an example of how RAG is applied and lists identi­ fied strategies within each stage. In the pre-retrieval stage, 50% of studies (n ¼ 10) reported strategies, such as query rewriting, document preprocessing, and assessing the necessity of retrieval. Zakka et al. simplified queries by rephrasing text into search terms that are better suited for website browsing,28 while Wang et al. focused on techniques such as correcting errors, expanding abbrevia­ tions, and matching synonyms in user queries.30 Soman et al. extracted disease entities in queries and retrieved correspond­ ing nodes from a knowledge graph.33 Document preprocess­ ing involved removing non-textual elements from PMC papers (eg, figures, references, and author disclosures),30 extracted tables from PDFs using pdfplumber, structured the content with pydantic for seamless integration.25 In addition to query modification and document preprocessing, Jeong et al. fine-tuned a model to determine whether retrieval was necessary for a given query.34 During the data retrieval stage, 85% of studies (n ¼ 17) reported strategies regarding indexing, aligning queries with documents, and ranking retrieval chunks. Chunking methods ranged from fixed-size chunks35 to recursive splits.36 Embed­ ding models such as Text-embedding-ada-002,24,28–30,36,37 MiniLM, and PubMedBERT33 were commonly used to con­ vert sentences into vectors. Cosine similarity was the primary metric for measuring query-document alignment. Two stud­ ies adopted Maximal Marginal Relevance for search and highlighted its improved performance over similarity-based methods.24,35 A domain-specific retriever, MedCPT, was used in one study.34 Another study used the multi-vector retriever that leveraged summarized document sections to identify the original content for final answer generation.25 The retrieval cutoff parameters varied widely, with probabil­ ity thresholds up to 0.83 and the number of retrieved chunks ranging from 3 to 90.28,36,38 Vector databases like FAISS and Chroma were frequently reported, and LangChain was widely used for document processing and retrieval.23,25,35,38 In the subgroup analysis, 12 studies used simple data retrieval strategies (OR 1.30, 95% CI [1.16, 1.45]), while 5 studies used complex data retrieval strategies (OR 1.30, 95% CI [1.07, 1.24]), with no statistically significant difference observed between the 2 approaches. In the post-retrieval stage, 65% of studies (n ¼ 13) imple­ mented specific strategies to refine outputs. Murugan et al. tailored prompts by providing clear context, defining roles (eg, distinguishing between healthcare providers and patients to deliver appropriately detailed information), and incorpo­ rating relevant citations from retrieval sources such as the Clinical Pharmacogenetics Implementation Consortium guidelines and Food and Drug Administration (FDA) label­ ing.24 Soman et al. utilized prompt engineering to integrate accurate knowledge sources and statistical evidence, such as P-values and z-scores, from the SPOKE knowledge graph into their outputs.33 Wang et al. outlined a detailed process in the post-retrieval stage using prompt engineering, which involved decomposing retrieved text into individual claims, verifying each claim with external knowledge sources,", "<2-hop>\n\nAbstract Recent advancements in large language mod- els (LLMs) have achieved promising perfor- mances across various applications. Nonethe- less, the ongoing challenge of integrating long-tail knowledge continues to impede the seamless adoption of LLMs in specialized do- mains. In this work, we introduce DALK, a.k.a. Dynamic Co-Augmentation of LLMs and KG, to address this limitation and demonstrate its ability on studying Alzheimer’s Disease (AD), a specialized sub-field in biomedicine and a global health priority. With a synergized frame- work of LLM and KG mutually enhancing each other, we first leverage LLM to construct an evolving AD-specific knowledge graph (KG) sourced from AD-related scientific literature, and then we utilize a coarse-to-fine sampling method with a novel self-aware knowledge re- trieval approach to select appropriate knowl- edge from the KG to augment LLM infer- ence capabilities. The experimental results, conducted on our constructed AD question answering (ADQA) benchmark, underscore the efficacy of DALK. Additionally, we per- form a series of detailed analyses that can of- fer valuable insights and guidelines for the emerging topic of mutually enhancing KG and LLM. We will release the code and data at https://github.com/David-Li0406/DALK. 1 Introduction Alzheimer’s Disease (AD) is a neurodegenerative disorder characterized by progressive declines in cognitive and functional status over a span of decades (Report, 2023). However, current AD ther- apy developments are facing critical challenges due to the lack of knowledge and understanding of the underlying etiological mechanisms of the dis- ease. Although scientific literature and dedicated biomedical databases could supply rich sources of * Equal Constributions † Corresponding authors AD knowledge, manual review of relevant informa- tion is impossible due to the large volume. As large language models (LLMs) (Brown et al., 2020; Zhang et al., 2022; Anil et al., 2023; Touvron et al., 2023) with chain-of-thought (CoT)-based prompting (Wei et al., 2022; Wang et al., 2022; Tong et al., 2023; Yao et al., 2023; Besta et al., 2023) demonstrate strong language capabilities across various tasks, there have been attempts to leverage LLMs-based systems in general biomedi- cal and AD-related applications (Mao et al., 2023; Li et al., 2023c; Yan et al., 2024; Feng et al., 2023). However, while the LLMs have shown promising performances in many general tasks, recent studies revealed LLMs’ limitations in long-tail (Kandpal et al., 2023) and domain-specific (Li et al., 2023b, 2024) knowledge, thereby significantly impeding their adaptations in vertical fields such as AD. To deal with this issue, the most common strategies are retrieval augmented generation (RAG) and domain- specific LLMs training. Nevertheless, directly applying these strategies in the context like AD would still suffer from sev- eral issues. First, Data Quality: As in many biomedical fields, scientific literature composes the largest publicly available corpus source in AD. Yet, the dense and information-overloaded nature of scientific literature, when combined with auto- matic retrieval methods, can lead to the retrieval of irrelevant and noisy information. Previous re- search has shown that noisy and irrelevant corpora can significantly undermine the performance of LLMs (Yu et al., 2023; Chen et al., 2024; Wu et al., 2024). Second, Efficiency & Scale Issues: Being an critical field of research, the knowledge of AD is rapidly evolving with scientific advancements at a remarkable pace and scale. However, retraining a domain-specific LLM or updating certain knowl- edge in it demands substantial computational re- sources (Hu et al., 2021; Ovadia et al., 2023; Zhang et al., 2024). This efficiency issue would also limit arXiv:2405.04819v1 [cs.CL] 8 May 2024"], "reference": "Zhang et al. contribute by developing large language models (LLMs) with chain-of-thought prompting that demonstrate strong language capabilities across various tasks, including biomedical applications, but also highlight challenges such as data quality and efficiency in domain-specific contexts like Alzheimer's Disease. Wang et al. focus on retrieval strategies in the pre-retrieval and post-retrieval stages by correcting errors, expanding abbreviations, matching synonyms in user queries, and employing prompt engineering to decompose retrieved text into individual claims and verify them with external knowledge sources. Together, these studies enhance retrieval strategies by addressing query refinement, error correction, and output verification within biomedical knowledge graph-augmented LLM systems.", "synthesizer_name": "multi_hop_specific_query_synthesizer"}
{"user_input": "How does the BioLunar platform integrate Python Coder components within its user interface to facilitate the creation of customizable biomedical workflows, and how is Python utilized in the system’s graphical user interface to enhance transparency and reliability in clinical question answering?", "reference_contexts": ["<1-hop>\n\nto support biological analyses. We demonstrate the key functionalities of the platform contextualised within a real-use case in the context of molecular- level evidence enrichment for biomarker discovery in oncology. 2 BioLunar BioLunar enables the creation of LLM-based biomedical scientific workflows using software components with standardised APIs. A workflow is composed of components and subworkflows con- nected through input-output relationships, and are capable of handling multiple inputs. In the user in- terface, components are clustered according to their function (see Fig.1). Creating a workflow does not require programming knowledge since components are predefined and merely require data inputs or parameter settings. However, for users who wish to write custom code, ’Python Coder’ and ’R Coder’ components are provided, enabling the definition of custom methods. These custom components can be saved and subsequently accessed in the ’Custom’ group tab. In the paper we describe an exemplar biomed- ical workflow designed to integrate evidence and infer conclusions from bioinformatics pipeline re- sults. Specifically, the biomedical workflow queries expert knowledge bases (KBs) that continuously compile clinical, experimental, and population ge- netic study outcomes, aligning them with assertions relevant to the significance of the observed gene or variant. It then employs Natural Language Infer- ence (NLI) (via LLM) to integrate and harmonise the evidence space and interpreting the results, cul- minating in a comprehensive summary for the en- tire gene set input. This interpretation takes into account the bioanalytical context supplied by the user. 2.1 Exemplar Workflow Next-generation sequencing (NGS) assays play a pivotal role in the precise characterisation of tu- mours and patients in experimental cancer treat- ments. NGS findings are essential to guide the design of novel biomarkers and cancer treatments. Nevertheless, the clinical elucidation of NGS find- ings subsequent to initial bioinformatics analysis often requires time-consuming manual analysis pro- cedures which are vulnerable to errors. The inter- pretation of molecular signatures that are typically yielded by genome-scale experiments are often supported by pathway-centric approaches through which mechanistic insights can be gained by point- ing at a set of biological processes. Moreover, gene and variant enrichment benefits from heteroge- neous curated data sources which pose challenges to seamless integration. Furthermore, there are different levels of supporting evidence and there- fore prioritising conclusions is crucial. Automating evidence interpretation, knowledge synthesis and leveraging evidence-rich gene set reports are fun- damental for addressing the challenges in precision oncology and the discovery of new biomarkers. 2.2 User interface The user interface facilitates an agile workflow construction by enabling users to select and ar- range components via drag-and-drop from func- tionally grouped categories, such as, i.a.: ’Prompt Query’ featuring NLI components, ’Knowledge Bases’ components, ’Extractors’ for retrieving files from zip archives or extracting text and tables from PDF files, and ’Coders’, which allow for the cre- ation of custom components using Python or R scripts. Components allow for individual execution, edi- tion, or configuration adjustment via a visual inter- face. Workflows can be executed, saved, or shared. Each component has designated input and output capabilities, enabling seamless integration where the output from one can directly feed into another. Users have the flexibility to manually input values if no direct connection is established. Additionally, a component’s output can feed into multiple compo- nents. The system’s architecture supports effortless expansion, adding branches and components with- out affecting the existing workflow, thus facilitating scalable customization to meet changing require- ments. The user interface with an example of a workflow is presented in Fig.1 and in demo video https://youtu.be/Hc6pAA_5Xu8. 2.3", "<2-hop>\n\nOmar & Mohammed / Journal of Applied Science and Technology Trends Vol. 06, No. 02, pp. 342 –357 (2025) 351 This query searches for a disease node whose name contains ’alzheimer’ and follows HAS_SYMPTOM edges to list related symptom names. The system then executes this cypher to retrieve answers. The prompts (such as few-shot examples and schema hints) were carefully designed to help LLaMA 3 generate correct Cypher queries. The model learns how to use the graph’s labels and relationships properly. For example, if a user asks, ’Which drugs treat diabetes?’, LLaMA might respond with a suitable Cypher query: MATCH (d:Drug)-[:TREATS]->(di:Disease) WHERE toLower(di.name) CONTAINS \"diabetes\" RETURN d.name This queries for drug nodes that have a TREATS edge to a diabetes disease node. By leveraging LLaMA 3 in this way, our system can flexibly handle many phrasing variations without manual mapping rules. D. Model Configuration & Decoding We run a local LLaMA 3.2-3B model in GGUF format (llama- 3.2-3b-instruct-q4_k_m.gguf) via llama.cpp, as shown in Table IV. TABLE IV. MODEL RUNTIME AND DECODING SETTINGS Runtime settings Decoding settings n_ctx = 1024 temperature = 0.2 n_threads = 12 top_p = 0.95 n_gpu_layers = 33 top_k = 40 n_batch = 512 repeat_penalty = 1.1 max_tokens = 80 seed = 42 E. Graph Subset and Versioning We use an iBKH derived subgraph (≈65.8k nodes; ≈3.0M edges) spanning DRUG, DISEASE, SYMPTOM, PATHWAY. IDs are normalized to CURIEs and duplicates collapsed across UMLS/DrugBank/DisGeNET/SIDER/KEGG. Each edge stores provenance/licensing metadata (source, source_version, license, retrieved_at, evidence_pmids/urls) and, when available, temporal fields (valid_from, valid_to). We report coverage as the percentage of evaluated questions whose gold entities/relations are present. F. Query Execution and Reliability After a Cypher query is generated, it is executed on the Neo4j database through the official Neo4j Python driver, which manages the secure connection and returns the results. Instead of restricting the output with a fixed LIMIT (e.g., LIMIT 5), the system retrieves candidate results and evaluates them using standardized retrieval metrics such as Hits@1, Hits@5, and Hits@10. This approach ensures that the system remains responsive while providing a fair assessment of ranking quality across different cutoff levels, rather than depending solely on a fixed number of returned items. Neo4j’s indexing on key node properties, such as name and primary identifiers, also helps speed up lookups as the knowledge graph grows. In cases where the language model generates an incomplete or incorrect query, such as referencing nodes or relationships that do not exist, the system catches the error and either retries with a simpler prompt or informs the user. Together, these steps make sure that queries run quickly, return valid results, and keep the overall experience smooth and reliable for biomedical question-answering. G. User Interface for Query Transparency The system includes a lightweight, cross-platform graphical user interface (GUI) implemented as a web application using the Flask framework in Python, with HTML and Bootstrap for interactive visualization. The interface is designed to make the question-answering process transparent and accessible to users without technical expertise. It consists of three main panels: 1. Input Panel: Where the user can enter a biomedical question in natural language. 2. Query Panel: Which displays the Cypher query generated by the language model, allowing users to verify how their question was interpreted. 3. Results Panel: Which presents the retrieved answers in a clear, readable format, accompanied by a brief natural language explanation generated by the system. By showing both the query and the answer, the GUI promotes user trust and enables validation of the system’s reasoning process. The interface is lightweight enough to run smoothly on standard desktop machines without additional dependencies, making it practical for local deployments in clinical or research settings. Fig. 4 illustrates the overall layout."], "reference": "The BioLunar platform enables the creation of LLM-based biomedical scientific workflows through a user interface that allows users to select and arrange predefined components via drag-and-drop, grouped by function. Among these components are 'Python Coder' and 'R Coder', which allow users to write custom code to define bespoke methods. These custom components can be saved and accessed later under a 'Custom' group tab, providing flexibility for users who wish to extend or tailor workflows beyond the predefined options. This design supports scalable customization without requiring programming knowledge for standard workflows but offers coding capabilities for advanced users.\n\nIn addition, the system’s graphical user interface for clinical question answering is implemented as a lightweight, cross-platform web application using the Flask framework in Python, combined with HTML and Bootstrap for interactive visualization. This interface includes panels for user input, displaying the generated Cypher query, and presenting retrieved answers with natural language explanations. By exposing the generated queries and results transparently, the Python-based GUI enhances user trust and allows validation of the system’s reasoning process. The use of Python in both the workflow customization components and the GUI contributes to the system’s reliability, responsiveness, and accessibility in clinical and research settings.", "synthesizer_name": "multi_hop_specific_query_synthesizer"}
{"user_input": "how LLMs and KGs help in understanding alzheimer’s disease risk factors like diabetes and head trauma?", "reference_contexts": ["<1-hop>\n\n… Successful application of AD preventative approaches may hinge on an accurate and comprehensive view of comorbidities, including cardiovascular disease, diabetes, and head trauma. Literature Corpus LLMs for KG KG for LLMs Head Relation Tail Diabetes AD Head trauma … … … risk factor AD risk factor Extracted Triplets Evolving KG Evolving KG AD Cognition level Down’s syndrome Head trauma Diabetes Selected Knowledge LLMs LLMs Extract Process Augment Q: Following are predispositions to Alzheimer's disease except ___. A). Down’s syndrome B). Head trauma C). Smoking D). Low education group Answer Rerank Sample AD Cognition level Down’s syndrome Head trauma Diabetes Subgraph Sampling Genetic factors Lifestyle Input Q: Following are predispositions to Alzheimer's disease except ___. A). Down’s syndrome B). Head trauma C). Smoking D). Low education group Question Input Figure 1: The overview pipeline of DALK. We first extract structural knowledge from unstructured corpora and construct a domain-specific knowledge graph tailored to AD (Section 3.1). Then, we utilize a coarse-to-fine sampling method with a novel self-aware knowledge retrieval approach to select appropriate knowledge from the knowledge graph (Section 3.2). … Successful application of AD preventative approaches… , including cardiovascular disease, diabetes, and head trauma. … Successful application of AD preventative approaches… , including cardiovascular disease, diabetes, and head trauma. Step1: Entity Recongnition Step2: Relation Extraction Pair-wised Relation Extraction Generative Relation Extraction … Reading the following abstract: [Abstract] Question: What is the relationship between AD and cardiovascular disease? Question: What is the relationship between AD and diabetes? Reading the following abstract: [Abstract] Here are all the biomedicine-related entities: [Entities] Question: Please extract all the valid relationship between the provided entities. Figure 2: The detailed process of AD-specific KG con- struction. study (Wen et al., 2023) and consider the following two kinds of explorations in our AD-KG: Path-based Exploration entails the extraction of a sub-graph from G to encompass all entities within EG. The process unfolds as follows: (a) Begin by selecting one node from e0 Q as the initial node, denoted as e1, and place the remaining nodes into a candidate node set, Ecand. Explore at most k hops from e1 to identify the subsequent node, e2, where e1 ∈Ecand. If e2 is successfully reached within k hops, update the start node to e2 and remove e2 from Ecand. In the event e2 cannot be found within k hops, concatenate the segment paths acquired thus far and store them in Gpath Q . Subsequently, choose another node e′ 1 from Vcand as the new start node, and eliminate both the original start node e1 and the current node e2 from Ecand. (b) Verify if Ecand is empty. If not, repeat step (a) to identify the next segment of the path. If Ecand is empty, combine all segments to construct a set of sub- graphs and place them into Gpath Q . Neighbor-based Exploration endeavors to aug- ment the evidence relevant to the query within GQ. This process consists of two steps: (a) Initially, expand each node e within EG by 1-hop to incor- porate their neighbors e′, thus appending triples (e, r, e′) to Gnei Q . (b) Then assess whether each e′ exhibits semantic relevance to the query. If affir- mative, further expand the 1-hop neighbors of e′, consequently adding triples (enei, r′, e′) to Gnei Q . After obtaining the two sub-graphs Gpath Q and Gnei Q , we perform post-processing to further prune redundant information in sub-graphs and prompt LLMs to describe the structure of each sub-graph.", "<2-hop>\n\nLLMs and KGs for AD research LLMs and KGs have both been applied to Alzheimer’s Dis- ease research in previous studies. Pre-trained lan- guage models are utilized to work on AD de- tection and many other related tasks based on speech recordings and transcripts (Balagopalan et al., 2020; Agbavor and Liang, 2022), electronic health records (EHRs) (Mao et al., 2023; Li et al., 2023c; Yan et al., 2024), and tabular data (Feng et al., 2023). KGs have been widely used in biomedical research, yet only a few are specifically for AD research (Romano et al., 2023; Pu et al., 2023; Hsieh et al., 2023; Nian et al., 2022; Daluwa- tumulle et al., 2023). These KGs were generally constructed from a variety of information derived from heterogeneous biomedical databases (e.g. for genes, drugs, pathways, etc.) or scientific literature related to AD. Despite the aforementioned efforts for LLMs and KGs in AD research, no prior study has explored using LLM to augment AD-KG, or"], "reference": "LLMs and KGs have been applied to Alzheimer’s Disease research by utilizing pre-trained language models on various data types such as speech recordings, electronic health records, and tabular data to detect AD and related tasks. Knowledge graphs (KGs) specifically constructed for AD integrate information from heterogeneous biomedical databases and scientific literature, including risk factors like diabetes and head trauma. The AD-specific KG construction involves extracting structural knowledge from unstructured corpora and using methods like path-based and neighbor-based exploration to build sub-graphs that capture relationships among entities such as diabetes, head trauma, and AD. This combined use of LLMs and KGs supports a more accurate and comprehensive understanding of AD risk factors, which is crucial for successful preventative approaches.", "synthesizer_name": "multi_hop_specific_query_synthesizer"}
