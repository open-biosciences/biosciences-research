user_input,reference_contexts,reference,synthesizer_name,response,retrieved_contexts
Who is Omar and what contributions are associated with him in biomedical knowledge graph research?,"['Omar & Mohammed / Journal of Applied Science and Technology Trends Vol. 06, No. 02, pp. 342 –357 (2025) 345 record-high accuracy, while open-source models achieved impressive gains through prompt optimization. Feng et al.[22] developed the Knowledge Graph-based Thought (KGT) framework that integrated LLMs with a pan- cancer knowledge graph for biomedical question answering. KGT was designed to reason on the knowledge graph schema and identify optimal subgraphs to use for directing accurate answer generation, all without fine-tuning the LLMs. The framework is benchmarked against a new dataset (PcQA) designed specifically for pan-cancer KGQA tasks and has outperformed all existing state-of-the-art approaches by a rather large margin. KGT’s practicality in biomedical issues was highlighted through case studies for drug repositioning, drug resistance, and biomarker discovery. Their approach exhibited robust adaptability among various LLMs. Rezaei et al.[26] developed AMG-RAG, a dynamic framework that utilizes autonomous LLM agents with medical search tools in the continuous construction and real-time updating of Medical Knowledge Graphs (MKGs). Their system incorporated confidence scoring and multi-hop reasoning to improve accuracy and interpretability in medical question answering. AMG-RAG outperformed size models on both very hard MEDQA benchmarks and more accessible MedMCQA ones, proving that it could conduct efficient reasoning based on current structured medical knowledge. They also used Neo4j to manage the knowledge graphs while adding external searches to ensure the latest data. Tiwari et al.[24] presented Auto-Cypher, a recent automated pipeline for producing high-quality synthetic data for training LLMs by mapping natural language to Cypher queries for graph databases like Neo4j. The pipeline deployed the novelty of LLM- as-database-filler to synthesize Neo4j databases for the execution of generated queries to ensure their correctness. A sizable dataset called SynthCypher was created, spanning multiple domains and complex queries, leading to a 40% improvement in LLM performance on Cypher generation. The datasets were used to fine-tune open-source models such as LLaMA, Mistral, and Qwen, and the SPIDER benchmark was adapted for evaluation purposes. Mohammed et al.[29] proposed a hybridized GraphRAG framework combining Neo4j-based UMLS knowledge graphs with a vector store for medical textbooks to create an improved U.S.M.L.E.-style clinical question-answering approach. The project integrated symbolic reasoning from knowledge graphs with semantic retrieval performed on text embeddings to enhance relevance and accuracy via adaptive re-ranking and query expansion. The system had the answers produced by GPT-4o- Mini, with different prompting strategies encouraging evidence- based and traceable responses grounded in verified medical knowledge. Experiments showed that the hybrid approach improved factual accuracy and citation fidelity as compared to the L.L.M.-only approach, enhancing transparency and reliability. It is shown that binding both structured and unstructured medical knowledge sources could aid in ameliorating hallucinations and hence improve clinical trustworthiness in AI-driven medical QA. Yang et al.[30] articulated sepsis knowledge graph was crafted by combining multicenter clinical data from over 10,000 patients with the help of GPT-4 for entity recognition and relationship extraction. Real-world data were collected from three hospitals and integrated with clinical guidelines and databases from the public domain. The knowledge graph contained 1,894 nodes and 2,021 relationships pertaining to diseases, symptoms, biomarkers, treatments, and complications. GPT outperformed other models in every resolution on sepsis- specific datasets to obtain high F1-score results. The constructed graph highlighted complex interactions in sepsis for assisting clinical decision-making and was implemented on Neo4j. Guan et al.[20] proposed a novel method for constructing a local knowledge graph from retrieved biomedical documents by extracting propositional claims. They carried out layer wise summarization on this graph to capture multi-document relationships and provide comprehensive contextual information to a language model for question-answering purposes. The method resolved issues in multi-document biomedical QA, such as noise cancellation and efficient context usage. They then tested their method on several benchmarks for biomedical question answering, achieving performance at least comparable to, if not better than, existing retrieval-augmented generation (RAG) baselines. The study established enhanced reasoning and answer accuracy of the model achieved through structured graph summarization. Previous studies have improved biomedical QA using KGs and LLMs, but important gaps remain. Most systems lack transparent, graph-based justifications, rely on limited evaluation methods, or depend on cloud resources that reduce privacy and reproducibility. Our framework addresses these gaps by providing visible Cypher queries with evidence subgraphs, applying comprehensive performance metrics across difficulty levels, and ensuring fully local, privacy-preserving deployment. Table I summarizes key previous studies on biomedical knowledge graphs and question answering, outlining their methods, datasets, and main limitations.']","Omar is a co-author mentioned in the context of biomedical knowledge graph research, specifically in the publication by Omar & Mohammed in the Journal of Applied Science and Technology Trends. Their work is referenced alongside other significant studies on integrating large language models with biomedical knowledge graphs for question answering, highlighting advances in frameworks that improve accuracy, reasoning, and clinical trustworthiness in medical QA systems.",single_hop_specifc_query_synthesizer,"Omar is one of the authors of a research work published in the *Journal of Applied Science and Technology Trends* (Vol. 06, No. 02, pp. 342–357, 2025) that focuses on biomedical knowledge graph (KG) research, specifically on question answering (QA) systems integrating large language models (LLMs) and biomedical knowledge graphs.

### Contributions associated with Omar in biomedical knowledge graph research:

1. **Hybrid LLM to Cypher QA System**: Omar and Mohammed developed a system that translates natural language biomedical questions into accurate, executable Cypher queries over a Neo4j knowledge graph containing drug, disease, and symptom data. This hybrid approach combines LLMs with graph databases for effective biomedical question answering.

2. **Prompt-driven Query Generation**: They introduced schema- and entity-aware prompting techniques that reliably map diverse biomedical questions into graph queries, improving the accuracy and relevance of the generated Cypher queries.

3. **Evidence Transparency**: Their system surfaces not only the answers but also the generated Cypher queries, the supporting justification subgraph (nodes and edges), and a brief natural language rationale. This transparency allows traceable, outcome-level validation of answers.

4. **Comprehensive Answer-level Evaluation**: The framework was evaluated end-to-end using multiple metrics such as Exact Match, Precision, Recall, F1, Hits@k, Mean Reciprocal Rank (MRR), and latency across simple, medium, and complex question sets, providing a thorough assessment of performance.

5. **Local, Reproducible Deployment**: The system uses on-premises LLaMA 3 inference, avoiding cloud dependencies. This design is suitable for biomedical settings requiring low latency, strong data control, privacy, and reproducibility.

6. **Integration of Structured and Unstructured Medical Knowledge**: In a related work by Mohammed et al. (likely the same Mohammed co-author), a hybridized GraphRAG framework was proposed that combines Neo4j-based UMLS knowledge graphs with vector stores of medical textbooks to improve clinical question answering, enhancing factual accuracy and reducing hallucinations.

### Summary

Omar’s work contributes a novel, hybrid biomedical QA framework that leverages LLMs and Neo4j-based biomedical knowledge graphs to translate natural language questions into Cypher queries with transparent evidence and rigorous evaluation. The approach emphasizes privacy-preserving, local deployment and improves the reliability and interpretability of biomedical question answering systems.","['Omar & Mohammed / Journal of Applied Science and Technology Trends Vol. 06, No. 02, pp. 342 –357 (2025) \n \n357 \n[4] Z. Ji, N. Lee, R. Frieske, T. Yu, D. Su, Y. Xu, E. Ishii, Y. J. Bang, A. Madotto, \nand P. Fung, “Survey of hallucination in natural language generation,” \nACM Computing Surveys, vol. 55, no. 12, pp. 1–38, 2023. \n[5] C. Malaviya, S. Lee, S. Chen, E. Sieber, M. Yatskar, and D. Roth, “Expertqa: \nexpert-curated questions and attributed answers,” arXiv preprint \narXiv:2309.07852, 2023. \n[6] L. Pusch and T. O. Conrad, “Combining llms and knowledge graphs to reduce \nhallucinations \nin \nquestion \nanswering,” \narXiv \npreprint \narXiv:2409.04181, 2024. \n[7] H. Nori, N. King, S. M. McKinney, D. Carignan, and E. Horvitz, \n“Capabilities of gpt-4 on medical challenge problems,” arXiv preprint \narXiv:2303.13375, 2023. \n[8] T. Sekar, Kushal, S. Shankar, S. Mohammed, and J. Fiaidhi, “Investigations \non using evidence-based graphrag pipeline using llm tailored for usmle \nstyle questions,” medRxiv, pp. 2025–05, 2025. \n[9] S. Farquhar, J. Kossen, L. Kuhn, and Y. Gal, “Detecting hallucinations in \nlarge language models using semantic entropy,” Nature, vol. 630, no. \n8017, pp. 625–630, 2024. \n[10] E. Asgari, N. Montaña-Brown, M. Dubois, S. Khalil, J. Balloch, J. A. \nYeung, and D. Pimenta, “A framework to assess clinical safety and \nhallucination rates of llms for medical text summarisation,” npj Digital \nMedicine, vol. 8, no. 1, p. 274, 2025. \n[11] C. Su, Y. Hou, M. Zhou, S. Rajendran, J. R. Maasch, Z. Abedi, H. Zhang, \nZ. Bai, A. Cuturrufo, W. Guo, et al., “Biomedical discovery through the \nintegrative biomedical knowledge hub (ibkh),” Iscience, vol. 26, no. 4, \n2023. \n[12] J. H. Morris, K. Soman, R. E. Akbas, X. Zhou, B. Smith, E. C. Meng, C. C. \nHuang, G. Cerono, G. Schenk, A. Rizk-Jackson, et al., “The scalable \nprecision medicine open knowledge engine (spoke): a massive \nknowledge graph of biomedical information,” Bioinformatics, vol. 39, \nno. 2, p. btad080, 2023. \n[13] K. Soman, P. W. Rose, J. H. Morris, R. E. Akbas, B. Smith, B. Peetoom, C. \nVillouta-Reyes, G. Cerono, Y. Shi, A. Rizk-Jackson, et al., “Biomedical \nknowledge graph-optimized prompt generation for large language \nmodels,” Bioinformatics, vol. 40, no. 9, p. btae560, 2024. \n[14] F. Frau, P. Loustalot, M. Törnqvist, N. Temam, J. Cupe, M. Montmerle, and \nF. Augé, “Connecting electronic health records to a biomedical \nknowledge graph to link clinical phenotypes and molecular endotypes \nin atopic dermatitis,” Scientific Reports, vol. 15, no. 1, p. 3082, 2025. \n[15] Y. Gao, R. Li, E. Croxford, J. Caskey, B. W. Patterson, M. Churpek, T. \nMiller, D. Dligach, and M. Afshar, “Leveraging medical knowledge \ngraphs into large language models for diagnosis prediction: design and \napplication study,” Jmir AI, vol. 4, p. e58670, 2025. \n[16] Y. Yan, Y. Hou, Y. Xiao, R. Zhang, and Q. Wang, “Knownet: guided health \ninformation seeking from llms via knowledge graph integration,” IEEE \nTransactions on Visualization and Computer Graphics, 2024. \n[17] Y. Deng, S. Zhao, Y. Miao, J. Zhu, and J. Li, “Medka: a knowledge graph-\naugmented approach to improve factuality in medical large language \nmodels,” Journal of Biomedical Informatics, p. 104871, 2025. \n[18] L. Ehrlinger and W. Wöß, “Towards a definition of knowledge graphs,” \nSEMANTiCS (Posters, Demos, SuCCESS), vol. 48, no. 1–4, p. 2, 2016. \n[19] E. Rajabi and S. Kafaie, “Building a disease knowledge graph,” in Caring \nis Sharing – Exploiting the Value in Data for Health and Innovation, \npp. 701–705, IOS Press, 2023. \n[20] L. Guan, Y. Huang, and J. Liu, “Biomedical question answering via multi-\nlevel summarization on a local knowledge graph,” arXiv preprint \narXiv:2504.01309, 2025. \n[21] D. Steinigen, R. Teucher, T. H. Ruland, M. Rudat, N. Flores-Herr, P. \nFischer, N. Milosevic, C. Schymura, and A. Ziletti, “Fact finder – \nenhancing domain expertise of large language models by incorporating \nknowledge graphs,” arXiv preprint arXiv:2408.03010, 2024. \n[22] Y. Feng, L. Zhou, C. Ma, Y. Zheng, R. He, and Y. Li, “Knowledge graph–\nbased thought: a knowledge graph–enhanced llm framework for pan-\ncancer question answering,” GigaScience, vol. 14, p. giae082, 2025. \n[23] H. Luo, Z. Tang, S. Peng, Y. Guo, W. Zhang, C. Ma, G. Dong, M. Song, \nW. Lin, Y. Zhu, et al., “Chatkbqa: a generate-then-retrieve framework \nfor knowledge base question answering with fine-tuned large language \nmodels,” arXiv preprint arXiv:2310.08975, 2023. \n[24] A. Tiwari, S. K. R. Malay, V. Yadav, M. Hashemi, and S. T. Madhusudhan, \n“Auto-cypher: improving llms on cypher generation via llm-supervised \ngeneration-verification framework,” in Proceedings of the 2025 \nConference of the Nations of the Americas Chapter of the Association \nfor Computational Linguistics: Human Language Technologies \n(Volume 2: Short Papers), pp. 623–640, 2025. \n[25] R. Wang, Z. Zhang, L. Rossetto, F. Ruosch, and A. Bernstein, “Nlqxform: \na language model-based question to sparql transformer,” arXiv preprint \narXiv:2311.07588, 2023. \n[26] M. R. Rezaei, R. S. Fard, J. L. Parker, R. G. Krishnan, and M. Lankarany, \n“Agentic medical knowledge graphs enhance medical question \nanswering: bridging the gap between llms and evolving medical \nknowledge,” arXiv preprint arXiv:2502.13010, 2025. \n[27] Z. Dong, B. Peng, Y. Wang, J. Fu, X. Wang, Y. Shan, and X. Zhou, “Effiqa: \nefficient question-answering with strategic multi-model collaboration \non knowledge graphs,” arXiv preprint arXiv:2406.01238, 2024. \n[28] Y. Duan, Q. Zhou, Y. Li, C. Qin, Z. Wang, H. Kan, and J. Hu, “Research \non a traditional chinese medicine case-based question-answering \nsystem integrating large language models and knowledge graphs,” \nFrontiers in Medicine, vol. 11, p. 1512329, 2025. \n[29] S. Mohammed, J. Fiaidhi, T. Sekar, K. Kushal, and S. Shankar, \n“Investigations on using evidence-based graphrag pipeline using llm \ntailored for answering usmle medical exam questions,” medRxiv, pp. \n2025–05, 2025. \n[30] H. Yang, J. Li, C. Zhang, A. P. Sierra, and B. Shen, “Large language model–\ndriven knowledge graph construction in sepsis care using multicenter \nclinical databases: development and usability study,” Journal of \nMedical Internet Research, vol. 27, p. e65537, 2025. \n[31] K.-L. Hsieh, G. Plascencia-Villa, K.-H. Lin, G. Perry, X. Jiang, and Y. Kim, \n“Synthesize heterogeneous biological knowledge via representation \nlearning for alzheimer’s disease drug repurposing,” Iscience, vol. 26, \nno. 1, 2023. \n[32] R. Angles and C. Gutierrez, “Survey of graph database models,” ACM \nComputing Surveys (CSUR), vol. 40, no. 1, pp. 1–39, 2008. \n[33] B. Chicho and A. O. Mohammed, “An empirical comparison of neo4j and \ntigergraph databases for network centrality,” Science Journal of \nUniversity of Zakho, vol. 11, no. 2, pp. 190–201, 2023. \n[34] I. Robinson, J. Webber, and E. Eifrem, Graph Databases: New \nOpportunities for Connected Data, O’Reilly Media, 2015. \n[35] A. Lysenko, I. A. Roznovăţ, M. Saqi, A. Mazein, C. J. Rawlings, and C. \nAuffray, “Representing and querying disease networks using graph \ndatabases,” BioData Mining, vol. 9, no. 1, p. 23, 2016. \n[36] M. Šestak, M. Heričko, T. W. Družovec, and M. Turkanović, “Applying k-\nvertex cardinality constraints on a neo4j graph database,” Future \nGeneration Computer Systems, vol. 115, pp. 459–474, 2021. \n[37] M. Desai, R. G. Mehta, and D. P. Rana, “An empirical analysis to identify \nthe effect of indexing on influence detection using graph databases,” \nInternational Journal of Innovative Technology and Exploring \nEngineering, vol. 8, no. 9S, pp. 414–421, 2019. \n[38] S. Beis, S. Papadopoulos, and Y. Kompatsiaris, “Benchmarking graph \ndatabases on the problem of community detection,” in New Trends in \nDatabase and Information Systems II, pp. 3–14, Springer, 2015. \n[39] R. Wang, Z. Yang, W. Zhang, and X. Lin, “An empirical study on recent \ngraph database systems,” in International Conference on Knowledge \nScience, Engineering and Management, pp. 328–340, Springer, 2020. \n[40] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “Bert: pre-training of \ndeep bidirectional transformers for language understanding,” in \nProceedings of the 2019 Conference of the North American Chapter of \nthe Association for Computational Linguistics: Human Language \nTechnologies, vol. 1, pp. 4171–4186, 2019. \n[41] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. \nNeelakantan, P. Shyam, G. Sastry, A. Askell, et al., “Language models \nare few-shot learners,” Advances in Neural Information Processing \nSystems, vol. 33, pp. 1877–1901, 2020. \n[42] J. Lee, W. Yoon, S. Kim, D. Kim, S. Kim, C. H. So, and J. Kang, “Biobert: \na pre-trained biomedical language representation model for biomedical \ntext mining,” Bioinformatics, vol. 36, no. 4, pp. 1234–1240, 2020. \n[43] E. Alsentzer, J. R. Murphy, W. Boag, W.-H. Weng, D. Jin, T. Naumann, \nand M. McDermott, “Publicly available clinical bert embeddings,” \narXiv preprint arXiv:1904.03323, 2019. \n[44] Z. He, S. Sunkara, X. Zang, Y. Xu, L. Liu, N. Wichers, G. Schubiner, R. \nLee, and J. Chen, “Actionbert: leveraging user actions for semantic \nunderstanding of user interfaces,” in Proceedings of the AAAI \nConference on Artificial Intelligence, vol. 35, pp. 5931–5938, 2021.'
 'Omar & Mohammed / Journal of Applied Science and Technology Trends Vol. 06, No. 02, pp. 342 –357 (2025) \n \n344 \nevaluated using Exact Match, Precision, Recall, F1, Hits@k, \nMRR, and latency across simple, medium, and complex question \nsets. Unlike prior template-based methods, our approach enables \ntraceable, outcome-level validation. In summary, the main \ncontributions of this work are as follows: \n• Hybrid LLM to Cypher QA: A system that translates \nnatural language questions into accurate, executable \nCypher over a Neo4j drug, disease, and symptom KG. \n• Prompt-driven query generation: Schema,  entity \naware prompting that reliably maps diverse biomedical \nquestions to graph queries. \n• Evidence transparency: Along with each answer, we \nsurface the generated Cypher and the supporting \njustification subgraph (nodes, edges) plus a brief natural \nlanguage rationale. \n• Answer-level evaluation: End-to-end assessment using \nExact Match, F1, Precision/Recall, Hits@k, MRR and \nlatency across simple, medium and complex tiers. \n• Local, reproducible deployment: On-prem LLaMA 3 \ninference (no cloud dependency) suitable for biomedical \nsettings requiring low latency and strong data control. \n \nThe remainder of this paper is organized as follows: Section \n2 reviews related work on biomedical knowledge graphs and \nLLM-based QA systems. Section 3 provides background on \nknowledge graphs, large language models, and question \nanswering frameworks. Section 4 details the proposed \nmethodology, \nincluding \nsystem \narchitecture, \ndataset \nconstruction, and query translation. Section 5 presents the \nexperimental results through both quantitative metrics and \nqualitative case studies. Section 6 discusses the findings, analyzes \nlimitations, and compares performance against baseline models. \nFinally, Section 7 concludes this paper and outlines directions for \nfuture work. \nII. RELATED WORK \nRecently, studies have concentrated on the integration of \nclinical and medical knowledge graphs (LLM) to improve the \nanswer to medical questions. Researchers have derived several \nbiomedical KGs using Neo4j and incorporated the application of \nLLMs like LLaMA and GPT to convert natural language \nquestions into graph queries. Improvements in answer \ncorrectness, reduction of hallucination errors, one-to-many \nrelationships, and support for complex reasoning were the \nobjectives of these efforts. Some frameworks also adopted \nretrieval methods to ground responses in secure data. \nSu et al.[11] developed an integrative Biomedical Knowledge \nHub (iBKH), a huge biomedical knowledge graph that comprises \n18 of the very best data sources. The deployment of the iBKH in \nNeo4j allows for a user-friendly web portal to allow fast and \ninteractive knowledge retrieval. The system implemented \nadvanced graph learning techniques to enable the discovery of \nbiomedical knowledge, illustrated by an example of repurposing \nin silico drugs for Alzheimer’s disease. iBKH achieved \npromising predictive performance for known drugs and proposed \npossible new drug candidates. \nRajabi and Kafaie[19] proposed a disease knowledge graph \nusing a cross-referential disease database comprising diseases, \nsymptoms, and drugs interconnected with relationships. They \ntransferred the data into Neo4j to create a graph of 9,400 nodes \nand 45,000 relationships representing the semantic links between \nmedical concepts. Applying Cypher queries enabled answering \ncomplex medical questions regarding identifying drugs that may \ncause certain diseases; it was demonstrated that the graph inferred \nnew relationships not explicitly existing in the original data. The \nconclusion was that disease knowledge graphs sped up clinical \ndiscovery and contributed to understanding complex medical \nrelationships. \nHou et al.[3] assessed and contrasted ChatGPT (both GPT-\n3.5 and GPT-4) and the biomedical knowledge graphs (BKGs) \nconcerning their ability to answer biomedical questions, generate \nnew knowledge, and reason. Their datasets were focused on \ndietary supplements and drugs, while evaluation criteria entailed \naccuracy, novelty, and reasoning ability. The results indicate that \nwhile GPT-4 surpassed GPT-3.5 and BKGs in knowledge \nprovision, it proved inconsistent with regard to citations and \nreasoning. Compared to them, BKGs scored higher in accuracy \nand reliability, especially in discovering novel links as well as \nwithin structured reasoning. \nSoman et al.[13] presented a novel framework called KG-\nRAG that integrates a large biomedical knowledge graph \n(SPOKE) with LLaMA 2, GPT-3.5, and GPT-4 (LLMs) to \nproduce accurate biomedical text. They optimized the retrieval of \nrelevant graph context to cut over 50% tokens without losing \naccuracy. It aided LLMs in performing better on biomedical \nquestion answering with very high accuracy boosts, especially in \nthe case of LLaMA 2. They compared KG-RAG to other retrieval \nmethods and indicated its comparatively more robust and \nefficient results. The framework produced reliable evidence-\nbased answers grounded in biomedical knowledge. \nLuo et al.[23] created ChatKBQA, a new framework with a \nquestion-and-answer approach over knowledge bases that first \ngenerates logical forms with the help of fine-tuned LLMs and \nthen retrieves the relevant entities and relations. This generate-\nthen-retrieve approach is supposed to handle a couple of issues \nwith the earlier methods concerning tedious retrieval and error \npropagation. They fine-tuned open-source LLMs like LLaMA 2 \nto change natural-language questions into logical forms with high \naccuracy. The retrieval phase uses unsupervised phrase-level \nsemantic matching in a way that enhances the alignment of \nentities and relations. Experiments on benchmark datasets \nindicate ChatKBQA to be superior to its predecessors, with the \nhighest accuracy to date. \nPusch and Conrad[6] conducted work under a hybrid \napproach conflating LLMs and biomedical Knowledge Graphs \n(KGs) to suppress hallucination errors in question-answering. \nThey proposed query-checking algorithms for validating, \ncorrecting, and executing the KG Cypher queries that LLMs \ngenerated, thereby attaining accurate and understandable \nanswers. The system used retrieval-augmented generation (RAG) \nto ground answers within KG data. The methodology was \nvalidated on a biomedical KG called PrimeKG using 50 \nbenchmark questions, assessing models like GPT-4 Turbo and \nLLaMA 3. Commercially available GPT-4 Turbo obtained'
 'Omar & Mohammed / Journal of Applied Science and Technology Trends Vol. 06, No. 02, pp. 342 –357 (2025) \n \n345 \nrecord-high accuracy, while open-source models achieved \nimpressive gains through prompt optimization. \nFeng et al.[22] developed the Knowledge Graph-based \nThought (KGT) framework that integrated LLMs with a pan-\ncancer knowledge graph for biomedical question answering. \nKGT was designed to reason on the knowledge graph schema and \nidentify optimal subgraphs to use for directing accurate answer \ngeneration, all without fine-tuning the LLMs. The framework is \nbenchmarked against a new dataset (PcQA) designed specifically \nfor pan-cancer KGQA tasks and has outperformed all existing \nstate-of-the-art approaches by a rather large margin. KGT’s \npracticality in biomedical issues was highlighted through case \nstudies for drug repositioning, drug resistance, and biomarker \ndiscovery. Their approach exhibited robust adaptability among \nvarious LLMs. \nRezaei et al.[26] developed AMG-RAG, a dynamic \nframework that utilizes autonomous LLM agents with medical \nsearch tools in the continuous construction and real-time updating \nof Medical Knowledge Graphs (MKGs). Their system \nincorporated confidence scoring and multi-hop reasoning to \nimprove accuracy and interpretability in medical question \nanswering. AMG-RAG outperformed size models on both very \nhard MEDQA benchmarks and more accessible MedMCQA \nones, proving that it could conduct efficient reasoning based on \ncurrent structured medical knowledge. They also used Neo4j to \nmanage the knowledge graphs while adding external searches to \nensure the latest data. \nTiwari et al.[24] presented Auto-Cypher, a recent automated \npipeline for producing high-quality synthetic data for training \nLLMs by mapping natural language to Cypher queries for graph \ndatabases like Neo4j. The pipeline deployed the novelty of LLM-\nas-database-filler to synthesize Neo4j databases for the execution \nof generated queries to ensure their correctness. A sizable dataset \ncalled SynthCypher was created, spanning multiple domains and \ncomplex queries, leading to a 40% improvement in LLM \nperformance on Cypher generation. The datasets were used to \nfine-tune open-source models such as LLaMA, Mistral, and \nQwen, and the SPIDER benchmark was adapted for evaluation \npurposes. \nMohammed et al.[29] proposed a hybridized GraphRAG \nframework combining Neo4j-based UMLS knowledge graphs \nwith a vector store for medical textbooks to create an improved \nU.S.M.L.E.-style clinical question-answering approach. The \nproject integrated symbolic reasoning from knowledge graphs \nwith semantic retrieval performed on text embeddings to enhance \nrelevance and accuracy via adaptive re-ranking and query \nexpansion. The system had the answers produced by GPT-4o-\nMini, with different prompting strategies encouraging evidence-\nbased and traceable responses grounded in verified medical \nknowledge. Experiments showed that the hybrid approach \nimproved factual accuracy and citation fidelity as compared to \nthe L.L.M.-only approach, enhancing transparency and \nreliability. It is shown that binding both structured and \nunstructured medical knowledge sources could aid in \nameliorating hallucinations and hence improve clinical \ntrustworthiness in AI-driven medical QA. \nYang et al.[30] articulated sepsis knowledge graph was \ncrafted by combining multicenter clinical data from over 10,000 \npatients with the help of GPT-4 for entity recognition and \nrelationship extraction. Real-world data were collected from \nthree hospitals and integrated with clinical guidelines and \ndatabases from the public domain. The knowledge graph \ncontained 1,894 nodes and 2,021 relationships pertaining to \ndiseases, symptoms, biomarkers, treatments, and complications. \nGPT outperformed other models in every resolution on sepsis-\nspecific datasets to obtain high F1-score results. The constructed \ngraph highlighted complex interactions in sepsis for assisting \nclinical decision-making and was implemented on Neo4j. \nGuan et al.[20] proposed a novel method for constructing a \nlocal knowledge graph from retrieved biomedical documents by \nextracting propositional claims. They carried out layer wise \nsummarization on this graph to capture multi-document \nrelationships and provide comprehensive contextual information \nto a language model for question-answering purposes. The \nmethod resolved issues in multi-document biomedical QA, such \nas noise cancellation and efficient context usage. They then tested \ntheir method on several benchmarks for biomedical question \nanswering, achieving performance at least comparable to, if not \nbetter than, existing retrieval-augmented generation (RAG) \nbaselines. The study established enhanced reasoning and answer \naccuracy of the model achieved through structured graph \nsummarization.   \nPrevious studies have improved biomedical QA using KGs \nand LLMs, but important gaps remain. Most systems lack \ntransparent, graph-based justifications, rely on limited evaluation \nmethods, or depend on cloud resources that reduce privacy and \nreproducibility. Our framework addresses these gaps by \nproviding visible Cypher queries with evidence subgraphs, \napplying comprehensive performance metrics across difficulty \nlevels, and ensuring fully local, privacy-preserving deployment. \nTable I summarizes key previous studies on biomedical \nknowledge graphs and question answering, outlining their \nmethods, datasets, and main limitations. \nIII. PRELIMINARIES \nThis section outlines the fundamental concepts required to \nunderstand the proposed system. It introduces biomedical \nknowledge graphs, explains how Neo4j stores data in graph \nform, and describes the use of Cypher for querying. It also \nprovides a brief overview of large language models (LLMs) and \ntheir role in interpreting natural language. \n \n \nA. Biomedical Knowledge Graphs \nBiomedical Knowledge Graphs (BKGs) provide a structured \nrepresentation of complex biomedical information by modeling \ndiverse medical entities, such as diseases, drugs, symptoms, and \nbiological pathways, as interconnected nodes within a graph \nstructure. The edges in these graphs represent the semantic \nrelationships between these entities, including ’treats’, ’causes’, \n’interacts with’ and many others, as illustrated in Fig 1. This \nform of representation enables the integration of heterogeneous \nbiomedical data from a wide range of sources, including']"
what Cypher do for biomedical data?,"['III. PRELIMINARIES This section outlines the fundamental concepts required to understand the proposed system. It introduces biomedical knowledge graphs, explains how Neo4j stores data in graph form, and describes the use of Cypher for querying. It also provides a brief overview of large language models (LLMs) and their role in interpreting natural language. A. Biomedical Knowledge Graphs Biomedical Knowledge Graphs (BKGs) provide a structured representation of complex biomedical information by modeling diverse medical entities, such as diseases, drugs, symptoms, and biological pathways, as interconnected nodes within a graph structure. The edges in these graphs represent the semantic relationships between these entities, including ’treats’, ’causes’, ’interacts with’ and many others, as illustrated in Fig 1. This form of representation enables the integration of heterogeneous biomedical data from a wide range of sources, including']","Cypher is used for querying biomedical knowledge graphs stored in Neo4j, enabling retrieval and manipulation of complex biomedical information represented as interconnected nodes and relationships.",single_hop_specifc_query_synthesizer,"Cypher is used in biomedical data to query biomedical knowledge graphs stored in graph databases like Neo4j. Specifically, Cypher enables the translation of natural language biomedical questions into graph queries that retrieve relevant, structured biomedical information. This includes identifying relationships between entities such as diseases, drugs, symptoms, side effects, and treatments.

For example, Cypher queries can:

- Retrieve symptoms associated with a specific disease (e.g., brain cancer).
- Find side effects caused by a particular drug (e.g., insulin).
- Extract side effects of drugs that treat a certain condition (e.g., epilepsy).

By using Cypher, the system can navigate the graph structure—following semantic relationships like HAS_SYMPTOM, CAUSES, and TREATS—to return clinically plausible, interpretable, and traceable answers. This approach supports accuracy and transparency in biomedical question answering by providing visible queries and evidence subgraphs that users can inspect.

In summary, Cypher serves as the query language that enables effective exploration, retrieval, and reasoning over complex biomedical knowledge graphs, facilitating accurate and explainable biomedical data analysis and question answering.","['Omar & Mohammed / Journal of Applied Science and Technology Trends Vol. 06, No. 02, pp. 342 –357 (2025) \n \n345 \nrecord-high accuracy, while open-source models achieved \nimpressive gains through prompt optimization. \nFeng et al.[22] developed the Knowledge Graph-based \nThought (KGT) framework that integrated LLMs with a pan-\ncancer knowledge graph for biomedical question answering. \nKGT was designed to reason on the knowledge graph schema and \nidentify optimal subgraphs to use for directing accurate answer \ngeneration, all without fine-tuning the LLMs. The framework is \nbenchmarked against a new dataset (PcQA) designed specifically \nfor pan-cancer KGQA tasks and has outperformed all existing \nstate-of-the-art approaches by a rather large margin. KGT’s \npracticality in biomedical issues was highlighted through case \nstudies for drug repositioning, drug resistance, and biomarker \ndiscovery. Their approach exhibited robust adaptability among \nvarious LLMs. \nRezaei et al.[26] developed AMG-RAG, a dynamic \nframework that utilizes autonomous LLM agents with medical \nsearch tools in the continuous construction and real-time updating \nof Medical Knowledge Graphs (MKGs). Their system \nincorporated confidence scoring and multi-hop reasoning to \nimprove accuracy and interpretability in medical question \nanswering. AMG-RAG outperformed size models on both very \nhard MEDQA benchmarks and more accessible MedMCQA \nones, proving that it could conduct efficient reasoning based on \ncurrent structured medical knowledge. They also used Neo4j to \nmanage the knowledge graphs while adding external searches to \nensure the latest data. \nTiwari et al.[24] presented Auto-Cypher, a recent automated \npipeline for producing high-quality synthetic data for training \nLLMs by mapping natural language to Cypher queries for graph \ndatabases like Neo4j. The pipeline deployed the novelty of LLM-\nas-database-filler to synthesize Neo4j databases for the execution \nof generated queries to ensure their correctness. A sizable dataset \ncalled SynthCypher was created, spanning multiple domains and \ncomplex queries, leading to a 40% improvement in LLM \nperformance on Cypher generation. The datasets were used to \nfine-tune open-source models such as LLaMA, Mistral, and \nQwen, and the SPIDER benchmark was adapted for evaluation \npurposes. \nMohammed et al.[29] proposed a hybridized GraphRAG \nframework combining Neo4j-based UMLS knowledge graphs \nwith a vector store for medical textbooks to create an improved \nU.S.M.L.E.-style clinical question-answering approach. The \nproject integrated symbolic reasoning from knowledge graphs \nwith semantic retrieval performed on text embeddings to enhance \nrelevance and accuracy via adaptive re-ranking and query \nexpansion. The system had the answers produced by GPT-4o-\nMini, with different prompting strategies encouraging evidence-\nbased and traceable responses grounded in verified medical \nknowledge. Experiments showed that the hybrid approach \nimproved factual accuracy and citation fidelity as compared to \nthe L.L.M.-only approach, enhancing transparency and \nreliability. It is shown that binding both structured and \nunstructured medical knowledge sources could aid in \nameliorating hallucinations and hence improve clinical \ntrustworthiness in AI-driven medical QA. \nYang et al.[30] articulated sepsis knowledge graph was \ncrafted by combining multicenter clinical data from over 10,000 \npatients with the help of GPT-4 for entity recognition and \nrelationship extraction. Real-world data were collected from \nthree hospitals and integrated with clinical guidelines and \ndatabases from the public domain. The knowledge graph \ncontained 1,894 nodes and 2,021 relationships pertaining to \ndiseases, symptoms, biomarkers, treatments, and complications. \nGPT outperformed other models in every resolution on sepsis-\nspecific datasets to obtain high F1-score results. The constructed \ngraph highlighted complex interactions in sepsis for assisting \nclinical decision-making and was implemented on Neo4j. \nGuan et al.[20] proposed a novel method for constructing a \nlocal knowledge graph from retrieved biomedical documents by \nextracting propositional claims. They carried out layer wise \nsummarization on this graph to capture multi-document \nrelationships and provide comprehensive contextual information \nto a language model for question-answering purposes. The \nmethod resolved issues in multi-document biomedical QA, such \nas noise cancellation and efficient context usage. They then tested \ntheir method on several benchmarks for biomedical question \nanswering, achieving performance at least comparable to, if not \nbetter than, existing retrieval-augmented generation (RAG) \nbaselines. The study established enhanced reasoning and answer \naccuracy of the model achieved through structured graph \nsummarization.   \nPrevious studies have improved biomedical QA using KGs \nand LLMs, but important gaps remain. Most systems lack \ntransparent, graph-based justifications, rely on limited evaluation \nmethods, or depend on cloud resources that reduce privacy and \nreproducibility. Our framework addresses these gaps by \nproviding visible Cypher queries with evidence subgraphs, \napplying comprehensive performance metrics across difficulty \nlevels, and ensuring fully local, privacy-preserving deployment. \nTable I summarizes key previous studies on biomedical \nknowledge graphs and question answering, outlining their \nmethods, datasets, and main limitations. \nIII. PRELIMINARIES \nThis section outlines the fundamental concepts required to \nunderstand the proposed system. It introduces biomedical \nknowledge graphs, explains how Neo4j stores data in graph \nform, and describes the use of Cypher for querying. It also \nprovides a brief overview of large language models (LLMs) and \ntheir role in interpreting natural language. \n \n \nA. Biomedical Knowledge Graphs \nBiomedical Knowledge Graphs (BKGs) provide a structured \nrepresentation of complex biomedical information by modeling \ndiverse medical entities, such as diseases, drugs, symptoms, and \nbiological pathways, as interconnected nodes within a graph \nstructure. The edges in these graphs represent the semantic \nrelationships between these entities, including ’treats’, ’causes’, \n’interacts with’ and many others, as illustrated in Fig 1. This \nform of representation enables the integration of heterogeneous \nbiomedical data from a wide range of sources, including'
 'Omar & Mohammed / Journal of Applied Science and Technology Trends Vol. 06, No. 02, pp. 342 –357 (2025) \n \n355 \nFor the question ’What are the symptoms of brain cancer?’ \nThe system generated a Cypher query that correctly followed the \nHAS_SYMPTOM relationship from disease nodes to symptoms \nnodes, filtering by the specified disease name. The results \nretrieved included terms such as aphasia (Broca, Wernicke), \nanomia, agraphia, agnosia, amnesia (including retrograde \namnesia), olfaction disorders, and apnea symptoms consistent \nwith established neurological manifestations of brain tumors. In \nNeo4j, the data formed a clear center-and-spoke pattern, with \nbrain cancer at the center and its associated nodes of symptoms \nradiating outward, as shown in Fig. 5. \nCypher: \nWITH toLower(""brain cancer"") AS disease_name \nMATCH (d:Disease)-[:HAS_SYMPTOM]->(s:Symptom) \nWHERE toLower(d.name) CONTAINS disease_name \nRETURN DISTINCT s,d \nLIMIT 10; \n \n \nFig. 5. Graphical user interface displaying Cypher query and results for breast \ncancer treatment \n \n \nA second query, What are the side effects of insulin?, \nproduced a Cypher statement starting from the drug node for \ninsulin and traversing the CAUSES relationship to the connected \nSideEffect nodes. The resulting list included dizziness, diarrhea, \ncough, back pain, weakness, rash/dermatitis, and hypersensitivity \nside effects well documented in insulin safety profiles. In the \nvisual representation in Fig. 6, insulin appeared centrally \nconnected to these side effect nodes, further strengthening the \ncorrectness of the relationship mapping of the system. \n \nCypher: \nWITH toLower(""insulin"") AS drug_name  \nMATCH (d:Drug)-[:CAUSES]->(se:SideEffect)  \nWHERE toLower(d.name) CONTAINS drug_name  \nRETURN se,d \nLIMIT 20; \n \nFig. 6. Knowledge Graph Visualization of Insulin and Its Related Side Effects \n \n \nThese examples demonstrate the system’s ability to interpret \nplain English biomedical questions, generate correct Cypher \nqueries, and return results that are clinically plausible, easy to \ninterpret, and directly traceable to specific graph relationships. \nThis supports both the accuracy and the transparency of the \nproposed approach. \nTo illustrate how biomedical questions can be translated into \ngraph queries, consider the natural language question: ’What are \nthe side effects of drugs that treat epilepsy?’. The following \nCypher query demonstrates how this question is mapped in the \nknowledge graph, where diseases are connected to drugs through \nthe TREATS relation and drugs are connected to side effects \nthrough the CAUSES relation. \n \nCypher: \nMATCH (d:Disease)<-[:TREATS]-(dr:Drug)  \nWHERE toLower(d.name) CONTAINS ""epilepsy"" \nMATCH (dr:Drug)-[:CAUSES]->(se:SideEffect) \nRETURN DISTINCT se,d,dr \nlimit 10'
 'Omar & Mohammed / Journal of Applied Science and Technology Trends Vol. 06, No. 02, pp. 342 –357 (2025) \n \n343 \nfor visualizing and validating LLM outputs [16], and MedKA for \nKG-enhanced question answering [17]. \nTo address these challenges, several recent works have \nexplored the integration of large language models with \nbiomedical knowledge graphs (KGs). A biomedical KG is a \nstructured network that connects entities such as diseases, drugs, \nand symptoms using defined relationships [18, 19]. These graphs \nstore verified medical knowledge from trusted databases, \nallowing for more accurate and explainable responses [12]. KGs \nare especially useful in multi-step reasoning tasks, where finding \nan answer requires connecting different pieces of information \n[20]. These entities and relationships can be visually represented \nin a biomedical knowledge graph, as shown in Fig. 1, where \nnodes represent medical concepts such as drugs, diseases, \nsymptoms, and pathways, and edges denote their semantic \nrelationships. \n \nFig. 1. Overview of Biomedical Knowledge Graph Entities and Relationships \n \nOne example of a widely used biomedical KG is SPOKE, \nwhich includes millions of nodes and relationships from over 40 \nbiomedical databases [12]. Integrating an LLM with a KG allows \nthe strengths of both technologies to work together: the LLM \nprovides language understanding, and the KG provides \nstructured, factual knowledge [21, 22]. A common method is \nretrieval-augmented generation (RAG), where the LLM retrieves \ninformation from the KG and uses it to generate more accurate \nresponses [13, 23]. In more advanced setups, the LLM can even \ngenerate queries like Cypher to fetch specific data from the graph \n[24, 25]. Neo4j is a popular graph database that supports fast and \nflexible storage and querying of knowledge graphs using Cypher \n[21]. It is well-suited for biomedical applications because it \nallows easy exploration of complex medical relationships. Recent \nwork has shown that combining Neo4j with LLMs can lead to \nbetter accuracy, fewer hallucinations, and more explainable \nresults [24, 26]. \nDespite improvements, building a reliable hybrid system that \ncombines an LLM with a biomedical KG remains a technical \nchallenge. Some approaches require complex pipelines or large \ntraining datasets, while others rely on fine-tuning specific to a \nnarrow set of questions [27, 28]. There is still a need for systems \nthat are both accurate and easy to scale, especially in domains like \nmedicine where the cost of errors is high [22]. \nRecent advances in KG-augmented LLMs have improved \nperformance, yet biomedical QA continues to face three practical \ngaps: \n1. Traceability: LLM-only or text-retrieval-only pipelines \nrarely provide graph-grounded justifications; users lack the \nability to inspect the exact nodes and edges that support an \nanswer. \n2. Evaluation: Prior work often judges quality via surface-\nform checks (e.g., matching a Cypher template), which fails to \ncapture end-to-end answer correctness or ranking quality \nacross different difficulty levels. \n3. Deployment: Many solutions assume cloud resources or \ndomain-specific fine-tuning, yet biomedical contexts typically \ndemand a local, privacy-preserving system with low latency \nand reproducible behavior. \nTimestamp-aware execution and periodic KG refresh help \navoid deprecated or contraindicated links, making the tool better \nsuited for safety-critical clinical contexts (e.g., drug–drug \ninteractions). \nTo address these limitations, Our work introduces a locally \ndeployable pipeline that translates biomedical questions into \nexecutable Cypher queries over a Neo4j knowledge graph. The \nsystem returns answers with supporting nodes and edges, and is']"
Wht is the role of the Journal of Applied Science and Technology Trends in biomedical data analysis?,"['Omar & Mohammed / Journal of Applied Science and Technology Trends Vol. 06, No. 02, pp. 342 –357 (2025) 348 drugs, diseases, symptoms) and execution returns structured data (tuples) relevant to the question. Step 5. Answer Synthesis: The structured tuples flow to Answer Synthesis, which aggregates and formats them into a concise raw answer. This raw answer is sent back to LLaMA 3 to optionally refine the phrasing while preserving the retrieved facts. Step 6. Result Presentation: LLaMA 3 produces the final answer, which the interface displays together with the executed Cypher query and an optional preview of the returned rows, improving transparency and trust. The pipeline couples LLM-based language understanding (LLaMA 3) with a schema-grounded Neo4j knowledge graph. The Cypher Query Gen refines the query formulation, Query Execution retrieves evidence and Answer Synthesis converts structured results into readable outputs that produce answers that are accurate, interpretable, and easy to audit directly from the displayed query and evidence.']","The Journal of Applied Science and Technology Trends publishes research such as the work by Omar & Mohammed, which details a pipeline that combines LLM-based language understanding (LLaMA 3) with a schema-grounded Neo4j knowledge graph to produce accurate, interpretable, and auditable answers in biomedical data analysis.",single_hop_specifc_query_synthesizer,"The role of the *Journal of Applied Science and Technology Trends* in biomedical data analysis, as reflected in the provided context, is primarily as a platform for publishing research that advances methods and frameworks for accurate, efficient, and interpretable biomedical question answering systems. Specifically, the journal features studies like the one by Omar & Mohammed (2025) that propose hybrid frameworks integrating Large Language Models (LLMs) with biomedical knowledge graphs (KGs) to improve the accuracy, reasoning, and reliability of biomedical data analysis.

Key contributions highlighted in the journal include:

- Presenting novel hybrid approaches that combine LLMs (e.g., LLaMA-3B) with Neo4j-based biomedical knowledge graphs to translate natural language biomedical questions into executable graph queries, enabling precise and evidence-backed answers.
- Demonstrating high performance metrics (e.g., Precision, Recall, F1-score) in biomedical question answering tasks across varying difficulty levels, indicating robustness and practical applicability.
- Addressing challenges in biomedical data analysis such as LLM hallucinations and limited reasoning by leveraging structured biomedical knowledge graphs.
- Emphasizing privacy-preserving, offline operation of biomedical AI systems, which is critical for sensitive healthcare data governance.
- Showcasing the use of graph databases like Neo4j to manage complex biomedical data and support multi-hop reasoning over interconnected biomedical entities (e.g., drugs, diseases, genes).

Overall, the journal serves as a scholarly venue that disseminates cutting-edge research integrating AI, knowledge graphs, and biomedical informatics to enhance data-driven discovery, clinical decision support, and personalized medicine in biomedical data analysis.","['Omar & Mohammed / Journal of Applied Science and Technology Trends Vol. 06, No. 02, pp. 342 –357 (2025) \n \n346 \nscientific literature, clinical records, genomic databases, and \nexperimental findings [19, 31]. \nSuch integration creates a comprehensive biomedical \nknowledge base that supports advanced analytics and discovery. \nFor example, biomedical knowledge graphs can reveal previously \nunknown relationships (e.g., between drugs and diseases) and \nhelp prioritize potential biomarkers for complex conditions. The \nIntegrative Biomedical Knowledge Hub (iBKH) is one such \nlarge-scale graph that consolidates diverse biomedical resources \ninto a unified hub, enabling discovery at scale [11]. Beyond \niBKH, large biomedical knowledge graphs such as SPOKE \nfurther illustrate how graph integration accelerates research and \nsupports precision-medicine use cases [12]. Overall, these graphs \nserve as foundational resources for data-driven and personalized \nmedicine. These knowledge graphs serve as foundational \nresources for precision medicine, where treatment can be tailored \nto the individual’s biological profile, improving outcomes and \nminimizing side effects [19, 31]. \n \nTABLE I.  SUMMARY OF RELATED RESEARCH ON BIOMEDICAL KGS AND QUESTION ANSWERING \nRef. \nYear \nData/Graph \nMethod \nBaselines \nKey Metric \nLimitation \n[11] \n2023 \niBKH (18 biomedical \nsources, Neo4j) \nIntegrative KG + Graph \nlearning; drug repurposing \ncase \nKnown drugs, \nAlzheimer’s \nstudy \nPredictive performance \n(drug repurposing) \nLimited to Alzheimer’s case \nstudy; scalability and updates not \ndetailed \n[19] \n2023 \nDisease KG (9,400 \nnodes, 45,000 relations \nin Neo4j) \nCypher queries for disease–\ndrug–symptom reasoning \nCross-referential \ndisease DB \nNew relation inference; \ncomplex query \nanswering \nLimited to single domain; lacks \nlarge-scale evaluation \n[3] \n2023 \nBKGs vs. GPT-3.5/4 \nComparative QA study: \nLLMs vs. KGs \nGPT-3.5, GPT-4, \nKG reasoning \nAccuracy, Novelty, \nReasoning \nGPT-4 inconsistent in \nreasoning/citations; KG less fluent \nbut more reliable \n[13] \n2024 \nSPOKE KG + \nLLaMA2, GPT-3.5, \nGPT-4 \nKG-optimized retrieval for \nLLMs (RAG) \nOther retrieval \nmethods \nAccuracy, token \nreduction >50% \nFocus on retrieval optimization, \nnot KG construction \n[23] \n2024 \nBenchmark KB datasets \nGenerate-then-retrieve \n(LLM → logical form → \nKB retrieval) \nPrior KBQA \nmethods \nAccuracy (highest to \ndate) \nRisk of error in logical form \ngeneration \n[6] \n2024 \nPrimeKG \nLLM + KG hybrid, Cypher \nquery validation, RAG \nGPT-4 Turbo, \nLLaMA 3 \nAccuracy, \nExplainability \nDependent on KG coverage; \ncomputationally intensive \n[22] \n2025 \nPan-cancer KG (PcQA \ndataset) \nKG-enhanced reasoning \n(subgraph selection) \nSOTA KGQA \nmethods \nOutperformed SOTA on \nPcQA \nLimited to pan-cancer focus; no \nfine-tuning explored \n[26] \n2025 \nDynamic Medical KG + \nNeo4j \nLLM agents + multi-hop \nreasoning \nMEDQA, \nMedMCQA \nbaselines \nAccuracy, \nInterpretability \nHigh system complexity; requires \ncontinuous updating \n[24] \n2025 \nSynthCypher dataset \n(Neo4j + synthetic \nqueries) \nLLM-supervised Cypher \ngeneration and verification \nSPIDER \nbenchmark \nCypher accuracy 40% \nSynthetic dataset may not capture \nall real-world cases \n[29] \n2025 \nUMLS KG + Neo4j \nHybrid GraphRAG \nLLM-only QA \nAccuracy, Citation \nfidelity \nMore complex pipeline; relies on \nexternal vector store \n[30] \n2025 \nClinical data (10k \npatients, 1,894 nodes, \nNeo4j) \nKG construction using \nGPT-4 for entity/relation \nextraction \nOther KG \nconstruction \nmethods \nHigh F1-scores \nFocus only on sepsis; limited \ngeneralization \n[20] \n2025 \nLocal KG from \nbiomedical documents \nMulti-level summarization \nover KG for QA \nRAG baselines \nQA accuracy, reasoning \nTested mainly on document QA; \nscalability not proven \n \nB. Neo4j Graph Database \nTo manage the complexity and large size of biomedical \nknowledge graphs, specialized graph databases are needed. \nNeo4j is one of the most popular graph databases designed to \nstore and query data structured as nodes (entities) and \nrelationships (edges), both of which can have descriptive \nproperties [32, 33]. It uses the property graph model, which \nmakes it easy to represent complex, connected biomedical data \nsuch as drug-gene interactions or disease pathways. Neo4j’s \nCypher query language is especially advantageous because it \nallows users to write expressive and efficient queries to explore \nmulti-step connections in the data [34]. \nNeo4j works well for biomedical data because it can quickly \nrun complicated queries over highly interconnected datasets. This \nis important in biology and medicine, where relationships \nbetween entities like proteins, diseases, and drugs are often \ncomplex and layered. Studies have shown that Neo4j handles \nlarge biomedical graphs efficiently, making it a favorite among \nresearchers and industry users alike [33, 35, 36]. Its indexing and \ncaching mechanisms also help speed up query processing and \ndata retrieval [37]. \nMoreover, \nNeo4j \nintegrates \nsmoothly \nwith \nmany \nprogramming languages and analytics tools, which makes it \neasier to build interactive biomedical applications and clinical \ndecision support systems that can turn complex graph data into \nuseful insights [38, 39]. \nC. Large Language Models (LLMs) in Biomedical Question \nAnswering \nLarge Language Models (LLMs) are powerful AI systems \ntrained on vast amounts of text data. They learn the structure and \npatterns of language, enabling them to understand questions, \ngenerate responses, summarize information, and perform other \ncomplex language tasks. Well-known models such as LLaMA \nand GPT-3 have greatly advanced the field of natural language \nprocessing by showing strong performance across many tasks \n[40, 41].'
 'Vol. 06, No. 02, pp. 342 –357 (2025) \nISSN: 2708-0757 \n \nJOURNAL OF APPLIED SCIENCE AND TECHNOLOGY TRENDS \n \nwww.jastt.org  \n \n                                                          342 \ndoi: 10.38094/jastt62404  \nA Hybrid LLM–Knowledge Graph Framework for Accurate \nBiomedical Question Answering \n \nHavraz Y. Omar1,²*, Abdulhakeem O. Mohammed³  \n \n¹Department of Information Technology, Technical College of Duhok, Duhok Polytechnic University, Duhok, Kurdistan Region, \nIraq. havraz.omar@dpu.edu.krd  \n² Department of Information Technology, Technical College of Informatics – Akre, Akre University for Applied Sciences, Akre, \nKurdistan Region, Iraq. \n³ Department of Computer Science, College of Science, University of Zakho, Zakho, Kurdistan Region, Iraq. \na.mohammed@uoz.edu.krd \n \n*Correspondence: havraz.omar@dpu.edu.krd \n \nAbstract \nBiomedical question answering requires accurate and interpretable systems; however, existing approaches often face challenges such as \nlanguage model hallucinations and limited reasoning when relying solely on standalone knowledge graphs. To address these limitations, \nthis study proposes a hybrid framework that integrates the LLaMA-3B language model with a Neo4j-based drug–disease–symptom \nknowledge graph. The system translates natural language questions into executable Cypher queries, operates on an iBKH-derived graph \ncomprising over 65,000 entities and 3 million relationships, and returns answers with supporting evidence through a transparent interface. \nExperiments conducted on 60 biomedical questions across three levels of difficulty demonstrate the robustness of the approach: 96% \nexact match for simple queries, 95% for medium queries, and 86.7% for complex queries. Overall, the system achieves Precision@5 of \n96.1%, Recall@5 of 89.0%, F1@5 of 91.0%, Hits@k of 96.1%, and an MRR of 94.4%, while maintaining an average response time of \nonly 6.07 seconds. These results indicate that the system retrieves nearly all relevant answers, ranks them correctly, and delivers them \nwith latency low enough for interactive use. Moreover, unlike cloud-based APIs such as ChatGPT, which require internet connectivity \nand external data transmission, the proposed framework operates fully offline, ensuring privacy, reproducibility, and compliance with \nbiomedical data governance. Overall, this pipeline provides an accurate, efficient, and privacy-preserving solution for biomedical question \nanswering, making it a practical alternative to cloud-dependent approaches in sensitive healthcare contexts. \n \nKeywords: Knowledge Graph, LLM, Question Answering, Neo4j, Biomedical Informatics, Healthcare AI, LLaMA 3. \n \nReceived: August 14th, 2025 / Revised: October 10th, 2025 / Accepted: October 16th, 2025 / Online: October 20th, 2025 \n \nI. INTRODUCTION \nAnswering questions in the biomedical field is a difficult task \ndue to the complexity of medical knowledge and the need for \nprecision. In recent years, large language models  (LLMs) like \nLLaMA, GPT-4 have made progress in understanding and \ngenerating human-like responses to medical questions  [1,  2]. \nThese models can process large amounts of information and \nrespond in natural language, which makes them helpful in \nhealthcare settings [3]. However, they often struggle to provide \naccurate answers when dealing with specialized biomedical \ncontent [4, 5]. \nOne major issue with LLMs is a problem called hallucination, \nwhere the model generates information that sounds right but is \nactually incorrect or unsupported [6]. In medical applications, \nthis can be dangerous, as healthcare professionals rely on precise \nand trustworthy information  [7]. Therefore, researchers are \nexploring ways to combine LLMs with structured sources of \nknowledge to improve their reliability [8]. \nLLM-only systems in biomedicine still hallucinate and are \nhard to verify, limiting safe use [9, 10]. Biomedical knowledge \ngraphs (BKGs) such as iBKH and SPOKE curate multi-source \nfacts and enable multi-hop reasoning, yet they neither interpret \nfree text nor generate answers [11, 12]. Recent hybrids (KG-\naware RAG) improve grounding but often lack explicit path-level \njustifications and robust end-to-end answer evaluation [13, 14]. \nRecent studies have increasingly integrated Knowledge \nGraphs (KGs) with Large Language Models (LLMs) to improve \nfactual accuracy, reasoning, and reduce hallucinations. Notable \nexamples include DR.KNOWS, which combines UMLS-based \nKGs with LLMs for better diagnostic reasoning [15], KnowNet'
 'Joy et. al \n18 \n \n26. \nEvangelista, J. E. et al. Toxicology knowledge graph for structural birth defects. \nCommunications Medicine 2023 3:1 3, 1–14 (2023). \n27. \nCallaghan, J. et al. BioThings Explorer: a query engine for a federated knowledge \ngraph of biomedical APIs. Bioinformatics 39, (2023). \n28. \nCarbon, S. et al. The Gene Ontology resource: enriching a GOld mine. Nucleic \nAcids Res 49, D325–D334 (2021). \n29. \nKnox, C. et al. DrugBank 6.0: the DrugBank Knowledgebase for 2024. Nucleic \nAcids Res 52, D1265–D1275 (2024). \n30. \nFecho, K. et al. Progress toward a universal biomedical data translator. Clin Transl \nSci 15, 1838–1847 (2022). \n31. \nGonzalez-Cavazos, A. C. et al. DrugMechDB: A Curated Database of Drug \nMechanisms. Sci Data 10, 1–7 (2023). \n32. \nOpenAI. GPT-4o System Card. (2024). \n33. \nDeka, P., Jurek-Loughrey, A. & Padmanabhan, D. Improved methods to aid \nunsupervised evidence-based fact checking for online health news. Journal of \nData Intelligence 3, 474–505 (2022). \n34. \nDeka, P., Jurek-Loughrey, A. & P, D. Evidence extraction to validate medical \nclaims in fake news detection. SpringerP Deka, A Jurek-Loughrey, DPInternational \nconference on health information science, 2022•Springer 13705 LNCS, 3–15 \n(2022). \n35. \nUnni, D. R. et al. Biolink Model: A universal schema for knowledge graphs in \nclinical, biomedical, and translational science. Clin Transl Sci 15, 1848 (2022). \n36. \nWu, C., MacLeod, I. & Su, A. I. BioGPS and MyGene.info: organizing online, \ngene-centric information. Nucleic Acids Res 41, D561–D565 (2013). \n37. \nOpenAI et al. GPT-4 Technical Report. (2023). \n38. \nChen, Y. et al. Iterative Prompt Refinement for Mining Gene Relationships from \nChatGPT. bioRxiv 2023.12.23.573201 (2023) doi:10.1101/2023.12.23.573201. \n39. \nMarvin, G., Hellen, N., Jjingo, D. & Nakatumba-Nabende, J. Prompt Engineering \nin Large Language Models. 387–402 (2024) doi:10.1007/978-981-99-7962-2_30. \n40. \nSahoo, P. et al. A Systematic Survey of Prompt Engineering in Large Language \nModels: Techniques and Applications. (2024). \n41. \nLi, M., Kilicoglu, H., Xu, H. & Zhang, R. BiomedRAG: A retrieval augmented large \nlanguage model for biomedicine. J Biomed Inform 162, 104769 (2025). \n.\nCC-BY 4.0 International license\navailable under a\n(which was not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made \nThe copyright holder for this preprint\nthis version posted August 2, 2025. \n; \nhttps://doi.org/10.1101/2025.08.01.668022\ndoi: \nbioRxiv preprint']"
How is the UMLS term utilized in the iBKH dataset for biomedical entities?,"['B. Dataset and Knowledge Graph Construction 1) Dataset The integrated Biomedical Knowledge Hub (iBKH), a large biomedical knowledge base, forms the first level of the system and integrates information from various curated high-quality biomedical databases. This implies that the data set includes various types of entities, such as diseases, symptoms, drugs, biological pathways, etc. This study used the representative subset of the iBKH dataset, which contained 65828 biomedical entities. These entities are semantically interconnected through a total of 3004166 relationships, thus creating a rich knowledge graph. The iBKH dataset was originally introduced in [11], and it is freely available at (https://github.com/wcm-wanglab/iBKH). This dataset is the core semantic foundation upon which this study is built. The knowledge graph is populated from multiple tabular sources (CSV files), each listing entities or relationships. The main input files and their contents are as follows: • Disease vocabulary(disease_vocab.csv): Contains columns such as primary (a unique disease ID), name, do_id (Disease Ontology ID), kegg_id, and umls_cui (UMLS Concept ID). Each row represents a disease node with external identifiers. • Drug vocabulary (drug_vocab.csv): Includes primary (unique drug ID), name, drugbank_id, kegg_id, pharmgkb_id, umls_cui, mesh_id, iDISK_id and CID (PubChem ID). Each row defines a drug node with standard database identifiers. • Symptom vocabulary (symptom_vocab.csv): Contains primary (unique symptom ID), name, mesh_id, umls_cui and iDISK_id. Each row defines a symptom node. • Side effect vocabulary (side_effect_vocab.csv): Includes primary (unique side-effect ID) and name. Each row represents a side-effect node (with UMLS ID when available). • Pathway vocabulary (pathway_vocab.csv): Contains primary (unique pathway ID), name, reactome_id, go_id, and kegg_id. Each row defines a biological pathway node. Relationship files (each row typically contains two entity IDs and one or more boolean flags or codes) include: • Disease–Symptom links (Di_Sy_res.csv): Rows include Disease and Symptom IDs, a presence flag (1 or 0) and a data source. If Present = 1, a HAS_SYMPTOM edge is created from the disease to the symptom, with properties for presence and source. • Disease–Disease links (di_di_res.csv): Rows include Disease_1 and Disease_2 IDs with binary flags for is_a and Resemble. If is_a = 1, an (IS_A) edge is created (Disease_1 → Disease_2); if Resemble = 1, a RESEMBLES edge is created. The source field is used for provenance. • Drug–Disease links (D_Di_res.csv): Includes Drug and Disease IDs with several binary flags. If a flag equals 1, a corresponding edge is created: o TREATS (Treats = 1) o PALLIATES (Palliates = 1) o ASSOCIATED_WITH (Associate = 1) o ALLEVIATES_REDUCES (alleviates = 1) o TREATMENT_THERAPY (treatment/therapy = 1) o INHIBITS_CELL_GROWTH (inhibits cell growth = 1) o HAS_BIOMARKER (biomarkers = 1) o PREVENTS_SUPPRESSES (prevents/suppresses = 1) o ROLE_IN_PATHOGENESIS (role in disease pathogenesis = 1) • Drug–SideEffect links (D_SE_res.csv): Contains Drug and SideEffect IDs with a Source column. Each row creates a CAUSES edge from the drug to the side effect, with source as an edge property. • Drug–Drug interactions (D_D_res.csv): Rows include Drug_1 and Drug_2 IDs with flags for Interaction and Resemble. If Interaction = 1, an INTERACTS_WITH edge is created (bidirectional). If Resemble = 1, a RESEMBLES edge is added. • Drug–Pathway links (D_Pwy_res.csv): Includes Drug ID and Pathway ID. Each row generates an ASSOCIATED_WITH edge from the drug to the pathway. • Disease–Pathway links (Di_Pwy_res.csv): Contains Disease ID and Pathway ID. Each row creates an ASSOCIATED_WITH edge from the disease to the pathway. 2) Data Upload Performance The time required to upload different types of entities and relationships into the Neo4j biomedical knowledge graph, measured in seconds. These measurements reflect both the size and complexity of the data being processed. As shown in Table II, the longest upload time is for Drug- Drug Relationships, which takes approximately 190 seconds due to the large number of edges (over 3 million). Following this, Disease-Disease and Drug-Disease Relationships also require considerable time for loading. On the other hand, individual']","In the iBKH dataset, the UMLS Concept Unique Identifier (UMLS CUI) is used as an external identifier for multiple biomedical entities. Specifically, it appears as a column in the disease vocabulary, drug vocabulary, and symptom vocabulary files, linking each entity to its corresponding UMLS concept.",single_hop_specifc_query_synthesizer,"The UMLS term (specifically the UMLS Concept Unique Identifier, or UMLS CUI) is utilized in the iBKH dataset as a standardized biomedical identifier to represent and normalize biomedical entities such as diseases, drugs, and symptoms. In the iBKH knowledge graph construction, the UMLS CUI is included as one of the key properties for nodes representing these entities. For example:

- In the disease vocabulary file (`disease_vocab.csv`), each disease node includes a `umls_cui` property alongside other identifiers like Disease Ontology ID and KEGG ID.
- In the drug vocabulary file (`drug_vocab.csv`), each drug node includes a `umls_cui` property along with DrugBank ID, KEGG ID, PharmGKB ID, etc.
- In the symptom vocabulary file (`symptom_vocab.csv`), symptom nodes also contain a `umls_cui` property.

These UMLS CUIs serve as external identifiers that help in concept-level normalization and semantic interoperability across different biomedical databases integrated into the iBKH. The use of UMLS CUIs supports synonym expansion and helps mitigate issues related to name-based matching (such as synonyms, abbreviations, and homonyms), which is important for improving the accuracy and robustness of query results in the biomedical knowledge graph.

Thus, the UMLS term in iBKH is used as a standardized identifier property for biomedical entity nodes, facilitating entity normalization and integration from multiple data sources within the knowledge graph.","['Omar & Mohammed / Journal of Applied Science and Technology Trends Vol. 06, No. 02, pp. 342 –357 (2025) \n \n348 \ndrugs, diseases, symptoms) and execution returns structured data \n(tuples) relevant to the question. \nStep 5. Answer Synthesis: The structured tuples flow to \nAnswer Synthesis, which aggregates and formats them into a \nconcise raw answer. This raw answer is sent back to LLaMA 3 \nto optionally refine the phrasing while preserving the retrieved \nfacts. \nStep 6. Result Presentation: LLaMA 3 produces the final \nanswer, which the interface displays together with the executed \nCypher query and an optional preview of the returned rows, \nimproving transparency and trust. \nThe pipeline couples LLM-based language understanding \n(LLaMA 3) with a schema-grounded Neo4j knowledge graph. \nThe Cypher Query Gen refines the query formulation, Query \nExecution retrieves evidence and Answer Synthesis converts \nstructured results into readable outputs that produce answers that \nare accurate, interpretable, and easy to audit directly from the \ndisplayed query and evidence. \nB. Dataset and Knowledge Graph Construction \n1) \nDataset \nThe integrated Biomedical Knowledge Hub (iBKH), a large \nbiomedical knowledge base, forms the first level of the system \nand integrates information from various curated high-quality \nbiomedical databases. This implies that the data set includes \nvarious types of entities, such as diseases, symptoms, drugs, \nbiological pathways, etc. This study used the representative \nsubset of the iBKH dataset, which contained 65828 biomedical \nentities. These entities are semantically interconnected through a \ntotal of 3004166 relationships, thus creating a rich knowledge \ngraph. The iBKH dataset was originally introduced in [11], and it \nis freely available at (https://github.com/wcm-wanglab/iBKH). \nThis dataset is the core semantic foundation upon which this \nstudy is built. The knowledge graph is populated from multiple \ntabular sources (CSV files), each listing entities or relationships. \nThe main input files and their contents are as follows: \n• Disease \nvocabulary(disease_vocab.csv): \nContains \ncolumns such as primary (a unique disease ID), name, \ndo_id (Disease Ontology ID), kegg_id, and umls_cui \n(UMLS Concept ID). Each row represents a disease node \nwith external identifiers. \n• Drug vocabulary (drug_vocab.csv): Includes primary \n(unique drug ID), name, drugbank_id, kegg_id, \npharmgkb_id, umls_cui, mesh_id, iDISK_id and CID \n(PubChem ID). Each row defines a drug node with \nstandard database identifiers. \n• Symptom vocabulary (symptom_vocab.csv): Contains \nprimary (unique symptom ID), name, mesh_id, umls_cui \nand iDISK_id. Each row defines a symptom node. \n• Side \neffect \nvocabulary \n(side_effect_vocab.csv): \nIncludes primary (unique side-effect ID) and name. Each \nrow represents a side-effect node (with UMLS ID when \navailable). \n• Pathway vocabulary (pathway_vocab.csv): Contains \nprimary (unique pathway ID), name, reactome_id, go_id, \nand kegg_id. Each row defines a biological pathway \nnode. \nRelationship files (each row typically contains two entity IDs \nand one or more boolean flags or codes) include: \n• Disease–Symptom links (Di_Sy_res.csv): Rows include \nDisease and Symptom IDs, a presence flag (1 or 0) and a \ndata source. If Present = 1, a HAS_SYMPTOM edge is \ncreated from the disease to the symptom, with properties \nfor presence and source. \n• Disease–Disease links (di_di_res.csv): Rows include \nDisease_1 and Disease_2 IDs with binary flags for is_a \nand Resemble. If is_a = 1, an (IS_A) edge is created \n(Disease_1 → Disease_2); if Resemble = 1, a \nRESEMBLES edge is created. The source field is used \nfor provenance. \n• Drug–Disease links (D_Di_res.csv): Includes Drug and \nDisease IDs with several binary flags. If a flag equals 1, \na corresponding edge is created: \no \nTREATS (Treats = 1) \no \nPALLIATES (Palliates = 1) \no \nASSOCIATED_WITH (Associate = 1) \no \nALLEVIATES_REDUCES (alleviates = 1) \no \nTREATMENT_THERAPY (treatment/therapy = \n1) \no \nINHIBITS_CELL_GROWTH \n(inhibits \ncell \ngrowth = 1) \no \nHAS_BIOMARKER (biomarkers = 1) \no \nPREVENTS_SUPPRESSES \n(prevents/suppresses = 1) \no \nROLE_IN_PATHOGENESIS (role in disease \npathogenesis = 1) \n• Drug–SideEffect links (D_SE_res.csv): Contains Drug \nand SideEffect IDs with a Source column. Each row \ncreates a CAUSES edge from the drug to the side effect, \nwith source as an edge property. \n• Drug–Drug interactions (D_D_res.csv): Rows include \nDrug_1 and Drug_2 IDs with flags for Interaction and \nResemble. If Interaction = 1, an INTERACTS_WITH \nedge is created (bidirectional). If Resemble = 1, a \nRESEMBLES edge is added. \n• Drug–Pathway links (D_Pwy_res.csv): Includes Drug \nID and Pathway ID. Each row generates an \nASSOCIATED_WITH edge from the drug to the \npathway. \n• Disease–Pathway links (Di_Pwy_res.csv): Contains \nDisease ID and Pathway ID. Each row creates an \nASSOCIATED_WITH edge from the disease to the \npathway. \n2) \nData Upload Performance \nThe time required to upload different types of entities and \nrelationships into the Neo4j biomedical knowledge graph, \nmeasured in seconds. These measurements reflect both the size \nand complexity of the data being processed. \nAs shown in Table II, the longest upload time is for Drug-\nDrug Relationships, which takes approximately 190 seconds due \nto the large number of edges (over 3 million). Following this, \nDisease-Disease and Drug-Disease Relationships also require \nconsiderable time for loading. On the other hand, individual'
 'Omar & Mohammed / Journal of Applied Science and Technology Trends Vol. 06, No. 02, pp. 342 –357 (2025) \n \n350 \ndocumentation, indexes “enable quicker and more efficient \npattern matching” by allowing the query planner to rapidly locate \nnodes by label and property. \nWith the schema in place, data was imported using Cypher’s \nLOAD CSV commands. For each vocabulary file, rows with \nnonnull primary IDs were processed: the code used MERGE to \ncreate (or match existing) nodes with the given label and primary \nproperty, and then SET the remaining attributes from the CSV \ncolumns. For example, in importing disease_vocab.csv, each row \nproduced a node (:Disease primary: <id>) with properties name, \ndo_id, kegg_id, and umls_cui set from the row (empty strings \nwere turned into null). Similarly, drug_vocab.csv rows produced \ndrug nodes with properties for DrugBank, KEGG, PharmGKB \nIDs, etc. This approach follows best practice: using MERGE on \nthe unique ID ensures that nodes are not duplicated during \nmultiple passes. \nAfter all nodes were created, the relationship CSVs were \nloaded. Each row in those files was matched to the corresponding \nsource and target nodes by their primary IDs, and conditional \nlogic was used to create edges. For example, the disease-\nsymptom file (Di_Sy_res.csv) was processed by matching a \ndisease node and a symptom node for each row, then executing \nMERGE (d)-[r:HAS_SYMPTOM]-(s) if the present column is \nnonzero; the edge was given a present property and a source \nproperty from the CSV. The disease-disease file (di_di_res.csv) \nwas processed by matching disease nodes d1 and d2:If is_a = 1, \na (:Disease)-[:IS_A]->(:Disease) edge was merged; if Resemble \n= 1, a (:Disease)-[:RESEMBLES]->(:Disease) edge was merged. \nDrug-disease relationships were handled similarly: the script \nmatched a Drug node and a Disease node for each row of \nD_Di_res.csv, then for each flag that was 1, it merged the \ncorresponding edge label (such as TREATS, PALLIATES, \nASSOCIATED_WITH, etc.) from the drug to the disease. Drug–\nside-effect rows produced (:Drug)-[:CAUSES]->(:SideEffect) \nedges with the source noted, and drug–drug rows produced either \nINTERACTS_WITH or RESEMBLES edges between matched \ndrug pairs. Finally, the drug-pathway and disease-pathway files \neach produced:ASSOCIATED_WITH edges linking drugs or \ndiseases to pathway nodes. \nIn this graph model, most relationships are directional (for \nexample, a drug TREATS a disease; a disease does not TREAT \na drug). This follows common practice in biomedical KGs. The \nsame relations (such as RESEMBLES or INTERACTS_WITH) \nare inherently symmetric, but were stored as two directed edges \nor one undirected edge depending on implementation. All \nrelationship creation steps used Cypher’s MERGE so that \nrepeated loads or out-of-order imports did not create duplicate \nedges. \nThis study used a static KG snapshot for reproducibility, but \nthe system also supports incremental updates through the Neo4j \nMERGE and batch import functions. A summary of the different \ntypes of nodes and relationships is provided, together with their \ncounts, descriptions, and examples in Table III.\n \nTABLE III.  DETAILED SUMMARY OF KNOWLEDGE GRAPH COMPONENTS IN IBKH SUBSET \nComponent Type \nEntity/Relationship \nCount \nDescription \nExamples \n \nDisease \n19,236 \nMedical conditions and disorders \nDiabetes, Hypertension, Cancer \n \nDrug \n37,997 \nPharmaceutical compounds \nAspirin, Insulin, Amoxicillin \nNodes \nSymptom \n1,361 \nClinical signs and symptoms \nHeadache, Fever, Nausea \n \nSideEffect \n4,251 \nNegative outcomes of drugs \nNausea, Drowsiness \n \nPathway \n2,983 \nBiological processes and pathways \nApoptosis, Glycolysis \n \nASSOCIATED_WITH \n101,534 \nGeneral associations (disease–\npathway, drug–pathway, etc.) \nDiabetes ASSOCIATED_WITH Pathway \n \nCAUSES \n145,321 \nDrug-SideEffect relationships \nAspirin CAUSES Gastric_Bleeding \n \nHAS_SYMPTOM \n3,357 \nDisease-symptom links \nCOVID-19 HAS_SYMPTOM Fever \n \nHAS_BIOMARKER \n412 \nBiomarkers linked to disease \nPSA HAS_BIOMARKER Prostate_Cancer \nRelationships \nINHIBITS_CELL_GROWTH \n1,508 \nDrugs inhibiting cell growth \nChemo INHIBITS_CELL_GROWTH Tumor \n \nINTERACTS_WITH \n2,682,142 \nDrug-drug interactions \nAspirin INTERACTS_WITH Warfarin \n \nIS_A \n10,529 \nSubtype hierarchy \nFlu IS_A Viral_Infection \n \nPALLIATES \n388 \nDrug palliates disease \nMorphine PALLIATES Cancer \n \nPREVENTS_SUPPRESSES \n859 \nPreventive links \nVaccine PREVENTS_SUPPRESSES Measles \n \nRESEMBLES \n7,000 \nSimilarity relationships \nDrugA RESEMBLES DrugB \n \nTREATMENT_THERAPY \n44,852 \nTherapy relationships \nRadiotherapy TREATMENT_THERAPY Tumor \n \nTREATS \n5,491 \nDrug-disease links \nInsulin TREATS Diabetes \n \nALLEVIATES_REDUCES \n∼180,000 \nSymptom relief \nParacetamol ALLEVIATES_REDUCES Fever \nTotal \nNodes \n65,828 \nTotal biomedical entities \n— \n \nRelationships \n3,004,166 \nTotal knowledge graph links \n— \nC. Natural Language to Cypher Query Translation \nA key feature of the system is its ability to accept questions \nwritten in plain English and automatically generate the \ncorresponding Cypher queries. This is accomplished using \nMeta’s LLaMA 3 large language model, which runs entirely on \na local machine through the open-source llama.cpp framework. \nRunning the model locally ensures low-latency execution and \nkeeps sensitive queries within the user’s environment. \nTo generate a Cypher query, LLaMA 3 is prompted with \nexamples of natural language questions along with their correct \nCypher translations. The prompt also includes instructions on \nhow to navigate the structure of the graph schema. When a user \nenters a question (e.g., ’What are the symptoms of Alzheimer’s \ndisease?’), the system inserts it into the prompt and asks LLaMA \n3 to produce a corresponding query. For example, the model may \nreturn: \nMATCH (d:Disease)-[:HAS_SYMPTOM]->(s:Symptom) \nWHERE toLower(d.name) CONTAINS ""alzheimer"" \nRETURN s.name'
 'Omar & Mohammed / Journal of Applied Science and Technology Trends Vol. 06, No. 02, pp. 342 –357 (2025) \n \n356 \n \nFig. 7. Side effects of drugs that treat epilepsy \n \nExecuting this query returns drugs associated with epilepsy \nand their corresponding side effects. For example, the query \nidentifies Pregabalin as a treatment for epilepsy and retrieves \nmultiple side effects such as anxiety, arrhythmia, gastrointestinal \npain, and others. The visualization of the graph in Fig. 7, \nhighlights this pattern, where the drug node is linked to epilepsy \nvia TREATS and to several side effect nodes via CAUSES, \nproviding an interpretable biomedical knowledge structure. \nVI. DISCUSSION \nUnder \na \nunified \nevaluation \nprotocol \nreporting \nP@k/R@k/F1@k, Hits@k, and MRR at 𝑘∈{1,5,10} , the \npipeline exhibits consistent end-to-end behavior across the three \ndifficulty tiers. The 𝑘-ablation shows strong early precision (high \nP@1 and elevated MRR), while recall increases with larger 𝑘, \nindicating that correct answers typically surface near the top yet \npersist deeper in the ranked slate. At the query-generation level, \nexact-match (EM) is high for simple questions (e.g., ∼96%) and \nlower for complex, multi-hop questions (e.g., ∼86.7%), which \naligns with increased compositionality. End-to-end latency \n(mean total ≈6.07\u2006s from question to final answer) remains within \ninteractive bounds on our local setup. Practically, a fully offline \ndeployment avoids internet connectivity, API keys, and external \ndata \ntransfer, \nstrengthening \nprivacy, \ncompliance, \nand \nreproducibility in biomedical contexts. \nSeveral challenges qualify these findings. First, the \ncomparability challenge: to our knowledge, no prior study \nevaluates iBKH using the same metric suite, making cross-paper \nnumeric comparisons not “apples to apples.” We therefore \ninterpret the results as controlled evidence about this pipeline \nunder a single, consistent protocol rather than as a cross-study \nleaderboard. Beyond comparability, performance may vary with \nother biomedical knowledge graphs; the current iBKH snapshot \nis static, limiting real-time updates; and scaling to larger or \ndynamically refreshed graphs can introduce latency and \nconsistency trade-offs. \nError analysis shows that residual failures concentrate in \ncomplex, multi-hop queries where missing relation hops or brittle \nname-based matching (synonyms, abbreviations, homonyms) \nlead to partial answers. These observations motivate concept-\nlevel normalization via biomedical identifiers (e.g., UMLS, \nSNOMED, RxNorm) with synonym expansion, as well as \nschema-constrained query generation and path-guided decoding \nto better satisfy multi-hop constraints. Finally, correctness is \nassessed primarily at the answer level (Hits@k, MRR, \nprecision/recall) and does not yet include full semantic-\nequivalence checks across alternative Cypher queries, which may \noverlook cases where different queries yield the same correct \nresults. Complementing template EM with result-set equivalence \nchecks, expanding the metric suite (e.g., nDCG/MAP) with per-\nquestion 95% bootstrap confidence intervals, and supporting \nincremental graph updates with distributed storage are promising \nsteps to enhance robustness, scalability, and external validity. \nVII. CONCLUSION \nThis study introduced a hybrid biomedical question \nanswering framework that couples the LLaMA-3B language \nmodel with a Neo4j-based iBKH knowledge graph to enable the \nautomatic generation of executable Cypher queries and to deliver \ntransparent, evidence-grounded answers through justification \nsubgraphs. Evaluation in simple, medium, and complex queries \ndemonstrated consistently high performance, with strong \nprecision, recall, F1, Hits@k, and MRR values, while \nmaintaining low latency suitable for interactive biomedical \napplications. Beyond quantitative performance, the proposed \nsystem provides a reproducible and privacy-preserving solution \nby operating fully offline, a property of particular importance in \nsensitive clinical and research contexts. However, certain \nlimitations remain. The reliance on a static iBKH snapshot \nconstrains coverage and adaptability, recall is lower for complex \nmulti-hop reasoning, and the absence of canonical entity \nnormalization (e.g., UMLS, SNOMED, RxNorm) may reduce \nsemantic robustness. Future research will therefore focus on \nintegrating standardized biomedical entity normalization, \nenabling dynamic and incremental knowledge graph updates, and \nleveraging domain-adapted or fine-tuned biomedical LLMs. \nThese directions are expected to further strengthen the \nrobustness, scalability, and applicability of the framework in real-\nworld biomedical and clinical environments. \n \nREFERENCES \n[1] S. Bubeck, V. Chandrasekaran, R. Eldan, J. Gehrke, E. Horvitz, E. Kamar, \nP. Lee, Y. T. Lee, Y. Li, S. Lundberg, et al., “Sparks of artificial general \nintelligence: \nEarly experiments with \ngpt-4,” arXiv preprint \narXiv:2303.12712, 2023. \n[2] J. Achiam, S. Adler, S. Agarwal, L. Ahmad, I. Akkaya, F. L. Aleman, D. \nAlmeida, J. Altenschmidt, S. Altman, S. Anadkat, et al., “Gpt-4 \ntechnical report,” arXiv preprint arXiv:2303.08774, 2023. \n[3] Y. Hou, J. Yeung, H. Xu, C. Su, F. Wang, and R. Zhang, “From answers to \ninsights: unveiling the strengths and limitations of chatgpt and \nbiomedical knowledge graphs,” Research Square, pp. rs–3, 2023.']"
How does the KGT framework leverage biomedical knowledge graphs to enhance large language models' reasoning capabilities and improve biomedical question answering performance?,"['<1-hop>\n\n8 | GigaScience , 2025, Vol. 14 Table 5. Ablation study of the KGT fr ame work under Code-Llama-13B ROUGE (%) Method GPT-4 Eval (%) BERTScore (%) Recall Precision F1 score KGT (ours) 92.4 97.7 87.4 87.7 86.8 Without GSBI 71.8 95.5 68.1 69.8 66.8 Without QA&GSBI 69.7 94.7 55.0 66.3 58.2 Without QA&GSBI&SC 24.7 77.4 14.8 12.3 12.2 compared to the complete KGT, indicating that each of the 3 mod- ules is vital for the final performance . Furthermore , such obser- vations confirm that our KGT can indeed le v er a ge knowledge to enhance the final performance of LLMs. Implementation settings Our knowledge gr a ph is quite large, with a complex schema, and typicall y involv es input tokens within 1,300. Our experiment does not r equir e fine-tuning, and the infer ence time is r elated to the model size and computational resources. For example, when us- ing our method, KGT, with the Code-Llama-13B model on an 80 GB A100 GPU, it occupies 33 GB of VRAM. Without any acceleration fr ame works, the infer ence r equir es 4 passes, eac h taking ar ound 20 seconds. Case studies Drug repositioning Drug r epositioning emer ges as a pr omising str ategy to acceler ate the process of drug development. This approach involves identi- fying new therapeutic uses for existing drugs, thereby saving time and r esources typicall y r equir ed for bringing a ne w drug to market [ 45 ]. Our system is capable of investigating the potential reposi- tioning of carteolol for the treatment of hemangiomas . T he exam- ple is shown in Supplementary Table S2 and r elational dia gr am is shown in Fig. 4 A. Utilizing the system’s knowledge gr a ph, a r ela- tional chain is delineated, illustrating that propranolol, another inhibitor of ADRB1, is effectiv el y emplo y ed in the treatment of hemangiomas . T he system harnesses this insight to formulate a hypothesis that carteolol, by virtue of its similar mechanism of inhibition, could be potentially repositioning for treating heman- giomas [ 46 ]. This hypothesis would serve as a precursor to clin- ical trials and r esearc h, potentiall y expediting the availability of an additional ther a peutic option for patients with hemangiomas. Drug resistance resear c h Drug resistance in cancer treatment poses a significant challenge in clinical oncology. Understanding the genetic basis of resistance can lead to mor e effectiv e tr eatment str ategies and personalized medicine a ppr oac hes. Researc h in drug r esistance involv es deter- mining why certain cancer-carrying mutated genes are not re- sponsive to specific drugs and finding ways to overcome this re- sistance [ 47 ]. Our system is capable of exploring drug resistance in cancer. The example is shown in Supplementary Table S3 , and a relational diagram is shown in Fig. 4 B. The KG data indicate that the ALK-p.L1196M m utation, whic h is associated with gas- tric cancer, has a known resistance to nalatinib [ 48 , 49 ]. The LLM processes this information and infers that due to this resistance, nalatinib might not be an effective medication for treating can- cers caused by the ALK-p.L1196M mutation. The case highlights the critical importance of understanding specific gene–drug in- teractions in drug resistance research. It demonstrates how cer- tain gene mutations could render a drug ineffective, which in turn could guide oncologists in choosing alternative treatments or de- v eloping ne w drugs that can bypass or tar get the r esistance mec h- anisms. By accelerating the process of understanding drug resis- tance, these artificial intelligence–driven systems can contribute to impr ov ed patient outcomes and the optimization of cancer tr eatment pr otocols.'
 '<2-hop>\n\nIndividualized treatment Details on individualized tr eatment ar e pr ovided in Supplementary Case Studies A . It is important to note that this example is included solely to illustrate the technical capabilities of the proposed method. The output generated in this example has not been validated for clinical use, and further validation in clinical settings would be r equir ed befor e an y suc h a pplication. Selection and understanding of biomarkers Details on selection and understanding of biomarkers are pro- vided in Supplementary Case Studies B . Discussion In this article, we introduce a novel framework KGT, which em- ploys LLMs for reasoning on the schema of KGs, to enhance the reasoning abilities of LLMs in areas with missing domain data by utilizing domain-specific knowledge gr a phs, suc h as oncology knowledge gr a phs, ther eby addr essing the issue of factual hal- lucinations in LLMs. Our method excels in extr acting, v alidat- ing, and refining factual knowledge throughout the LLMs’ rea- soning process. It seamlessly integrates with various LLMs, in- cluding open-source models like Code-Llama, and enhances the capabilities of LLMs solely through prompt engineering and in- context learning without any fine-tuning. This grants it significant generalizability. We possess an extensive oncology knowledge graph and have established a benchmark based on it to e v aluate the ca pabilities of various methods. When tested on PcQA using various open-source LLMs, the KGT fr ame work performs exceptionall y well, sur passing the current best methods by 33%. This significant improvement positions our a ppr oac h as a pioneering benchmark in biomedical KGQA, setting a new standard that advances beyond previously established best pr actices. Additionall y, thr ough case studies, our a ppr oac h has been shown to effectiv el y pr ovide ther a peutic plans, gener ate v aluable hypotheses for drug r epositioning, identify po- tential drug targets, and study drug resistance . T his underscores the pr actical v alue of the KGT fr ame work in deliv ering insight- ful contributions that aid in the de v elopment and optimization of treatment strategies. Each case study’s conclusions are further v alidated by e vidence fr om pr e viousl y published r esearc h pa pers, enhancing the credibility and impact of our findings. Ho w e v er, it is important to note that the constructed QA dataset and the corresponding published subset of the SOKG Downloaded from https://academic.oup.com/gigascience/article/doi/10.1093/gigascience/giae082/7943459 by guest on 11 January 2026'
 '<3-hop>\n\nOmar & Mohammed / Journal of Applied Science and Technology Trends Vol. 06, No. 02, pp. 342 –357 (2025) 345 record-high accuracy, while open-source models achieved impressive gains through prompt optimization. Feng et al.[22] developed the Knowledge Graph-based Thought (KGT) framework that integrated LLMs with a pan- cancer knowledge graph for biomedical question answering. KGT was designed to reason on the knowledge graph schema and identify optimal subgraphs to use for directing accurate answer generation, all without fine-tuning the LLMs. The framework is benchmarked against a new dataset (PcQA) designed specifically for pan-cancer KGQA tasks and has outperformed all existing state-of-the-art approaches by a rather large margin. KGT’s practicality in biomedical issues was highlighted through case studies for drug repositioning, drug resistance, and biomarker discovery. Their approach exhibited robust adaptability among various LLMs. Rezaei et al.[26] developed AMG-RAG, a dynamic framework that utilizes autonomous LLM agents with medical search tools in the continuous construction and real-time updating of Medical Knowledge Graphs (MKGs). Their system incorporated confidence scoring and multi-hop reasoning to improve accuracy and interpretability in medical question answering. AMG-RAG outperformed size models on both very hard MEDQA benchmarks and more accessible MedMCQA ones, proving that it could conduct efficient reasoning based on current structured medical knowledge. They also used Neo4j to manage the knowledge graphs while adding external searches to ensure the latest data. Tiwari et al.[24] presented Auto-Cypher, a recent automated pipeline for producing high-quality synthetic data for training LLMs by mapping natural language to Cypher queries for graph databases like Neo4j. The pipeline deployed the novelty of LLM- as-database-filler to synthesize Neo4j databases for the execution of generated queries to ensure their correctness. A sizable dataset called SynthCypher was created, spanning multiple domains and complex queries, leading to a 40% improvement in LLM performance on Cypher generation. The datasets were used to fine-tune open-source models such as LLaMA, Mistral, and Qwen, and the SPIDER benchmark was adapted for evaluation purposes. Mohammed et al.[29] proposed a hybridized GraphRAG framework combining Neo4j-based UMLS knowledge graphs with a vector store for medical textbooks to create an improved U.S.M.L.E.-style clinical question-answering approach. The project integrated symbolic reasoning from knowledge graphs with semantic retrieval performed on text embeddings to enhance relevance and accuracy via adaptive re-ranking and query expansion. The system had the answers produced by GPT-4o- Mini, with different prompting strategies encouraging evidence- based and traceable responses grounded in verified medical knowledge. Experiments showed that the hybrid approach improved factual accuracy and citation fidelity as compared to the L.L.M.-only approach, enhancing transparency and reliability. It is shown that binding both structured and unstructured medical knowledge sources could aid in ameliorating hallucinations and hence improve clinical trustworthiness in AI-driven medical QA. Yang et al.[30] articulated sepsis knowledge graph was crafted by combining multicenter clinical data from over 10,000 patients with the help of GPT-4 for entity recognition and relationship extraction. Real-world data were collected from three hospitals and integrated with clinical guidelines and databases from the public domain. The knowledge graph contained 1,894 nodes and 2,021 relationships pertaining to diseases, symptoms, biomarkers, treatments, and complications. GPT outperformed other models in every resolution on sepsis- specific datasets to obtain high F1-score results. The constructed graph highlighted complex interactions in sepsis for assisting clinical decision-making and was implemented on Neo4j. Guan et al.[20] proposed a novel method for constructing a local knowledge graph from retrieved biomedical documents by extracting propositional claims. They carried out layer wise summarization on this graph to capture multi-document relationships and provide comprehensive contextual information to a language model for question-answering purposes. The method resolved issues in multi-document biomedical QA, such as noise cancellation and efficient context usage. They then tested their method on several benchmarks for biomedical question answering, achieving performance at least comparable to, if not better than, existing retrieval-augmented generation (RAG) baselines. The study established enhanced reasoning and answer accuracy of the model achieved through structured graph summarization. Previous studies have improved biomedical QA using KGs and LLMs, but important gaps remain. Most systems lack transparent, graph-based justifications, rely on limited evaluation methods, or depend on cloud resources that reduce privacy and reproducibility. Our framework addresses these gaps by providing visible Cypher queries with evidence subgraphs, applying comprehensive performance metrics across difficulty levels, and ensuring fully local, privacy-preserving deployment. Table I summarizes key previous studies on biomedical knowledge graphs and question answering, outlining their methods, datasets, and main limitations.'
 '<4-hop>\n\nIII. PRELIMINARIES This section outlines the fundamental concepts required to understand the proposed system. It introduces biomedical knowledge graphs, explains how Neo4j stores data in graph form, and describes the use of Cypher for querying. It also provides a brief overview of large language models (LLMs) and their role in interpreting natural language. A. Biomedical Knowledge Graphs Biomedical Knowledge Graphs (BKGs) provide a structured representation of complex biomedical information by modeling diverse medical entities, such as diseases, drugs, symptoms, and biological pathways, as interconnected nodes within a graph structure. The edges in these graphs represent the semantic relationships between these entities, including ’treats’, ’causes’, ’interacts with’ and many others, as illustrated in Fig 1. This form of representation enables the integration of heterogeneous biomedical data from a wide range of sources, including']","The KGT framework enhances large language models' (LLMs) reasoning capabilities by employing biomedical knowledge graphs (KGs) to provide structured domain-specific knowledge. It reasons on the schema of these KGs to identify optimal subgraphs that guide accurate answer generation without requiring fine-tuning of the LLMs. This approach addresses the issue of factual hallucinations by extracting, validating, and refining factual knowledge during the reasoning process. The framework integrates seamlessly with various LLMs, including open-source models like Code-Llama, and improves performance solely through prompt engineering and in-context learning. Evaluations on a pan-cancer question answering benchmark (PcQA) demonstrate that KGT surpasses existing state-of-the-art methods by 33%, establishing a new standard in biomedical KGQA. Additionally, case studies show KGT's practical value in generating therapeutic plans, drug repositioning hypotheses, identifying drug resistance mechanisms, and biomarker discovery, all supported by evidence from prior research. Thus, by combining LLMs with biomedical knowledge graphs, KGT significantly improves the accuracy, interpretability, and efficiency of biomedical question answering.",multi_hop_abstract_query_synthesizer,"The KGT (Knowledge Graph–based Thought) framework enhances large language models' (LLMs) reasoning capabilities and improves biomedical question answering performance by integrating LLMs with biomedical knowledge graphs (KGs) in a novel and effective way. Here is how KGT leverages biomedical knowledge graphs to achieve these improvements, based solely on the provided context:

1. **Integration of LLMs with Biomedical Knowledge Graphs**  
   KGT combines the natural language understanding and generation strengths of LLMs with the structured, verifiable, and factual information encoded in biomedical knowledge graphs. This integration allows the framework to ground LLM responses in reliable biomedical data, reducing hallucinations and factual errors common in LLM outputs.

2. **Reasoning on Knowledge Graph Schema and Subgraph Identification**  
   Instead of fine-tuning LLMs, KGT reasons over the knowledge graph schema to identify optimal subgraphs relevant to the biomedical question at hand. By selecting these subgraphs, KGT directs the LLM to generate answers based on precise, contextually relevant biomedical facts and relationships, enhancing reasoning accuracy.

3. **Use of Pan-Cancer Knowledge Graph for Biomedical QA**  
   KGT utilizes a comprehensive pan-cancer knowledge graph that models complex biomedical entities (e.g., diseases, drugs, biomarkers) and their semantic relationships (e.g., ""treats,"" ""causes,"" ""interacts with""). This rich graph structure provides a detailed and interconnected biomedical knowledge base that supports multi-hop reasoning and complex query answering.

4. **Improved Factual Accuracy and Reduced Hallucination**  
   By grounding LLM outputs in verifiable information extracted from the knowledge graph, KGT significantly reduces factual hallucinations and erroneous outputs. The framework ensures that answers are evidence-based and traceable to specific graph substructures, improving trustworthiness and reliability in biomedical contexts.

5. **Flexible Architecture Supporting Various LLMs**  
   KGT is designed as a flexible architecture that can seamlessly integrate with different open-source LLMs without requiring model fine-tuning. This adaptability allows it to leverage the strengths of various LLMs while consistently enhancing their biomedical reasoning through knowledge graph augmentation.

6. **Benchmarking and Superior Performance**  
   The framework was evaluated on a newly developed pan-cancer question answering benchmark (PcQA) specifically designed for knowledge graph question answering in biomedicine. KGT outperformed existing state-of-the-art methods by a large margin, demonstrating its effectiveness in improving biomedical QA performance.

7. **Practical Biomedical Applications**  
   Case studies show that KGT can assist in real-world biomedical challenges such as drug repositioning (discovering new uses for existing drugs), predicting drug resistance by analyzing biomarkers and genetic mechanisms, and individualized treatment planning. This highlights the framework’s utility beyond theoretical improvements.

8. **Transparent and Explainable Reasoning**  
   KGT provides visible Cypher queries and evidence subgraphs as part of its reasoning process, offering transparent and interpretable justifications for the answers generated. This transparency is crucial in biomedical applications where explainability and evidence-based reasoning are essential.

**In summary**, the KGT framework leverages biomedical knowledge graphs by reasoning over their structured data to guide LLMs in generating accurate, evidence-based answers. It identifies relevant subgraphs to inform responses, reduces hallucinations, supports multiple LLMs without fine-tuning, and demonstrates superior performance on biomedical QA benchmarks, thereby substantially enhancing the reasoning capabilities and practical utility of LLMs in biomedical question answering.","['GigaScience , 2025, 14 , 1–12 \nDOI: 10.1093/gigascience/giae082 \nRESEARCH \nKno wledge gr aph–based thought: a kno wledge \ngraph–enhanced LLM framework for pan-cancer \nquestion ans w ering \nYichun Feng \n1 ,2 ,‡ , Lu Zhou 2 ,‡ , Chao Ma 3 ,‡ , Yikai Zheng 2 , Ruikun He 4 ,5 , * , and Yixue Li 1 ,2 , * \n1 Hangzhou Institute for Advanced Study, University of Chinese Academy of Sciences, 310024 Hangzhou, China \n2 Guangzhou National Laboratory, Guangzhou International Bio Island, 510005 Guangzhou, China \n3 Smartquerier Gene Technology (Shanghai) Co ., Ltd. , 200100 Shanghai, China \n4 BYHEALTH Institute of Nutrition & Health, 510663 Guangzhou, China \n5 Shanghai Institute of Nutrition and Health, Chinese Academy of Sciences Shanghai, 200030 Shanghai, China \n∗Corr espondence addr ess . Ruikun He , BYHEALTH Institute of Nutrition & Health, 510663 Guangzhou, China. E-mail: herk@by-health.com ; Yixue Li, Guangzhou \nNational Laboratory, No. 9 XingDaoHuanBei Road, Guangzhou International Bio Island, 510005 Guangzhou. E-mail: yxli@sibs.ac.cn .\n‡ Contributed equally. \nAbstract \nBac kgr ound: In recent years, large language models (LLMs) have shown promise in various domains, notably in biomedical sciences. \nHowev er, their r eal-world application is often limited by issues like erroneous outputs and hallucinatory responses. \nResults: We developed the knowledge graph–based thought (KGT) framework, an innov ati v e solution that integrates LLMs with knowl- \nedge graphs (KGs) to impr ov e their initial r esponses by utilizing v erifia b le information from KGs, thus significantl y r educing factual \nerr ors in r easoning. The KGT fr amew ork demonstr ates str ong adapta bility and performs well acr oss v arious open-source LLMs. No- \nta b l y, KGT can facilitate the discov er y of new uses for existing drugs through potential drug–cancer associations and can assist in \npr edicting r esistance by anal yzing r elev ant biomarkers and genetic mechanisms. To ev aluate the kno wledge graph question ans wer- \ning task within biomedicine , w e utilize a pan-cancer knowledge graph to develop a pan-cancer question answering benchmark, named \npan-cancer question answering. \nConclusions: The KGT fr amew ork substantiall y impr ov es the accuracy and utility of LLMs in the biomedical field. This study serves \nas a proof of concept, demonstrating its exceptional performance in biomedical question answering. \nKe yw ords: pan-cancer knowledge graph, large language model, knowledge graph question answering, prompt engineering \nKe y P oints: \nr We introduce a framework combining large language \nmodels (LLMs) with knowledge gr a phs (KGs) to impr ov e \nfactual accuracy in LLM reasoning. \nr Our system is a flexible arc hitectur e that seamlessly in- \ntegr ates v arious LLMs. \nr Utilizing a pan-cancer knowledge gr a ph, we hav e pr o- \nposed the first knowledge gr a ph question answering \nbenchmark in the field of biomedicine. \nr Case studies r e v eal our method enhanced LLMs in ad- \ndressing biomedical challenges such as drug reposition- \ning, r esistance r esearc h, individualized tr eatment, and \nbiomarker analysis. \nr The method performs favor abl y in comparison to exist- \ning methods. \nIntroduction \nWith the increasing prominence of large language models (LLMs) \nin the field of artificial intelligence, the advent of influential mod- \nels such as ChatGPT [ 1 ] and Llama [ 2 ] consequently catalyze the \nde v elopment of a wide array of applications in biomedicine and \nhealth care. Ho w ever, LLMs still face the challenge of factual hal- \nlucination, wher e they gener ate incorr ect statements due to lim- \nited inherent knowledge [ 3 ]. Factual hallucination presents a sig- \nnificant challenge for the practical use of LLMs, especially in real- \nworld scenarios where factual accuracy is crucial. Consequently, \nthere is a growing focus on addressing factual hallucinations in \nLLMs within the field of natural language processing (NLP) [ 4 , 5 ]. \nLLMs often struggle to ca ptur e and access factual knowledge, \nprimarily due to 3 aspects: the inability to comprehend ques- \ntions due to the lack of contextual information, the insufficient \nknowledge to generate accurate ans wers , and the incapacity to \nrecall specific facts [ 6 ]. Consequently, researchers consider the \nfine-tuning technique as a solution to address these issues. For \nexample, MedAlpaca [ 7 ] builds upon medical data to fine-tune \nStanford Alpaca for applications related to medical question \nans wering and dialogue . ChatDoctor [ 8 ] is designed to simulate \na conversation between a doctor and a patient by fine-tuning \nRecei v ed: May 27, 2024. Re vised: A ugust 14, 2024. Accepted: October 2, 2024 \n© The Author(s) 2025. Published by Oxford Uni v ersity Pr ess GigaScience. This is an Open Access article distributed under the terms of the Cr eati v e Commons \nAttribution License ( https://cr eati v ecommons.org/licenses/by/4.0/ ), which permits unrestricted reuse, distribution, and reproduction in any medium, provided \nthe original work is pr operl y cited. \nDownloaded from https://academic.oup.com/gigascience/article/doi/10.1093/gigascience/giae082/7943459 by guest on 11 January 2026'
 'Omar & Mohammed / Journal of Applied Science and Technology Trends Vol. 06, No. 02, pp. 342 –357 (2025) \n \n345 \nrecord-high accuracy, while open-source models achieved \nimpressive gains through prompt optimization. \nFeng et al.[22] developed the Knowledge Graph-based \nThought (KGT) framework that integrated LLMs with a pan-\ncancer knowledge graph for biomedical question answering. \nKGT was designed to reason on the knowledge graph schema and \nidentify optimal subgraphs to use for directing accurate answer \ngeneration, all without fine-tuning the LLMs. The framework is \nbenchmarked against a new dataset (PcQA) designed specifically \nfor pan-cancer KGQA tasks and has outperformed all existing \nstate-of-the-art approaches by a rather large margin. KGT’s \npracticality in biomedical issues was highlighted through case \nstudies for drug repositioning, drug resistance, and biomarker \ndiscovery. Their approach exhibited robust adaptability among \nvarious LLMs. \nRezaei et al.[26] developed AMG-RAG, a dynamic \nframework that utilizes autonomous LLM agents with medical \nsearch tools in the continuous construction and real-time updating \nof Medical Knowledge Graphs (MKGs). Their system \nincorporated confidence scoring and multi-hop reasoning to \nimprove accuracy and interpretability in medical question \nanswering. AMG-RAG outperformed size models on both very \nhard MEDQA benchmarks and more accessible MedMCQA \nones, proving that it could conduct efficient reasoning based on \ncurrent structured medical knowledge. They also used Neo4j to \nmanage the knowledge graphs while adding external searches to \nensure the latest data. \nTiwari et al.[24] presented Auto-Cypher, a recent automated \npipeline for producing high-quality synthetic data for training \nLLMs by mapping natural language to Cypher queries for graph \ndatabases like Neo4j. The pipeline deployed the novelty of LLM-\nas-database-filler to synthesize Neo4j databases for the execution \nof generated queries to ensure their correctness. A sizable dataset \ncalled SynthCypher was created, spanning multiple domains and \ncomplex queries, leading to a 40% improvement in LLM \nperformance on Cypher generation. The datasets were used to \nfine-tune open-source models such as LLaMA, Mistral, and \nQwen, and the SPIDER benchmark was adapted for evaluation \npurposes. \nMohammed et al.[29] proposed a hybridized GraphRAG \nframework combining Neo4j-based UMLS knowledge graphs \nwith a vector store for medical textbooks to create an improved \nU.S.M.L.E.-style clinical question-answering approach. The \nproject integrated symbolic reasoning from knowledge graphs \nwith semantic retrieval performed on text embeddings to enhance \nrelevance and accuracy via adaptive re-ranking and query \nexpansion. The system had the answers produced by GPT-4o-\nMini, with different prompting strategies encouraging evidence-\nbased and traceable responses grounded in verified medical \nknowledge. Experiments showed that the hybrid approach \nimproved factual accuracy and citation fidelity as compared to \nthe L.L.M.-only approach, enhancing transparency and \nreliability. It is shown that binding both structured and \nunstructured medical knowledge sources could aid in \nameliorating hallucinations and hence improve clinical \ntrustworthiness in AI-driven medical QA. \nYang et al.[30] articulated sepsis knowledge graph was \ncrafted by combining multicenter clinical data from over 10,000 \npatients with the help of GPT-4 for entity recognition and \nrelationship extraction. Real-world data were collected from \nthree hospitals and integrated with clinical guidelines and \ndatabases from the public domain. The knowledge graph \ncontained 1,894 nodes and 2,021 relationships pertaining to \ndiseases, symptoms, biomarkers, treatments, and complications. \nGPT outperformed other models in every resolution on sepsis-\nspecific datasets to obtain high F1-score results. The constructed \ngraph highlighted complex interactions in sepsis for assisting \nclinical decision-making and was implemented on Neo4j. \nGuan et al.[20] proposed a novel method for constructing a \nlocal knowledge graph from retrieved biomedical documents by \nextracting propositional claims. They carried out layer wise \nsummarization on this graph to capture multi-document \nrelationships and provide comprehensive contextual information \nto a language model for question-answering purposes. The \nmethod resolved issues in multi-document biomedical QA, such \nas noise cancellation and efficient context usage. They then tested \ntheir method on several benchmarks for biomedical question \nanswering, achieving performance at least comparable to, if not \nbetter than, existing retrieval-augmented generation (RAG) \nbaselines. The study established enhanced reasoning and answer \naccuracy of the model achieved through structured graph \nsummarization.   \nPrevious studies have improved biomedical QA using KGs \nand LLMs, but important gaps remain. Most systems lack \ntransparent, graph-based justifications, rely on limited evaluation \nmethods, or depend on cloud resources that reduce privacy and \nreproducibility. Our framework addresses these gaps by \nproviding visible Cypher queries with evidence subgraphs, \napplying comprehensive performance metrics across difficulty \nlevels, and ensuring fully local, privacy-preserving deployment. \nTable I summarizes key previous studies on biomedical \nknowledge graphs and question answering, outlining their \nmethods, datasets, and main limitations. \nIII. PRELIMINARIES \nThis section outlines the fundamental concepts required to \nunderstand the proposed system. It introduces biomedical \nknowledge graphs, explains how Neo4j stores data in graph \nform, and describes the use of Cypher for querying. It also \nprovides a brief overview of large language models (LLMs) and \ntheir role in interpreting natural language. \n \n \nA. Biomedical Knowledge Graphs \nBiomedical Knowledge Graphs (BKGs) provide a structured \nrepresentation of complex biomedical information by modeling \ndiverse medical entities, such as diseases, drugs, symptoms, and \nbiological pathways, as interconnected nodes within a graph \nstructure. The edges in these graphs represent the semantic \nrelationships between these entities, including ’treats’, ’causes’, \n’interacts with’ and many others, as illustrated in Fig 1. This \nform of representation enables the integration of heterogeneous \nbiomedical data from a wide range of sources, including'
 'Knowledge gr a ph–based thought | 11 \narXiv:230515075. 24 May 2023. https:// doi.org/ 10.48550/arXiv.2 \n305.15075 .\n13. Yang S, Zhao H, Zhu S, et al. Zhongjing: enhancing the Chi- \nnese medical capabilities of large language model through \nexpert feedback and real-world multi-turn dialogue. In: Pro- \nceedings of the AAAI Conference on Artificial Intelligence. \n2023;38(17):19368–76. https:// doi.org/ 10.1609/ aaai.v38i17.2990 \n7 .\n14. Tian Y, Gan R, Song Y, et al. ChiMed-GPT: a Chinese medical large \nlanguage model with full training regime and better alignment \nto human pr efer ences. 2023. arXiv pr e print arXi v:231106025. 15 \nJul 2024. https:// doi.org/ 10.48550/arXiv.2311.06025 .\n15. Ouyang L, Wu J, Jiang X, et al. Tr aining langua ge models \nto follow instructions with human feedback. Adv Neur Inf \nProc Syst 2022;35:27730–44. https:// doi.org/ 10.48550/arXiv.2203. \n02155 .\n16. Luo Y, Yang Z, Meng F, et al. An empirical study of catas- \ntr ophic for getting in lar ge langua ge models during continual \nfine-tuning. 2023. arXi v pre print arXi v:230808747. 17 Aug 2023. \nhttps:// doi.org/ 10.48550/arXiv.2308.08747 .\n17. Li Z, Hoiem D. Learning without forgetting. IEEE Trans Pattern \nAnal Machine Intell 2017;40(12):2935–47. https:// doi.org/ 10.110 \n9/TPAMI.2017.2773081 .\n18. Liu V, Chilton LB. Design guidelines for prompt engineering text- \nto-ima ge gener ativ e models. In: Pr oceedings of the 2022 CHI \nConference on Human Factors in Computing Systems. 2022:1–\n23. https:// doi.org/ 10.1145/ 3491102.3501825 .\n19. W ei J, W ang X, Schuurmans D, et al. Chain-of-thought prompt- \ning elicits reasoning in large language models. Adv Neur Inf Proc \nSyst 2022;35:24824–37. https:// doi.org/ 10.48550/arXiv.2201.1190 \n3 .\n20. Zhou Y, Muresanu AI, Han Z, et al. Large language mod- \nels ar e human-le v el pr ompt engineers. 2022. arXiv pr eprint \narXiv:221101910. 3 Nov 2022. https:// doi.org/ 10.48550/arXiv.221 \n1.01910 .\n21. Sun H, Dhingra B, Zaheer M, et al. Open domain question an- \nswering using early fusion of knowledge bases and text. In: \nRiloff E., Chiang D., Hockenmaier J., and Tsujii J.eds. Proceed- \nings of the 2018 Conference on Empirical Methods in Natural \nLangua ge Pr ocessing. 2018:4231–42. https:// doi.org/ 10.18653/v \n1/D18-1455 .\n22. Sun H, Bedrax-Weiss T, Cohen WW. Pullnet: open domain ques- \ntion answering with iter ativ e r etrie v al on knowledge bases and \ntext. In: Proceedings of the 2019 Conference on Empirical Meth- \nods in Natural Language Processing and the 9th International \nJoint Conference on Natural Language Processing (EMNLP- \nIJCNLP). Association for Computational Linguistics; 2019:2380–\n90. https:// doi.org/ 10.18653/v1/ D19-1242 .\n23. Zhang J, Zhang X, Yu J, et al. Subgr a ph r etrie v al enhanced model \nfor multi-hop knowledge base question answering. 2022. arXiv \npre print arXi v:220213296. 27 Jul 2022. https:// doi.org/ 10.48550/a \nrXiv.2202.13296 .\n24. Chen Y, Wu L, Zaki MJ. Bidir ectional attentiv e memory networks \nfor question answering over knowledge bases. In: Proceedings \nof the 2019 Conference of the North American Chapter of the \nAssociation for Computational Linguistics: Human Language \nTechnologies, Volume 1 (Long and Short Papers); Minneapolis, \nMinnesota: Association for Computational Linguistics; 2913–23. \nhttps:// doi.org/ 10.18653/v1/ N19-1299 .\n25. Saxena A, Tripathi A, Talukdar P. Impr oving m ulti-hop question \nans wering o v er knowledge gr a phs using knowledge base em- \nbeddings . In: J urafsky D., Chai J., Schluter N., and T etreault J .eds. \nProceedings of the 58th Annual Meeting of the Association for \nComputational Linguistics. Association for Computational Lin- \nguistics; 2020:4498–507. https:// doi.org/ 10.18653/v1/ 2020.acl-m \nain.412 .\n26. Lan Y, He G, Jiang J, et al. A survey on complex knowledge base \nquestion ans wering: methods , challenges and solutions . In: Pro- \nceedings of the 30th International Joint Conference on Artificial \nIntelligence (IJC AI-21). IJC AI; 4483–91. https:// doi.org/ 10.24963/i \njcai.2021/611 .\n27. Das R, Zaheer M, Thai D, et al. Case-based reasoning for nat- \nur al langua ge queries ov er knowledge bases. In: Pr oceedings \nof the 2021 Conference on Empirical Methods in Natural Lan- \ngua ge Pr ocessing. Online and Punta Cana: Association for Com- \nputational Linguistics; 2021:9594–611. https:// doi.org/ 10.18653 \n/v1/2021.emnlp-main.755 .\n28. Jiang J, Zhou K, Zhao WX, et al. Unikgqa: unified r etrie v al and \nreasoning for solving multi-hop question ans wering o ver knowl- \nedge gr a ph. 2022. arXiv pr eprint. arXiv:221200959. 2 Dec 2022. \nhttps:// doi.org/ 10.48550/arXiv.2212.00959 .\n29. Jiang J, Zhou K, Dong Z, et al. Structgpt: a gener al fr ame work \nfor lar ge langua ge model to r eason ov er structur ed data. In: Pr o- \nceedings of the 2023 Conference on Empirical Methods in Nat- \nur al Langua ge Pr ocessing. Singa por e. Association for Computa- \ntional Linguistics; 2023:9237–51. https:// doi.org/ 10.18653/v1/ 20 \n23.emnlp-main.574 .\n30. Sun J, Xu C, Tang L, et al. Think-on-gr a ph: deep and responsible \nreasoning of large language model on knowledge graph. In: The \nTwelfth International Conference on Learning Representations. \nVienna, Austria: arXiv; 2024. https:// doi.org/ 10.48550/arXiv.230 \n7.07697 .\n31. Luo L, Li YF, Haf R, et al. Reasoning on gr a phs: faithful and \ninter pr etable lar ge langua ge model r easoning. In: The Twelfth \nInternational Conference on Learning Representations.Vienna, \nAustria. arXiv; 2024. https:// doi.org/ 10.48550/arXiv.2310.01061 .\n32. Kim J, Kwon Y, Jo Y, et al. KG-GPT: A general framework for \nreasoning on knowledge graphs using large language models. \nIn: Bouamor H., Pino J., and Bali K.eds. Findings of the Asso- \nciation for Computational Linguistics: EMNLP. Singa por e: As- \nsociation for Computational Linguistics. 2023; 9410–21. https: \n// doi.org/ 10.48550/arXiv.2310.11220 .\n33. Feng Y, Zhou L, Ma C, et al.. Knowledge gr a ph-based thought: \na knowledge gr a ph enhanced LLMs fr ame work for pan-cancer \nquestion answering (Version 1). 2024 Softwar e Herita ge [Com- \nputer softwar e]. https://arc hive.softwar eheritage.or g/br owse/ \nsna pshot/1906dbbfc88c9d1c8b7acf7deb7495e8002cbafa/dir ect \nory/ ?origin _ url=https:// github.com/yichun10/ bioKGQA-KGT .\n34. W ang J, W u M, Huang X, et al. SynLethDB 2.0: a web-based \nknowledge gr a ph database on synthetic lethality for novel anti- \ncancer drug discovery. Database 2022;2022:baac030. https://doi. \norg/ 10.1093/ database/baac030 .\n35. Zhu C, Yang Z, Xia X, et al. Multimodal reasoning based on \nknowledge gr a ph embedding for specific diseases. Bioinformat- \nics 2022;38(8):2235–45. https:// doi.org/ 10.1093/ bioinformatics/b \ntac085 .\n36. Zhang Y, Dai H, Kozar e v a Z, et al. Variational reasoning \nfor question answering with knowledge gr a ph. In: McIlr aith \nS.A. and Weinberger K.Q. eds. Proceedings of the AAAI confer- \nence on artificial intelligence. Vol. 32. New Orleans, Louisiana, \nUSA: {AAAI} Press; 2018. https:// doi.org/ 10.48550/arXiv.1709.04 \n071 .\n37. Kim J, Park S, Kwon Y, et al. FactKG: fact verification via rea- \nsoning on knowledge gr a phs. In: Rogers A., Boyd-Gr aber J.L., and \nOkazaki N.eds. Proceedings of the 61st Annual Meeting of the \nAssociation for Computational Linguistics (Volume 1: Long Pa- \nDownloaded from https://academic.oup.com/gigascience/article/doi/10.1093/gigascience/giae082/7943459 by guest on 11 January 2026']"
"What evaluation methods and performance metrics have been used to compare RAG-enhanced LLMs with baseline LLMs in clinical tasks, and how do these methods reflect the accuracy and usefulness of the models?","['<1-hop>\n\nLimitations This study was limited to peer-reviewed publications avail\xad able in biomedical databases (eg, PubMed, Embase), exclud\xad ing preprint articles from repositories like ArXiv. Additionally, only studies in English language were included, which might have excluded relevant studies in other lan\xad guages. We did not include sources such as IEEE Xplore or Google Scholar, which might have additional relevant stud\xad ies. However, our focus was on biomedicine, and we priori\xad tized databases specifically tailored to biomedical research to maintain the relevance and quality of the included studies. Furthermore, we used free-text searches in the databases, which activated automatic mapping to Medical Subject Headings (MeSH) and Emtree terms, improving retrieval accuracy. However, the limitations of automatic term map\xad ping cannot be ignored, as it may introduce variability if the underlying algorithms change. To address this, we have docu\xad mented all identified papers from our search. The title, publi\xad cation year, PMID, PUI, and database source for each study are provided in File S3. Conclusion We conducted a systematic literature review of studies exploring the use of RAG and LLM in clinical tasks. RAG implementation showed a 1.35 odds ratio increase in per\xad formance compared to baseline LLMs. To improve perform\xad ance and transparency in future studies, we developed guidelines for improving clinical RAG applications based on current research findings. Future research could focus on these 3 directions: (1) system-level enhancement: the combi\xad nation of RAG and agent, (2) knowledge-level enhancements: deep integration of knowledge into LLM, and (3) integration- level enhancements: integrating RAG systems within EHRs. Author contributions Siru Liu (Conceptualization, Data curation, Formal analysis, Investigation, Methodology, Software, Visualization, Writing – original draft), Allison B. McCoy (Conceptualization, Writ\xad ing – review & editing), Adam Wright (Conceptualization, Writing – review & editing). Supplementary material Supplementary material is available at Journal of the Ameri\xad can Medical Informatics Association online.'
 '<2-hop>\n\nwas asked about medications for peripheral artery disease patients without increased bleeding risk, it initially omitted low-dose rivaroxaban. After integrating retrieved text from the 2024 American College of Cardiology / American Heart Association Guideline for the Management of Lower Extrem\xad ity Peripheral Artery Disease,10 the model correctly recom\xad mended rivaroxaban. Several guidelines exist for evaluating Artificial Intelligence (AI) applications and LLMs in healthcare, including DECIDE-AI (Developmental and Exploratory Clinical Inves\xad tigations of DEcision support systems driven by Artificial Intelligence),11 CLAIM (Checklist for Artificial Intelligence in Medical Imaging),12 and CONSORT-AI (Consolidated Standards of Reporting Trials-AI).13 In addition, Tam et al. introduced QUEST, a framework specifically for human eval\xad uation of LLMs in healthcare.14 However, these guidelines do not cover RAG applications in clinical settings, emphasiz\xad ing the need for a more specific guideline. Despite the promise of RAG in improving LLM performance in clinical settings, there is limited understanding of its overall effectiveness comparing with the baseline LLM, adoption in clinical domains, and optimal strategies for its development in biomedical applications. The aim of this study is to synthesize findings from recent research of RAG and LLM in biomedicine and provide clinical development guidelines to improve effec\xad tiveness as well as transparency in future research. Materials and methods Study design We conducted a systematic literature review. The report was created in adherence to the Preferred Reporting Items for Sys\xad tematic Reviews and Meta-Analyses 2020 analysis checklist (File S1).15 We also performed a meta-analysis following the Cochrane Handbook guidelines.16 Screening papers We searched in 3 databases (PubMed, Embase, and Psy\xad cINFO), using terms related to “retrieval augmented gener\xad ation” and “large language model.” PubMed and Embase were selected based on recommendations from the Cochrane Handbook, as they are widely recognized for their extensive coverage of biomedical literature.17 PsycINFO was included to capture articles from the psychological and behavioral sci\xad ences. To maintain a high standard of quality and reliability, we focused on peer-reviewed articles and excluded preprints. The specific search terms used for each database are provided in File S2. Given that ChatGPT was released on November 30, 2022, we set the publication filter to search papers pub\xad lished in 2023 and 2024. The search was performed on December 12, 2024. The inclusion criteria were: (1) the study must compare baseline LLM performance with RAG per\xad formance and (2) the study must address a biomedical ques\xad tion. The exclusion criteria were: (1) literature reviews, editorial comments, or viewpoint papers, (2) studies focusing on LLMs in languages other than English, or (3) studies cen\xad tered on a multi-agent system without a focus on RAG. SL screened titles and abstracts, then conducted a full-text review of papers meeting the criteria. Data extraction For each included study, we extracted the following informa\xad tion: author, title, publication year, journal, clinical task, and specialty. Regarding RAG techniques, we gathered details about the baseline LLM, retrieval sources, and strategies used in the pre-retrieval, retrieval, and post-retrieval stages. For evaluation, we extracted the evaluation method (human, automated, or a combination of both), the number of evalua\xad tors, the evaluation dataset, and the evaluation metrics.'
 '<3-hop>\n\nMeta-analysis Effect size was defined as a metric quantifying the relation\xad ship between variables, including both direction and magni\xad tude.18 For each included study, we calculated the effect size between baseline LLM performance and RAG-enhanced LLM performance. The outcomes focused on the perform\xad ance of generation results, such as accuracy and usefulness. Metrics related to the retrieval process, cost, or speed were not included as outcomes in the meta-analysis. For continu\xad ous outcomes, we used Cohen’s d, standardized between- group mean difference (SMD), calculated as the difference in means divided by the pooled standard deviation. The stand\xad ard error (SE) of SMD was calculated using the following for\xad mula (1), where n1 and n2 represent the sample sizes of each group.19 For dichotomous measurements, we calculated the log-odds ratio, obtained by transforming the odds ratio (OR) with the natural logarithm, and the associated SE was calcu\xad lated using formula (2), where a, b, c, and d represent the number of successful and failed events in the baseline LLM and RAG-enhanced LLM approaches. For studies reporting multiple outcomes, we used the overall outcome to calculate effect size. If no overall outcome was reported, we averaged the effect sizes of all reported outcomes. We excluded out\xad comes with a sample size of less than 30 to avoid small- sample bias. SESMD ¼ ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi n1 þ n2 n1n2 þ SMD2 2 n1 þ n2 ð Þ s (1) SElogOR ¼ ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi 1 a þ 1 b þ 1 c þ 1 d r (2) We developed a random-effect meta-analysis model, because of the variability in RAG architectures and evalua\xad tion datasets among the included studies. The random-effect model was used when individual study effects likely con\xad tained additional sources of variance beyond sampling error. Between-study heterogeneity was assessed using Higgins & Thompson’s I2 statistic, where 25% indicated low heteroge\xad neity, 50% moderate, and 75% substantial.20 We conducted subgroup analyses to explore performance variations across different factors. First, we analyzed the influence of the baseline LLM, referring to the foundation model (eg, GPT-4 or Llama2) that provides the core architec\xad ture for the system. Second, we examined data retrieval strat\xad egies, categorizing them as simple or complex. Simple strategies included fixed-length chunking and basic similarity search, and we performed a subgroup analysis to compare these with complex retrieval strategies. Third, we analyzed differences based on evaluation methods, distinguishing between human evaluations, such as Likert scale ratings for helpfulness and accuracy, and automatic evaluation metrics, including ROUGE-1 and BLEU. Finally, we conducted a sub\xad group analysis based on the type of task, classifying studies 606 Journal of the American Medical Informatics Association, 2025, Vol. 32, No. 4 Downloaded from https://academic.oup.com/jamia/article/32/4/605/7954485 by guest on 11 January 2026']","Studies comparing RAG-enhanced LLMs with baseline LLMs in clinical tasks have employed various evaluation methods, including human evaluations such as Likert scale ratings for helpfulness and accuracy, and automated evaluation metrics like ROUGE-1 and BLEU. Several guidelines exist for evaluating AI applications and LLMs in healthcare, including DECIDE-AI, CLAIM, CONSORT-AI, and the QUEST framework for human evaluation of LLMs, though these do not specifically cover RAG applications, highlighting the need for more tailored guidelines. Performance metrics focused on generation results such as accuracy and usefulness were analyzed in a meta-analysis using effect size measures like Cohen’s d and log-odds ratio. The meta-analysis excluded metrics related to retrieval process, cost, or speed, concentrating on the quality of generated outputs. Subgroup analyses further explored variations in performance based on baseline LLM models, retrieval strategies, evaluation methods, and task types, demonstrating a comprehensive approach to assessing RAG and LLM performance in biomedical applications.",multi_hop_abstract_query_synthesizer,"The evaluation methods and performance metrics used to compare retrieval-augmented generation (RAG)-enhanced large language models (LLMs) with baseline LLMs in clinical tasks, as described in the provided context, include the following:

### Evaluation Methods:
1. **Human Evaluation:**
   - Used in 9 studies.
   - Typically involved physicians or domain experts as evaluators (average of 4 evaluators, range 1 to 10).
   - Evaluators assessed aspects such as accuracy, helpfulness, completeness, understandability, and whether responses met user intent.
   - Some evaluations included patients (e.g., diabetic patients assessing understandability).
   - Human evaluation often used Likert scale ratings.
   - Outcomes from human evaluation showed an overall odds ratio (OR) of 1.65 (95% CI: 1.36-2.03), indicating a significant improvement of RAG-enhanced LLMs over baseline LLMs.

2. **Automated Evaluation:**
   - Used in 8 studies.
   - Metrics included similarity comparisons between generated responses and expert-provided answers.
   - Common automated metrics were ROUGE (ROUGE-1, ROUGE-2, ROUGE-L), BLEU, METEOR, and BERTScore.
   - Automated evaluation showed an OR of 1.20 (95% CI: 1.1-1.41), also indicating improvement but less pronounced than human evaluation.
   
3. **Combined Human and Automated Evaluation:**
   - Used in 3 studies.
   - Combined both subjective human judgments and objective automated metrics for a more comprehensive assessment.

### Performance Metrics:
1. **Accuracy:**
   - Most frequently reported metric.
   - Included Likert scale ratings, match rates, correct treatment percentages, AUC (Area Under the Curve), AUPRC (Area Under the Precision-Recall Curve), and F1 scores.
   - Also included text similarity metrics (ROUGE, BLEU, METEOR, BERTScore) comparing generated responses to expert answers.

2. **Completeness:**
   - Assessed whether responses included all necessary information.
   - Typically measured using Likert scales.

3. **User Perception:**
   - Captured subjective feedback on understandability, helpfulness, and whether responses met user intent.
   - Usually measured via Likert scales from healthcare providers and patients.

4. **Safety:**
   - Evaluated potential harm, adversarial safety, and risk management.
   - Included assessments of factual accuracy and ethical considerations.
   - Some studies used adversarial prompting to test system safety.

5. **Hallucination:**
   - Identified primarily through manual review.
   - Defined as generation of nonfactual or conflicting information.

6. **Citation Accuracy:**
   - Measured the correctness of references provided by the model.
   - Valid citations pointed to established publications, guidelines, or research.

7. **Bias and Language:**
   - Evaluated clarity, neutrality, and empathy in responses to ensure unbiased and patient-sensitive communication.

### Reflection on Accuracy and Usefulness:
- The combination of human and automated evaluation methods provides a balanced assessment of both factual correctness and practical usefulness.
- Human evaluations capture nuanced judgments about clinical relevance, safety, and user satisfaction that automated metrics may miss.
- Automated metrics offer scalable, objective measures of textual similarity and accuracy against gold-standard answers.
- The statistically significant higher odds ratios in human evaluations (OR=1.65) compared to automated evaluations (OR=1.20) suggest that RAG-enhanced LLMs not only produce more accurate responses but also generate outputs that are more clinically useful and trustworthy from a human perspective.
- Safety and hallucination assessments ensure that improvements in accuracy do not come at the cost of generating harmful or misleading information.
- Citation and bias evaluations further support the clinical reliability and ethical appropriateness of RAG-enhanced LLM outputs.

### Summary:
The studies reviewed used a combination of human expert assessments and automated text similarity metrics to evaluate RAG-enhanced LLMs against baseline LLMs in clinical tasks. These methods comprehensively measure accuracy, completeness, safety, and user satisfaction, reflecting both the factual correctness and practical usefulness of the models. The meta-analysis showed that RAG-enhanced LLMs significantly outperform baseline LLMs, particularly in human-evaluated clinical relevance and helpfulness.","['was asked about medications for peripheral artery disease \npatients without increased bleeding risk, it initially omitted \nlow-dose rivaroxaban. After integrating retrieved text from \nthe 2024 American College of Cardiology / American Heart \nAssociation Guideline for the Management of Lower Extrem\xad\nity Peripheral Artery Disease,10 the model correctly recom\xad\nmended rivaroxaban.\nSeveral guidelines exist for evaluating Artificial Intelligence \n(AI) applications and LLMs in healthcare, including \nDECIDE-AI (Developmental and Exploratory Clinical Inves\xad\ntigations of DEcision support systems driven by Artificial \nIntelligence),11 CLAIM (Checklist for Artificial Intelligence in \nMedical Imaging),12\nand CONSORT-AI (Consolidated \nStandards of Reporting Trials-AI).13 In addition, Tam et al. \nintroduced QUEST, a framework specifically for human eval\xad\nuation of LLMs in healthcare.14 However, these guidelines \ndo not cover RAG applications in clinical settings, emphasiz\xad\ning the need for a more specific guideline.\nDespite the promise of RAG in improving LLM performance \nin clinical settings, there is limited understanding of its overall \neffectiveness comparing with the baseline LLM, adoption in \nclinical domains, and optimal strategies for its development in \nbiomedical applications. The aim of this study is to synthesize \nfindings from recent research of RAG and LLM in biomedicine \nand provide clinical development guidelines to improve effec\xad\ntiveness as well as transparency in future research.\nMaterials and methods\nStudy design\nWe conducted a systematic literature review. The report was \ncreated in adherence to the Preferred Reporting Items for Sys\xad\ntematic Reviews and Meta-Analyses 2020 analysis checklist \n(File S1).15 We also performed a meta-analysis following the \nCochrane Handbook guidelines.16\nScreening papers\nWe searched in 3 databases (PubMed, Embase, and Psy\xad\ncINFO), using terms related to “retrieval augmented gener\xad\nation” and “large language model.” PubMed and Embase \nwere selected based on recommendations from the Cochrane \nHandbook, as they are widely recognized for their extensive \ncoverage of biomedical literature.17 PsycINFO was included \nto capture articles from the psychological and behavioral sci\xad\nences. To maintain a high standard of quality and reliability, \nwe focused on peer-reviewed articles and excluded preprints. \nThe specific search terms used for each database are provided \nin File S2. Given that ChatGPT was released on November \n30, 2022, we set the publication filter to search papers pub\xad\nlished in 2023 and 2024. The search was performed on \nDecember 12, 2024. The inclusion criteria were: (1) the study \nmust compare baseline LLM performance with RAG per\xad\nformance and (2) the study must address a biomedical ques\xad\ntion. The exclusion criteria were: (1) literature reviews, \neditorial comments, or viewpoint papers, (2) studies focusing \non LLMs in languages other than English, or (3) studies cen\xad\ntered on a multi-agent system without a focus on RAG. SL \nscreened titles and abstracts, then conducted a full-text \nreview of papers meeting the criteria.\nData extraction\nFor each included study, we extracted the following informa\xad\ntion: author, title, publication year, journal, clinical task, and \nspecialty. Regarding RAG techniques, we gathered details \nabout the baseline LLM, retrieval sources, and strategies used \nin the pre-retrieval, retrieval, and post-retrieval stages. For \nevaluation, we extracted the evaluation method (human, \nautomated, or a combination of both), the number of evalua\xad\ntors, the evaluation dataset, and the evaluation metrics.\nMeta-analysis\nEffect size was defined as a metric quantifying the relation\xad\nship between variables, including both direction and magni\xad\ntude.18 For each included study, we calculated the effect size \nbetween baseline LLM performance and RAG-enhanced \nLLM performance. The outcomes focused on the perform\xad\nance of generation results, such as accuracy and usefulness. \nMetrics related to the retrieval process, cost, or speed were \nnot included as outcomes in the meta-analysis. For continu\xad\nous outcomes, we used Cohen’s d, standardized between- \ngroup mean difference (SMD), calculated as the difference in \nmeans divided by the pooled standard deviation. The stand\xad\nard error (SE) of SMD was calculated using the following for\xad\nmula (1), where n1 and n2 represent the sample sizes of each \ngroup.19 For dichotomous measurements, we calculated the \nlog-odds ratio, obtained by transforming the odds ratio (OR) \nwith the natural logarithm, and the associated SE was calcu\xad\nlated using formula (2), where a, b, c, and d represent the \nnumber of successful and failed events in the baseline LLM \nand RAG-enhanced LLM approaches. For studies reporting \nmultiple outcomes, we used the overall outcome to calculate \neffect size. If no overall outcome was reported, we averaged \nthe effect sizes of all reported outcomes. We excluded out\xad\ncomes with a sample size of less than 30 to avoid small- \nsample bias. \nSESMD ¼\nffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi\nn1 þ n2\nn1n2\nþ\nSMD2\n2 n1 þ n2\nð\nÞ\ns\n(1) \nSElogOR ¼\nffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi\n1\na þ 1\nb þ 1\nc þ 1\nd\nr\n(2) \nWe developed a random-effect meta-analysis model, \nbecause of the variability in RAG architectures and evalua\xad\ntion datasets among the included studies. The random-effect \nmodel was used when individual study effects likely con\xad\ntained additional sources of variance beyond sampling error. \nBetween-study heterogeneity was assessed using Higgins & \nThompson’s I2 statistic, where 25% indicated low heteroge\xad\nneity, 50% moderate, and 75% substantial.20\nWe conducted subgroup analyses to explore performance \nvariations across different factors. First, we analyzed the \ninfluence of the baseline LLM, referring to the foundation \nmodel (eg, GPT-4 or Llama2) that provides the core architec\xad\nture for the system. Second, we examined data retrieval strat\xad\negies, categorizing them as simple or complex. Simple \nstrategies included fixed-length chunking and basic similarity \nsearch, and we performed a subgroup analysis to compare \nthese with complex retrieval strategies. Third, we analyzed \ndifferences based on evaluation methods, distinguishing \nbetween human evaluations, such as Likert scale ratings for \nhelpfulness and accuracy, and automatic evaluation metrics, \nincluding ROUGE-1 and BLEU. Finally, we conducted a sub\xad\ngroup analysis based on the type of task, classifying studies \n606                                                                                                      \nJournal of the American Medical Informatics Association, 2025, Vol. 32, No. 4 \nDownloaded from https://academic.oup.com/jamia/article/32/4/605/7954485 by guest on 11 January 2026'
 'conducting safety checks by applying 24 predefined rules to \nensure ethical and factual accuracy, and summarizing the \nresults.30 Glicksberg et al. developed an ensemble model that \ncombined structured and unstructured data to predict hospi\xad\ntal admission probabilities. These predicted probabilities, \nalong with similar historical cases, were incorporated into the \nprompt to enhance the performance of LLM.37 Chen et al. \nused Chain-of-Thought (CoT) prompting to improve LLM \nreasoning capabilities.39 Kresevic et al. customized prompts \nto help the model interpret structured guidelines, combined \nwith few-shot learning using 54 question-answer pairs.27\nJeong et al. fine-tuned LLMs to assess the relevance of \nretrieved evidence, ensure all statements were evidence-based, \nand confirm that the response effectively addressed the \nquery.34\nEvaluation\nNine studies used human evaluation, 8 relied on automated \nevaluation (eg, similarity comparisons between generated \nsentences and original answers), and 3 used a mix of both. \nOutcomes from human evaluation showed an overall OR of \n1.65 (95% CI: 1.36-2.03), while automatic evaluation \nresulted in an OR of 1.20 (95% CI: 1.1-1.41). The differen\xad\nces between the 2 were statistically significant (P < .01). \nThere were 4 human evaluators on average, with the range \nspanning from 1 to 10. Most human evaluators were \nphysicians from relevant specialties according to the \nstudy focus. In one case, 3 diabetic patients were involved in \nevaluating the understandability of diabetes-related patient \nqueries.30\nTwelve studies used self-curated datasets focused on \nresearch tasks. Examples included the ClinicalQA bench\xad\nmark, which comprised 314 open-ended questions about \ntreatment guidelines and clinical calculations generated by \nphysicians,28 and 43 diabetes-related questions sourced from \nthe National Institute of Diabetes and Digestive and Kidney \nDiseases website.30 Simulated cases from medical examina\xad\ntions were also utilized.25\nThree studies used EHR \ndata.35,37,40 Six studies used public benchmark datasets, such \nUS board exam practice questions, MedMCQA29,34 and \nlongform question-answering benchmarks (eg, LiveQA, Med\xad\nicationQA).34 The self-curated datasets averaged 76 ques\xad\ntions, ranging from 7 to 314. The length of public benchmark \ndatasets varied significantly, from 102 questions in the \nLiveQA dataset28 to 194 000 questions in the MedMCQA \ndataset.34\nMost studies reported evaluation metrics for the final \nresponse generation, while 4 (25%) also included specific \nmetrics to evaluate the retrieval process. For instance, 1 study \nmeasured recall in context retrieval,24 another evaluated \nretrieval accuracy,33 and a fine-tuned LLM was developed to \nassess the relevance of retrieved information to the user’s \nquery.34 Additionally, 1 study evaluated the accuracy of \nusing LLMs to extract text from figures and tables during \ndocument preprocessing.27 The final evaluation metrics \nfocused on the generated responses, consistent with those \nused in LLM-only systems. These metrics could be catego\xad\nrized as accuracy, completeness, user perception, safety, hal\xad\nlucination, citation, bias, and language. Accuracy was the \nmost frequently reported metric, covering Likert scale rat\xad\nings, match rates, correct treatment percentages,9 AUC, \nAUPRC, and F1 scores, as well as text similarity metrics like \nROUGE \n(ROUGE-1, \nROUGE-2, \nROUGE-L), \nBLEU, \nMETEOR, and BERTScore,21\nwhich compared LLM- \ngenerated responses to expert-provided answers. Complete\xad\nness metrics assessed whether responses included all neces\xad\nsary information, typically using Likert scales. User \nperception captured subjective feedback from both healthcare \nproviders and patients on understandability, helpfulness, and \nwhether responses met user intent, usually using Likert scales. \nSafety metrics focused both on user-related and system- \nrelated aspects. These metrics assessed potential harm, adver\xad\nsarial safety, and risk management,24 ensuring that outputs \nwere free of harmful content or risks. Scientific validity and \nadherence to evidence were also evaluated.29 One study used \nadversarial prompting, defined as intentionally adding harm\xad\nful directives to a prompt, to evaluate the safety of the RAG \nsystem.28 Hallucinations were primarily identified through \nmanual review, with definitions varying across studies. Some \nstudies defined hallucinations as nonfactual information, \nwhile one study added 2 other types of hallucinations: input- \nconflicting (content deviating from user-provided input) and \ncontextual-conflicting (content conflicting with previously \ngenerated information).27,41 Citation metrics measured the \naccuracy of provided references, with valid references consid\xad\nered those that pointed to established publications, guide\xad\nlines, or research. Bias and language were evaluated for \nclarity and neutrality, ensuring responses were unbiased and \nempathetic to patient concerns.24\nDiscussion\nThis study presents a systematic review of current research \non RAG for clinical tasks. Overall, RAG implementation \nincreased outcomes by 1.35 times compared to baseline \nLLM. We analyzed clinical tasks, baseline LLMs, retrieval \nsources and strategies, as well as evaluation methods. Despite \nthe potential benefits of RAG systems, there remains room \nfor improvement. Building on our literature review, we devel\xad\noped GUIDE-RAG (Guidelines for Unified Implementation \nand Development of Enhanced LLM Applications with RAG \nin Clinical Settings) for future clinical applications using \nRAG (Figure 4).\nGUIDE-RAG:\n1) Define clear clinical tasks and evaluation datasets. \nFuture research should clearly define clinical tasks and \nquestions to maximize the effectiveness of RAGs. Ambi\xad\nguity in questions can hinder performance, particularly \nin less powerful LLMs, making it challenging to achieve \nsignificant improvements in responses generation, even \nwith improved knowledge selection.42 For example, one \nstudy in the review constructed the evaluation dataset \nusing 30 case reports on rare diseases from PubMed.38\nThe authors did not report human performance on the \nself-build dataset. The questions themselves might have \nbeen inherently challenging or ambiguous. As expected, \nthe reported performance showed modest improvement, \nwith an OR of 1.31. \n2) Identify appropriate external resources for specific clini\xad\ncal tasks. \nThe first step in developing a RAG-based clinical system \nis to identify external resources that fill the knowledge \ngaps of the baseline LLM in relation to specific clinical \ntasks. The external knowledge should complement the \nLLM’s existing capabilities to effectively address task \n610                                                                                                      \nJournal of the American Medical Informatics Association, 2025, Vol. 32, No. 4 \nDownloaded from https://academic.oup.com/jamia/article/32/4/605/7954485 by guest on 11 January 2026'
 'Research and Applications\nImproving large language model applications in \nbiomedicine with retrieval-augmented generation: a \nsystematic review, meta-analysis, and clinical \ndevelopment guidelines\nSiru Liu, PhD\x03,1,2, Allison B. McCoy \n, PhD1, Adam Wright \n, PhD1,3 \n1Department of Biomedical Informatics, Vanderbilt University Medical Center, Nashville, TN 37212, United States, 2Department of Computer \nScience, Vanderbilt University, Nashville, TN 37212, United States, 3Department of Medicine, Vanderbilt University Medical Center, \nNashville, TN 37212, United States\n\x03Corresponding author: Siru Liu, PhD, Department of Biomedical Informatics, Vanderbilt University Medical Center, 2525 West End Ave #1475, Nashville, \nTN 37212, United States (siru.liu@vumc.org)\nAbstract\nObjective: The objectives of this study are to synthesize findings from recent research of retrieval-augmented generation (RAG) and large lan\xad\nguage models (LLMs) in biomedicine and provide clinical development guidelines to improve effectiveness.\nMaterials and Methods: We conducted a systematic literature review and a meta-analysis. The report was created in adherence to the Pre\xad\nferred Reporting Items for Systematic Reviews and Meta-Analyses 2020 analysis. Searches were performed in 3 databases (PubMed, Embase, \nPsycINFO) using terms related to “retrieval augmented generation” and “large language model,” for articles published in 2023 and 2024. We \nselected studies that compared baseline LLM performance with RAG performance. We developed a random-effect meta-analysis model, using \nodds ratio as the effect size.\nResults: Among 335 studies, 20 were included in this literature review. The pooled effect size was 1.35, with a 95% confidence interval of \n1.19-1.53, indicating a statistically significant effect (P ¼ .001). We reported clinical tasks, baseline LLMs, retrieval sources and strategies, as \nwell as evaluation methods.\nDiscussion: Building on our literature review, we developed Guidelines for Unified Implementation and Development of Enhanced LLM Appli\xad\ncations with RAG in Clinical Settings to inform clinical applications using RAG.\nConclusion: Overall, RAG implementation showed a 1.35 odds ratio increase in performance compared to baseline LLMs. Future research \nshould focus on (1) system-level enhancement: the combination of RAG and agent, (2) knowledge-level enhancement: deep integration of \nknowledge into LLM, and (3) integration-level enhancement: integrating RAG systems within electronic health records.\nKey words: large language model; retrieval augmented generation; systematic review; meta-analysis. \nIntroduction\nLarge language models (LLMs) have reported remarkable \nperformance in question-answering, summarization, and text \ngeneration.1 Given this, researchers have explored its poten\xad\ntial in biomedical areas.2 For example, several studies \nreported the ability of using LLM to answer patient mes\xad\nsages,3 to analyze alert logic in clinical decision support,4 and \nto make discharge summaries more readable to patients.5\nHowever, several challenges remain.\nLLMs are trained on fixed datasets, which restrict their \nknowledge to information available up to the training cut-off \ndate. For example, GPT-4o’s training data only includes \ninformation up to October 2023, making it unable to \nrespond accurately to findings that emerged afterward. LLM \ntraining datasets are also generally broad and lack the specif\xad\nicity required for biomedical applications. Finally, not all \nsources used to train the LLMs are reliable and trustworthy. \nTo address these limitations, researchers have performed \nfine-tuning and retrieval-augmented generation (RAG) tech\xad\nniques. Fine-tuning can adapt LLMs to specific domains, but \nit is resource-intensive and does not allow for real-time \nupdates. In contrast, RAG maintains the original LLM archi\xad\ntecture while incorporating relevant context directly into \nqueries, offering more flexibility and control. In addition, \nRAG’s unique advantage in biomedical applications lies in its \nability to adapt to dynamic environments by delivering up-to- \ndate information and efficiently integrating external knowl\xad\nedge sources with high interpretability.6\nAnother limitation of using LLMs directly is the risk of hal\xad\nlucination, where the model generates incorrect or fabricated \ninformation.7 To mitigate such issues, researchers have pro\xad\nposed RAG as a solution that integrates up-to-date, relevant \ninformation, enhancing both the accuracy and reliability of \nLLM generated responses.8,9 For example, when ChatGPT \nReceived: November 19, 2024; Revised: December 17, 2024; Editorial Decision: January 2, 2025; Accepted: January 3, 2025 \n© The Author(s) 2025. Published by Oxford University Press on behalf of the American Medical Informatics Association.  \nThis is an Open Access article distributed under the terms of the Creative Commons Attribution-NonCommercial License (https://creativecommons.org/licenses/ \nby-nc/4.0/), which permits non-commercial re-use, distribution, and reproduction in any medium, provided the original work is properly cited. For commercial \nre-use, please contact reprints@oup.com for reprints and translation rights for reprints. All other permissions can be obtained through our RightsLink service via \nthe Permissions link on the article page on our site—for further information please contact journals.permissions@oup.com. \nJournal of the American Medical Informatics Association, 2025, 32(4), 605–615 \nhttps://doi.org/10.1093/jamia/ocaf008 \nAdvance access publication 15 January 2025 \nResearch and Applications \nDownloaded from https://academic.oup.com/jamia/article/32/4/605/7954485 by guest on 11 January 2026']"
How does the system leverage cypher query generation and multi-hop reasoning with confidence scoring to ensure accurate and reliable biomedical question answering over the iBKH knowledge graph?,"['<1-hop>\n\nOmar & Mohammed / Journal of Applied Science and Technology Trends Vol. 06, No. 02, pp. 342 –357 (2025) 351 This query searches for a disease node whose name contains ’alzheimer’ and follows HAS_SYMPTOM edges to list related symptom names. The system then executes this cypher to retrieve answers. The prompts (such as few-shot examples and schema hints) were carefully designed to help LLaMA 3 generate correct Cypher queries. The model learns how to use the graph’s labels and relationships properly. For example, if a user asks, ’Which drugs treat diabetes?’, LLaMA might respond with a suitable Cypher query: MATCH (d:Drug)-[:TREATS]->(di:Disease) WHERE toLower(di.name) CONTAINS ""diabetes"" RETURN d.name This queries for drug nodes that have a TREATS edge to a diabetes disease node. By leveraging LLaMA 3 in this way, our system can flexibly handle many phrasing variations without manual mapping rules. D. Model Configuration & Decoding We run a local LLaMA 3.2-3B model in GGUF format (llama- 3.2-3b-instruct-q4_k_m.gguf) via llama.cpp, as shown in Table IV. TABLE IV. MODEL RUNTIME AND DECODING SETTINGS Runtime settings Decoding settings n_ctx = 1024 temperature = 0.2 n_threads = 12 top_p = 0.95 n_gpu_layers = 33 top_k = 40 n_batch = 512 repeat_penalty = 1.1 max_tokens = 80 seed = 42 E. Graph Subset and Versioning We use an iBKH derived subgraph (≈65.8k nodes; ≈3.0M edges) spanning DRUG, DISEASE, SYMPTOM, PATHWAY. IDs are normalized to CURIEs and duplicates collapsed across UMLS/DrugBank/DisGeNET/SIDER/KEGG. Each edge stores provenance/licensing metadata (source, source_version, license, retrieved_at, evidence_pmids/urls) and, when available, temporal fields (valid_from, valid_to). We report coverage as the percentage of evaluated questions whose gold entities/relations are present. F. Query Execution and Reliability After a Cypher query is generated, it is executed on the Neo4j database through the official Neo4j Python driver, which manages the secure connection and returns the results. Instead of restricting the output with a fixed LIMIT (e.g., LIMIT 5), the system retrieves candidate results and evaluates them using standardized retrieval metrics such as Hits@1, Hits@5, and Hits@10. This approach ensures that the system remains responsive while providing a fair assessment of ranking quality across different cutoff levels, rather than depending solely on a fixed number of returned items. Neo4j’s indexing on key node properties, such as name and primary identifiers, also helps speed up lookups as the knowledge graph grows. In cases where the language model generates an incomplete or incorrect query, such as referencing nodes or relationships that do not exist, the system catches the error and either retries with a simpler prompt or informs the user. Together, these steps make sure that queries run quickly, return valid results, and keep the overall experience smooth and reliable for biomedical question-answering. G. User Interface for Query Transparency The system includes a lightweight, cross-platform graphical user interface (GUI) implemented as a web application using the Flask framework in Python, with HTML and Bootstrap for interactive visualization. The interface is designed to make the question-answering process transparent and accessible to users without technical expertise. It consists of three main panels: 1. Input Panel: Where the user can enter a biomedical question in natural language. 2. Query Panel: Which displays the Cypher query generated by the language model, allowing users to verify how their question was interpreted. 3. Results Panel: Which presents the retrieved answers in a clear, readable format, accompanied by a brief natural language explanation generated by the system. By showing both the query and the answer, the GUI promotes user trust and enables validation of the system’s reasoning process. The interface is lightweight enough to run smoothly on standard desktop machines without additional dependencies, making it practical for local deployments in clinical or research settings. Fig. 4 illustrates the overall layout.'
 '<2-hop>\n\nV. EXPERIMENTAL RESULTS In this section, the proposed system is evaluated based on its ability to translate natural-language biomedical questions into executable Cypher queries over the iBKH knowledge graph. The assessment is conducted end-to-end and focuses on three main aspects: (i) the accuracy of query generation, measured by the proportion of correctly produced Cypher queries; (ii) system efficiency, quantified through total response time from question submission to final answer delivery, covering both query generation and execution; and (iii) the quality of retrieved answers, evaluated using standardized information retrieval metrics including Precision@k, Recall@k, F1@k, Hits@k, and Mean Reciprocal Rank (MRR). For clarity, all these metrics are formally defined in the following subsections, with their corresponding equations, and will be used consistently throughout the results section. Together, these dimensions provide a comprehensive view of both correctness and efficiency across simple, medium, and complex biomedical queries. A. Description of the Experimental Data To evaluate the proposed system, this work used a carefully selected set of biomedical questions designed to test how well the system understands natural language and converts it into Cypher queries for a biomedical knowledge graph. To evaluate the system, a benchmark of 60 biomedical questions was constructed by the authors, guided by the schema and relations in iBKH. The questions were grouped into simple, medium, and complex levels to assess performance across different reasoning requirements. Gold-standard answers were manually prepared for each question to enable quantitative evaluation using Exact Match, Precision@k, Recall@k, F1@k, Hits@k, and MRR. The complete set of 60 questions is available at this link https://drive.google.com/drive/my-drive.'
 '<3-hop>\n\nOmar & Mohammed / Journal of Applied Science and Technology Trends Vol. 06, No. 02, pp. 342 –357 (2025) 345 record-high accuracy, while open-source models achieved impressive gains through prompt optimization. Feng et al.[22] developed the Knowledge Graph-based Thought (KGT) framework that integrated LLMs with a pan- cancer knowledge graph for biomedical question answering. KGT was designed to reason on the knowledge graph schema and identify optimal subgraphs to use for directing accurate answer generation, all without fine-tuning the LLMs. The framework is benchmarked against a new dataset (PcQA) designed specifically for pan-cancer KGQA tasks and has outperformed all existing state-of-the-art approaches by a rather large margin. KGT’s practicality in biomedical issues was highlighted through case studies for drug repositioning, drug resistance, and biomarker discovery. Their approach exhibited robust adaptability among various LLMs. Rezaei et al.[26] developed AMG-RAG, a dynamic framework that utilizes autonomous LLM agents with medical search tools in the continuous construction and real-time updating of Medical Knowledge Graphs (MKGs). Their system incorporated confidence scoring and multi-hop reasoning to improve accuracy and interpretability in medical question answering. AMG-RAG outperformed size models on both very hard MEDQA benchmarks and more accessible MedMCQA ones, proving that it could conduct efficient reasoning based on current structured medical knowledge. They also used Neo4j to manage the knowledge graphs while adding external searches to ensure the latest data. Tiwari et al.[24] presented Auto-Cypher, a recent automated pipeline for producing high-quality synthetic data for training LLMs by mapping natural language to Cypher queries for graph databases like Neo4j. The pipeline deployed the novelty of LLM- as-database-filler to synthesize Neo4j databases for the execution of generated queries to ensure their correctness. A sizable dataset called SynthCypher was created, spanning multiple domains and complex queries, leading to a 40% improvement in LLM performance on Cypher generation. The datasets were used to fine-tune open-source models such as LLaMA, Mistral, and Qwen, and the SPIDER benchmark was adapted for evaluation purposes. Mohammed et al.[29] proposed a hybridized GraphRAG framework combining Neo4j-based UMLS knowledge graphs with a vector store for medical textbooks to create an improved U.S.M.L.E.-style clinical question-answering approach. The project integrated symbolic reasoning from knowledge graphs with semantic retrieval performed on text embeddings to enhance relevance and accuracy via adaptive re-ranking and query expansion. The system had the answers produced by GPT-4o- Mini, with different prompting strategies encouraging evidence- based and traceable responses grounded in verified medical knowledge. Experiments showed that the hybrid approach improved factual accuracy and citation fidelity as compared to the L.L.M.-only approach, enhancing transparency and reliability. It is shown that binding both structured and unstructured medical knowledge sources could aid in ameliorating hallucinations and hence improve clinical trustworthiness in AI-driven medical QA. Yang et al.[30] articulated sepsis knowledge graph was crafted by combining multicenter clinical data from over 10,000 patients with the help of GPT-4 for entity recognition and relationship extraction. Real-world data were collected from three hospitals and integrated with clinical guidelines and databases from the public domain. The knowledge graph contained 1,894 nodes and 2,021 relationships pertaining to diseases, symptoms, biomarkers, treatments, and complications. GPT outperformed other models in every resolution on sepsis- specific datasets to obtain high F1-score results. The constructed graph highlighted complex interactions in sepsis for assisting clinical decision-making and was implemented on Neo4j. Guan et al.[20] proposed a novel method for constructing a local knowledge graph from retrieved biomedical documents by extracting propositional claims. They carried out layer wise summarization on this graph to capture multi-document relationships and provide comprehensive contextual information to a language model for question-answering purposes. The method resolved issues in multi-document biomedical QA, such as noise cancellation and efficient context usage. They then tested their method on several benchmarks for biomedical question answering, achieving performance at least comparable to, if not better than, existing retrieval-augmented generation (RAG) baselines. The study established enhanced reasoning and answer accuracy of the model achieved through structured graph summarization. Previous studies have improved biomedical QA using KGs and LLMs, but important gaps remain. Most systems lack transparent, graph-based justifications, rely on limited evaluation methods, or depend on cloud resources that reduce privacy and reproducibility. Our framework addresses these gaps by providing visible Cypher queries with evidence subgraphs, applying comprehensive performance metrics across difficulty levels, and ensuring fully local, privacy-preserving deployment. Table I summarizes key previous studies on biomedical knowledge graphs and question answering, outlining their methods, datasets, and main limitations.']","The system uses LLaMA 3 to generate Cypher queries by learning to utilize the graph's labels and relationships properly, enabling flexible handling of various natural language biomedical questions without manual mapping rules. After generating a Cypher query, it is executed on the Neo4j database, which manages secure connections and returns results efficiently. The system evaluates retrieved answers using standardized metrics such as Hits@k and Mean Reciprocal Rank (MRR) to assess ranking quality rather than relying on fixed limits, ensuring responsiveness and fairness. Additionally, the system catches errors from incomplete or incorrect queries and retries with simpler prompts or informs the user, maintaining reliability. Multi-hop reasoning and confidence scoring are incorporated as described in related frameworks like AMG-RAG, which utilize autonomous LLM agents and medical search tools to improve accuracy and interpretability in medical question answering. This combination of transparent Cypher query generation, multi-hop reasoning, and confidence scoring over the iBKH knowledge graph enables accurate, efficient, and trustworthy biomedical question answering.",multi_hop_abstract_query_synthesizer,"The system ensures accurate and reliable biomedical question answering over the iBKH knowledge graph by combining Cypher query generation with multi-hop reasoning and confidence scoring in the following ways:

1. **Cypher Query Generation via LLaMA-3B**  
   - The system uses a locally run LLaMA-3B large language model to translate natural language biomedical questions into executable Cypher queries.  
   - Carefully designed prompts, including few-shot examples and schema hints, guide the LLaMA model to generate syntactically correct and semantically meaningful Cypher queries that properly utilize the graph’s node labels and relationships (e.g., `MATCH (d:Drug)-[:TREATS]->(di:Disease) WHERE toLower(di.name) CONTAINS ""diabetes"" RETURN d.name`).  
   - This approach allows flexible handling of diverse question phrasings without manual rule-based mappings, improving robustness and adaptability.

2. **Multi-hop Reasoning on the iBKH Knowledge Graph**  
   - The iBKH-derived Neo4j graph contains over 65,000 entities and 3 million relationships spanning drugs, diseases, symptoms, and pathways, enabling complex multi-hop reasoning.  
   - The Cypher queries generated can traverse multiple edges and nodes to infer indirect relationships, supporting answers that require reasoning over several connected biomedical concepts.  
   - This multi-hop capability is essential for answering medium and complex questions that involve chains of biomedical facts.

3. **Confidence Scoring and Result Evaluation**  
   - After executing the Cypher queries on the Neo4j database, the system retrieves candidate answers without imposing a fixed limit, allowing comprehensive result sets.  
   - It then evaluates the retrieved answers using standardized information retrieval metrics such as Precision@k, Recall@k, F1@k, Hits@k, and Mean Reciprocal Rank (MRR). These metrics serve as confidence scores reflecting the quality and relevance of the answers.  
   - This evaluation ensures that the system ranks answers correctly and retrieves nearly all relevant information, contributing to reliability.

4. **Error Handling and Query Validation**  
   - If the language model generates incomplete or incorrect Cypher queries (e.g., referencing non-existent nodes or relationships), the system detects these errors during query execution.  
   - It either retries query generation with simpler prompts or informs the user, preventing invalid queries from producing misleading answers.  
   - This mechanism maintains the integrity and accuracy of the question-answering process.

5. **Transparent User Interface for Verification**  
   - The system provides a graphical user interface that displays the original natural language question, the generated Cypher query, and the retrieved answers with supporting evidence.  
   - This transparency allows users to verify how their question was interpreted and understand the reasoning path taken by the system, fostering trust and enabling validation of the answers.

6. **Offline Operation for Privacy and Reproducibility**  
   - Unlike cloud-based LLM APIs, the system operates fully offline, ensuring data privacy and compliance with biomedical data governance.  
   - This local deployment also guarantees reproducibility and consistent performance without reliance on external services.

**Summary:**  
By integrating LLaMA-3B for precise Cypher query generation, leveraging the rich multi-hop relational structure of the iBKH knowledge graph, applying rigorous confidence scoring through retrieval metrics, and incorporating error handling and transparent user interfaces, the system achieves accurate, interpretable, and reliable biomedical question answering. This hybrid approach effectively mitigates LLM hallucinations and enhances reasoning capabilities, making it suitable for sensitive healthcare applications.","['Vol. 06, No. 02, pp. 342 –357 (2025) \nISSN: 2708-0757 \n \nJOURNAL OF APPLIED SCIENCE AND TECHNOLOGY TRENDS \n \nwww.jastt.org  \n \n                                                          342 \ndoi: 10.38094/jastt62404  \nA Hybrid LLM–Knowledge Graph Framework for Accurate \nBiomedical Question Answering \n \nHavraz Y. Omar1,²*, Abdulhakeem O. Mohammed³  \n \n¹Department of Information Technology, Technical College of Duhok, Duhok Polytechnic University, Duhok, Kurdistan Region, \nIraq. havraz.omar@dpu.edu.krd  \n² Department of Information Technology, Technical College of Informatics – Akre, Akre University for Applied Sciences, Akre, \nKurdistan Region, Iraq. \n³ Department of Computer Science, College of Science, University of Zakho, Zakho, Kurdistan Region, Iraq. \na.mohammed@uoz.edu.krd \n \n*Correspondence: havraz.omar@dpu.edu.krd \n \nAbstract \nBiomedical question answering requires accurate and interpretable systems; however, existing approaches often face challenges such as \nlanguage model hallucinations and limited reasoning when relying solely on standalone knowledge graphs. To address these limitations, \nthis study proposes a hybrid framework that integrates the LLaMA-3B language model with a Neo4j-based drug–disease–symptom \nknowledge graph. The system translates natural language questions into executable Cypher queries, operates on an iBKH-derived graph \ncomprising over 65,000 entities and 3 million relationships, and returns answers with supporting evidence through a transparent interface. \nExperiments conducted on 60 biomedical questions across three levels of difficulty demonstrate the robustness of the approach: 96% \nexact match for simple queries, 95% for medium queries, and 86.7% for complex queries. Overall, the system achieves Precision@5 of \n96.1%, Recall@5 of 89.0%, F1@5 of 91.0%, Hits@k of 96.1%, and an MRR of 94.4%, while maintaining an average response time of \nonly 6.07 seconds. These results indicate that the system retrieves nearly all relevant answers, ranks them correctly, and delivers them \nwith latency low enough for interactive use. Moreover, unlike cloud-based APIs such as ChatGPT, which require internet connectivity \nand external data transmission, the proposed framework operates fully offline, ensuring privacy, reproducibility, and compliance with \nbiomedical data governance. Overall, this pipeline provides an accurate, efficient, and privacy-preserving solution for biomedical question \nanswering, making it a practical alternative to cloud-dependent approaches in sensitive healthcare contexts. \n \nKeywords: Knowledge Graph, LLM, Question Answering, Neo4j, Biomedical Informatics, Healthcare AI, LLaMA 3. \n \nReceived: August 14th, 2025 / Revised: October 10th, 2025 / Accepted: October 16th, 2025 / Online: October 20th, 2025 \n \nI. INTRODUCTION \nAnswering questions in the biomedical field is a difficult task \ndue to the complexity of medical knowledge and the need for \nprecision. In recent years, large language models  (LLMs) like \nLLaMA, GPT-4 have made progress in understanding and \ngenerating human-like responses to medical questions  [1,  2]. \nThese models can process large amounts of information and \nrespond in natural language, which makes them helpful in \nhealthcare settings [3]. However, they often struggle to provide \naccurate answers when dealing with specialized biomedical \ncontent [4, 5]. \nOne major issue with LLMs is a problem called hallucination, \nwhere the model generates information that sounds right but is \nactually incorrect or unsupported [6]. In medical applications, \nthis can be dangerous, as healthcare professionals rely on precise \nand trustworthy information  [7]. Therefore, researchers are \nexploring ways to combine LLMs with structured sources of \nknowledge to improve their reliability [8]. \nLLM-only systems in biomedicine still hallucinate and are \nhard to verify, limiting safe use [9, 10]. Biomedical knowledge \ngraphs (BKGs) such as iBKH and SPOKE curate multi-source \nfacts and enable multi-hop reasoning, yet they neither interpret \nfree text nor generate answers [11, 12]. Recent hybrids (KG-\naware RAG) improve grounding but often lack explicit path-level \njustifications and robust end-to-end answer evaluation [13, 14]. \nRecent studies have increasingly integrated Knowledge \nGraphs (KGs) with Large Language Models (LLMs) to improve \nfactual accuracy, reasoning, and reduce hallucinations. Notable \nexamples include DR.KNOWS, which combines UMLS-based \nKGs with LLMs for better diagnostic reasoning [15], KnowNet'
 'Omar & Mohammed / Journal of Applied Science and Technology Trends Vol. 06, No. 02, pp. 342 –357 (2025) \n \n351 \nThis query searches for a disease node whose name contains \n’alzheimer’ and follows HAS_SYMPTOM edges to list related \nsymptom names. The system then executes this cypher to retrieve \nanswers. The prompts (such as few-shot examples and schema \nhints) were carefully designed to help LLaMA 3 generate correct \nCypher queries. The model learns how to use the graph’s labels \nand relationships properly. For example, if a user asks, ’Which \ndrugs treat diabetes?’, LLaMA might respond with a suitable \nCypher query: \n \nMATCH (d:Drug)-[:TREATS]->(di:Disease) \nWHERE toLower(di.name) CONTAINS ""diabetes"" \nRETURN d.name  \n \nThis queries for drug nodes that have a TREATS edge to a \ndiabetes disease node. By leveraging LLaMA 3 in this way, our \nsystem can flexibly handle many phrasing variations without \nmanual mapping rules. \nD. Model Configuration & Decoding \nWe run a local LLaMA 3.2-3B model in GGUF format (llama-\n3.2-3b-instruct-q4_k_m.gguf) via llama.cpp, as shown in Table \nIV. \nTABLE IV.  MODEL RUNTIME AND DECODING SETTINGS \nRuntime settings \nDecoding settings \nn_ctx = 1024 \ntemperature = 0.2 \nn_threads = 12 \ntop_p = 0.95 \nn_gpu_layers = 33 \ntop_k = 40 \nn_batch = 512 \nrepeat_penalty = 1.1 \n \nmax_tokens = 80 \n \nseed = 42 \n \nE. Graph Subset and Versioning \nWe use an iBKH derived subgraph (≈65.8k nodes; ≈3.0M \nedges) spanning DRUG, DISEASE, SYMPTOM, PATHWAY. IDs are \nnormalized to CURIEs and duplicates collapsed across \nUMLS/DrugBank/DisGeNET/SIDER/KEGG. Each edge stores \nprovenance/licensing metadata (source, source_version, license, \nretrieved_at, \nevidence_pmids/urls) \nand, \nwhen \navailable, \ntemporal fields (valid_from, valid_to). We report coverage as \nthe \npercentage \nof \nevaluated \nquestions \nwhose \ngold \nentities/relations are present. \nF. Query Execution and Reliability \nAfter a Cypher query is generated, it is executed on the Neo4j \ndatabase through the official Neo4j Python driver, which \nmanages the secure connection and returns the results. Instead of \nrestricting the output with a fixed LIMIT (e.g., LIMIT 5), the \nsystem retrieves candidate results and evaluates them using \nstandardized retrieval metrics such as Hits@1, Hits@5, and \nHits@10. This approach ensures that the system remains \nresponsive while providing a fair assessment of ranking quality \nacross different cutoff levels, rather than depending solely on a \nfixed number of returned items. Neo4j’s indexing on key node \nproperties, such as name and primary identifiers, also helps speed \nup lookups as the knowledge graph grows. In cases where the \nlanguage model generates an incomplete or incorrect query, such \nas referencing nodes or relationships that do not exist, the system \ncatches the error and either retries with a simpler prompt or \ninforms the user. Together, these steps make sure that queries run \nquickly, return valid results, and keep the overall experience \nsmooth and reliable for biomedical question-answering. \nG. User Interface for Query Transparency \nThe system includes a lightweight, cross-platform graphical \nuser interface (GUI) implemented as a web application using the \nFlask framework in Python, with HTML and Bootstrap for \ninteractive visualization. The interface is designed to make the \nquestion-answering process transparent and accessible to users \nwithout technical expertise. It consists of three main panels: \n1. Input Panel: Where the user can enter a biomedical \nquestion in natural language. \n2. Query Panel: Which displays the Cypher query generated \nby the language model, allowing users to verify how their \nquestion was interpreted. \n3. Results Panel: Which presents the retrieved answers in a \nclear, readable format, accompanied by a brief natural \nlanguage explanation generated by the system. \nBy showing both the query and the answer, the GUI promotes \nuser trust and enables validation of the system’s reasoning \nprocess. The interface is lightweight enough to run smoothly on \nstandard desktop machines without additional dependencies, \nmaking it practical for local deployments in clinical or research \nsettings. Fig. 4 illustrates the overall layout. \nV. EXPERIMENTAL RESULTS \nIn this section, the proposed system is evaluated based on its \nability to translate natural-language biomedical questions into \nexecutable Cypher queries over the iBKH knowledge graph. The \nassessment is conducted end-to-end and focuses on three main \naspects: (i) the accuracy of query generation, measured by the \nproportion of correctly produced Cypher queries; (ii) system \nefficiency, quantified through total response time from question \nsubmission to final answer delivery, covering both query \ngeneration and execution; and (iii) the quality of retrieved \nanswers, evaluated using standardized information retrieval \nmetrics including Precision@k, Recall@k, F1@k, Hits@k, and \nMean Reciprocal Rank (MRR). For clarity, all these metrics are \nformally defined in the following subsections, with their \ncorresponding equations, and will be used consistently \nthroughout the results section. Together, these dimensions \nprovide a comprehensive view of both correctness and efficiency \nacross simple, medium, and complex biomedical queries. \nA. Description of the Experimental Data \nTo evaluate the proposed system, this work used a carefully \nselected set of biomedical questions designed to test how well the \nsystem understands natural language and converts it into Cypher \nqueries for a biomedical knowledge graph. \nTo evaluate the system, a benchmark of 60 biomedical \nquestions was constructed by the authors, guided by the schema \nand relations in iBKH. The questions were grouped into simple, \nmedium, and complex levels to assess performance across \ndifferent reasoning requirements. Gold-standard answers were \nmanually prepared for each question to enable quantitative \nevaluation using Exact Match, Precision@k, Recall@k, F1@k, \nHits@k, and MRR. The complete set of 60 questions is available \nat this link https://drive.google.com/drive/my-drive.'
 'Omar & Mohammed / Journal of Applied Science and Technology Trends Vol. 06, No. 02, pp. 342 –357 (2025) \n \n345 \nrecord-high accuracy, while open-source models achieved \nimpressive gains through prompt optimization. \nFeng et al.[22] developed the Knowledge Graph-based \nThought (KGT) framework that integrated LLMs with a pan-\ncancer knowledge graph for biomedical question answering. \nKGT was designed to reason on the knowledge graph schema and \nidentify optimal subgraphs to use for directing accurate answer \ngeneration, all without fine-tuning the LLMs. The framework is \nbenchmarked against a new dataset (PcQA) designed specifically \nfor pan-cancer KGQA tasks and has outperformed all existing \nstate-of-the-art approaches by a rather large margin. KGT’s \npracticality in biomedical issues was highlighted through case \nstudies for drug repositioning, drug resistance, and biomarker \ndiscovery. Their approach exhibited robust adaptability among \nvarious LLMs. \nRezaei et al.[26] developed AMG-RAG, a dynamic \nframework that utilizes autonomous LLM agents with medical \nsearch tools in the continuous construction and real-time updating \nof Medical Knowledge Graphs (MKGs). Their system \nincorporated confidence scoring and multi-hop reasoning to \nimprove accuracy and interpretability in medical question \nanswering. AMG-RAG outperformed size models on both very \nhard MEDQA benchmarks and more accessible MedMCQA \nones, proving that it could conduct efficient reasoning based on \ncurrent structured medical knowledge. They also used Neo4j to \nmanage the knowledge graphs while adding external searches to \nensure the latest data. \nTiwari et al.[24] presented Auto-Cypher, a recent automated \npipeline for producing high-quality synthetic data for training \nLLMs by mapping natural language to Cypher queries for graph \ndatabases like Neo4j. The pipeline deployed the novelty of LLM-\nas-database-filler to synthesize Neo4j databases for the execution \nof generated queries to ensure their correctness. A sizable dataset \ncalled SynthCypher was created, spanning multiple domains and \ncomplex queries, leading to a 40% improvement in LLM \nperformance on Cypher generation. The datasets were used to \nfine-tune open-source models such as LLaMA, Mistral, and \nQwen, and the SPIDER benchmark was adapted for evaluation \npurposes. \nMohammed et al.[29] proposed a hybridized GraphRAG \nframework combining Neo4j-based UMLS knowledge graphs \nwith a vector store for medical textbooks to create an improved \nU.S.M.L.E.-style clinical question-answering approach. The \nproject integrated symbolic reasoning from knowledge graphs \nwith semantic retrieval performed on text embeddings to enhance \nrelevance and accuracy via adaptive re-ranking and query \nexpansion. The system had the answers produced by GPT-4o-\nMini, with different prompting strategies encouraging evidence-\nbased and traceable responses grounded in verified medical \nknowledge. Experiments showed that the hybrid approach \nimproved factual accuracy and citation fidelity as compared to \nthe L.L.M.-only approach, enhancing transparency and \nreliability. It is shown that binding both structured and \nunstructured medical knowledge sources could aid in \nameliorating hallucinations and hence improve clinical \ntrustworthiness in AI-driven medical QA. \nYang et al.[30] articulated sepsis knowledge graph was \ncrafted by combining multicenter clinical data from over 10,000 \npatients with the help of GPT-4 for entity recognition and \nrelationship extraction. Real-world data were collected from \nthree hospitals and integrated with clinical guidelines and \ndatabases from the public domain. The knowledge graph \ncontained 1,894 nodes and 2,021 relationships pertaining to \ndiseases, symptoms, biomarkers, treatments, and complications. \nGPT outperformed other models in every resolution on sepsis-\nspecific datasets to obtain high F1-score results. The constructed \ngraph highlighted complex interactions in sepsis for assisting \nclinical decision-making and was implemented on Neo4j. \nGuan et al.[20] proposed a novel method for constructing a \nlocal knowledge graph from retrieved biomedical documents by \nextracting propositional claims. They carried out layer wise \nsummarization on this graph to capture multi-document \nrelationships and provide comprehensive contextual information \nto a language model for question-answering purposes. The \nmethod resolved issues in multi-document biomedical QA, such \nas noise cancellation and efficient context usage. They then tested \ntheir method on several benchmarks for biomedical question \nanswering, achieving performance at least comparable to, if not \nbetter than, existing retrieval-augmented generation (RAG) \nbaselines. The study established enhanced reasoning and answer \naccuracy of the model achieved through structured graph \nsummarization.   \nPrevious studies have improved biomedical QA using KGs \nand LLMs, but important gaps remain. Most systems lack \ntransparent, graph-based justifications, rely on limited evaluation \nmethods, or depend on cloud resources that reduce privacy and \nreproducibility. Our framework addresses these gaps by \nproviding visible Cypher queries with evidence subgraphs, \napplying comprehensive performance metrics across difficulty \nlevels, and ensuring fully local, privacy-preserving deployment. \nTable I summarizes key previous studies on biomedical \nknowledge graphs and question answering, outlining their \nmethods, datasets, and main limitations. \nIII. PRELIMINARIES \nThis section outlines the fundamental concepts required to \nunderstand the proposed system. It introduces biomedical \nknowledge graphs, explains how Neo4j stores data in graph \nform, and describes the use of Cypher for querying. It also \nprovides a brief overview of large language models (LLMs) and \ntheir role in interpreting natural language. \n \n \nA. Biomedical Knowledge Graphs \nBiomedical Knowledge Graphs (BKGs) provide a structured \nrepresentation of complex biomedical information by modeling \ndiverse medical entities, such as diseases, drugs, symptoms, and \nbiological pathways, as interconnected nodes within a graph \nstructure. The edges in these graphs represent the semantic \nrelationships between these entities, including ’treats’, ’causes’, \n’interacts with’ and many others, as illustrated in Fig 1. This \nform of representation enables the integration of heterogeneous \nbiomedical data from a wide range of sources, including']"
How does LLaMA 3 utilize the Integrated Biomedical Knowledge Hub (iBKH) dataset to improve the accuracy and transparency of biomedical question answering in the described system?,"['<1-hop>\n\nOmar & Mohammed / Journal of Applied Science and Technology Trends Vol. 06, No. 02, pp. 342 –357 (2025) 348 drugs, diseases, symptoms) and execution returns structured data (tuples) relevant to the question. Step 5. Answer Synthesis: The structured tuples flow to Answer Synthesis, which aggregates and formats them into a concise raw answer. This raw answer is sent back to LLaMA 3 to optionally refine the phrasing while preserving the retrieved facts. Step 6. Result Presentation: LLaMA 3 produces the final answer, which the interface displays together with the executed Cypher query and an optional preview of the returned rows, improving transparency and trust. The pipeline couples LLM-based language understanding (LLaMA 3) with a schema-grounded Neo4j knowledge graph. The Cypher Query Gen refines the query formulation, Query Execution retrieves evidence and Answer Synthesis converts structured results into readable outputs that produce answers that are accurate, interpretable, and easy to audit directly from the displayed query and evidence.'
 '<2-hop>\n\nB. Dataset and Knowledge Graph Construction 1) Dataset The integrated Biomedical Knowledge Hub (iBKH), a large biomedical knowledge base, forms the first level of the system and integrates information from various curated high-quality biomedical databases. This implies that the data set includes various types of entities, such as diseases, symptoms, drugs, biological pathways, etc. This study used the representative subset of the iBKH dataset, which contained 65828 biomedical entities. These entities are semantically interconnected through a total of 3004166 relationships, thus creating a rich knowledge graph. The iBKH dataset was originally introduced in [11], and it is freely available at (https://github.com/wcm-wanglab/iBKH). This dataset is the core semantic foundation upon which this study is built. The knowledge graph is populated from multiple tabular sources (CSV files), each listing entities or relationships. The main input files and their contents are as follows: • Disease vocabulary(disease_vocab.csv): Contains columns such as primary (a unique disease ID), name, do_id (Disease Ontology ID), kegg_id, and umls_cui (UMLS Concept ID). Each row represents a disease node with external identifiers. • Drug vocabulary (drug_vocab.csv): Includes primary (unique drug ID), name, drugbank_id, kegg_id, pharmgkb_id, umls_cui, mesh_id, iDISK_id and CID (PubChem ID). Each row defines a drug node with standard database identifiers. • Symptom vocabulary (symptom_vocab.csv): Contains primary (unique symptom ID), name, mesh_id, umls_cui and iDISK_id. Each row defines a symptom node. • Side effect vocabulary (side_effect_vocab.csv): Includes primary (unique side-effect ID) and name. Each row represents a side-effect node (with UMLS ID when available). • Pathway vocabulary (pathway_vocab.csv): Contains primary (unique pathway ID), name, reactome_id, go_id, and kegg_id. Each row defines a biological pathway node. Relationship files (each row typically contains two entity IDs and one or more boolean flags or codes) include: • Disease–Symptom links (Di_Sy_res.csv): Rows include Disease and Symptom IDs, a presence flag (1 or 0) and a data source. If Present = 1, a HAS_SYMPTOM edge is created from the disease to the symptom, with properties for presence and source. • Disease–Disease links (di_di_res.csv): Rows include Disease_1 and Disease_2 IDs with binary flags for is_a and Resemble. If is_a = 1, an (IS_A) edge is created (Disease_1 → Disease_2); if Resemble = 1, a RESEMBLES edge is created. The source field is used for provenance. • Drug–Disease links (D_Di_res.csv): Includes Drug and Disease IDs with several binary flags. If a flag equals 1, a corresponding edge is created: o TREATS (Treats = 1) o PALLIATES (Palliates = 1) o ASSOCIATED_WITH (Associate = 1) o ALLEVIATES_REDUCES (alleviates = 1) o TREATMENT_THERAPY (treatment/therapy = 1) o INHIBITS_CELL_GROWTH (inhibits cell growth = 1) o HAS_BIOMARKER (biomarkers = 1) o PREVENTS_SUPPRESSES (prevents/suppresses = 1) o ROLE_IN_PATHOGENESIS (role in disease pathogenesis = 1) • Drug–SideEffect links (D_SE_res.csv): Contains Drug and SideEffect IDs with a Source column. Each row creates a CAUSES edge from the drug to the side effect, with source as an edge property. • Drug–Drug interactions (D_D_res.csv): Rows include Drug_1 and Drug_2 IDs with flags for Interaction and Resemble. If Interaction = 1, an INTERACTS_WITH edge is created (bidirectional). If Resemble = 1, a RESEMBLES edge is added. • Drug–Pathway links (D_Pwy_res.csv): Includes Drug ID and Pathway ID. Each row generates an ASSOCIATED_WITH edge from the drug to the pathway. • Disease–Pathway links (Di_Pwy_res.csv): Contains Disease ID and Pathway ID. Each row creates an ASSOCIATED_WITH edge from the disease to the pathway. 2) Data Upload Performance The time required to upload different types of entities and relationships into the Neo4j biomedical knowledge graph, measured in seconds. These measurements reflect both the size and complexity of the data being processed. As shown in Table II, the longest upload time is for Drug- Drug Relationships, which takes approximately 190 seconds due to the large number of edges (over 3 million). Following this, Disease-Disease and Drug-Disease Relationships also require considerable time for loading. On the other hand, individual']","LLaMA 3 is integrated into a biomedical question answering pipeline that leverages the Integrated Biomedical Knowledge Hub (iBKH) dataset as its core semantic foundation. The iBKH dataset contains 65,828 biomedical entities interconnected by over 3 million relationships, forming a rich knowledge graph with entities such as diseases, symptoms, drugs, and biological pathways. In the system, structured data relevant to a biomedical question is retrieved from this knowledge graph using Cypher queries refined by the Query Generation component. The retrieved structured tuples are then aggregated and formatted into a concise raw answer, which is sent to LLaMA 3. LLaMA 3 optionally refines the phrasing of this answer while preserving the retrieved facts, producing the final answer. This final output is presented alongside the executed Cypher query and an optional preview of the returned rows, enhancing transparency and trust. Thus, LLaMA 3 couples large language model-based understanding with the schema-grounded iBKH knowledge graph to produce answers that are accurate, interpretable, and easy to audit.",multi_hop_abstract_query_synthesizer,"LLaMA 3 utilizes the Integrated Biomedical Knowledge Hub (iBKH) dataset to improve the accuracy and transparency of biomedical question answering through a hybrid framework that tightly couples the language model with a Neo4j-based biomedical knowledge graph derived from iBKH. The key ways this integration enhances the system are as follows:

1. **Rich, Structured Biomedical Knowledge Base**:  
   - The iBKH dataset provides a comprehensive, semantically rich knowledge graph containing approximately 65,828 biomedical entities (such as diseases, drugs, symptoms, pathways) interconnected by over 3 million relationships.  
   - This structured data foundation enables precise retrieval of factual information relevant to biomedical questions, reducing reliance on the language model’s internal knowledge alone and mitigating hallucinations.

2. **Schema-Grounded Query Generation**:  
   - LLaMA 3 is prompted and trained to translate natural language biomedical questions into executable Cypher queries that operate directly on the iBKH knowledge graph.  
   - The model learns to use the graph’s schema (node labels, relationship types) correctly, allowing it to flexibly handle diverse question phrasings and generate accurate queries that retrieve relevant data.

3. **Answer Synthesis from Structured Data**:  
   - The system executes the generated Cypher queries on the Neo4j graph to retrieve structured tuples of biomedical facts.  
   - These tuples are then aggregated and formatted into concise raw answers, which LLaMA 3 optionally refines in phrasing while strictly preserving the retrieved facts, ensuring factual accuracy.

4. **Transparent Result Presentation**:  
   - The final answer is presented alongside the exact Cypher query used and a preview of the returned data rows.  
   - This transparency allows users to verify how the question was interpreted and what evidence supports the answer, increasing trust and enabling auditability.

5. **Robustness and Reliability**:  
   - The system handles query errors gracefully by retrying or informing the user, ensuring smooth and reliable operation.  
   - Neo4j’s indexing and efficient query execution enable fast response times suitable for interactive use.

6. **Offline Operation and Data Provenance**:  
   - The entire pipeline operates fully offline, preserving privacy and compliance with biomedical data governance.  
   - Each edge in the knowledge graph stores provenance metadata (source, version, evidence), further supporting answer reliability and transparency.

In summary, by leveraging the iBKH dataset as a rich, curated biomedical knowledge graph and tightly integrating it with LLaMA 3’s natural language understanding and generation capabilities, the system achieves highly accurate, interpretable, and auditable biomedical question answering. The explicit use of structured biomedical data combined with transparent query and evidence presentation significantly reduces hallucinations and builds user trust.","['Omar & Mohammed / Journal of Applied Science and Technology Trends Vol. 06, No. 02, pp. 342 –357 (2025) \n \n348 \ndrugs, diseases, symptoms) and execution returns structured data \n(tuples) relevant to the question. \nStep 5. Answer Synthesis: The structured tuples flow to \nAnswer Synthesis, which aggregates and formats them into a \nconcise raw answer. This raw answer is sent back to LLaMA 3 \nto optionally refine the phrasing while preserving the retrieved \nfacts. \nStep 6. Result Presentation: LLaMA 3 produces the final \nanswer, which the interface displays together with the executed \nCypher query and an optional preview of the returned rows, \nimproving transparency and trust. \nThe pipeline couples LLM-based language understanding \n(LLaMA 3) with a schema-grounded Neo4j knowledge graph. \nThe Cypher Query Gen refines the query formulation, Query \nExecution retrieves evidence and Answer Synthesis converts \nstructured results into readable outputs that produce answers that \nare accurate, interpretable, and easy to audit directly from the \ndisplayed query and evidence. \nB. Dataset and Knowledge Graph Construction \n1) \nDataset \nThe integrated Biomedical Knowledge Hub (iBKH), a large \nbiomedical knowledge base, forms the first level of the system \nand integrates information from various curated high-quality \nbiomedical databases. This implies that the data set includes \nvarious types of entities, such as diseases, symptoms, drugs, \nbiological pathways, etc. This study used the representative \nsubset of the iBKH dataset, which contained 65828 biomedical \nentities. These entities are semantically interconnected through a \ntotal of 3004166 relationships, thus creating a rich knowledge \ngraph. The iBKH dataset was originally introduced in [11], and it \nis freely available at (https://github.com/wcm-wanglab/iBKH). \nThis dataset is the core semantic foundation upon which this \nstudy is built. The knowledge graph is populated from multiple \ntabular sources (CSV files), each listing entities or relationships. \nThe main input files and their contents are as follows: \n• Disease \nvocabulary(disease_vocab.csv): \nContains \ncolumns such as primary (a unique disease ID), name, \ndo_id (Disease Ontology ID), kegg_id, and umls_cui \n(UMLS Concept ID). Each row represents a disease node \nwith external identifiers. \n• Drug vocabulary (drug_vocab.csv): Includes primary \n(unique drug ID), name, drugbank_id, kegg_id, \npharmgkb_id, umls_cui, mesh_id, iDISK_id and CID \n(PubChem ID). Each row defines a drug node with \nstandard database identifiers. \n• Symptom vocabulary (symptom_vocab.csv): Contains \nprimary (unique symptom ID), name, mesh_id, umls_cui \nand iDISK_id. Each row defines a symptom node. \n• Side \neffect \nvocabulary \n(side_effect_vocab.csv): \nIncludes primary (unique side-effect ID) and name. Each \nrow represents a side-effect node (with UMLS ID when \navailable). \n• Pathway vocabulary (pathway_vocab.csv): Contains \nprimary (unique pathway ID), name, reactome_id, go_id, \nand kegg_id. Each row defines a biological pathway \nnode. \nRelationship files (each row typically contains two entity IDs \nand one or more boolean flags or codes) include: \n• Disease–Symptom links (Di_Sy_res.csv): Rows include \nDisease and Symptom IDs, a presence flag (1 or 0) and a \ndata source. If Present = 1, a HAS_SYMPTOM edge is \ncreated from the disease to the symptom, with properties \nfor presence and source. \n• Disease–Disease links (di_di_res.csv): Rows include \nDisease_1 and Disease_2 IDs with binary flags for is_a \nand Resemble. If is_a = 1, an (IS_A) edge is created \n(Disease_1 → Disease_2); if Resemble = 1, a \nRESEMBLES edge is created. The source field is used \nfor provenance. \n• Drug–Disease links (D_Di_res.csv): Includes Drug and \nDisease IDs with several binary flags. If a flag equals 1, \na corresponding edge is created: \no \nTREATS (Treats = 1) \no \nPALLIATES (Palliates = 1) \no \nASSOCIATED_WITH (Associate = 1) \no \nALLEVIATES_REDUCES (alleviates = 1) \no \nTREATMENT_THERAPY (treatment/therapy = \n1) \no \nINHIBITS_CELL_GROWTH \n(inhibits \ncell \ngrowth = 1) \no \nHAS_BIOMARKER (biomarkers = 1) \no \nPREVENTS_SUPPRESSES \n(prevents/suppresses = 1) \no \nROLE_IN_PATHOGENESIS (role in disease \npathogenesis = 1) \n• Drug–SideEffect links (D_SE_res.csv): Contains Drug \nand SideEffect IDs with a Source column. Each row \ncreates a CAUSES edge from the drug to the side effect, \nwith source as an edge property. \n• Drug–Drug interactions (D_D_res.csv): Rows include \nDrug_1 and Drug_2 IDs with flags for Interaction and \nResemble. If Interaction = 1, an INTERACTS_WITH \nedge is created (bidirectional). If Resemble = 1, a \nRESEMBLES edge is added. \n• Drug–Pathway links (D_Pwy_res.csv): Includes Drug \nID and Pathway ID. Each row generates an \nASSOCIATED_WITH edge from the drug to the \npathway. \n• Disease–Pathway links (Di_Pwy_res.csv): Contains \nDisease ID and Pathway ID. Each row creates an \nASSOCIATED_WITH edge from the disease to the \npathway. \n2) \nData Upload Performance \nThe time required to upload different types of entities and \nrelationships into the Neo4j biomedical knowledge graph, \nmeasured in seconds. These measurements reflect both the size \nand complexity of the data being processed. \nAs shown in Table II, the longest upload time is for Drug-\nDrug Relationships, which takes approximately 190 seconds due \nto the large number of edges (over 3 million). Following this, \nDisease-Disease and Drug-Disease Relationships also require \nconsiderable time for loading. On the other hand, individual'
 'Vol. 06, No. 02, pp. 342 –357 (2025) \nISSN: 2708-0757 \n \nJOURNAL OF APPLIED SCIENCE AND TECHNOLOGY TRENDS \n \nwww.jastt.org  \n \n                                                          342 \ndoi: 10.38094/jastt62404  \nA Hybrid LLM–Knowledge Graph Framework for Accurate \nBiomedical Question Answering \n \nHavraz Y. Omar1,²*, Abdulhakeem O. Mohammed³  \n \n¹Department of Information Technology, Technical College of Duhok, Duhok Polytechnic University, Duhok, Kurdistan Region, \nIraq. havraz.omar@dpu.edu.krd  \n² Department of Information Technology, Technical College of Informatics – Akre, Akre University for Applied Sciences, Akre, \nKurdistan Region, Iraq. \n³ Department of Computer Science, College of Science, University of Zakho, Zakho, Kurdistan Region, Iraq. \na.mohammed@uoz.edu.krd \n \n*Correspondence: havraz.omar@dpu.edu.krd \n \nAbstract \nBiomedical question answering requires accurate and interpretable systems; however, existing approaches often face challenges such as \nlanguage model hallucinations and limited reasoning when relying solely on standalone knowledge graphs. To address these limitations, \nthis study proposes a hybrid framework that integrates the LLaMA-3B language model with a Neo4j-based drug–disease–symptom \nknowledge graph. The system translates natural language questions into executable Cypher queries, operates on an iBKH-derived graph \ncomprising over 65,000 entities and 3 million relationships, and returns answers with supporting evidence through a transparent interface. \nExperiments conducted on 60 biomedical questions across three levels of difficulty demonstrate the robustness of the approach: 96% \nexact match for simple queries, 95% for medium queries, and 86.7% for complex queries. Overall, the system achieves Precision@5 of \n96.1%, Recall@5 of 89.0%, F1@5 of 91.0%, Hits@k of 96.1%, and an MRR of 94.4%, while maintaining an average response time of \nonly 6.07 seconds. These results indicate that the system retrieves nearly all relevant answers, ranks them correctly, and delivers them \nwith latency low enough for interactive use. Moreover, unlike cloud-based APIs such as ChatGPT, which require internet connectivity \nand external data transmission, the proposed framework operates fully offline, ensuring privacy, reproducibility, and compliance with \nbiomedical data governance. Overall, this pipeline provides an accurate, efficient, and privacy-preserving solution for biomedical question \nanswering, making it a practical alternative to cloud-dependent approaches in sensitive healthcare contexts. \n \nKeywords: Knowledge Graph, LLM, Question Answering, Neo4j, Biomedical Informatics, Healthcare AI, LLaMA 3. \n \nReceived: August 14th, 2025 / Revised: October 10th, 2025 / Accepted: October 16th, 2025 / Online: October 20th, 2025 \n \nI. INTRODUCTION \nAnswering questions in the biomedical field is a difficult task \ndue to the complexity of medical knowledge and the need for \nprecision. In recent years, large language models  (LLMs) like \nLLaMA, GPT-4 have made progress in understanding and \ngenerating human-like responses to medical questions  [1,  2]. \nThese models can process large amounts of information and \nrespond in natural language, which makes them helpful in \nhealthcare settings [3]. However, they often struggle to provide \naccurate answers when dealing with specialized biomedical \ncontent [4, 5]. \nOne major issue with LLMs is a problem called hallucination, \nwhere the model generates information that sounds right but is \nactually incorrect or unsupported [6]. In medical applications, \nthis can be dangerous, as healthcare professionals rely on precise \nand trustworthy information  [7]. Therefore, researchers are \nexploring ways to combine LLMs with structured sources of \nknowledge to improve their reliability [8]. \nLLM-only systems in biomedicine still hallucinate and are \nhard to verify, limiting safe use [9, 10]. Biomedical knowledge \ngraphs (BKGs) such as iBKH and SPOKE curate multi-source \nfacts and enable multi-hop reasoning, yet they neither interpret \nfree text nor generate answers [11, 12]. Recent hybrids (KG-\naware RAG) improve grounding but often lack explicit path-level \njustifications and robust end-to-end answer evaluation [13, 14]. \nRecent studies have increasingly integrated Knowledge \nGraphs (KGs) with Large Language Models (LLMs) to improve \nfactual accuracy, reasoning, and reduce hallucinations. Notable \nexamples include DR.KNOWS, which combines UMLS-based \nKGs with LLMs for better diagnostic reasoning [15], KnowNet'
 'Omar & Mohammed / Journal of Applied Science and Technology Trends Vol. 06, No. 02, pp. 342 –357 (2025) \n \n351 \nThis query searches for a disease node whose name contains \n’alzheimer’ and follows HAS_SYMPTOM edges to list related \nsymptom names. The system then executes this cypher to retrieve \nanswers. The prompts (such as few-shot examples and schema \nhints) were carefully designed to help LLaMA 3 generate correct \nCypher queries. The model learns how to use the graph’s labels \nand relationships properly. For example, if a user asks, ’Which \ndrugs treat diabetes?’, LLaMA might respond with a suitable \nCypher query: \n \nMATCH (d:Drug)-[:TREATS]->(di:Disease) \nWHERE toLower(di.name) CONTAINS ""diabetes"" \nRETURN d.name  \n \nThis queries for drug nodes that have a TREATS edge to a \ndiabetes disease node. By leveraging LLaMA 3 in this way, our \nsystem can flexibly handle many phrasing variations without \nmanual mapping rules. \nD. Model Configuration & Decoding \nWe run a local LLaMA 3.2-3B model in GGUF format (llama-\n3.2-3b-instruct-q4_k_m.gguf) via llama.cpp, as shown in Table \nIV. \nTABLE IV.  MODEL RUNTIME AND DECODING SETTINGS \nRuntime settings \nDecoding settings \nn_ctx = 1024 \ntemperature = 0.2 \nn_threads = 12 \ntop_p = 0.95 \nn_gpu_layers = 33 \ntop_k = 40 \nn_batch = 512 \nrepeat_penalty = 1.1 \n \nmax_tokens = 80 \n \nseed = 42 \n \nE. Graph Subset and Versioning \nWe use an iBKH derived subgraph (≈65.8k nodes; ≈3.0M \nedges) spanning DRUG, DISEASE, SYMPTOM, PATHWAY. IDs are \nnormalized to CURIEs and duplicates collapsed across \nUMLS/DrugBank/DisGeNET/SIDER/KEGG. Each edge stores \nprovenance/licensing metadata (source, source_version, license, \nretrieved_at, \nevidence_pmids/urls) \nand, \nwhen \navailable, \ntemporal fields (valid_from, valid_to). We report coverage as \nthe \npercentage \nof \nevaluated \nquestions \nwhose \ngold \nentities/relations are present. \nF. Query Execution and Reliability \nAfter a Cypher query is generated, it is executed on the Neo4j \ndatabase through the official Neo4j Python driver, which \nmanages the secure connection and returns the results. Instead of \nrestricting the output with a fixed LIMIT (e.g., LIMIT 5), the \nsystem retrieves candidate results and evaluates them using \nstandardized retrieval metrics such as Hits@1, Hits@5, and \nHits@10. This approach ensures that the system remains \nresponsive while providing a fair assessment of ranking quality \nacross different cutoff levels, rather than depending solely on a \nfixed number of returned items. Neo4j’s indexing on key node \nproperties, such as name and primary identifiers, also helps speed \nup lookups as the knowledge graph grows. In cases where the \nlanguage model generates an incomplete or incorrect query, such \nas referencing nodes or relationships that do not exist, the system \ncatches the error and either retries with a simpler prompt or \ninforms the user. Together, these steps make sure that queries run \nquickly, return valid results, and keep the overall experience \nsmooth and reliable for biomedical question-answering. \nG. User Interface for Query Transparency \nThe system includes a lightweight, cross-platform graphical \nuser interface (GUI) implemented as a web application using the \nFlask framework in Python, with HTML and Bootstrap for \ninteractive visualization. The interface is designed to make the \nquestion-answering process transparent and accessible to users \nwithout technical expertise. It consists of three main panels: \n1. Input Panel: Where the user can enter a biomedical \nquestion in natural language. \n2. Query Panel: Which displays the Cypher query generated \nby the language model, allowing users to verify how their \nquestion was interpreted. \n3. Results Panel: Which presents the retrieved answers in a \nclear, readable format, accompanied by a brief natural \nlanguage explanation generated by the system. \nBy showing both the query and the answer, the GUI promotes \nuser trust and enables validation of the system’s reasoning \nprocess. The interface is lightweight enough to run smoothly on \nstandard desktop machines without additional dependencies, \nmaking it practical for local deployments in clinical or research \nsettings. Fig. 4 illustrates the overall layout. \nV. EXPERIMENTAL RESULTS \nIn this section, the proposed system is evaluated based on its \nability to translate natural-language biomedical questions into \nexecutable Cypher queries over the iBKH knowledge graph. The \nassessment is conducted end-to-end and focuses on three main \naspects: (i) the accuracy of query generation, measured by the \nproportion of correctly produced Cypher queries; (ii) system \nefficiency, quantified through total response time from question \nsubmission to final answer delivery, covering both query \ngeneration and execution; and (iii) the quality of retrieved \nanswers, evaluated using standardized information retrieval \nmetrics including Precision@k, Recall@k, F1@k, Hits@k, and \nMean Reciprocal Rank (MRR). For clarity, all these metrics are \nformally defined in the following subsections, with their \ncorresponding equations, and will be used consistently \nthroughout the results section. Together, these dimensions \nprovide a comprehensive view of both correctness and efficiency \nacross simple, medium, and complex biomedical queries. \nA. Description of the Experimental Data \nTo evaluate the proposed system, this work used a carefully \nselected set of biomedical questions designed to test how well the \nsystem understands natural language and converts it into Cypher \nqueries for a biomedical knowledge graph. \nTo evaluate the system, a benchmark of 60 biomedical \nquestions was constructed by the authors, guided by the schema \nand relations in iBKH. The questions were grouped into simple, \nmedium, and complex levels to assess performance across \ndifferent reasoning requirements. Gold-standard answers were \nmanually prepared for each question to enable quantitative \nevaluation using Exact Match, Precision@k, Recall@k, F1@k, \nHits@k, and MRR. The complete set of 60 questions is available \nat this link https://drive.google.com/drive/my-drive.']"
"How does the PharmGKB identifier integration in the drug vocabulary contribute to the construction and semantic richness of the iBKH biomedical knowledge graph, particularly in relation to drug-disease and drug-pathway relationships?","['<1-hop>\n\nOmar & Mohammed / Journal of Applied Science and Technology Trends Vol. 06, No. 02, pp. 342 –357 (2025) 350 documentation, indexes “enable quicker and more efficient pattern matching” by allowing the query planner to rapidly locate nodes by label and property. With the schema in place, data was imported using Cypher’s LOAD CSV commands. For each vocabulary file, rows with nonnull primary IDs were processed: the code used MERGE to create (or match existing) nodes with the given label and primary property, and then SET the remaining attributes from the CSV columns. For example, in importing disease_vocab.csv, each row produced a node (:Disease primary: <id>) with properties name, do_id, kegg_id, and umls_cui set from the row (empty strings were turned into null). Similarly, drug_vocab.csv rows produced drug nodes with properties for DrugBank, KEGG, PharmGKB IDs, etc. This approach follows best practice: using MERGE on the unique ID ensures that nodes are not duplicated during multiple passes. After all nodes were created, the relationship CSVs were loaded. Each row in those files was matched to the corresponding source and target nodes by their primary IDs, and conditional logic was used to create edges. For example, the disease- symptom file (Di_Sy_res.csv) was processed by matching a disease node and a symptom node for each row, then executing MERGE (d)-[r:HAS_SYMPTOM]-(s) if the present column is nonzero; the edge was given a present property and a source property from the CSV. The disease-disease file (di_di_res.csv) was processed by matching disease nodes d1 and d2:If is_a = 1, a (:Disease)-[:IS_A]->(:Disease) edge was merged; if Resemble = 1, a (:Disease)-[:RESEMBLES]->(:Disease) edge was merged. Drug-disease relationships were handled similarly: the script matched a Drug node and a Disease node for each row of D_Di_res.csv, then for each flag that was 1, it merged the corresponding edge label (such as TREATS, PALLIATES, ASSOCIATED_WITH, etc.) from the drug to the disease. Drug– side-effect rows produced (:Drug)-[:CAUSES]->(:SideEffect) edges with the source noted, and drug–drug rows produced either INTERACTS_WITH or RESEMBLES edges between matched drug pairs. Finally, the drug-pathway and disease-pathway files each produced:ASSOCIATED_WITH edges linking drugs or diseases to pathway nodes. In this graph model, most relationships are directional (for example, a drug TREATS a disease; a disease does not TREAT a drug). This follows common practice in biomedical KGs. The same relations (such as RESEMBLES or INTERACTS_WITH) are inherently symmetric, but were stored as two directed edges or one undirected edge depending on implementation. All relationship creation steps used Cypher’s MERGE so that repeated loads or out-of-order imports did not create duplicate edges. This study used a static KG snapshot for reproducibility, but the system also supports incremental updates through the Neo4j MERGE and batch import functions. A summary of the different types of nodes and relationships is provided, together with their counts, descriptions, and examples in Table III. TABLE III. DETAILED SUMMARY OF KNOWLEDGE GRAPH COMPONENTS IN IBKH SUBSET Component Type Entity/Relationship Count Description Examples Disease 19,236 Medical conditions and disorders Diabetes, Hypertension, Cancer Drug 37,997 Pharmaceutical compounds Aspirin, Insulin, Amoxicillin'
 '<2-hop>\n\nB. Dataset and Knowledge Graph Construction 1) Dataset The integrated Biomedical Knowledge Hub (iBKH), a large biomedical knowledge base, forms the first level of the system and integrates information from various curated high-quality biomedical databases. This implies that the data set includes various types of entities, such as diseases, symptoms, drugs, biological pathways, etc. This study used the representative subset of the iBKH dataset, which contained 65828 biomedical entities. These entities are semantically interconnected through a total of 3004166 relationships, thus creating a rich knowledge graph. The iBKH dataset was originally introduced in [11], and it is freely available at (https://github.com/wcm-wanglab/iBKH). This dataset is the core semantic foundation upon which this study is built. The knowledge graph is populated from multiple tabular sources (CSV files), each listing entities or relationships. The main input files and their contents are as follows: • Disease vocabulary(disease_vocab.csv): Contains columns such as primary (a unique disease ID), name, do_id (Disease Ontology ID), kegg_id, and umls_cui (UMLS Concept ID). Each row represents a disease node with external identifiers. • Drug vocabulary (drug_vocab.csv): Includes primary (unique drug ID), name, drugbank_id, kegg_id, pharmgkb_id, umls_cui, mesh_id, iDISK_id and CID (PubChem ID). Each row defines a drug node with standard database identifiers. • Symptom vocabulary (symptom_vocab.csv): Contains primary (unique symptom ID), name, mesh_id, umls_cui and iDISK_id. Each row defines a symptom node. • Side effect vocabulary (side_effect_vocab.csv): Includes primary (unique side-effect ID) and name. Each row represents a side-effect node (with UMLS ID when available). • Pathway vocabulary (pathway_vocab.csv): Contains primary (unique pathway ID), name, reactome_id, go_id, and kegg_id. Each row defines a biological pathway node. Relationship files (each row typically contains two entity IDs and one or more boolean flags or codes) include: • Disease–Symptom links (Di_Sy_res.csv): Rows include Disease and Symptom IDs, a presence flag (1 or 0) and a data source. If Present = 1, a HAS_SYMPTOM edge is created from the disease to the symptom, with properties for presence and source. • Disease–Disease links (di_di_res.csv): Rows include Disease_1 and Disease_2 IDs with binary flags for is_a and Resemble. If is_a = 1, an (IS_A) edge is created (Disease_1 → Disease_2); if Resemble = 1, a RESEMBLES edge is created. The source field is used for provenance. • Drug–Disease links (D_Di_res.csv): Includes Drug and Disease IDs with several binary flags. If a flag equals 1, a corresponding edge is created: o TREATS (Treats = 1) o PALLIATES (Palliates = 1) o ASSOCIATED_WITH (Associate = 1) o ALLEVIATES_REDUCES (alleviates = 1) o TREATMENT_THERAPY (treatment/therapy = 1) o INHIBITS_CELL_GROWTH (inhibits cell growth = 1) o HAS_BIOMARKER (biomarkers = 1) o PREVENTS_SUPPRESSES (prevents/suppresses = 1) o ROLE_IN_PATHOGENESIS (role in disease pathogenesis = 1) • Drug–SideEffect links (D_SE_res.csv): Contains Drug and SideEffect IDs with a Source column. Each row creates a CAUSES edge from the drug to the side effect, with source as an edge property. • Drug–Drug interactions (D_D_res.csv): Rows include Drug_1 and Drug_2 IDs with flags for Interaction and Resemble. If Interaction = 1, an INTERACTS_WITH edge is created (bidirectional). If Resemble = 1, a RESEMBLES edge is added. • Drug–Pathway links (D_Pwy_res.csv): Includes Drug ID and Pathway ID. Each row generates an ASSOCIATED_WITH edge from the drug to the pathway. • Disease–Pathway links (Di_Pwy_res.csv): Contains Disease ID and Pathway ID. Each row creates an ASSOCIATED_WITH edge from the disease to the pathway. 2) Data Upload Performance The time required to upload different types of entities and relationships into the Neo4j biomedical knowledge graph, measured in seconds. These measurements reflect both the size and complexity of the data being processed. As shown in Table II, the longest upload time is for Drug- Drug Relationships, which takes approximately 190 seconds due to the large number of edges (over 3 million). Following this, Disease-Disease and Drug-Disease Relationships also require considerable time for loading. On the other hand, individual']","The PharmGKB identifier is included as a property in the drug nodes created from the drug_vocab.csv file, which contains unique drug IDs along with standard database identifiers such as drugbank_id, kegg_id, pharmgkb_id, umls_cui, mesh_id, iDISK_id, and CID. This integration ensures that each drug node in the iBKH knowledge graph is linked to PharmGKB, a curated biomedical database, enhancing the semantic interoperability and accuracy of the graph. The iBKH knowledge graph uses these drug nodes to establish multiple types of relationships, including drug-disease links (such as TREATS, PALLIATES, ASSOCIATED_WITH) and drug-pathway associations, by matching drug nodes with disease and pathway nodes through their unique IDs. This structured incorporation of PharmGKB identifiers facilitates precise merging of nodes and edges using Cypher's MERGE command, preventing duplication and enabling rich, directional relationships that reflect real biomedical interactions. Consequently, the PharmGKB integration significantly contributes to the comprehensive and semantically rich representation of drug-related biomedical knowledge within the iBKH knowledge graph.",multi_hop_specific_query_synthesizer,"The integration of the PharmGKB identifier within the drug vocabulary significantly enhances the construction and semantic richness of the iBKH biomedical knowledge graph, particularly in relation to drug-disease and drug-pathway relationships, in the following ways:

1. **Standardized and Rich Drug Node Representation**:  
   The drug vocabulary (`drug_vocab.csv`) includes multiple standardized identifiers for each drug node, such as DrugBank ID, KEGG ID, UMLS CUI, MeSH ID, PubChem CID, and importantly, the PharmGKB ID. Incorporating the PharmGKB identifier ensures that each drug entity in the iBKH is linked to a well-curated pharmacogenomics resource, which enriches the semantic context of the drug nodes beyond basic chemical or clinical information.

2. **Improved Cross-Database Integration and Interoperability**:  
   By including PharmGKB IDs, the iBKH knowledge graph can seamlessly integrate and cross-reference pharmacogenomic data with other biomedical databases. This integration supports more comprehensive and precise mappings between drugs and their genetic, pathway, and disease associations, facilitating multi-dimensional biomedical insights.

3. **Enhanced Drug-Disease Relationship Semantics**:  
   The drug-disease relationships in iBKH are represented by various edge types such as TREATS, PALLIATES, ASSOCIATED_WITH, and others, created by matching drug and disease nodes via their unique IDs. The presence of PharmGKB IDs in drug nodes allows these relationships to be grounded in pharmacogenomic evidence, potentially linking drugs to diseases with known genetic or molecular mechanisms. This adds a layer of biological relevance and precision to the drug-disease edges.

4. **Refined Drug-Pathway Associations**:  
   Drug-pathway relationships are represented by ASSOCIATED_WITH edges linking drug nodes to biological pathway nodes. The PharmGKB identifier, which catalogs drug interactions with genes and pathways, enriches the drug nodes with pharmacogenomic context, enabling more accurate and biologically meaningful associations between drugs and pathways. This supports better understanding of drug mechanisms and their effects on biological processes.

5. **Facilitation of Advanced Queries and Reasoning**:  
   With PharmGKB IDs integrated, queries that involve pharmacogenomic aspects—such as identifying drugs targeting specific genetic variants or pathways implicated in diseases—can be more effectively executed using Cypher queries. This enhances the knowledge graph’s utility for precision medicine applications, drug repurposing, and biomarker discovery.

In summary, the inclusion of PharmGKB identifiers in the drug vocabulary enriches the iBKH knowledge graph by providing a pharmacogenomic dimension to drug entities. This integration strengthens the semantic connections between drugs, diseases, and pathways, thereby improving the graph’s comprehensiveness, biological relevance, and utility for complex biomedical queries and analyses.","['Omar & Mohammed / Journal of Applied Science and Technology Trends Vol. 06, No. 02, pp. 342 –357 (2025) \n \n348 \ndrugs, diseases, symptoms) and execution returns structured data \n(tuples) relevant to the question. \nStep 5. Answer Synthesis: The structured tuples flow to \nAnswer Synthesis, which aggregates and formats them into a \nconcise raw answer. This raw answer is sent back to LLaMA 3 \nto optionally refine the phrasing while preserving the retrieved \nfacts. \nStep 6. Result Presentation: LLaMA 3 produces the final \nanswer, which the interface displays together with the executed \nCypher query and an optional preview of the returned rows, \nimproving transparency and trust. \nThe pipeline couples LLM-based language understanding \n(LLaMA 3) with a schema-grounded Neo4j knowledge graph. \nThe Cypher Query Gen refines the query formulation, Query \nExecution retrieves evidence and Answer Synthesis converts \nstructured results into readable outputs that produce answers that \nare accurate, interpretable, and easy to audit directly from the \ndisplayed query and evidence. \nB. Dataset and Knowledge Graph Construction \n1) \nDataset \nThe integrated Biomedical Knowledge Hub (iBKH), a large \nbiomedical knowledge base, forms the first level of the system \nand integrates information from various curated high-quality \nbiomedical databases. This implies that the data set includes \nvarious types of entities, such as diseases, symptoms, drugs, \nbiological pathways, etc. This study used the representative \nsubset of the iBKH dataset, which contained 65828 biomedical \nentities. These entities are semantically interconnected through a \ntotal of 3004166 relationships, thus creating a rich knowledge \ngraph. The iBKH dataset was originally introduced in [11], and it \nis freely available at (https://github.com/wcm-wanglab/iBKH). \nThis dataset is the core semantic foundation upon which this \nstudy is built. The knowledge graph is populated from multiple \ntabular sources (CSV files), each listing entities or relationships. \nThe main input files and their contents are as follows: \n• Disease \nvocabulary(disease_vocab.csv): \nContains \ncolumns such as primary (a unique disease ID), name, \ndo_id (Disease Ontology ID), kegg_id, and umls_cui \n(UMLS Concept ID). Each row represents a disease node \nwith external identifiers. \n• Drug vocabulary (drug_vocab.csv): Includes primary \n(unique drug ID), name, drugbank_id, kegg_id, \npharmgkb_id, umls_cui, mesh_id, iDISK_id and CID \n(PubChem ID). Each row defines a drug node with \nstandard database identifiers. \n• Symptom vocabulary (symptom_vocab.csv): Contains \nprimary (unique symptom ID), name, mesh_id, umls_cui \nand iDISK_id. Each row defines a symptom node. \n• Side \neffect \nvocabulary \n(side_effect_vocab.csv): \nIncludes primary (unique side-effect ID) and name. Each \nrow represents a side-effect node (with UMLS ID when \navailable). \n• Pathway vocabulary (pathway_vocab.csv): Contains \nprimary (unique pathway ID), name, reactome_id, go_id, \nand kegg_id. Each row defines a biological pathway \nnode. \nRelationship files (each row typically contains two entity IDs \nand one or more boolean flags or codes) include: \n• Disease–Symptom links (Di_Sy_res.csv): Rows include \nDisease and Symptom IDs, a presence flag (1 or 0) and a \ndata source. If Present = 1, a HAS_SYMPTOM edge is \ncreated from the disease to the symptom, with properties \nfor presence and source. \n• Disease–Disease links (di_di_res.csv): Rows include \nDisease_1 and Disease_2 IDs with binary flags for is_a \nand Resemble. If is_a = 1, an (IS_A) edge is created \n(Disease_1 → Disease_2); if Resemble = 1, a \nRESEMBLES edge is created. The source field is used \nfor provenance. \n• Drug–Disease links (D_Di_res.csv): Includes Drug and \nDisease IDs with several binary flags. If a flag equals 1, \na corresponding edge is created: \no \nTREATS (Treats = 1) \no \nPALLIATES (Palliates = 1) \no \nASSOCIATED_WITH (Associate = 1) \no \nALLEVIATES_REDUCES (alleviates = 1) \no \nTREATMENT_THERAPY (treatment/therapy = \n1) \no \nINHIBITS_CELL_GROWTH \n(inhibits \ncell \ngrowth = 1) \no \nHAS_BIOMARKER (biomarkers = 1) \no \nPREVENTS_SUPPRESSES \n(prevents/suppresses = 1) \no \nROLE_IN_PATHOGENESIS (role in disease \npathogenesis = 1) \n• Drug–SideEffect links (D_SE_res.csv): Contains Drug \nand SideEffect IDs with a Source column. Each row \ncreates a CAUSES edge from the drug to the side effect, \nwith source as an edge property. \n• Drug–Drug interactions (D_D_res.csv): Rows include \nDrug_1 and Drug_2 IDs with flags for Interaction and \nResemble. If Interaction = 1, an INTERACTS_WITH \nedge is created (bidirectional). If Resemble = 1, a \nRESEMBLES edge is added. \n• Drug–Pathway links (D_Pwy_res.csv): Includes Drug \nID and Pathway ID. Each row generates an \nASSOCIATED_WITH edge from the drug to the \npathway. \n• Disease–Pathway links (Di_Pwy_res.csv): Contains \nDisease ID and Pathway ID. Each row creates an \nASSOCIATED_WITH edge from the disease to the \npathway. \n2) \nData Upload Performance \nThe time required to upload different types of entities and \nrelationships into the Neo4j biomedical knowledge graph, \nmeasured in seconds. These measurements reflect both the size \nand complexity of the data being processed. \nAs shown in Table II, the longest upload time is for Drug-\nDrug Relationships, which takes approximately 190 seconds due \nto the large number of edges (over 3 million). Following this, \nDisease-Disease and Drug-Disease Relationships also require \nconsiderable time for loading. On the other hand, individual'
 'Omar & Mohammed / Journal of Applied Science and Technology Trends Vol. 06, No. 02, pp. 342 –357 (2025) \n \n350 \ndocumentation, indexes “enable quicker and more efficient \npattern matching” by allowing the query planner to rapidly locate \nnodes by label and property. \nWith the schema in place, data was imported using Cypher’s \nLOAD CSV commands. For each vocabulary file, rows with \nnonnull primary IDs were processed: the code used MERGE to \ncreate (or match existing) nodes with the given label and primary \nproperty, and then SET the remaining attributes from the CSV \ncolumns. For example, in importing disease_vocab.csv, each row \nproduced a node (:Disease primary: <id>) with properties name, \ndo_id, kegg_id, and umls_cui set from the row (empty strings \nwere turned into null). Similarly, drug_vocab.csv rows produced \ndrug nodes with properties for DrugBank, KEGG, PharmGKB \nIDs, etc. This approach follows best practice: using MERGE on \nthe unique ID ensures that nodes are not duplicated during \nmultiple passes. \nAfter all nodes were created, the relationship CSVs were \nloaded. Each row in those files was matched to the corresponding \nsource and target nodes by their primary IDs, and conditional \nlogic was used to create edges. For example, the disease-\nsymptom file (Di_Sy_res.csv) was processed by matching a \ndisease node and a symptom node for each row, then executing \nMERGE (d)-[r:HAS_SYMPTOM]-(s) if the present column is \nnonzero; the edge was given a present property and a source \nproperty from the CSV. The disease-disease file (di_di_res.csv) \nwas processed by matching disease nodes d1 and d2:If is_a = 1, \na (:Disease)-[:IS_A]->(:Disease) edge was merged; if Resemble \n= 1, a (:Disease)-[:RESEMBLES]->(:Disease) edge was merged. \nDrug-disease relationships were handled similarly: the script \nmatched a Drug node and a Disease node for each row of \nD_Di_res.csv, then for each flag that was 1, it merged the \ncorresponding edge label (such as TREATS, PALLIATES, \nASSOCIATED_WITH, etc.) from the drug to the disease. Drug–\nside-effect rows produced (:Drug)-[:CAUSES]->(:SideEffect) \nedges with the source noted, and drug–drug rows produced either \nINTERACTS_WITH or RESEMBLES edges between matched \ndrug pairs. Finally, the drug-pathway and disease-pathway files \neach produced:ASSOCIATED_WITH edges linking drugs or \ndiseases to pathway nodes. \nIn this graph model, most relationships are directional (for \nexample, a drug TREATS a disease; a disease does not TREAT \na drug). This follows common practice in biomedical KGs. The \nsame relations (such as RESEMBLES or INTERACTS_WITH) \nare inherently symmetric, but were stored as two directed edges \nor one undirected edge depending on implementation. All \nrelationship creation steps used Cypher’s MERGE so that \nrepeated loads or out-of-order imports did not create duplicate \nedges. \nThis study used a static KG snapshot for reproducibility, but \nthe system also supports incremental updates through the Neo4j \nMERGE and batch import functions. A summary of the different \ntypes of nodes and relationships is provided, together with their \ncounts, descriptions, and examples in Table III.\n \nTABLE III.  DETAILED SUMMARY OF KNOWLEDGE GRAPH COMPONENTS IN IBKH SUBSET \nComponent Type \nEntity/Relationship \nCount \nDescription \nExamples \n \nDisease \n19,236 \nMedical conditions and disorders \nDiabetes, Hypertension, Cancer \n \nDrug \n37,997 \nPharmaceutical compounds \nAspirin, Insulin, Amoxicillin \nNodes \nSymptom \n1,361 \nClinical signs and symptoms \nHeadache, Fever, Nausea \n \nSideEffect \n4,251 \nNegative outcomes of drugs \nNausea, Drowsiness \n \nPathway \n2,983 \nBiological processes and pathways \nApoptosis, Glycolysis \n \nASSOCIATED_WITH \n101,534 \nGeneral associations (disease–\npathway, drug–pathway, etc.) \nDiabetes ASSOCIATED_WITH Pathway \n \nCAUSES \n145,321 \nDrug-SideEffect relationships \nAspirin CAUSES Gastric_Bleeding \n \nHAS_SYMPTOM \n3,357 \nDisease-symptom links \nCOVID-19 HAS_SYMPTOM Fever \n \nHAS_BIOMARKER \n412 \nBiomarkers linked to disease \nPSA HAS_BIOMARKER Prostate_Cancer \nRelationships \nINHIBITS_CELL_GROWTH \n1,508 \nDrugs inhibiting cell growth \nChemo INHIBITS_CELL_GROWTH Tumor \n \nINTERACTS_WITH \n2,682,142 \nDrug-drug interactions \nAspirin INTERACTS_WITH Warfarin \n \nIS_A \n10,529 \nSubtype hierarchy \nFlu IS_A Viral_Infection \n \nPALLIATES \n388 \nDrug palliates disease \nMorphine PALLIATES Cancer \n \nPREVENTS_SUPPRESSES \n859 \nPreventive links \nVaccine PREVENTS_SUPPRESSES Measles \n \nRESEMBLES \n7,000 \nSimilarity relationships \nDrugA RESEMBLES DrugB \n \nTREATMENT_THERAPY \n44,852 \nTherapy relationships \nRadiotherapy TREATMENT_THERAPY Tumor \n \nTREATS \n5,491 \nDrug-disease links \nInsulin TREATS Diabetes \n \nALLEVIATES_REDUCES \n∼180,000 \nSymptom relief \nParacetamol ALLEVIATES_REDUCES Fever \nTotal \nNodes \n65,828 \nTotal biomedical entities \n— \n \nRelationships \n3,004,166 \nTotal knowledge graph links \n— \nC. Natural Language to Cypher Query Translation \nA key feature of the system is its ability to accept questions \nwritten in plain English and automatically generate the \ncorresponding Cypher queries. This is accomplished using \nMeta’s LLaMA 3 large language model, which runs entirely on \na local machine through the open-source llama.cpp framework. \nRunning the model locally ensures low-latency execution and \nkeeps sensitive queries within the user’s environment. \nTo generate a Cypher query, LLaMA 3 is prompted with \nexamples of natural language questions along with their correct \nCypher translations. The prompt also includes instructions on \nhow to navigate the structure of the graph schema. When a user \nenters a question (e.g., ’What are the symptoms of Alzheimer’s \ndisease?’), the system inserts it into the prompt and asks LLaMA \n3 to produce a corresponding query. For example, the model may \nreturn: \nMATCH (d:Disease)-[:HAS_SYMPTOM]->(s:Symptom) \nWHERE toLower(d.name) CONTAINS ""alzheimer"" \nRETURN s.name'
 'Omar & Mohammed / Journal of Applied Science and Technology Trends Vol. 06, No. 02, pp. 342 –357 (2025) \n \n346 \nscientific literature, clinical records, genomic databases, and \nexperimental findings [19, 31]. \nSuch integration creates a comprehensive biomedical \nknowledge base that supports advanced analytics and discovery. \nFor example, biomedical knowledge graphs can reveal previously \nunknown relationships (e.g., between drugs and diseases) and \nhelp prioritize potential biomarkers for complex conditions. The \nIntegrative Biomedical Knowledge Hub (iBKH) is one such \nlarge-scale graph that consolidates diverse biomedical resources \ninto a unified hub, enabling discovery at scale [11]. Beyond \niBKH, large biomedical knowledge graphs such as SPOKE \nfurther illustrate how graph integration accelerates research and \nsupports precision-medicine use cases [12]. Overall, these graphs \nserve as foundational resources for data-driven and personalized \nmedicine. These knowledge graphs serve as foundational \nresources for precision medicine, where treatment can be tailored \nto the individual’s biological profile, improving outcomes and \nminimizing side effects [19, 31]. \n \nTABLE I.  SUMMARY OF RELATED RESEARCH ON BIOMEDICAL KGS AND QUESTION ANSWERING \nRef. \nYear \nData/Graph \nMethod \nBaselines \nKey Metric \nLimitation \n[11] \n2023 \niBKH (18 biomedical \nsources, Neo4j) \nIntegrative KG + Graph \nlearning; drug repurposing \ncase \nKnown drugs, \nAlzheimer’s \nstudy \nPredictive performance \n(drug repurposing) \nLimited to Alzheimer’s case \nstudy; scalability and updates not \ndetailed \n[19] \n2023 \nDisease KG (9,400 \nnodes, 45,000 relations \nin Neo4j) \nCypher queries for disease–\ndrug–symptom reasoning \nCross-referential \ndisease DB \nNew relation inference; \ncomplex query \nanswering \nLimited to single domain; lacks \nlarge-scale evaluation \n[3] \n2023 \nBKGs vs. GPT-3.5/4 \nComparative QA study: \nLLMs vs. KGs \nGPT-3.5, GPT-4, \nKG reasoning \nAccuracy, Novelty, \nReasoning \nGPT-4 inconsistent in \nreasoning/citations; KG less fluent \nbut more reliable \n[13] \n2024 \nSPOKE KG + \nLLaMA2, GPT-3.5, \nGPT-4 \nKG-optimized retrieval for \nLLMs (RAG) \nOther retrieval \nmethods \nAccuracy, token \nreduction >50% \nFocus on retrieval optimization, \nnot KG construction \n[23] \n2024 \nBenchmark KB datasets \nGenerate-then-retrieve \n(LLM → logical form → \nKB retrieval) \nPrior KBQA \nmethods \nAccuracy (highest to \ndate) \nRisk of error in logical form \ngeneration \n[6] \n2024 \nPrimeKG \nLLM + KG hybrid, Cypher \nquery validation, RAG \nGPT-4 Turbo, \nLLaMA 3 \nAccuracy, \nExplainability \nDependent on KG coverage; \ncomputationally intensive \n[22] \n2025 \nPan-cancer KG (PcQA \ndataset) \nKG-enhanced reasoning \n(subgraph selection) \nSOTA KGQA \nmethods \nOutperformed SOTA on \nPcQA \nLimited to pan-cancer focus; no \nfine-tuning explored \n[26] \n2025 \nDynamic Medical KG + \nNeo4j \nLLM agents + multi-hop \nreasoning \nMEDQA, \nMedMCQA \nbaselines \nAccuracy, \nInterpretability \nHigh system complexity; requires \ncontinuous updating \n[24] \n2025 \nSynthCypher dataset \n(Neo4j + synthetic \nqueries) \nLLM-supervised Cypher \ngeneration and verification \nSPIDER \nbenchmark \nCypher accuracy 40% \nSynthetic dataset may not capture \nall real-world cases \n[29] \n2025 \nUMLS KG + Neo4j \nHybrid GraphRAG \nLLM-only QA \nAccuracy, Citation \nfidelity \nMore complex pipeline; relies on \nexternal vector store \n[30] \n2025 \nClinical data (10k \npatients, 1,894 nodes, \nNeo4j) \nKG construction using \nGPT-4 for entity/relation \nextraction \nOther KG \nconstruction \nmethods \nHigh F1-scores \nFocus only on sepsis; limited \ngeneralization \n[20] \n2025 \nLocal KG from \nbiomedical documents \nMulti-level summarization \nover KG for QA \nRAG baselines \nQA accuracy, reasoning \nTested mainly on document QA; \nscalability not proven \n \nB. Neo4j Graph Database \nTo manage the complexity and large size of biomedical \nknowledge graphs, specialized graph databases are needed. \nNeo4j is one of the most popular graph databases designed to \nstore and query data structured as nodes (entities) and \nrelationships (edges), both of which can have descriptive \nproperties [32, 33]. It uses the property graph model, which \nmakes it easy to represent complex, connected biomedical data \nsuch as drug-gene interactions or disease pathways. Neo4j’s \nCypher query language is especially advantageous because it \nallows users to write expressive and efficient queries to explore \nmulti-step connections in the data [34]. \nNeo4j works well for biomedical data because it can quickly \nrun complicated queries over highly interconnected datasets. This \nis important in biology and medicine, where relationships \nbetween entities like proteins, diseases, and drugs are often \ncomplex and layered. Studies have shown that Neo4j handles \nlarge biomedical graphs efficiently, making it a favorite among \nresearchers and industry users alike [33, 35, 36]. Its indexing and \ncaching mechanisms also help speed up query processing and \ndata retrieval [37]. \nMoreover, \nNeo4j \nintegrates \nsmoothly \nwith \nmany \nprogramming languages and analytics tools, which makes it \neasier to build interactive biomedical applications and clinical \ndecision support systems that can turn complex graph data into \nuseful insights [38, 39]. \nC. Large Language Models (LLMs) in Biomedical Question \nAnswering \nLarge Language Models (LLMs) are powerful AI systems \ntrained on vast amounts of text data. They learn the structure and \npatterns of language, enabling them to understand questions, \ngenerate responses, summarize information, and perform other \ncomplex language tasks. Well-known models such as LLaMA \nand GPT-3 have greatly advanced the field of natural language \nprocessing by showing strong performance across many tasks \n[40, 41].']"
"How do LLaMA and Llama2 models differ in their integration with biomedical knowledge graphs to mitigate hallucinations in large language models, and what impact does this integration have on the performance of retrieval-augmented generation (RAG) systems as demonstrated by meta-analyses?","['<1-hop>\n\n2 | GigaScience , 2025, Vol. 14 LLaMA with medical liter atur e. Additionall y, Med-P aLM [ 9 ] shows promising performance on the MedQA exam based on clinical cor por a and human feedback. Meanwhile, aiming at the Chinese medical domain, LLMs such as BenTsao [ 10 ], DoctorGLM [ 11 ], and HuatuoGPT [ 12 ] are developed on the Chinese medical dialogue data. Mor e r ecentl y, Zhongjing [ 13 ] and ChiMed-GPT [ 14 ] adopted full pipeline training from pretraining, SFT, to reinforcement learning with human feedback (RLHF) [ 15 ]. While fine-tuning can reduce hallucinations in LLMs, it brings about considerable train- ing expenses. Additionally, it poses a critical challenge known as catastrophic forgetting. This issue manifests when a model for gets its pr e viousl y learned information as a consequence of parameter modifications during the acquisition of new tasks. This forgetfulness results in a deterioration of performance on prior tasks, consequently constraining the model’s practical applicability [ 16 , 17 ]. In addition to fine-tuning, r esearc hers also enhance the out- put of LLMs through the field of prompt engineering. Prompt en- gineering focuses on the creation and optimization of prompts to impr ov e the effectiv eness of LLMs acr oss v arious a pplications and r esearc h domains [ 18 ]. It can enhance the ca pabilities of LLMs in a wide range of complex tasks, including question answering, sentiment classification, and commonsense reasoning. Chain-of- thought (CoT) prompts [ 19 ] enable complex reasoning capabili- ties by incor por ating intermediate r easoning steps . T he Automatic Pr ompt Engineer (APE) pr oposes an automatic pr ompt gener ation method aimed at enhancing the performance of LLMs [ 20 ]. Prompt engineering offers a straightforw ar d approach to harnessing the potential of LLMs without fine-tuning. On the other hand, knowledge gr a phs (KGs) ar e r epositories of vast quantities of high-quality structured data, offering the po- tential to effectiv el y mitigate the issue of factual hallucinations when integrated with LLMs . Hence , employing KGs for question answering can enhance the precision of the responses and fur- nish a dependable foundation for the factual verification of in- formation produced by LLMs. Knowledge gr a ph question answer- ing (KGQA) has long been a hot r esearc h topic. Befor e the ad- vent of LLMs, certain studies [ 21–23 ] typically begin by r etrie ving a subgr a ph r elated to the question to r educe the searc h space, then perform m ultihop r easoning on this basis . T his r etrie v al- plus-r easoning par adigm has shown its adv anta ges ov er dir ect r easoning acr oss the entir e KG [ 24 , 25 ]. Additionall y, r esearc hers tackle KGQA by parsing the question into a structured query lan- guage (e.g., SPARQL) and using a query engine to obtain accu- rate answers [ 26 , 27 ]. UniKGQA [ 28 ] introduces a unified fine- tuning fr ame work for r etrie v al and r easoning, mor e closel y linking these 2 stages. Ho w ever, traditional KGQA methods usually per- form poorly in accurate semantic understanding and high-quality text generation due to the lack of LLMs for r etrie v al and r eason- ing. Hence, r ecent r esearc h is incr easingl y utilizing external KGs to enhance LLMs in addressing KGQA challenges. For instance, StructGPT [ 29 ] navigates through knowledge graphs by identify- ing pathways from an initial seed entity to the target answer en- tity, while Think-on-Gr a ph (ToG) [ 30 ] intr oduces iter ativ e explo- ration of the knowledge gr a ph, whic h can become inefficient with v ery lar ge KGs. Additionall y, Reasoning on Gr a phs (RoG) [ 31 ] ne- cessitates fine-tuning to accur atel y gener ate and plan the r elation paths. KG-GPT [ 32 ] opts for r etrie ving an entire subgraph from the knowledge gr a ph and then deduces the answer thr ough infer ence. Although these methods hav e ac hie v ed gr atifying r esults in gen- er al ar eas, as shown in Fig. 1 B, when the intermediate entity in the multihop question is unknown, it is impossible to retrieve the a ppr opriate knowledge from the KG. In this article, we intr oduce an innov ativ e fr ame work called knowledge gr a ph–based thought (KGT), whic h integr ates LLMs with KGs through employing LLMs for reasoning on the schema of KGs to mitigate factual hallucinations of LLMs, as shown in Fig. 1 C. Unlike traditional methods, KGT does not dir ectl y r etrie v e factual information based on the question. Instead, it uses LLMs to infer entity information on the schema of the knowledge graph, generating an optimal subgraph based on k e y information di- r ectl y extr acted fr om the question and inferr ed information fr om the sc hema. Subsequentl y, the optimal subgr a ph is used to in- fer the answer to the question through LLMs. KGT r equir es no fine-tuning, offers seamless integration with multiple LLMs, and is plug-and-play, facilitating easy deployment. It demonstrates generalizability, making it adaptable for use with diverse knowl- edge gr a phs . T his fr ame work is tailor ed for wide-r anging a ppli- cations in numerous biomedical challenges, such as (i) enhanc- ing clinical decision-making for physicians and medical organi- zations, (ii) delivering medical advice to patients and health care pro viders , (iii) unco vering crucial biomarkers for early disease de- tection and tailored therapy, and (iv) exploring novel therapeutic applications for existing medications through insights into their mechanisms , side effects , and the biological processes of associ- ated diseases . Furthermore , we utilize the'
 '<2-hop>\n\nMeta-analysis Effect size was defined as a metric quantifying the relation\xad ship between variables, including both direction and magni\xad tude.18 For each included study, we calculated the effect size between baseline LLM performance and RAG-enhanced LLM performance. The outcomes focused on the perform\xad ance of generation results, such as accuracy and usefulness. Metrics related to the retrieval process, cost, or speed were not included as outcomes in the meta-analysis. For continu\xad ous outcomes, we used Cohen’s d, standardized between- group mean difference (SMD), calculated as the difference in means divided by the pooled standard deviation. The stand\xad ard error (SE) of SMD was calculated using the following for\xad mula (1), where n1 and n2 represent the sample sizes of each group.19 For dichotomous measurements, we calculated the log-odds ratio, obtained by transforming the odds ratio (OR) with the natural logarithm, and the associated SE was calcu\xad lated using formula (2), where a, b, c, and d represent the number of successful and failed events in the baseline LLM and RAG-enhanced LLM approaches. For studies reporting multiple outcomes, we used the overall outcome to calculate effect size. If no overall outcome was reported, we averaged the effect sizes of all reported outcomes. We excluded out\xad comes with a sample size of less than 30 to avoid small- sample bias. SESMD ¼ ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi n1 þ n2 n1n2 þ SMD2 2 n1 þ n2 ð Þ s (1) SElogOR ¼ ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi 1 a þ 1 b þ 1 c þ 1 d r (2) We developed a random-effect meta-analysis model, because of the variability in RAG architectures and evalua\xad tion datasets among the included studies. The random-effect model was used when individual study effects likely con\xad tained additional sources of variance beyond sampling error. Between-study heterogeneity was assessed using Higgins & Thompson’s I2 statistic, where 25% indicated low heteroge\xad neity, 50% moderate, and 75% substantial.20 We conducted subgroup analyses to explore performance variations across different factors. First, we analyzed the influence of the baseline LLM, referring to the foundation model (eg, GPT-4 or Llama2) that provides the core architec\xad ture for the system. Second, we examined data retrieval strat\xad egies, categorizing them as simple or complex. Simple strategies included fixed-length chunking and basic similarity search, and we performed a subgroup analysis to compare these with complex retrieval strategies. Third, we analyzed differences based on evaluation methods, distinguishing between human evaluations, such as Likert scale ratings for helpfulness and accuracy, and automatic evaluation metrics, including ROUGE-1 and BLEU. Finally, we conducted a sub\xad group analysis based on the type of task, classifying studies 606 Journal of the American Medical Informatics Association, 2025, Vol. 32, No. 4 Downloaded from https://academic.oup.com/jamia/article/32/4/605/7954485 by guest on 11 January 2026']","LLaMA-based models in the biomedical domain have been enhanced through various approaches, including fine-tuning and prompt engineering, to reduce hallucinations and improve performance. The knowledge graph–based thought (KGT) framework integrates LLMs like LLaMA with biomedical knowledge graphs by using LLMs to reason on the schema of knowledge graphs rather than directly retrieving factual information. This method generates an optimal subgraph based on key information extracted from the question and inferred schema data, which is then used by LLMs to infer answers. KGT requires no fine-tuning, supports seamless integration with multiple LLMs including LLaMA, and is plug-and-play, facilitating deployment and generalizability across diverse biomedical knowledge graphs. On the other hand, Llama2 serves as a foundational model in retrieval-augmented generation (RAG) systems, where meta-analyses have quantified the effect size of RAG-enhanced LLM performance compared to baseline LLMs. These analyses, using metrics such as Cohen’s d and log-odds ratios, demonstrate that integrating retrieval mechanisms with Llama2 improves generation accuracy and usefulness. Subgroup analyses further reveal that the choice of baseline LLM, including Llama2, and the complexity of data retrieval strategies influence performance outcomes. Thus, while LLaMA models benefit from knowledge graph integration via frameworks like KGT to mitigate hallucinations and enhance reasoning, Llama2’s role as a base model in RAG systems is supported by meta-analytic evidence showing improved generation performance when retrieval is incorporated.",multi_hop_specific_query_synthesizer,"Based on the provided context, the differences between LLaMA and LLaMA 2 models in their integration with biomedical knowledge graphs (KGs) to mitigate hallucinations in large language models (LLMs), and the impact of this integration on retrieval-augmented generation (RAG) system performance as demonstrated by meta-analyses, can be summarized as follows:

### Differences in Integration of LLaMA and LLaMA 2 with Biomedical Knowledge Graphs

1. **Model Versions and Fine-tuning:**
   - **LLaMA 3** is mentioned as being deployed locally for biomedical settings requiring low latency and strong data control, enabling on-premises inference without cloud dependency (Omar & Mohammed, 2025).
   - **LLaMA 2** has been fine-tuned specifically to convert natural language biomedical questions into logical forms with high accuracy, which then facilitates retrieval of relevant entities and relations from biomedical KGs (Luo et al. [23]).
   - LLaMA 2 is also integrated within frameworks like KG-RAG, which combines a large biomedical knowledge graph (SPOKE) with LLaMA 2 and other LLMs (GPT-3.5, GPT-4) to generate accurate biomedical text (Soman et al. [13]).

2. **Approach to Query Generation and Retrieval:**
   - LLaMA 2 benefits from a **generate-then-retrieve** approach (ChatKBQA framework) that first generates logical forms and then retrieves relevant graph data, improving alignment of biomedical entities and relations and reducing error propagation (Luo et al. [23]).
   - LLaMA 3 is used in a **hybrid LLM to Cypher QA system** that translates natural language questions directly into executable Cypher queries over Neo4j-based biomedical KGs, with schema- and entity-aware prompting to reliably map diverse biomedical questions (Omar & Mohammed, 2025).
   - Both models leverage **prompt-driven query generation** and **evidence transparency** by surfacing generated Cypher queries and supporting subgraphs as justifications, which helps mitigate hallucinations by grounding answers in explicit KG data.

3. **Mitigation of Hallucinations:**
   - Integration of biomedical KGs with LLaMA 2 and LLaMA 3 reduces hallucination errors by grounding responses in structured, validated biomedical knowledge rather than relying solely on the LLM’s internal parameters.
   - Query-checking algorithms and retrieval-augmented generation (RAG) techniques validate and correct KG queries generated by LLMs, ensuring accurate and understandable answers (Pusch and Conrad [6]).
   - KG-RAG with LLaMA 2 demonstrated significant token reduction (over 50%) without loss of accuracy, indicating efficient retrieval and generation that reduces hallucination risk (Soman et al. [13]).

### Impact on Retrieval-Augmented Generation (RAG) System Performance

1. **Meta-Analysis Findings:**
   - A systematic review and meta-analysis (Liu et al., 2025) of 20 studies comparing baseline LLMs with RAG-enhanced LLMs in biomedical tasks found a statistically significant improvement in performance with RAG integration.
   - The pooled effect size was an odds ratio of 1.35 (95% CI: 1.19-1.53, P = .001), indicating that RAG systems outperform baseline LLMs in biomedical question answering and related tasks.
   - This improvement is attributed to RAG’s ability to incorporate up-to-date, relevant biomedical knowledge from external sources like KGs, which enhances accuracy, reduces hallucinations, and supports complex reasoning.

2. **Specific Benefits Observed:**
   - LLaMA 2 combined with biomedical KGs in KG-RAG frameworks showed very high accuracy boosts compared to other retrieval methods and baseline LLMs (Soman et al. [13]).
   - The integration supports **evidence-based, transparent answers** by providing the underlying KG queries and subgraphs, which is critical in clinical and biomedical contexts.
   - RAG systems enable **dynamic updating** of knowledge without retraining the LLM, addressing the limitation of fixed training data and improving reliability in fast-evolving biomedical domains.

3. **Clinical and Biomedical Application Guidelines:**
   - The meta-analysis suggests future directions including deeper integration of knowledge into LLMs, system-level enhancements combining RAG with agents, and embedding RAG within electronic health records for clinical use (Liu et al., 2025).
   - These guidelines emphasize the importance of integrating biomedical KGs with LLMs like LLaMA and LLaMA 2 to mitigate hallucinations and improve clinical decision support.

---

### Summary

- **LLaMA 2** is often fine-tuned and used in frameworks that generate logical forms before retrieval, enhancing accuracy and reducing hallucinations through precise entity and relation alignment.
- **LLaMA 3** is deployed locally with hybrid LLM-to-Cypher QA systems that translate questions directly into graph queries with schema-aware prompting, supporting low-latency, secure biomedical applications.
- Both models benefit from integration with biomedical knowledge graphs to ground answers, reduce hallucinations, and improve interpretability.
- Meta-analyses demonstrate that RAG systems incorporating these models and biomedical KGs significantly outperform baseline LLMs, with an odds ratio improvement of 1.35, validating the effectiveness of this integration in biomedical question answering and clinical applications.

This integration thus represents a critical advancement in mitigating hallucinations and enhancing the reliability and accuracy of biomedical LLM applications.","['Omar & Mohammed / Journal of Applied Science and Technology Trends Vol. 06, No. 02, pp. 342 –357 (2025) \n \n344 \nevaluated using Exact Match, Precision, Recall, F1, Hits@k, \nMRR, and latency across simple, medium, and complex question \nsets. Unlike prior template-based methods, our approach enables \ntraceable, outcome-level validation. In summary, the main \ncontributions of this work are as follows: \n• Hybrid LLM to Cypher QA: A system that translates \nnatural language questions into accurate, executable \nCypher over a Neo4j drug, disease, and symptom KG. \n• Prompt-driven query generation: Schema,  entity \naware prompting that reliably maps diverse biomedical \nquestions to graph queries. \n• Evidence transparency: Along with each answer, we \nsurface the generated Cypher and the supporting \njustification subgraph (nodes, edges) plus a brief natural \nlanguage rationale. \n• Answer-level evaluation: End-to-end assessment using \nExact Match, F1, Precision/Recall, Hits@k, MRR and \nlatency across simple, medium and complex tiers. \n• Local, reproducible deployment: On-prem LLaMA 3 \ninference (no cloud dependency) suitable for biomedical \nsettings requiring low latency and strong data control. \n \nThe remainder of this paper is organized as follows: Section \n2 reviews related work on biomedical knowledge graphs and \nLLM-based QA systems. Section 3 provides background on \nknowledge graphs, large language models, and question \nanswering frameworks. Section 4 details the proposed \nmethodology, \nincluding \nsystem \narchitecture, \ndataset \nconstruction, and query translation. Section 5 presents the \nexperimental results through both quantitative metrics and \nqualitative case studies. Section 6 discusses the findings, analyzes \nlimitations, and compares performance against baseline models. \nFinally, Section 7 concludes this paper and outlines directions for \nfuture work. \nII. RELATED WORK \nRecently, studies have concentrated on the integration of \nclinical and medical knowledge graphs (LLM) to improve the \nanswer to medical questions. Researchers have derived several \nbiomedical KGs using Neo4j and incorporated the application of \nLLMs like LLaMA and GPT to convert natural language \nquestions into graph queries. Improvements in answer \ncorrectness, reduction of hallucination errors, one-to-many \nrelationships, and support for complex reasoning were the \nobjectives of these efforts. Some frameworks also adopted \nretrieval methods to ground responses in secure data. \nSu et al.[11] developed an integrative Biomedical Knowledge \nHub (iBKH), a huge biomedical knowledge graph that comprises \n18 of the very best data sources. The deployment of the iBKH in \nNeo4j allows for a user-friendly web portal to allow fast and \ninteractive knowledge retrieval. The system implemented \nadvanced graph learning techniques to enable the discovery of \nbiomedical knowledge, illustrated by an example of repurposing \nin silico drugs for Alzheimer’s disease. iBKH achieved \npromising predictive performance for known drugs and proposed \npossible new drug candidates. \nRajabi and Kafaie[19] proposed a disease knowledge graph \nusing a cross-referential disease database comprising diseases, \nsymptoms, and drugs interconnected with relationships. They \ntransferred the data into Neo4j to create a graph of 9,400 nodes \nand 45,000 relationships representing the semantic links between \nmedical concepts. Applying Cypher queries enabled answering \ncomplex medical questions regarding identifying drugs that may \ncause certain diseases; it was demonstrated that the graph inferred \nnew relationships not explicitly existing in the original data. The \nconclusion was that disease knowledge graphs sped up clinical \ndiscovery and contributed to understanding complex medical \nrelationships. \nHou et al.[3] assessed and contrasted ChatGPT (both GPT-\n3.5 and GPT-4) and the biomedical knowledge graphs (BKGs) \nconcerning their ability to answer biomedical questions, generate \nnew knowledge, and reason. Their datasets were focused on \ndietary supplements and drugs, while evaluation criteria entailed \naccuracy, novelty, and reasoning ability. The results indicate that \nwhile GPT-4 surpassed GPT-3.5 and BKGs in knowledge \nprovision, it proved inconsistent with regard to citations and \nreasoning. Compared to them, BKGs scored higher in accuracy \nand reliability, especially in discovering novel links as well as \nwithin structured reasoning. \nSoman et al.[13] presented a novel framework called KG-\nRAG that integrates a large biomedical knowledge graph \n(SPOKE) with LLaMA 2, GPT-3.5, and GPT-4 (LLMs) to \nproduce accurate biomedical text. They optimized the retrieval of \nrelevant graph context to cut over 50% tokens without losing \naccuracy. It aided LLMs in performing better on biomedical \nquestion answering with very high accuracy boosts, especially in \nthe case of LLaMA 2. They compared KG-RAG to other retrieval \nmethods and indicated its comparatively more robust and \nefficient results. The framework produced reliable evidence-\nbased answers grounded in biomedical knowledge. \nLuo et al.[23] created ChatKBQA, a new framework with a \nquestion-and-answer approach over knowledge bases that first \ngenerates logical forms with the help of fine-tuned LLMs and \nthen retrieves the relevant entities and relations. This generate-\nthen-retrieve approach is supposed to handle a couple of issues \nwith the earlier methods concerning tedious retrieval and error \npropagation. They fine-tuned open-source LLMs like LLaMA 2 \nto change natural-language questions into logical forms with high \naccuracy. The retrieval phase uses unsupervised phrase-level \nsemantic matching in a way that enhances the alignment of \nentities and relations. Experiments on benchmark datasets \nindicate ChatKBQA to be superior to its predecessors, with the \nhighest accuracy to date. \nPusch and Conrad[6] conducted work under a hybrid \napproach conflating LLMs and biomedical Knowledge Graphs \n(KGs) to suppress hallucination errors in question-answering. \nThey proposed query-checking algorithms for validating, \ncorrecting, and executing the KG Cypher queries that LLMs \ngenerated, thereby attaining accurate and understandable \nanswers. The system used retrieval-augmented generation (RAG) \nto ground answers within KG data. The methodology was \nvalidated on a biomedical KG called PrimeKG using 50 \nbenchmark questions, assessing models like GPT-4 Turbo and \nLLaMA 3. Commercially available GPT-4 Turbo obtained'
 'Research and Applications\nImproving large language model applications in \nbiomedicine with retrieval-augmented generation: a \nsystematic review, meta-analysis, and clinical \ndevelopment guidelines\nSiru Liu, PhD\x03,1,2, Allison B. McCoy \n, PhD1, Adam Wright \n, PhD1,3 \n1Department of Biomedical Informatics, Vanderbilt University Medical Center, Nashville, TN 37212, United States, 2Department of Computer \nScience, Vanderbilt University, Nashville, TN 37212, United States, 3Department of Medicine, Vanderbilt University Medical Center, \nNashville, TN 37212, United States\n\x03Corresponding author: Siru Liu, PhD, Department of Biomedical Informatics, Vanderbilt University Medical Center, 2525 West End Ave #1475, Nashville, \nTN 37212, United States (siru.liu@vumc.org)\nAbstract\nObjective: The objectives of this study are to synthesize findings from recent research of retrieval-augmented generation (RAG) and large lan\xad\nguage models (LLMs) in biomedicine and provide clinical development guidelines to improve effectiveness.\nMaterials and Methods: We conducted a systematic literature review and a meta-analysis. The report was created in adherence to the Pre\xad\nferred Reporting Items for Systematic Reviews and Meta-Analyses 2020 analysis. Searches were performed in 3 databases (PubMed, Embase, \nPsycINFO) using terms related to “retrieval augmented generation” and “large language model,” for articles published in 2023 and 2024. We \nselected studies that compared baseline LLM performance with RAG performance. We developed a random-effect meta-analysis model, using \nodds ratio as the effect size.\nResults: Among 335 studies, 20 were included in this literature review. The pooled effect size was 1.35, with a 95% confidence interval of \n1.19-1.53, indicating a statistically significant effect (P ¼ .001). We reported clinical tasks, baseline LLMs, retrieval sources and strategies, as \nwell as evaluation methods.\nDiscussion: Building on our literature review, we developed Guidelines for Unified Implementation and Development of Enhanced LLM Appli\xad\ncations with RAG in Clinical Settings to inform clinical applications using RAG.\nConclusion: Overall, RAG implementation showed a 1.35 odds ratio increase in performance compared to baseline LLMs. Future research \nshould focus on (1) system-level enhancement: the combination of RAG and agent, (2) knowledge-level enhancement: deep integration of \nknowledge into LLM, and (3) integration-level enhancement: integrating RAG systems within electronic health records.\nKey words: large language model; retrieval augmented generation; systematic review; meta-analysis. \nIntroduction\nLarge language models (LLMs) have reported remarkable \nperformance in question-answering, summarization, and text \ngeneration.1 Given this, researchers have explored its poten\xad\ntial in biomedical areas.2 For example, several studies \nreported the ability of using LLM to answer patient mes\xad\nsages,3 to analyze alert logic in clinical decision support,4 and \nto make discharge summaries more readable to patients.5\nHowever, several challenges remain.\nLLMs are trained on fixed datasets, which restrict their \nknowledge to information available up to the training cut-off \ndate. For example, GPT-4o’s training data only includes \ninformation up to October 2023, making it unable to \nrespond accurately to findings that emerged afterward. LLM \ntraining datasets are also generally broad and lack the specif\xad\nicity required for biomedical applications. Finally, not all \nsources used to train the LLMs are reliable and trustworthy. \nTo address these limitations, researchers have performed \nfine-tuning and retrieval-augmented generation (RAG) tech\xad\nniques. Fine-tuning can adapt LLMs to specific domains, but \nit is resource-intensive and does not allow for real-time \nupdates. In contrast, RAG maintains the original LLM archi\xad\ntecture while incorporating relevant context directly into \nqueries, offering more flexibility and control. In addition, \nRAG’s unique advantage in biomedical applications lies in its \nability to adapt to dynamic environments by delivering up-to- \ndate information and efficiently integrating external knowl\xad\nedge sources with high interpretability.6\nAnother limitation of using LLMs directly is the risk of hal\xad\nlucination, where the model generates incorrect or fabricated \ninformation.7 To mitigate such issues, researchers have pro\xad\nposed RAG as a solution that integrates up-to-date, relevant \ninformation, enhancing both the accuracy and reliability of \nLLM generated responses.8,9 For example, when ChatGPT \nReceived: November 19, 2024; Revised: December 17, 2024; Editorial Decision: January 2, 2025; Accepted: January 3, 2025 \n© The Author(s) 2025. Published by Oxford University Press on behalf of the American Medical Informatics Association.  \nThis is an Open Access article distributed under the terms of the Creative Commons Attribution-NonCommercial License (https://creativecommons.org/licenses/ \nby-nc/4.0/), which permits non-commercial re-use, distribution, and reproduction in any medium, provided the original work is properly cited. For commercial \nre-use, please contact reprints@oup.com for reprints and translation rights for reprints. All other permissions can be obtained through our RightsLink service via \nthe Permissions link on the article page on our site—for further information please contact journals.permissions@oup.com. \nJournal of the American Medical Informatics Association, 2025, 32(4), 605–615 \nhttps://doi.org/10.1093/jamia/ocaf008 \nAdvance access publication 15 January 2025 \nResearch and Applications \nDownloaded from https://academic.oup.com/jamia/article/32/4/605/7954485 by guest on 11 January 2026'
 '17. Lefebvre C, Glanville J, Briscoe S, et al. Chapter 4: searching for \nand selecting studies. Cochrane Handbook for Systematic Reviews \nof Interventions Version, Vol. 6. Cochrane, 2024. https://training. \ncochrane.org/handbook/current/chapter-04\n18. Chapter 3 Effect Sizes j Doing Meta-Analysis in R. Accessed Octo\xad\nber 13, 2024. https://bookdown.org/MathiasHarrer/Doing_Meta_ \nAnalysis_in_R/effects.html\n19. Borenstein M, Hedges LV, Higgins JPT, et al. Introduction to \nMeta-Analysis. John Wiley & Sons; 2011.\n20. Higgins JPT, Thompson SG. Quantifying heterogeneity in a meta- \nanalysis. Stat Med. 2002;21:1539-1558. https://doi.org/10.1002/ \nsim.1186\n21. Peters JL, Sutton AJ, Jones DR, et al. Contour-enhanced meta- \nanalysis funnel plots help distinguish publication bias from other \ncauses of asymmetry. J Clin Epidemiol. 2008;61:991-996. https:// \ndoi.org/10.1016/j.jclinepi.2007.11.010\n22. Egger M, Smith GD, Schneider M, et al. Bias in meta-analysis detected \nby a simple, graphical test measures of funnel plot asymmetry. BMJ. \n1997;315:629-634. https://doi.org/10.1136/bmj.315.7109.629\n23. Kreimeyer K, Canzoniero JV, Fatteh M, et al. Using retrieval- \naugmented generation to capture molecularly-driven treatment \nrelationships for precision oncology. Stud Health Technol Inform. \n2024;316:983-987. https://doi.org/10.3233/SHTI240575\n24. Murugan M, Yuan B, Venner E, et al. Empowering personalized \npharmacogenomics with generative AI solutions. J Am Med Inform \nAssoc. 2024;31:1356-1366. https://doi.org/10.1093/jamia/ocae039\n25. Yazaki M, Maki S, Furuya T, et al. Emergency patient triage \nimprovement through a retrieval-augmented generation enhanced \nlarge-scale language model. Prehosp Emerg Care. 2024;1-7. \nhttps://doi.org/10.1080/10903127.2024.2374400\n26. Malik S, Kharel H, Dahiya DS, et al. Assessing ChatGPT4 with \nand without retrieval-augmented generation in anticoagulation \nmanagement for gastrointestinal procedures. Ann Gastroenterol. \n2024;37:514-526. https://doi.org/10.20524/aog.2024.0907\n27. Kresevic S, Giuffr\x12e M, Ajcevic M, et al. Optimization of hepatolog\xad\nical clinical guidelines interpretation by large language models: a \nretrieval augmented generation-based framework. NPJ Digit Med. \n2024;7:102-109. https://doi.org/10.1038/s41746-024-01091-y\n28. Zakka C, Shad R, Chaurasia A, et al. Almanac–retrieval-aug\xad\nmented language models for clinical medicine. NEJM AI. 2024;1 \n(2):10.1056/aioa2300068. https://doi.org/10.1056/aioa2300068\n29. Long C, Subburam D, Lowe K, et al. ChatENT: augmented large \nlanguage model for expert knowledge retrieval in otolaryngology– \nhead and neck surgery. Otolaryngol Head Neck Surg. \n2024;171:1042-1051. https://doi.org/10.1002/ohn.864\n30. Wang D, Liang J, Ye J, et al. Enhancement of the performance of \nlarge language models in diabetes education through retrieval-aug\xad\nmented generation: comparative study. J Med Internet Res. \n2024;26:e58041. https://doi.org/10.2196/58041\n31. Rau S, Rau A, Nattenm€uller J, et al. A retrieval-augmented chatbot \nbased on GPT-4 provides appropriate differential diagnosis in gas\xad\ntrointestinal radiology: a proof of concept study. Eur Radiol Exp. \n2024;8:60. https://doi.org/10.1186/s41747-024-00457-x\n32. Morris JH, Soman K, Akbas RE, et al. The scalable precision \nmedicine open knowledge engine (SPOKE): a massive knowl\xad\nedge \ngraph \nof \nbiomedical \ninformation. \nBioinformatics. \n2023;39(2):btad080. \nhttps://doi.org/10.1093/BIOINFOR\xad\nMATICS/BTAD080\n33. Soman K, Rose PW, Morris JH, et al. Biomedical knowl\xad\nedge graph-optimized prompt generation for large language \nmodels. Commun ACM. 2023;66:7-7. https://doi.org/10.1145/ \n3606337\n34. Jeong M, Sohn J, Sung M, et al. Improving medical reasoning \nthrough retrieval and self-reflection with retrieval-augmented large \nlanguage models. Bioinformatics. 2024;40:i119-i129. https://doi. \norg/10.1093/bioinformatics/btae238\n35. Alkhalaf M, Yu P, Yin M, et al. Applying generative AI with \nretrieval augmented generation to summarize and extract key \nclinical information from electronic health records. J Biomed \nInform. \n2024;156:104662. \nhttps://doi.org/10.1016/j.jbi.2024. \n104662\n36. Tarabanis C, Zahid S, Mamalis M, et al. Performance of publicly \navailable large language models on internal medicine board-style \nquestions. PLOS Digit Health. 2024;3:e0000604. https://doi.org/ \n10.1371/journal.pdig.0000604\n37. Glicksberg BS, Timsina P, Patel D, et al. Evaluating the accuracy of \na state-of-the-art large language model for prediction of admis\xad\nsions from the emergency room. J Am Med Inform Assoc. \n2024;31:1921-1928. https://doi.org/10.1093/jamia/ocae103\n38. Zelin C, Chung WK, Jeanne M, et al. Rare disease diagnosis using \nknowledge guided retrieval augmentation for ChatGPT. J Biomed \nInform. 2024;157:104702. https://doi.org/10.1016/j.jbi.2024.104702\n39. Chen X, Wang L, You MK, et al. Evaluating and enhancing large \nlanguage models’ performance in domain-specific medicine: devel\xad\nopment and usability study with DocOA. J Med Internet Res. \n2024;26:e58158. https://doi.org/10.2196/58158\n40. Du X, Novoa-Laurentiev J, Plasaek JM, et al. Enhancing early \ndetection of cognitive decline in the elderly: a comparative study \nutilizing large language models in clinical notes. medRxiv. \n2024;109:105401. https://doi.org/10.1101/2024.04.03.24305298\n41. Zhang Y, Li Y, Cui L, et al. Siren’s song in the AI Ocean: a survey \non hallucination in large language models. 2023, preprint: not \npeer reviewed. https://arxiv.org/abs/2309.01219\n42. Li X, Ouyang JA. Systematic investigation of knowledge retrieval \nand selection for retrieval augmented generation. 2024, preprint: \nnot peer reviewed. https://arxiv.org/abs/2410.13258\n43. Xiong G, Jin Q, Lu Z, et al. Benchmarking retrieval-augmented \ngeneration for medicine. Findings of the Association for Computa\xad\ntional Linguistics: ACL 2024, 6233-6251. Bangkok, Thailand: \nAssociation for Computational Linguistics. \n44. Hewitt KJ, Wiest IC, Carrero ZI, et al. Large language models as a \ndiagnostic support tool in neuropathology. J Pathol Clin Res. \n2024;10:e70009. https://doi.org/10.1002/2056-4538.70009\n45. Allahverdiyev R, Taha M, Akalin A, et al. ChunkRAG: novel \nLLM-chunk filtering method for RAG systems. October 25, 2024, \npreprint: not peer reviewed. https://arxiv.org/abs/2410.19572\n46. Cai B, Zhang FL, Wang C. Research on chunking algorithms of \ndata de-duplication. Advances in Intelligent Systems and Comput\xad\ning, Vol. 181. 2013:1019-1025. Springer, Berlin, Heidelberg. \nhttps://doi.org/10.1007/978-3-642-31698-2_144\n47. Optimizing RAG with Advanced Chunking Techniques. Accessed \nDecember 15, 2024. https://antematter.io/blogs/optimizing-rag- \nadvanced-chunking-techniques-study\n48. Chen J, Xiao S, Zhang P, et al. M3-Embedding: multi-lingual, \nmulti-functionality, multi-granularity text embeddings through \nself-knowledge distillation. Findings of the Association for Com\xad\nputational Linguistics ACL 2024. 2024:2318-2335. Association \nfor Computational Linguistics. https://doi.org/10.18653/V1/2024. \nFINDINGS-ACL.137\n49. Sawarkar K, Mangal A, Solanki SR. Blended RAG: improving \nRAG (Retriever-Augmented Generation) accuracy with semantic \nsearch and hybrid query-based retrievers. 2024 IEEE 7th Interna\xad\ntional Conference on Multimedia Information Processing and \nRetrieval (MIPR), 155-161. San Jose, CA, USA: IEEE. https://doi. \norg/10.1109/MIPR62202.2024.00031\n50. Edge D, Trinh H, Cheng N, et al. From local to global: a graph \nRAG approach to query-focused summarization. 2024, preprint: \nnot peer reviewed. https://arxiv.org/abs/2404.16130\n51. Wu Z, Hasan A, Wu J, et al. KnowLab_AIMed at MEDIQA- \nCORR 2024: Chain-of-Though (CoT) prompting strategies for \nmedical error detection and correction. Proceedings of the 6th \nClinical Natural Language Processing Workshop. 2024:353-359. \nAssociation for Computational Linguistics. 10.18653/v1/2024. \nclinicalnlp-1.33\n52. Kwon T, Tzu-Iunn Ong K, Kang D, et al. Large language models \nare clinical reasoners: reasoning-aware diagnosis framework with \n614                                                                                                      \nJournal of the American Medical Informatics Association, 2025, Vol. 32, No. 4 \nDownloaded from https://academic.oup.com/jamia/article/32/4/605/7954485 by guest on 11 January 2026']"
"How does the integration of single-cell RNA sequencing data from the cellxgene Atlas enhance the co-expression networks in STRING version 12.5, and what advantages does this expansion provide for understanding gene expression dynamics across different organisms?","['<1-hop>\n\nNucleic Acids Research , 2025, Vol. 53, Database issue D 737 networks in all domains of life, supporting directed links and tissue-specificity. J. Mol. Biol., 433 , 166835. 13. Kim, C.Y. , Baek, S. , Cha, J. , Yang, S. , Kim, E. , Marcotte, E.M. , Hart, T. and Lee,I. (2022) HumanNet v3: an improved database of human gene networks for disease research. Nucleic Acids Res., 50 , D632–D639. 14. Huang, J.K. , Carlin, D.E. , Yu, M.K. , Zhang, W. , Kreisberg, J.F. , Tamayo, P. and Ideker, T. (2018) Systematic evaluation of molecular networks for discovery of disease genes. Cell Syst. , 6 , 484–495.e5. 15. Wright, S.N. , Colton, S. , Schaffer, L.V. , Pillich, R.T. , Churas, C. , Pratt, D. and Ideker, T. (2024) State of the interactomes: an evaluation of molecular networks for generating biological insights. bioRxiv doi: https:// doi.org/ 10.1101/ 2024.04.26.587073 , 29 April 2024, preprint: not peer reviewed. 16. Ontology Consortium, G. , Aleksander, S.A. , Balhoff, J. , Carbon, S. , Cherry, J.M. , Drabkin, H.J. , Ebert, D. , Feuermann, M. , Gaudet, P. , Harris, N.L. , et al. (2023) The Gene Ontology knowledgebase in 2023. Genetics , 224 , iyad031. 17. Liberzon, A. , Birger, C. , Thorvaldsdóttir, H. , Ghandi, M. , Mesirov, J.P. and Tamayo, P. (2015) The Molecular Signatures Database hallmark gene set collection. Cell Syst. , 1 , 417–425. 18. Amberger, J.S. , Bocchini, C.A. , Schiettecatte, F. , Scott, A.F. and Hamosh,A. (2015) OMIM.org: Online Mendelian Inheritance in Man (OMIM ®), an online catalog of human genes and genetic disorders. Nucleic Acids Res. , 43 , D789–D798. 19. Snel, B. , Bork, P. and Huynen, M.A. (2002) The identification of functional modules from the genomic association of genes. Proc. Natl Acad. Sci. U.S.A., 99 , 5890–5895. 20. Guala, D. , Ogris, C. , Müller, N. and Sonnhammer, E.L.L. (2020) Genome-wide functional association networks: background, data & state-of-the-art resources. Brief. Bioinform., 21 , 1224. 21. Szklarczyk, D. , Gable, A.L. , Nastou, K.C. , Lyon, D. , Kirsch, R. , Pyysalo, S. , Doncheva, N.T. , Legeay, M. , Fang, T. , Bork, P. , et al. (2021) The STRING database in 2021: customizable protein–protein networks, and functional characterization of user-uploaded gene / measurement sets. Nucleic Acids Res., 49 , D605–D612. 22. Mering,C.V. (2003) STRING: a database of predicted functional associations between proteins. Nucleic Acids Res. , 31 , 258–261. 23. Enright, A.J. , Iliopoulos, I. , Kyrpides, N.C. and Ouzounis, C.A. (1999) Protein interaction maps for complete genomes based on gene fusion events. Nature , 402 , 86–90. 24. Franceschini, A. , Lin, J. , von Mering, C. and Jensen, L.J. (2016) SVD-phy: improved prediction of protein functional associations through singular value decomposition of phylogenetic profiles. Bioinformatics , 32 , 1085–1087. 25. Koutrouli, M. , Nastou, K. , Piera Líndez, P. , Bouwmeester, R. , Rasmussen, S. , Martens, L. and Jensen, L.J. (2024) FAVA: high-quality functional association networks inferred from scRNA-seq and proteomics data. Bioinformatics , 40 , btae010. 26. Porras, P. , Barrera, E. , Bridge, A. , Del-Toro, N. , Cesareni, G. , Duesbury, M. , Hermjakob, H. , Iannuccelli, M. , Jurisica, I. , Kotlyar, M. , et al. (2020) Towards a unified open access dataset of molecular interactions. Nat. Commun., 11 , 6144. 27. Franceschini, A. , Szklarczyk, D. , Frankild, S. , Kuhn, M. , Simonovic, M. , Roth, A. , Lin, J. , Minguez, P. , Bork, P. , von Mering, C. , et al. (2013) STRING v9.1: protein–protein interaction networks, with increased coverage and integration. Nucleic Acids Res., 41 , D808–D815. 28. Yu, H. , Luscombe, N.M. , Lu, H.X. , Zhu, X. , Xia, Y. , Han, J .-D.J ., Bertin, N. , Chung, S. , V idal, M. and Gerstein, M. (2004) Annotation transfer between genomes: protein–protein interologs and protein–DNA regulogs. Genome Res. , 14 , 1107–1118. 29. Szklarczyk, D. , Gable, A.L. , Lyon, D. , Junge, A. , Wyder, S. , Huerta-Cepas, J. , Simonovic, M. , Doncheva, N.T. , Morris, J.H. , Bork, P. , et al. (2019) STRING v11: protein–protein association networks with increased coverage, supporting functional discovery in genome-wide experimental datasets. Nucleic Acids Res., 47 , D607–D613. 30. Doncheva, N.T. , Morris, J.H. , Gorodkin, J. and Jensen, L.J. (2019) Cytoscape StringApp: network analysis and visualization of proteomics data. J. Proteome Res., 18 , 623–632. 31. Jensen, L.J. , Kuhn, M. , Stark, M. , Chaffron, S. , Creevey, C. , Muller, J. , Doerks, T. , Julien, P. , Roth, A. , Simonovic, M. , et al. (2009) STRING 8—a global view on proteins and their functional interactions in 630 organisms. Nucleic Acids Res. , 37 , D412–D416. 32. Nastou, K. , Mehryary, F. , Ohta, T. , Luoma, J. , Pyysalo, S. and Jensen,L.J. (2024) RegulaTome: a corpus of typed, directed, and signed relations between biomedical entities in the scientific literature. Database , 2024 , baae095. 33. Megill, C. , Martin, B. , Weaver, C. , Bell, S. , Prins, L. , Badajoz, S. , McCandless, B. , Pisco, A.O. , Kinsella, M. , Griffin, F. , et al. (2021) cellxgene: a performant, scalable exploration platform for high dimensional sparse matrices. bioRxiv doi: https:// doi.org/ 10.1101/ 2021.04.05.438318 , 6 April 2021, preprint: not peer reviewed. 34. Moreno, P. , Fexova, S. , George, N. , Manning, J.R. , Miao, Z. , Mohammed, S. , Muñoz-Pomer, A. , Fullgrabe, A. , Bi, Y. , Bush, N. , et al. (2022) Expression Atlas update: gene and protein expression in multiple species. Nucleic Acids Res. , 50 , D129–D140. 35. T immons, J.A. , Szkop, K.J. and Gallagher, I.J. (2015) Multiple sources of bias confound functional enrichment analysis of global -omics data. Genome Biol. , 16 , 186. 36. Benjamini, Y. and Hochberg, Y. (1995) Controlling the false discovery rate: a practical and powerful approach to multiple testing. J. R. Stat. Soc. Ser. B Stat. Methodol., 57 ,'
 '<2-hop>\n\nExpanded co-expression networks The co-expression channel in STRING identifies genes with similar expression profiles across various tissues and condi- tions, revealing their involvement in shared biological pro- cesses. It compiles data from large-scale gene expression stud- ies, analyzing transcript and protein levels to detect gene pairs with coordinated expression. This method highlights poten- tial functional relationships between genes. Importantly, the analysis is free from study bias, as it directly uses raw omics data instead of relying on manually curated or experimental datasets, providing a more precise and objective view of gene interactions. For the upcoming version 12.5, we are expanding the co-expression channel by generating additional networks through the application of FAVA (Functional Association of Variational Autoencoders) ( 25 ) on a wider range of single- cell RNA sequencing data. Specifically, we are incorporating single-cell data from the cellxgene Atlas ( 33 ) to enhance the human and mouse networks and from the EBI Single Cell Ex- pression Atlas to extend coverage to more model organisms ( 34 ). This expansion will provide more detailed, organism- specific co-expression maps, enabling more precise identifi- cation of gene co-expression patterns and cross-species com- parisons, particularly for development, immune response and disease-related gene expression dynamics. Impro v ed enric hment analysis Gene set enrichment analysis is a critical component of high- throughput biological studies, providing key insights into the biological characteristics of datasets. This analysis is pivotal in identifying essential aspects such as biological processes, path- ways and phenotypes, which are crucial for understanding the biological implications of experiments. Typically, overrepre- sentation analysis is employed, wherein a subset of data ex- ceeding a specific cutoff (empirical or customary) is compared against an experimental background. By default, STRING Downloaded from https://academic.oup.com/nar/article/53/D1/D730/7903368 by guest on 11 January 2026']","The integration of single-cell RNA sequencing data from the cellxgene Atlas into STRING version 12.5 enhances the co-expression networks by generating additional networks through the application of FAVA (Functional Association of Variational Autoencoders) on a wider range of single-cell data. This incorporation specifically improves the human and mouse networks and, together with data from the EBI Single Cell Expression Atlas, extends coverage to more model organisms. This expansion provides more detailed, organism-specific co-expression maps, enabling more precise identification of gene co-expression patterns and facilitating cross-species comparisons. These improvements are particularly valuable for studying development, immune response, and disease-related gene expression dynamics, offering a more precise and objective view of gene interactions by directly using raw omics data free from study bias.",multi_hop_specific_query_synthesizer,"The integration of single-cell RNA sequencing (scRNA-seq) data from the cellxgene Atlas into STRING version 12.5 enhances the co-expression networks by providing additional, high-resolution gene expression data that captures gene expression profiles at the single-cell level. Specifically, STRING 12.5 applies the FAVA (Functional Association of Variational Autoencoders) method to this expanded dataset, which includes scRNA-seq data from the cellxgene Atlas for human and mouse, as well as data from the EBI Single Cell Expression Atlas for other model organisms.

**How this integration enhances co-expression networks:**

1. **Increased Data Diversity and Resolution:**  
   Incorporating single-cell data allows STRING to analyze gene expression patterns with finer granularity compared to bulk tissue data. This leads to more detailed and accurate identification of genes that are co-expressed within specific cell types or states.

2. **Organism-Specific Networks:**  
   The inclusion of cellxgene Atlas data improves the human and mouse co-expression networks, while data from the EBI Single Cell Expression Atlas extends coverage to additional model organisms. This results in more comprehensive and organism-specific co-expression maps.

3. **Improved Functional Associations:**  
   By leveraging scRNA-seq data, STRING can detect coordinated gene expression patterns that reflect functional relationships more precisely, especially in complex biological contexts such as development, immune response, and disease.

**Advantages of this expansion for understanding gene expression dynamics:**

- **More Precise Identification of Co-expression Patterns:**  
  The high resolution of single-cell data enables the detection of subtle and cell-type-specific co-expression relationships that might be obscured in bulk data.

- **Cross-Species Comparisons:**  
  With expanded coverage across multiple organisms, researchers can perform comparative analyses of gene expression dynamics, facilitating the study of conserved and divergent regulatory mechanisms.

- **Enhanced Insights into Biological Processes:**  
  The detailed co-expression networks support better understanding of gene regulation during critical biological processes such as development and immune responses, as well as in disease contexts.

In summary, integrating single-cell RNA sequencing data from the cellxgene Atlas into STRING 12.5 enriches the co-expression networks by providing high-resolution, organism-specific gene expression information. This expansion enables more accurate and detailed mapping of gene co-expression patterns, thereby improving the understanding of gene expression dynamics across different organisms and biological conditions.","['Nucleic Acids Research , 2025, Vol. 53, Database issue \nD 733 \nFigure 1. Illustration of the new ‘regulatory network’ mode in STRING, where the network edges visually indicate the direction, confidence and sources \nof each regulatory interaction. Clicking on an edge within the network brings up a pop-up window with a detailed o v ervie w of the a v ailable e vidence and \nthe annotated type of regulation. Users can further explore the data behind the interactions by accessing the specific evidence viewers linked within the \npop-up, including the text-mining evidence viewer. This viewer presents excerpts from literature pertaining to the inspected interactions, as well as an \nautomatically generated large language model summary of the presented e x cerpts. \nrylation , Catalysis of Small Molecule Conjugation and Cataly- \nsis of Small Molecule Removal . To enhance the interoperabil- \nity of the annotations, these chosen relation types align with \nGene Ontology ( 32 ). \nTo identify and classify a wide array of interaction types, we \nfine-tuned the RoBERT a-large-PM-M3-V oc language model, \na transformer-based model well suited for biological tasks. \nThis model was trained on the RegulaTome dataset for multi- \nlabel extraction of the directed, typed and signed interactions \nmentioned above, and achieved an average F 1 score of 73.5% \n(with a precision of 75.2% and a recall of 71.8%) on a held- \nout test set, although the exact performance varied across dif- \nferent types of relationships. This level of accuracy is substan- \ntial, considering the complexity of the underlying task. Using \nthis model, we processed over 1.2 billion sentence-level pairs \nextracted from all available PubMed abstracts and PMC Open \nAccess full-text documents, and assigned 3.5% of them ( ∼43 \nmillion, of which ∼18 million in human) with at least one \npositive label indicating directed (72.9%) or signed (33.1%) \nrelationships among the proteins. Furthermore, we made use \nof the existing curated knowledge of regulatory interactions in \nthe database channel, by parsing regulatory information from \nthe SIGNOR, KEGG and Reactome databases. \nTo integrate these interactions into STRING, we followed \na similar score aggregation and benchmarking approach as \nfor physical interactions ( 21 ). Here, we perform benchmark- \ning in five categories for which we can derive gold stan- \ndard datasets of known human regulatory interactions from \nSIGNOR: regulation , upregulation , downregulation , tran- \nscriptional regulation and phosphorylation . The resulting \ncalibration functions are then applied to extracted regula- \ntions of all types to produce the final regulatory confidence \nscores. \nThe new regulatory network has been seamlessly integrated \ninto the existing STRING framework, complementing the \n‘functional’ and ‘physical’ network types. Directional edges \nwithin this network are visually represented by arrows, depict- \ning both bidirectional and unidirectional relationships. The \nnetwork visualization can show varying confidence levels be- \ntween directions (in the ‘confidence’ view) or the specific di- \nrectionality of the sources (in the ‘evidence’ view). The user \ninterface retains its intuitive design, enabling users to access \nthe type of regulatory event and the evidence for the interac- \ntion by clicking on the edge (Figure 1 ). Additionally, all API \nfunctions have been updated to fully support the new net- \nwork type, which can be accessed by specifying the parameter \nnetw or k_type = regulatory in the API call. \nExpanded co-expression networks \nThe co-expression channel in STRING identifies genes with \nsimilar expression profiles across various tissues and condi- \ntions, revealing their involvement in shared biological pro- \ncesses. It compiles data from large-scale gene expression stud- \nies, analyzing transcript and protein levels to detect gene pairs \nwith coordinated expression. This method highlights poten- \ntial functional relationships between genes. Importantly, the \nanalysis is free from study bias, as it directly uses raw omics \ndata instead of relying on manually curated or experimental \ndatasets, providing a more precise and objective view of gene \ninteractions. \nFor the upcoming version 12.5, we are expanding the \nco-expression channel by generating additional networks \nthrough the application of FAVA (Functional Association of \nVariational Autoencoders) ( 25 ) on a wider range of single- \ncell RNA sequencing data. Specifically, we are incorporating \nsingle-cell data from the cellxgene Atlas ( 33 ) to enhance the \nhuman and mouse networks and from the EBI Single Cell Ex- \npression Atlas to extend coverage to more model organisms \n( 34 ). This expansion will provide more detailed, organism- \nspecific co-expression maps, enabling more precise identifi- \ncation of gene co-expression patterns and cross-species com- \nparisons, particularly for development, immune response and \ndisease-related gene expression dynamics. \nImpro v ed enric hment analysis \nGene set enrichment analysis is a critical component of high- \nthroughput biological studies, providing key insights into the \nbiological characteristics of datasets. This analysis is pivotal in \nidentifying essential aspects such as biological processes, path- \nways and phenotypes, which are crucial for understanding the \nbiological implications of experiments. Typically, overrepre- \nsentation analysis is employed, wherein a subset of data ex- \nceeding a specific cutoff (empirical or customary) is compared \nagainst an experimental background. By default, STRING \nDownloaded from https://academic.oup.com/nar/article/53/D1/D730/7903368 by guest on 11 January 2026'
 'Nucleic Acids Research , 2025, Vol. 53, Database issue \nD 737 \nnetworks in all domains of life, supporting directed links and \ntissue-specificity. J. Mol. Biol., 433 , 166835.\n13. Kim, C.Y. , Baek, S. , Cha, J. , Yang, S. , Kim, E. , Marcotte, E.M. , Hart, T. \nand Lee,I. (2022) HumanNet v3: an improved database of human \ngene networks for disease research. Nucleic Acids Res., 50 , \nD632–D639.\n14. Huang, J.K. , Carlin, D.E. , Yu, M.K. , Zhang, W. , Kreisberg, J.F. , \nTamayo, P. and Ideker, T. (2018) Systematic evaluation of molecular \nnetworks for discovery of disease genes. Cell Syst. , 6 , 484–495.e5. \n15. Wright, S.N. , Colton, S. , Schaffer, L.V. , Pillich, R.T. , Churas, C. , \nPratt, D. and Ideker, T. (2024) State of the interactomes: an \nevaluation of molecular networks for generating biological \ninsights. bioRxiv doi: \nhttps:// doi.org/ 10.1101/ 2024.04.26.587073 , 29 April 2024, \npreprint: not peer reviewed.\n16. Ontology Consortium, G. , Aleksander, S.A. , Balhoff, J. , Carbon, S. , \nCherry, J.M. , Drabkin, H.J. , Ebert, D. , Feuermann, M. , Gaudet, P. , \nHarris, N.L. , et al. (2023) The Gene Ontology knowledgebase in \n2023. Genetics , 224 , iyad031.\n17. Liberzon, A. , Birger, C. , Thorvaldsdóttir, H. , Ghandi, M. , \nMesirov, J.P. and Tamayo, P. (2015) The Molecular Signatures \nDatabase hallmark gene set collection. Cell Syst. , 1 , 417–425. \n18. Amberger, J.S. , Bocchini, C.A. , Schiettecatte, F. , Scott, A.F. and \nHamosh,A. (2015) OMIM.org: Online Mendelian Inheritance in \nMan (OMIM ®), an online catalog of human genes and genetic \ndisorders. Nucleic Acids Res. , 43 , D789–D798. \n19. Snel, B. , Bork, P. and Huynen, M.A. (2002) The identification of \nfunctional modules from the genomic association of genes. Proc. \nNatl Acad. Sci. U.S.A., 99 , 5890–5895.\n20. Guala, D. , Ogris, C. , Müller, N. and Sonnhammer, E.L.L. (2020) \nGenome-wide functional association networks: background, data \n& state-of-the-art resources. Brief. Bioinform., 21 , 1224.\n21. Szklarczyk, D. , Gable, A.L. , Nastou, K.C. , Lyon, D. , Kirsch, R. , \nPyysalo, S. , Doncheva, N.T. , Legeay, M. , Fang, T. , Bork, P. , et al. \n(2021) The STRING database in 2021: customizable \nprotein–protein networks, and functional characterization of \nuser-uploaded gene / measurement sets. Nucleic Acids Res., 49 , \nD605–D612.\n22. Mering,C.V. (2003) STRING: a database of predicted functional \nassociations between proteins. Nucleic Acids Res. , 31 , 258–261. \n23. Enright, A.J. , Iliopoulos, I. , Kyrpides, N.C. and Ouzounis, C.A. \n(1999) Protein interaction maps for complete genomes based on \ngene fusion events. Nature , 402 , 86–90.\n24. Franceschini, A. , Lin, J. , von Mering, C. and Jensen, L.J. (2016) \nSVD-phy: improved prediction of protein functional associations \nthrough singular value decomposition of phylogenetic profiles. \nBioinformatics , 32 , 1085–1087.\n25. Koutrouli, M. , Nastou, K. , Piera Líndez, P. , Bouwmeester, R. , \nRasmussen, S. , Martens, L. and Jensen, L.J. (2024) FAVA: \nhigh-quality functional association networks inferred from \nscRNA-seq and proteomics data. Bioinformatics , 40 , btae010.\n26. Porras, P. , Barrera, E. , Bridge, A. , Del-Toro, N. , Cesareni, G. , \nDuesbury, M. , Hermjakob, H. , Iannuccelli, M. , Jurisica, I. , \nKotlyar, M. , et al. (2020) Towards a unified open access dataset of \nmolecular interactions. Nat. Commun., 11 , 6144.\n27. Franceschini, A. , Szklarczyk, D. , Frankild, S. , Kuhn, M. , \nSimonovic, M. , Roth, A. , Lin, J. , Minguez, P. , Bork, P. , von Mering, C. , \net al. (2013) STRING v9.1: protein–protein interaction networks, \nwith increased coverage and integration. Nucleic Acids Res., 41 , \nD808–D815.\n28. Yu, H. , Luscombe, N.M. , Lu, H.X. , Zhu, X. , Xia, Y. , Han, J .-D.J ., \nBertin, N. , Chung, S. , V idal, M. and Gerstein, M. (2004) Annotation \ntransfer between genomes: protein–protein interologs and \nprotein–DNA regulogs. Genome Res. , 14 , 1107–1118. \n29. Szklarczyk, D. , Gable, A.L. , Lyon, D. , Junge, A. , Wyder, S. , \nHuerta-Cepas, J. , Simonovic, M. , Doncheva, N.T. , Morris, J.H. , \nBork, P. , et al. (2019) STRING v11: protein–protein association \nnetworks with increased coverage, supporting functional discovery \nin genome-wide experimental datasets. Nucleic Acids Res., 47 , \nD607–D613.\n30. Doncheva, N.T. , Morris, J.H. , Gorodkin, J. and Jensen, L.J. (2019) \nCytoscape StringApp: network analysis and visualization of \nproteomics data. J. Proteome Res., 18 , 623–632.\n31. Jensen, L.J. , Kuhn, M. , Stark, M. , Chaffron, S. , Creevey, C. , Muller, J. , \nDoerks, T. , Julien, P. , Roth, A. , Simonovic, M. , et al. (2009) STRING \n8—a global view on proteins and their functional interactions in \n630 organisms. Nucleic Acids Res. , 37 , D412–D416. \n32. Nastou, K. , Mehryary, F. , Ohta, T. , Luoma, J. , Pyysalo, S. and \nJensen,L.J. (2024) RegulaTome: a corpus of typed, directed, and \nsigned relations between biomedical entities in the scientific \nliterature. Database , 2024 , baae095.\n33. Megill, C. , Martin, B. , Weaver, C. , Bell, S. , Prins, L. , Badajoz, S. , \nMcCandless, B. , Pisco, A.O. , Kinsella, M. , Griffin, F. , et al. (2021) \ncellxgene: a performant, scalable exploration platform for high \ndimensional sparse matrices. bioRxiv doi: \nhttps:// doi.org/ 10.1101/ 2021.04.05.438318 , 6 April 2021, \npreprint: not peer reviewed.\n34. Moreno, P. , Fexova, S. , George, N. , Manning, J.R. , Miao, Z. , \nMohammed, S. , Muñoz-Pomer, A. , Fullgrabe, A. , Bi, Y. , Bush, N. , \net al. (2022) Expression Atlas update: gene and protein expression \nin multiple species. Nucleic Acids Res. , 50 , D129–D140. \n35. T immons, J.A. , Szkop, K.J. and Gallagher, I.J. (2015) Multiple \nsources of bias confound functional enrichment analysis of global \n-omics data. Genome Biol. , 16 , 186. \n36. Benjamini, Y. and Hochberg, Y. (1995) Controlling the false \ndiscovery rate: a practical and powerful approach to multiple \ntesting. J. R. Stat. Soc. Ser. B Stat. Methodol., 57 , 289–300.\n37. Novikov,A. (2019) PyClustering: data mining library. J. Open \nSource Softw., 4 , 1230.\n38. Van Dongen,S. (2008) Graph clustering via a discrete uncoupling \nprocess. SIAM J. Matrix Anal. Appl., 30 , 121–141.\n39. Elnaggar, A. , Heinzinger, M. , Dallago, C. , Rehawi, G. , Wang, Y. , \nJones, L. , Gibbs, T. , Feher, T. , Angerer, C. , Steinegger, M. , et al. \n(2022) ProtTrans: toward understanding the language of life \nthrough self-supervised learning. IEEE Trans. Pattern Anal. Mach. \nIntell., 44 , 7112–7127.\n40. Baumgartner, M. , Dell’Aglio, D. , Paulheim, H. and Bernstein, A. \n(2023) Towards the Web of Embeddings: integrating multiple \nknowledge graph embedding spaces with FedCoder. J. Web \nSemant., 75 , 100741.\nReceived: September 15, 2024. Revised: October 18, 2024. Editorial Decision: October 21, 2024. Accepted: October 29, 2024 \n© The Author(s) 2024. Published by Oxford University Press on behalf of Nucleic Acids Research. \nThis is an Open Access article distributed under the terms of the Creative Commons Attribution License (https: // creativecommons.org / licenses / by / 4.0 / ), which permits unrestricted reuse, \ndistribution, and reproduction in any medium, provided the original work is properly cited. \nDownloaded from https://academic.oup.com/nar/article/53/D1/D730/7903368 by guest on 11 January 2026'
 'D 732 \nNucleic Acids Research , 2025, Vol. 53, Database issue \ncific network modes are fully consistent with the functional \nnetwork, such that if an interaction is present in either the \nphysical or regulatory network, it will, by definition, also be \npresent in the full functional association network (with an \nequal or higher confidence score). To compute the various con- \nfidence scores, the available evidence supporting a given inter- \naction is first separated into ‘evidence channels’, by the type \nof evidence. For the basic functional association confidence \nscore, seven such channels are used. These include three that \nare based on genomic context predictions—neighborhood, fu- \nsion and gene co-occurrence—as well as channels dedicated to \nco-expression, experimental data, curated databases and text \nmining. \nThe genomic context channels focus on associations in- \nferred from genome sequences alone, detecting evolution- \nary patterns such as gene proximity, fusion events and co- \noccurrence across different organisms. For instance, the neigh- \nborhood channel assigns an association score to gene pairs \nthat are located close to each other on the chromosome in \nthe same orientation (in prokaryotic genomes) ( 22 ). The fu- \nsion channel identifies open reading frames that result from \ngene fusion events ( 23 ), while the co-occurrence channel ex- \namines whether genes have a nontrivial but shared distribu- \ntion across genomes ( 24 ), implying a shared history of hori- \nzontal transfers, losses or duplication events and thus likely \nrelated functions. The co-expression channel compiles data \nfrom gene expression studies, analyzing both transcript and \nprotein abundances across various conditions. By comparing \nexpression profiles, it identifies gene pairs with similar expres- \nsion patterns, suggesting functional linkage ( 10 ,25 ). The ex- \nperiments channel aggregates interaction evidence from labo- \nratory assays, including biochemical, biophysical and genetic \nassays. Data are imported from primary repositories such as \nBioGRID ( 5 ) and the IMEx consortium ( 26 ), and the confi- \ndence scores are estimated by globally benchmarking the accu- \nracy of annotated experimental techniques, as well as within- \ndataset performance and consistency for the case of high- \nthroughput experiments ( 10 ). The database channel is based \non well-described, curated protein–protein associations from \nexpert-compiled resources, such as KEGG ( 4 ), Reactome ( 3 ) \nand Gene Ontology Complexes ( 16 ). These resources provide \nwell-established pathways, complexes and functional relation- \nships, offering a high level of reliability. Unlike other chan- \nnels, the database channel assigns a uniformly high confidence \nscore to associations, reflecting their established nature. Fi- \nnally, the text-mining channel utilizes a large corpus of sci- \nentific literature, including PubMed abstracts and full-text ar- \nticles, to identify co-mentions of protein names ( 27 ). By statis- \ntically analyzing the frequency of these co-mentions in various \ntextual contexts, this channel uncovers potential associations \nthat may not be evident from structured data alone. In addi- \ntion, for the more specific association modes, dedicated large \nlanguage models are employed to detect sentences supporting \neither physical ( 21 ) or regulatory associations (see below). \nFor each evidence channel, the available interaction evi- \ndence is translated into a confidence score by first quantifying \nthe evidence using channel-specific metrics and then convert- \ning these metrics into likelihoods using calibration curves de- \nrived from prior knowledge (from pathway-map memberships \nin KEGG). For the more specific association modes ‘physical’ \nand ‘regulatory’, only channels and evidence that are appli- \ncable to these modes are considered. After this, all channel \nscores that have been computed for a given protein pair in \na given organism are transferred onto related protein pairs \nin other organisms, based on the ‘interolog’ concept ( 27 ,28 ). \nLastly, a final, combined confidence score is computed by inte- \ngrating the channel-specific subscores probabilistically, under \nthe assumption that evidence in different channels is largely \nindependent. Users of STRING can directly rely on this com- \nbined score for browsing and filtering networks, or they can \nalternatively customize their analyses by enabling or disabling \nspecific channels separately, after which the combined score is \nrecomputed according to their chosen settings. \nAll primary evidence underlying a given interaction can \nbe inspected interactively, in dedicated evidence viewers on- \nline. Furthermore, accessory information is available for \neach protein, such as its three-dimensional structure, do- \nmain composition, annotation and cross-references. Apart \nfrom its core protein-network functionality, STRING also \nimplements features that allow extensive analysis of up- \nloaded user data. These include functional enrichment de- \ntection ( 29 ), experimental bias detection ( 21 ), homology \nsearches, and clustering and organizing large query pro- \ntein sets. All data contained in STRING are freely avail- \nable for download, under a Creative Commons BY 4.0 li- \ncense. Apart from its website, STRING can also be accessed \nvia a dedicated Cytoscape plugin ( 30 ), through an applica- \ntion programming interface (API) ( 31 ), as well as through \nan R / Bioconductor package ( https://www.bioconductor.org/ \npackages/ release/ bioc/ html/ STRINGdb.html ). \nRegulat ory netw orks \nMolecular networks, like those available through STRING, \nhave become invaluable tools in biomedical research, offering \npowerful insights into how molecules work together within \ncells. The functional association networks, while useful for \nmany applications, do not specify the interaction type (e.g. \ncomplex formation or transcriptional regulation) nor the di- \nrection of interaction (i.e. who regulates whom). Constructing \nnetworks with more detailed interaction types and directions \ncan significantly expand their utility, particularly for building \nmathematical models of biological systems or for making log- \nical inferences. The directionality of interactions is essential \nfor interpretation of omics data, which typically reveals more \nabout downstream effects of conditions such as diseases than \nit does about the upstream events that led to it and thus could \nbe targeted therapeutically. \nTo incorporate interactions with specific directionality, sign \nand type into STRING, we leveraged the novel dataset and \ndeep learning-based relation extraction method described in \nRegulaTome ( 32 ). The RegulaTome corpus provides a rich \nand diverse dataset that includes 16 961 relations between \n54 951 entities annotated across over 2500 documents. This \ncorpus is utilized to train a deep learning-based method used \nto annotate and extract the regulatory events throughout the \nentire available literature. \nOn top of undirected physical interactions—which are al- \nready included in STRING—the following types of directed \ninteractions can be extracted using the system developed for \nRegulaTome: Regulation , with the signed subtypes of Posi- \ntive Regulation and Negative Regulation , Regulation of Gene \nExpression , Regulation of Degradation and Catalysis of Post- \nTranslational Modifications , covering six subtypes: Catalysis \nof Small Protein Conjugation , Catalysis of Small Protein Re- \nmoval , Catalysis of Phosphorylation , Catalysis of Dephospho- \nDownloaded from https://academic.oup.com/nar/article/53/D1/D730/7903368 by guest on 11 January 2026']"
"How do the improvements introduced in Vol. 53 of Nucleic Acids Research enhance gene set enrichment analysis, and how does the RAG system described in Vol. 32 of JAMIA apply retrieval strategies in clinical decision-making?","['<1-hop>\n\nD 734 Nucleic Acids Research , 2025, Vol. 53, Database issue uses the whole genome / proteome background; however, it is recommended that users provide a custom background list representing a more realistic universe of genes / proteins detected by their assay ( 35 ). Alternatively, the entire sorted dataset can be analyzed using gene set enrichment analysis to determine whether the distribution of genes at the top, bottom or both ends deviates significantly from what would be ex- pected by chance. STRING supports both of these enrichment methods. The most recent update to STRING introduces vari- ous improvements, including a new false discovery rate (FDR) correction, enhanced filtering options and a new graphical vi- sualization.'
 '<2-hop>\n\nFigure 3. An example of using RAG in clinical applications, with identified strategies in 3 stages: pre-retrieval, retrieval, and post-retrieval. The user query seeks recommended medications for symptomatic peripheral artery disease without increased bleeding risk. The system retrieves evidence from clinical guidelines, processes it through embeddings and a vector database, and outputs a response, including rivaroxaban with low-dose aspirin, as recommended by the retrieved guideline information. In this example, GPT-4 suggested a dose of 75 mg Aspirin, but the common low-dose Aspirin is 81 mg. Journal of the American Medical Informatics Association, 2025, Vol. 32, No. 4 609 Downloaded from https://academic.oup.com/jamia/article/32/4/605/7954485 by guest on 11 January 2026']","Vol. 53 of Nucleic Acids Research introduces improvements to STRING, including a new false discovery rate (FDR) correction, enhanced filtering options, and a new graphical visualization, which collectively enhance gene set enrichment analysis by allowing users to analyze either a whole genome/proteome background or a custom background list more realistically representing detected genes or proteins. Meanwhile, Vol. 32 of the Journal of the American Medical Informatics Association describes the use of a Retrieval-Augmented Generation (RAG) system in clinical applications, employing three stages—pre-retrieval, retrieval, and post-retrieval—to answer queries such as recommending medications for symptomatic peripheral artery disease without increased bleeding risk. This system retrieves evidence from clinical guidelines, processes it through embeddings and a vector database, and outputs responses like rivaroxaban with low-dose aspirin, demonstrating how retrieval strategies support accurate clinical decision-making.",multi_hop_specific_query_synthesizer,"The improvements introduced in Vol. 53 of *Nucleic Acids Research* enhance gene set enrichment analysis primarily through more sophisticated statistical and visualization methods implemented in the STRING database:

1. **Flexible Term Testing Based on Statistical Potential**:  
   Instead of testing all terms in an ontology, STRING now only tests terms that have a priori statistical potential for enrichment. This is determined by parameters such as term size, query set size, background size, and the number of tests conducted. By excluding terms that cannot theoretically be enriched after false discovery rate (FDR) correction, this approach significantly increases the statistical power of enrichment analysis without omitting potentially relevant terms. This method is ontology-agnostic and can be applied to any pathway or term collection with an imbalance of small terms.

2. **Enhanced Filtering Options**:  
   New filters allow users to sort and filter enrichment results by FDR, strength, signal, term size, and term similarity. The term similarity filter uses the Jaccard index to remove redundant terms with overlapping gene sets, while the signal filter balances observed-to-expected gene ratios and statistical significance (−log(FDR)) to prioritize meaningful terms. These filters help reduce the overwhelming number of results and focus on the most relevant enriched terms.

3. **Improved Graphical Visualization**:  
   STRING now provides an interactive dot plot visualization for enrichment results, displaying enrichment signal (X-axis), FDR (color coding), and term protein count (dot size). This visualization is customizable and reflects user-selected filters, making it easier to interpret and communicate enrichment findings.

---

The RAG (Retrieval-Augmented Generation) system described in Vol. 32 of *JAMIA* applies retrieval strategies in clinical decision-making by integrating external knowledge sources to enhance the performance of large language models (LLMs) in medical tasks:

1. **Application in Clinical Decision-Making**:  
   RAG techniques have been used to support personalized treatment, emergency triage, and disease management across various medical specialties. By retrieving relevant clinical guidelines, academic articles, specialized knowledge bases, and biomedical knowledge graphs, RAG systems provide evidence-based information to assist clinicians in making informed decisions.

2. **Use of Diverse Retrieval Sources**:  
   The system retrieves information from pre-stored documents such as clinical guidelines (e.g., Emergency Severity Index), PubMed abstracts and full texts, specialized databases (e.g., OncoKB, RareDis Corpus), and biomedical knowledge graphs (e.g., SPOKE). This retrieval enriches the LLM's responses with up-to-date and domain-specific knowledge.

3. **Demonstrated Effectiveness**:  
   Studies show that RAG-enhanced systems improve clinical decision-making outcomes compared to baseline LLMs, with an overall pooled odds ratio of 1.35 (95% CI: 1.19–1.53), indicating a statistically significant benefit.

In summary, the RAG system applies retrieval strategies by dynamically incorporating relevant, authoritative medical information into the language model's output, thereby improving accuracy and reliability in clinical decision-making contexts.","['D 734 \nNucleic Acids Research , 2025, Vol. 53, Database issue \nuses the whole genome / proteome background; however, it \nis recommended that users provide a custom background \nlist representing a more realistic universe of genes / proteins \ndetected by their assay ( 35 ). Alternatively, the entire sorted \ndataset can be analyzed using gene set enrichment analysis to \ndetermine whether the distribution of genes at the top, bottom \nor both ends deviates significantly from what would be ex- \npected by chance. STRING supports both of these enrichment \nmethods. The most recent update to STRING introduces vari- \nous improvements, including a new false discovery rate (FDR) \ncorrection, enhanced filtering options and a new graphical vi- \nsualization. \nFDR correction \nIn its analyses, STRING performs a test for each path- \nway (‘term’) within a given pathway collection (‘category’). \nLike many other enrichment tools, STRING employs the \nBenjamini–Hochberg correction ( 36 ) to adjust for multiple \nhypothesis testing. This statistical correction is essential for \nmanaging the FDR effectively. However, the larger the term \ncount and the greater the diversity of the terms, the higher \nthe required correction and the lower the chance of exceed- \ning the alpha level (significance threshold) for a given term. \nOne method used to address this issue involves restricting the \nanalysis to a subset of terms, such as ‘GO slims’ that focus \non broad, high-level terms with a simplified ontology struc- \nture ( 16 ). However, this approach has some disadvantages: it \nmight exclude terms particularly relevant to a tested gene set, \nit may not cover all functions by design and it is mainly ap- \nplicable to hierarchical classification systems such as the Gene \nOntology. \nSTRING utilizes several hierarchical ontologies, such as \nthe Brenda Tissue Ontology, Disease Ontology, Human Phe- \nnotype Ontology and the aforementioned Gene Ontology; \nthey typically contain many more leaf-ward nodes than nodes \ncloser to the root. The parental nodes must contain all the \ngenes included in the child terms, creating an imbalance in \nthe ontology with many more smaller terms and fewer larger \nterms. Removing smaller terms can substantially increase sta- \ntistical power . However , simply eliminating all small terms \ncould negatively impact discoverability, especially for small \nquery gene sets where the smaller terms might be the most rel- \nevant for biological interpretation. The ontologies are usually \nlarge, with > 10 000 terms (and therefore tests) in the Biolog- \nical Process branch of the Gene Ontology tree. Such strong \ncorrections have the effect that for larger query gene sets, or \nfor backgrounds that cover only part of the proteome, it might \nnot be statistically possible to detect enrichments for a subset \nof the terms; these can therefore be removed from considera- \ntion. \nIn its new version, STRING takes a more flexible \napproach—by only testing terms that have a priori any statis- \ntical potential for enrichment. This decision is based on sev- \neral parameters: the size of the term, the size of the query set, \nthe background size and the number of tests conducted. By \ncalculating the smallest and largest term sizes that could the- \noretically be enriched given the FDR correction, STRING can \ndetermine which terms are viable for inclusion in the anal- \nysis. Terms outside these bounds are excluded from testing. \nThis methodical exclusion significantly enhances the statisti- \ncal power of the enrichment analysis, without omitting any \nterms that could be enriched. For smaller query sets, this strat- \negy will maintain all terms, as even the smallest tested sets \n(term size = 2) might still be theoretically enriched. How- \never , for larger , less specific sets or for tests against a cus- \ntom smaller background, this approach markedly improves \nthe statistical power, enabling STRING to perform more fo- \ncused analyses without the drawbacks of increased false posi- \ntive rates or omitting potentially biologically interesting terms. \nAlthough this method leverages the characteristics of ontolo- \ngies, it does not require a hierarchy (parent–child relationships \nbetween the terms) to function and can be applied to any \npathway / term collection with an imbalance of small terms, \nsuch as PMID-derived gene sets and STRING neighborhood \nclusters. \nTerm filtering \nOne key insight from our online user surveys is that the of- \nten overwhelming number of enrichment results can make it \nchallenging to discern the most relevant enriched terms. We \nhave thus enhanced the display of enrichment results, allow- \ning users to filter and sort them based on (i) FDR, (ii) strength, \n(iii) signal, (iv) term size and (v) term similarity. \nThe newest addition to our filtering options are the ‘term \nsimilarity’ and ‘signal’ filters. The ‘term similarity’ filter uses \nthe Jaccard index to measure the similarity between the gene \nsets of terms within the same category. The method proceeds \nby sorting the terms according to their P -values, prioritizing \nthose with the strongest statistical significance. The algorithm \nthen iterates through the list of terms; any term that exhibits \na similarity exceeding a predetermined, user-defined threshold \nrelative to terms already included in the results is subsequently \nexcluded. This approach requires no additional information \nbeyond what is already available in the dataset and can be \nuniformly applied across all categories of terms. The ‘signal’ \nfilter is defined as a weighted harmonic mean between the ra- \ntio of observed to expected gene occurrences in an enriched \nterm and its −log(FDR), respectively. This filter balances the \nimpact of both metrics and provides a more intuitive ordering \nof enriched terms and has been implemented as the default \nsorting criterion. \nThese filters are designed to exclude potentially less relevant \nresults, such as terms that are marginally significant, small or \nshow significant overlap in terms of gene content. This en- \nhancement streamlines the search for relevant observations, \nhelping users focus on the most meaningful results. \nGraphical visualization \nIn response to user feedback requesting more intuitive, graph- \nical visualizations of the detected enrichments, STRING now \nhas adopted an interactive dot plot as a primary display \nmethod for enrichment analysis outcomes. We chose the dot \nplot for its clarity in representing enriched functions across \nthree dimensions: (i) enrichment signal along the X -axis, (ii) \nthe FDR indicated through the color coding of the dots and \n(iii) the term’s protein count in the network represented by the \nsize of each dot. The terms (listed on the Y -axis) are ranked \nby their enrichment signal or, if grouped by similarity, by the \nenrichment signal of their group first (Figure 2 ). This visual- \nization reflects all filter settings chosen by the user to manage \nthe volume of results displayed. These plots can be addition- \nally customized in the interface and the resulting graphs can be \ndownloaded as PNG for immediate use or as SVG for further \nmodifications. \nDownloaded from https://academic.oup.com/nar/article/53/D1/D730/7903368 by guest on 11 January 2026'
 'Clinical applications of RAG\nRAG techniques have been applied across a broad range of \nmedical specialties, as shown in Table 1. These applications \ninclude clinical decision-making and medical question- \nanswering. In clinical decision making, RAG has supported \npersonalized treatment,23,24 emergency triage,25 and disease \nmanagement.26,27 For medical question-answering, RAG’s \ncapability has been explored to address complex treatment \nguidelines questions,28 as well as queries focused on specific \nareas, such as head and neck surgery-related questions,29 and \npatient questions regarding diabetes.30 In the subgroup anal\xad\nysis, 13 studies focused on clinical decision-making (OR \n1.46, 95% CI [1.16, 1.71]) and 7 studies focused on medical \nquestion-answering (OR 1.32, 95% CI [1.08, 1.63]), with no \nstatistically significant difference observed between these 2 \ngroups.\nBaseline LLMs\nThe baseline LLMs varied across studies, with GPT-4 being \nthe most common, used in 14 studies, (OR: 1.58, 95% CI: \n1.21-2.04). GPT-3.5, used in 6 studies, showed an OR of \n1.43 (95% CI: 1.06-1.93). Llama2 was applied in 5 studies \n(OR: 1.25, 95% CI: 1.08-1.44).\nRetrieval sources\nRetrieval sources were categorized as pre-stored documents \nand real-time online browsing. Regarding pre-stored docu\xad\nments, 6 studies used clinical guidelines, such as the Emer\xad\ngency Severity Index (ESI) Ver.3 Field Triage.25 Five studies \nused academic articles from sources like PubMed abstracts or \nfull texts, or document sets such as the Radiographics Top 10 \nReading List on Gastrointestinal Imaging.31 Three studies \nused specialized knowledge bases, including ChatENT, \nOncoKB, and RareDis Corpus, while one study employed a \ngeneral biomedical knowledge graph (Scalable Precision \nMedicine Open Knowledge Engine [SPOKE]). SPOKE \nTable 1. Distribution of RAG applications by medical specialty and task \ntype.\nMedical Specialty\nFrequency\nInternal medicine\n4\nGeneral medicine\n3\nOncology\n3\nEmergency medicine\n2\nGastroenterology\n2\nOtolaryngology\n1\nHepatology\n1\nRare diseases\n1\nOrthopedics\n1\nNeurology\n1\nOphthalmology\n1\nTask\nFrequency (%)\nClinical decision-making\n13 (65%)\nMedical question-answering\n7 (35%)\nFigure 2. A forest plot showing the odds ratio (OR) of total impacts of the RAG-enhanced system compared with the baseline LLM system in clinical \ntasks. The left panel lists individual studies (e.g., Zakka et al., Murugan et al., Long et al.) along with their corresponding ORs and 95% confidence \nintervals (CIs). The right panel visualizes these ORs, with square markers proportional to study weights and horizontal lines representing CIs. The overall \npooled OR is 1.35 (95% CI: 1.19–1.53), represented by a diamond at the bottom.\n608                                                                                                      \nJournal of the American Medical Informatics Association, 2025, Vol. 32, No. 4 \nDownloaded from https://academic.oup.com/jamia/article/32/4/605/7954485 by guest on 11 January 2026'
 'Nucleic Acids Research , 2025, Vol. 53, Database issue \nD 733 \nFigure 1. Illustration of the new ‘regulatory network’ mode in STRING, where the network edges visually indicate the direction, confidence and sources \nof each regulatory interaction. Clicking on an edge within the network brings up a pop-up window with a detailed o v ervie w of the a v ailable e vidence and \nthe annotated type of regulation. Users can further explore the data behind the interactions by accessing the specific evidence viewers linked within the \npop-up, including the text-mining evidence viewer. This viewer presents excerpts from literature pertaining to the inspected interactions, as well as an \nautomatically generated large language model summary of the presented e x cerpts. \nrylation , Catalysis of Small Molecule Conjugation and Cataly- \nsis of Small Molecule Removal . To enhance the interoperabil- \nity of the annotations, these chosen relation types align with \nGene Ontology ( 32 ). \nTo identify and classify a wide array of interaction types, we \nfine-tuned the RoBERT a-large-PM-M3-V oc language model, \na transformer-based model well suited for biological tasks. \nThis model was trained on the RegulaTome dataset for multi- \nlabel extraction of the directed, typed and signed interactions \nmentioned above, and achieved an average F 1 score of 73.5% \n(with a precision of 75.2% and a recall of 71.8%) on a held- \nout test set, although the exact performance varied across dif- \nferent types of relationships. This level of accuracy is substan- \ntial, considering the complexity of the underlying task. Using \nthis model, we processed over 1.2 billion sentence-level pairs \nextracted from all available PubMed abstracts and PMC Open \nAccess full-text documents, and assigned 3.5% of them ( ∼43 \nmillion, of which ∼18 million in human) with at least one \npositive label indicating directed (72.9%) or signed (33.1%) \nrelationships among the proteins. Furthermore, we made use \nof the existing curated knowledge of regulatory interactions in \nthe database channel, by parsing regulatory information from \nthe SIGNOR, KEGG and Reactome databases. \nTo integrate these interactions into STRING, we followed \na similar score aggregation and benchmarking approach as \nfor physical interactions ( 21 ). Here, we perform benchmark- \ning in five categories for which we can derive gold stan- \ndard datasets of known human regulatory interactions from \nSIGNOR: regulation , upregulation , downregulation , tran- \nscriptional regulation and phosphorylation . The resulting \ncalibration functions are then applied to extracted regula- \ntions of all types to produce the final regulatory confidence \nscores. \nThe new regulatory network has been seamlessly integrated \ninto the existing STRING framework, complementing the \n‘functional’ and ‘physical’ network types. Directional edges \nwithin this network are visually represented by arrows, depict- \ning both bidirectional and unidirectional relationships. The \nnetwork visualization can show varying confidence levels be- \ntween directions (in the ‘confidence’ view) or the specific di- \nrectionality of the sources (in the ‘evidence’ view). The user \ninterface retains its intuitive design, enabling users to access \nthe type of regulatory event and the evidence for the interac- \ntion by clicking on the edge (Figure 1 ). Additionally, all API \nfunctions have been updated to fully support the new net- \nwork type, which can be accessed by specifying the parameter \nnetw or k_type = regulatory in the API call. \nExpanded co-expression networks \nThe co-expression channel in STRING identifies genes with \nsimilar expression profiles across various tissues and condi- \ntions, revealing their involvement in shared biological pro- \ncesses. It compiles data from large-scale gene expression stud- \nies, analyzing transcript and protein levels to detect gene pairs \nwith coordinated expression. This method highlights poten- \ntial functional relationships between genes. Importantly, the \nanalysis is free from study bias, as it directly uses raw omics \ndata instead of relying on manually curated or experimental \ndatasets, providing a more precise and objective view of gene \ninteractions. \nFor the upcoming version 12.5, we are expanding the \nco-expression channel by generating additional networks \nthrough the application of FAVA (Functional Association of \nVariational Autoencoders) ( 25 ) on a wider range of single- \ncell RNA sequencing data. Specifically, we are incorporating \nsingle-cell data from the cellxgene Atlas ( 33 ) to enhance the \nhuman and mouse networks and from the EBI Single Cell Ex- \npression Atlas to extend coverage to more model organisms \n( 34 ). This expansion will provide more detailed, organism- \nspecific co-expression maps, enabling more precise identifi- \ncation of gene co-expression patterns and cross-species com- \nparisons, particularly for development, immune response and \ndisease-related gene expression dynamics. \nImpro v ed enric hment analysis \nGene set enrichment analysis is a critical component of high- \nthroughput biological studies, providing key insights into the \nbiological characteristics of datasets. This analysis is pivotal in \nidentifying essential aspects such as biological processes, path- \nways and phenotypes, which are crucial for understanding the \nbiological implications of experiments. Typically, overrepre- \nsentation analysis is employed, wherein a subset of data ex- \nceeding a specific cutoff (empirical or customary) is compared \nagainst an experimental background. By default, STRING \nDownloaded from https://academic.oup.com/nar/article/53/D1/D730/7903368 by guest on 11 January 2026']"
