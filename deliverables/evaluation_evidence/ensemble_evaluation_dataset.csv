user_input,reference_contexts,reference,synthesizer_name,response,retrieved_contexts
Who is Omar and what contributions are associated with him in biomedical knowledge graph research?,"['Omar & Mohammed / Journal of Applied Science and Technology Trends Vol. 06, No. 02, pp. 342 ‚Äì357 (2025) 345 record-high accuracy, while open-source models achieved impressive gains through prompt optimization. Feng et al.[22] developed the Knowledge Graph-based Thought (KGT) framework that integrated LLMs with a pan- cancer knowledge graph for biomedical question answering. KGT was designed to reason on the knowledge graph schema and identify optimal subgraphs to use for directing accurate answer generation, all without fine-tuning the LLMs. The framework is benchmarked against a new dataset (PcQA) designed specifically for pan-cancer KGQA tasks and has outperformed all existing state-of-the-art approaches by a rather large margin. KGT‚Äôs practicality in biomedical issues was highlighted through case studies for drug repositioning, drug resistance, and biomarker discovery. Their approach exhibited robust adaptability among various LLMs. Rezaei et al.[26] developed AMG-RAG, a dynamic framework that utilizes autonomous LLM agents with medical search tools in the continuous construction and real-time updating of Medical Knowledge Graphs (MKGs). Their system incorporated confidence scoring and multi-hop reasoning to improve accuracy and interpretability in medical question answering. AMG-RAG outperformed size models on both very hard MEDQA benchmarks and more accessible MedMCQA ones, proving that it could conduct efficient reasoning based on current structured medical knowledge. They also used Neo4j to manage the knowledge graphs while adding external searches to ensure the latest data. Tiwari et al.[24] presented Auto-Cypher, a recent automated pipeline for producing high-quality synthetic data for training LLMs by mapping natural language to Cypher queries for graph databases like Neo4j. The pipeline deployed the novelty of LLM- as-database-filler to synthesize Neo4j databases for the execution of generated queries to ensure their correctness. A sizable dataset called SynthCypher was created, spanning multiple domains and complex queries, leading to a 40% improvement in LLM performance on Cypher generation. The datasets were used to fine-tune open-source models such as LLaMA, Mistral, and Qwen, and the SPIDER benchmark was adapted for evaluation purposes. Mohammed et al.[29] proposed a hybridized GraphRAG framework combining Neo4j-based UMLS knowledge graphs with a vector store for medical textbooks to create an improved U.S.M.L.E.-style clinical question-answering approach. The project integrated symbolic reasoning from knowledge graphs with semantic retrieval performed on text embeddings to enhance relevance and accuracy via adaptive re-ranking and query expansion. The system had the answers produced by GPT-4o- Mini, with different prompting strategies encouraging evidence- based and traceable responses grounded in verified medical knowledge. Experiments showed that the hybrid approach improved factual accuracy and citation fidelity as compared to the L.L.M.-only approach, enhancing transparency and reliability. It is shown that binding both structured and unstructured medical knowledge sources could aid in ameliorating hallucinations and hence improve clinical trustworthiness in AI-driven medical QA. Yang et al.[30] articulated sepsis knowledge graph was crafted by combining multicenter clinical data from over 10,000 patients with the help of GPT-4 for entity recognition and relationship extraction. Real-world data were collected from three hospitals and integrated with clinical guidelines and databases from the public domain. The knowledge graph contained 1,894 nodes and 2,021 relationships pertaining to diseases, symptoms, biomarkers, treatments, and complications. GPT outperformed other models in every resolution on sepsis- specific datasets to obtain high F1-score results. The constructed graph highlighted complex interactions in sepsis for assisting clinical decision-making and was implemented on Neo4j. Guan et al.[20] proposed a novel method for constructing a local knowledge graph from retrieved biomedical documents by extracting propositional claims. They carried out layer wise summarization on this graph to capture multi-document relationships and provide comprehensive contextual information to a language model for question-answering purposes. The method resolved issues in multi-document biomedical QA, such as noise cancellation and efficient context usage. They then tested their method on several benchmarks for biomedical question answering, achieving performance at least comparable to, if not better than, existing retrieval-augmented generation (RAG) baselines. The study established enhanced reasoning and answer accuracy of the model achieved through structured graph summarization. Previous studies have improved biomedical QA using KGs and LLMs, but important gaps remain. Most systems lack transparent, graph-based justifications, rely on limited evaluation methods, or depend on cloud resources that reduce privacy and reproducibility. Our framework addresses these gaps by providing visible Cypher queries with evidence subgraphs, applying comprehensive performance metrics across difficulty levels, and ensuring fully local, privacy-preserving deployment. Table I summarizes key previous studies on biomedical knowledge graphs and question answering, outlining their methods, datasets, and main limitations.']","Omar is a co-author mentioned in the context of biomedical knowledge graph research, specifically in the publication by Omar & Mohammed in the Journal of Applied Science and Technology Trends. Their work is referenced alongside other significant studies on integrating large language models with biomedical knowledge graphs for question answering, highlighting advances in frameworks that improve accuracy, reasoning, and clinical trustworthiness in medical QA systems.",single_hop_specifc_query_synthesizer,"Omar is a researcher who, along with Mohammed, has contributed to biomedical knowledge graph (KG) research, particularly in the development of hybrid biomedical question answering (QA) frameworks that integrate large language models (LLMs) with biomedical knowledge graphs. Their work is documented in the 2025 article in the *Journal of Applied Science and Technology Trends*.

### Contributions Associated with Omar in Biomedical Knowledge Graph Research:

1. **Hybrid Biomedical QA Framework**:  
   Omar and Mohammed proposed a hybrid system that couples the LLaMA-3B language model with a Neo4j-based integrative Biomedical Knowledge Hub (iBKH) knowledge graph. This system automatically generates executable Cypher queries from natural language biomedical questions and returns transparent, evidence-grounded answers supported by justification subgraphs.

2. **Focus on Transparency and Traceability**:  
   Their framework emphasizes providing graph-grounded justifications for answers, allowing users to inspect the exact nodes and edges in the knowledge graph that support the response. This addresses a common limitation in LLM-only or text-retrieval-only QA systems, which often lack explainability.

3. **Evaluation Across Difficulty Levels**:  
   The system was evaluated on a dataset with questions categorized into simple, medium, and complex levels, demonstrating high performance in precision, recall, F1 score, Hits@k, and Mean Reciprocal Rank (MRR). The system maintained low latency (~6 seconds) suitable for interactive use and operated fully offline, enhancing privacy and reproducibility.

4. **Use of Neo4j and Cypher Query Generation**:  
   Their approach involves translating biomedical questions into Cypher queries that can be executed on the Neo4j graph database, enabling efficient retrieval of structured biomedical knowledge.

5. **Case Studies Demonstrating Practical Utility**:  
   The framework was applied in case studies including:
   - Drug repositioning hypotheses (e.g., carteolol for hemangiomas),
   - Drug resistance research in cancer (e.g., resistance of ALK-p.L1196M mutation to nalatinib),
   - Biomarker discovery and individualized treatment planning.

6. **Addressing Limitations and Future Directions**:  
   They acknowledged current limitations such as reliance on a static snapshot of the knowledge graph, challenges in multi-hop reasoning, and lack of canonical entity normalization (e.g., UMLS, SNOMED). Future work aims to integrate standardized biomedical entity normalization, enable dynamic graph updates, and leverage domain-adapted or fine-tuned biomedical LLMs.

### Summary

Omar's contributions lie in advancing biomedical question answering by integrating LLMs with structured biomedical knowledge graphs, focusing on transparency, accuracy, and privacy-preserving local deployment. Their hybrid framework improves the interpretability and reliability of biomedical QA systems and demonstrates practical applications in drug repositioning, drug resistance, and biomarker research.","['Omar & Mohammed / Journal of Applied Science and Technology Trends Vol. 06, No. 02, pp. 342 ‚Äì357 (2025) \n \n345 \nrecord-high accuracy, while open-source models achieved \nimpressive gains through prompt optimization. \nFeng et al.[22] developed the Knowledge Graph-based \nThought (KGT) framework that integrated LLMs with a pan-\ncancer knowledge graph for biomedical question answering. \nKGT was designed to reason on the knowledge graph schema and \nidentify optimal subgraphs to use for directing accurate answer \ngeneration, all without fine-tuning the LLMs. The framework is \nbenchmarked against a new dataset (PcQA) designed specifically \nfor pan-cancer KGQA tasks and has outperformed all existing \nstate-of-the-art approaches by a rather large margin. KGT‚Äôs \npracticality in biomedical issues was highlighted through case \nstudies for drug repositioning, drug resistance, and biomarker \ndiscovery. Their approach exhibited robust adaptability among \nvarious LLMs. \nRezaei et al.[26] developed AMG-RAG, a dynamic \nframework that utilizes autonomous LLM agents with medical \nsearch tools in the continuous construction and real-time updating \nof Medical Knowledge Graphs (MKGs). Their system \nincorporated confidence scoring and multi-hop reasoning to \nimprove accuracy and interpretability in medical question \nanswering. AMG-RAG outperformed size models on both very \nhard MEDQA benchmarks and more accessible MedMCQA \nones, proving that it could conduct efficient reasoning based on \ncurrent structured medical knowledge. They also used Neo4j to \nmanage the knowledge graphs while adding external searches to \nensure the latest data. \nTiwari et al.[24] presented Auto-Cypher, a recent automated \npipeline for producing high-quality synthetic data for training \nLLMs by mapping natural language to Cypher queries for graph \ndatabases like Neo4j. The pipeline deployed the novelty of LLM-\nas-database-filler to synthesize Neo4j databases for the execution \nof generated queries to ensure their correctness. A sizable dataset \ncalled SynthCypher was created, spanning multiple domains and \ncomplex queries, leading to a 40% improvement in LLM \nperformance on Cypher generation. The datasets were used to \nfine-tune open-source models such as LLaMA, Mistral, and \nQwen, and the SPIDER benchmark was adapted for evaluation \npurposes. \nMohammed et al.[29] proposed a hybridized GraphRAG \nframework combining Neo4j-based UMLS knowledge graphs \nwith a vector store for medical textbooks to create an improved \nU.S.M.L.E.-style clinical question-answering approach. The \nproject integrated symbolic reasoning from knowledge graphs \nwith semantic retrieval performed on text embeddings to enhance \nrelevance and accuracy via adaptive re-ranking and query \nexpansion. The system had the answers produced by GPT-4o-\nMini, with different prompting strategies encouraging evidence-\nbased and traceable responses grounded in verified medical \nknowledge. Experiments showed that the hybrid approach \nimproved factual accuracy and citation fidelity as compared to \nthe L.L.M.-only approach, enhancing transparency and \nreliability. It is shown that binding both structured and \nunstructured medical knowledge sources could aid in \nameliorating hallucinations and hence improve clinical \ntrustworthiness in AI-driven medical QA. \nYang et al.[30] articulated sepsis knowledge graph was \ncrafted by combining multicenter clinical data from over 10,000 \npatients with the help of GPT-4 for entity recognition and \nrelationship extraction. Real-world data were collected from \nthree hospitals and integrated with clinical guidelines and \ndatabases from the public domain. The knowledge graph \ncontained 1,894 nodes and 2,021 relationships pertaining to \ndiseases, symptoms, biomarkers, treatments, and complications. \nGPT outperformed other models in every resolution on sepsis-\nspecific datasets to obtain high F1-score results. The constructed \ngraph highlighted complex interactions in sepsis for assisting \nclinical decision-making and was implemented on Neo4j. \nGuan et al.[20] proposed a novel method for constructing a \nlocal knowledge graph from retrieved biomedical documents by \nextracting propositional claims. They carried out layer wise \nsummarization on this graph to capture multi-document \nrelationships and provide comprehensive contextual information \nto a language model for question-answering purposes. The \nmethod resolved issues in multi-document biomedical QA, such \nas noise cancellation and efficient context usage. They then tested \ntheir method on several benchmarks for biomedical question \nanswering, achieving performance at least comparable to, if not \nbetter than, existing retrieval-augmented generation (RAG) \nbaselines. The study established enhanced reasoning and answer \naccuracy of the model achieved through structured graph \nsummarization.   \nPrevious studies have improved biomedical QA using KGs \nand LLMs, but important gaps remain. Most systems lack \ntransparent, graph-based justifications, rely on limited evaluation \nmethods, or depend on cloud resources that reduce privacy and \nreproducibility. Our framework addresses these gaps by \nproviding visible Cypher queries with evidence subgraphs, \napplying comprehensive performance metrics across difficulty \nlevels, and ensuring fully local, privacy-preserving deployment. \nTable I summarizes key previous studies on biomedical \nknowledge graphs and question answering, outlining their \nmethods, datasets, and main limitations. \nIII. PRELIMINARIES \nThis section outlines the fundamental concepts required to \nunderstand the proposed system. It introduces biomedical \nknowledge graphs, explains how Neo4j stores data in graph \nform, and describes the use of Cypher for querying. It also \nprovides a brief overview of large language models (LLMs) and \ntheir role in interpreting natural language. \n \n \nA. Biomedical Knowledge Graphs \nBiomedical Knowledge Graphs (BKGs) provide a structured \nrepresentation of complex biomedical information by modeling \ndiverse medical entities, such as diseases, drugs, symptoms, and \nbiological pathways, as interconnected nodes within a graph \nstructure. The edges in these graphs represent the semantic \nrelationships between these entities, including ‚Äôtreats‚Äô, ‚Äôcauses‚Äô, \n‚Äôinteracts with‚Äô and many others, as illustrated in Fig 1. This \nform of representation enables the integration of heterogeneous \nbiomedical data from a wide range of sources, including'
 ""Omar & Mohammed / Journal of Applied Science and Technology Trends Vol. 06, No. 02, pp. 342 ‚Äì357 (2025) \n \n352 \nThe dataset contains 60 questions divided into three difficulty \nlevels based on how complex the language is and how deep the \nbiomedical reasoning needs to be: \n‚Ä¢ Level 1: 25 simple questions focusing mostly on easy-to-\nunderstand facts, such as symptoms of certain diseases or \ndrugs used to treat specific conditions. \n‚Ä¢ Level 2: 20 medium-level questions that involve more \ndetailed relationships, such as drug interactions and \nSideEffect. \n‚Ä¢ Level 3: 15 hard questions requiring multi-step reasoning \nacross multiple biomedical concepts or biological \npathways, similar to the complexity found in real clinical \ncases. \nThe set of evaluation questions was designed to span a wide \nrange of common biomedical topics and to reflect clinically \nrelevant query types reported in prior literature. Each question is \npaired with its corresponding gold standard cypher query and \ncategorized by difficulty level, as summarized in Table V, where \nthree illustrative examples are shown. The questions were derived \nfrom publicly available biomedical QA benchmarks and adapted \nfrom established knowledge bases (e.g., iBKH schema relations), \nensuring both coverage and diversity across diseases, drugs, \nsymptoms, and pathways.\nTABLE V.  QUESTION DIFFICULTY LEVELS AND SAMPLE CYPHER QUERIES \nLevel \nDefinition \nExample \nCypher Query \n1 \nSingle-hop question using \none relationship \nWhat are the symptoms of \nAlzheimer? \nMATCH (d:Disease)-[:HAS_SYMPTOM]->(s:Symptom) \nWHERE toLower(d.name) CONTAINS 'alzheimer' \nRETURN s.name AS symptom \n \n2 \nQuestions requiring one \nrelationship \nWhat are the side effects of \ndrugs used to treat asthma? \nWITH toLower('asthma') AS disease_name \nMATCH (d:Disease)<-[:TREATS]-(dr:Drug) \nWHERE toLower(d.name) CONTAINS disease_name \nMATCH (dr:Drug)-[:CAUSES]->(se:SideEffect) \nRETURN DISTINCT se.name AS side_effect, dr.name AS drug \n \n3 \nQuestions requiring two or \nmore relationships \nWhat cholesterol medications \ncause side effects, and what are \nsome of those effects? \nWITH toLower('cholesterol') AS disease_name \nMATCH (d:Disease)<-[:TREATS]-(dr:Drug) \nWHERE toLower(d.name) CONTAINS disease_name \nMATCH (dr:Drug)-[:CAUSES]->(se:SideEffect) \nRETURN DISTINCT dr.name AS drug, se.name AS side_effect \n \n \n \nFig. 4. Graphical User Interface of the Biomedical Knowledge Explorer System \nB. Quantitative Evaluation \nThe performance of the proposed system is evaluated on the \niBKH knowledge graph using Exact Match, Precision@k, \nRecall@k, F1@k, Hits@k, MRR, and total response time across \nsimple, medium, and complex biomedical queries. These metrics \nwere measured for each difficulty level (simple, medium, and \ncomplex) to see how performance changes as questions become \nmore challenging.\n \nTABLE VI.  QUERY GENERATION EXACT MATCH BY DIFFICULTY LEVEL""
 'Nucleic Acids Research , 2025, Vol. 53, Database issue \nD 737 \nnetworks in all domains of life, supporting directed links and \ntissue-specificity. J. Mol. Biol., 433 , 166835.\n13. Kim, C.Y. , Baek, S. , Cha, J. , Yang, S. , Kim, E. , Marcotte, E.M. , Hart, T. \nand Lee,I. (2022) HumanNet v3: an improved database of human \ngene networks for disease research. Nucleic Acids Res., 50 , \nD632‚ÄìD639.\n14. Huang, J.K. , Carlin, D.E. , Yu, M.K. , Zhang, W. , Kreisberg, J.F. , \nTamayo, P. and Ideker, T. (2018) Systematic evaluation of molecular \nnetworks for discovery of disease genes. Cell Syst. , 6 , 484‚Äì495.e5. \n15. Wright, S.N. , Colton, S. , Schaffer, L.V. , Pillich, R.T. , Churas, C. , \nPratt, D. and Ideker, T. (2024) State of the interactomes: an \nevaluation of molecular networks for generating biological \ninsights. bioRxiv doi: \nhttps:// doi.org/ 10.1101/ 2024.04.26.587073 , 29 April 2024, \npreprint: not peer reviewed.\n16. Ontology Consortium, G. , Aleksander, S.A. , Balhoff, J. , Carbon, S. , \nCherry, J.M. , Drabkin, H.J. , Ebert, D. , Feuermann, M. , Gaudet, P. , \nHarris, N.L. , et al. (2023) The Gene Ontology knowledgebase in \n2023. Genetics , 224 , iyad031.\n17. Liberzon, A. , Birger, C. , Thorvaldsd√≥ttir, H. , Ghandi, M. , \nMesirov, J.P. and Tamayo, P. (2015) The Molecular Signatures \nDatabase hallmark gene set collection. Cell Syst. , 1 , 417‚Äì425. \n18. Amberger, J.S. , Bocchini, C.A. , Schiettecatte, F. , Scott, A.F. and \nHamosh,A. (2015) OMIM.org: Online Mendelian Inheritance in \nMan (OMIM ¬Æ), an online catalog of human genes and genetic \ndisorders. Nucleic Acids Res. , 43 , D789‚ÄìD798. \n19. Snel, B. , Bork, P. and Huynen, M.A. (2002) The identification of \nfunctional modules from the genomic association of genes. Proc. \nNatl Acad. Sci. U.S.A., 99 , 5890‚Äì5895.\n20. Guala, D. , Ogris, C. , M√ºller, N. and Sonnhammer, E.L.L. (2020) \nGenome-wide functional association networks: background, data \n& state-of-the-art resources. Brief. Bioinform., 21 , 1224.\n21. Szklarczyk, D. , Gable, A.L. , Nastou, K.C. , Lyon, D. , Kirsch, R. , \nPyysalo, S. , Doncheva, N.T. , Legeay, M. , Fang, T. , Bork, P. , et al. \n(2021) The STRING database in 2021: customizable \nprotein‚Äìprotein networks, and functional characterization of \nuser-uploaded gene / measurement sets. Nucleic Acids Res., 49 , \nD605‚ÄìD612.\n22. Mering,C.V. (2003) STRING: a database of predicted functional \nassociations between proteins. Nucleic Acids Res. , 31 , 258‚Äì261. \n23. Enright, A.J. , Iliopoulos, I. , Kyrpides, N.C. and Ouzounis, C.A. \n(1999) Protein interaction maps for complete genomes based on \ngene fusion events. Nature , 402 , 86‚Äì90.\n24. Franceschini, A. , Lin, J. , von Mering, C. and Jensen, L.J. (2016) \nSVD-phy: improved prediction of protein functional associations \nthrough singular value decomposition of phylogenetic profiles. \nBioinformatics , 32 , 1085‚Äì1087.\n25. Koutrouli, M. , Nastou, K. , Piera L√≠ndez, P. , Bouwmeester, R. , \nRasmussen, S. , Martens, L. and Jensen, L.J. (2024) FAVA: \nhigh-quality functional association networks inferred from \nscRNA-seq and proteomics data. Bioinformatics , 40 , btae010.\n26. Porras, P. , Barrera, E. , Bridge, A. , Del-Toro, N. , Cesareni, G. , \nDuesbury, M. , Hermjakob, H. , Iannuccelli, M. , Jurisica, I. , \nKotlyar, M. , et al. (2020) Towards a unified open access dataset of \nmolecular interactions. Nat. Commun., 11 , 6144.\n27. Franceschini, A. , Szklarczyk, D. , Frankild, S. , Kuhn, M. , \nSimonovic, M. , Roth, A. , Lin, J. , Minguez, P. , Bork, P. , von Mering, C. , \net al. (2013) STRING v9.1: protein‚Äìprotein interaction networks, \nwith increased coverage and integration. Nucleic Acids Res., 41 , \nD808‚ÄìD815.\n28. Yu, H. , Luscombe, N.M. , Lu, H.X. , Zhu, X. , Xia, Y. , Han, J .-D.J ., \nBertin, N. , Chung, S. , V idal, M. and Gerstein, M. (2004) Annotation \ntransfer between genomes: protein‚Äìprotein interologs and \nprotein‚ÄìDNA regulogs. Genome Res. , 14 , 1107‚Äì1118. \n29. Szklarczyk, D. , Gable, A.L. , Lyon, D. , Junge, A. , Wyder, S. , \nHuerta-Cepas, J. , Simonovic, M. , Doncheva, N.T. , Morris, J.H. , \nBork, P. , et al. (2019) STRING v11: protein‚Äìprotein association \nnetworks with increased coverage, supporting functional discovery \nin genome-wide experimental datasets. Nucleic Acids Res., 47 , \nD607‚ÄìD613.\n30. Doncheva, N.T. , Morris, J.H. , Gorodkin, J. and Jensen, L.J. (2019) \nCytoscape StringApp: network analysis and visualization of \nproteomics data. J. Proteome Res., 18 , 623‚Äì632.\n31. Jensen, L.J. , Kuhn, M. , Stark, M. , Chaffron, S. , Creevey, C. , Muller, J. , \nDoerks, T. , Julien, P. , Roth, A. , Simonovic, M. , et al. (2009) STRING \n8‚Äîa global view on proteins and their functional interactions in \n630 organisms. Nucleic Acids Res. , 37 , D412‚ÄìD416. \n32. Nastou, K. , Mehryary, F. , Ohta, T. , Luoma, J. , Pyysalo, S. and \nJensen,L.J. (2024) RegulaTome: a corpus of typed, directed, and \nsigned relations between biomedical entities in the scientific \nliterature. Database , 2024 , baae095.\n33. Megill, C. , Martin, B. , Weaver, C. , Bell, S. , Prins, L. , Badajoz, S. , \nMcCandless, B. , Pisco, A.O. , Kinsella, M. , Griffin, F. , et al. (2021) \ncellxgene: a performant, scalable exploration platform for high \ndimensional sparse matrices. bioRxiv doi: \nhttps:// doi.org/ 10.1101/ 2021.04.05.438318 , 6 April 2021, \npreprint: not peer reviewed.\n34. Moreno, P. , Fexova, S. , George, N. , Manning, J.R. , Miao, Z. , \nMohammed, S. , Mu√±oz-Pomer, A. , Fullgrabe, A. , Bi, Y. , Bush, N. , \net al. (2022) Expression Atlas update: gene and protein expression \nin multiple species. Nucleic Acids Res. , 50 , D129‚ÄìD140. \n35. T immons, J.A. , Szkop, K.J. and Gallagher, I.J. (2015) Multiple \nsources of bias confound functional enrichment analysis of global \n-omics data. Genome Biol. , 16 , 186. \n36. Benjamini, Y. and Hochberg, Y. (1995) Controlling the false \ndiscovery rate: a practical and powerful approach to multiple \ntesting. J. R. Stat. Soc. Ser. B Stat. Methodol., 57 , 289‚Äì300.\n37. Novikov,A. (2019) PyClustering: data mining library. J. Open \nSource Softw., 4 , 1230.\n38. Van Dongen,S. (2008) Graph clustering via a discrete uncoupling \nprocess. SIAM J. Matrix Anal. Appl., 30 , 121‚Äì141.\n39. Elnaggar, A. , Heinzinger, M. , Dallago, C. , Rehawi, G. , Wang, Y. , \nJones, L. , Gibbs, T. , Feher, T. , Angerer, C. , Steinegger, M. , et al. \n(2022) ProtTrans: toward understanding the language of life \nthrough self-supervised learning. IEEE Trans. Pattern Anal. Mach. \nIntell., 44 , 7112‚Äì7127.\n40. Baumgartner, M. , Dell‚ÄôAglio, D. , Paulheim, H. and Bernstein, A. \n(2023) Towards the Web of Embeddings: integrating multiple \nknowledge graph embedding spaces with FedCoder. J. Web \nSemant., 75 , 100741.\nReceived: September 15, 2024. Revised: October 18, 2024. Editorial Decision: October 21, 2024. Accepted: October 29, 2024 \n¬© The Author(s) 2024. Published by Oxford University Press on behalf of Nucleic Acids Research. \nThis is an Open Access article distributed under the terms of the Creative Commons Attribution License (https: // creativecommons.org / licenses / by / 4.0 / ), which permits unrestricted reuse, \ndistribution, and reproduction in any medium, provided the original work is properly cited. \nDownloaded from https://academic.oup.com/nar/article/53/D1/D730/7903368 by guest on 11 January 2026'
 'Omar & Mohammed / Journal of Applied Science and Technology Trends Vol. 06, No. 02, pp. 342 ‚Äì357 (2025) \n \n355 \nFor the question ‚ÄôWhat are the symptoms of brain cancer?‚Äô \nThe system generated a Cypher query that correctly followed the \nHAS_SYMPTOM relationship from disease nodes to symptoms \nnodes, filtering by the specified disease name. The results \nretrieved included terms such as aphasia (Broca, Wernicke), \nanomia, agraphia, agnosia, amnesia (including retrograde \namnesia), olfaction disorders, and apnea symptoms consistent \nwith established neurological manifestations of brain tumors. In \nNeo4j, the data formed a clear center-and-spoke pattern, with \nbrain cancer at the center and its associated nodes of symptoms \nradiating outward, as shown in Fig. 5. \nCypher: \nWITH toLower(""brain cancer"") AS disease_name \nMATCH (d:Disease)-[:HAS_SYMPTOM]->(s:Symptom) \nWHERE toLower(d.name) CONTAINS disease_name \nRETURN DISTINCT s,d \nLIMIT 10; \n \n \nFig. 5. Graphical user interface displaying Cypher query and results for breast \ncancer treatment \n \n \nA second query, What are the side effects of insulin?, \nproduced a Cypher statement starting from the drug node for \ninsulin and traversing the CAUSES relationship to the connected \nSideEffect nodes. The resulting list included dizziness, diarrhea, \ncough, back pain, weakness, rash/dermatitis, and hypersensitivity \nside effects well documented in insulin safety profiles. In the \nvisual representation in Fig. 6, insulin appeared centrally \nconnected to these side effect nodes, further strengthening the \ncorrectness of the relationship mapping of the system. \n \nCypher: \nWITH toLower(""insulin"") AS drug_name  \nMATCH (d:Drug)-[:CAUSES]->(se:SideEffect)  \nWHERE toLower(d.name) CONTAINS drug_name  \nRETURN se,d \nLIMIT 20; \n \nFig. 6. Knowledge Graph Visualization of Insulin and Its Related Side Effects \n \n \nThese examples demonstrate the system‚Äôs ability to interpret \nplain English biomedical questions, generate correct Cypher \nqueries, and return results that are clinically plausible, easy to \ninterpret, and directly traceable to specific graph relationships. \nThis supports both the accuracy and the transparency of the \nproposed approach. \nTo illustrate how biomedical questions can be translated into \ngraph queries, consider the natural language question: ‚ÄôWhat are \nthe side effects of drugs that treat epilepsy?‚Äô. The following \nCypher query demonstrates how this question is mapped in the \nknowledge graph, where diseases are connected to drugs through \nthe TREATS relation and drugs are connected to side effects \nthrough the CAUSES relation. \n \nCypher: \nMATCH (d:Disease)<-[:TREATS]-(dr:Drug)  \nWHERE toLower(d.name) CONTAINS ""epilepsy"" \nMATCH (dr:Drug)-[:CAUSES]->(se:SideEffect) \nRETURN DISTINCT se,d,dr \nlimit 10'
 'format. Second, because the entire federated KG is never in-\nstantiated in a single place, reasoning and scoring methods\nthat rely on having the entire knowledge graph in memory\ncannot be used with BioThings Explorer.\nIn sum, we believe that knowledge graphs enable many ex-\nciting use cases in biomedical research (Nicholson and Greene\n2020), and that BioThings Explorer is complementary to\nexisting approaches for assembling knowledge graphs, offer-\ning powerful and unique capabilities for both scientific data\nanalysts and tool developers.\nSupplementary data\nSupplementary data are available at Bioinformatics online.\nConflict of interest\nNone declared.\nFunding\nSupport for this work was provided by the National Center\nfor Advancing Translational Sciences, National Institutes of\nHealth, through the Biomedical Data Translator program,\nawards OT2TR003427 and OT2TR003445.\nData availability\nBioThings Explorer is implemented as a NodeJS application.\nThe primary repository for the BioThings Explorer project is\nat https://github.com/biothings/biothings_explorer, which in\nturn links to and incorporates other repositories as sub-\nmodules. All code is released under the Apache 2.0 open-\nsource software license.\nReferences\nCilibrasi RL, Vitanyi PMB. The google similarity distance. IEEE Trans\nKnowl Data Eng 2007;19:370‚Äì83.\nDavis AP, Wiegers TC, Johnson RJ et al. Comparative toxicogenomics\ndatabase\n(CTD):\nupdate\n2023.\nNucleic\nAcids\nRes\n2023;51:\nD1257‚Äì62.\nDowell RD, Jokerst RM, Day A et al. The distributed annotation system.\nBMC Bioinformatics 2001;2:7.\nFecho K, Bizon C, Miller F et al. A biomedical knowledge graph system\nto propose mechanistic hypotheses for real-world environmental\nhealth observations: cohort study and informatics application. JMIR\nMed Inform 2021;9:e26714.\nFecho K, Thessen AE, Baranzini SE et al. Progress toward a universal\nbiomedical data translator. Clin Transl Sci 2022;15:1838‚Äì47.\nHimmelstein DS, Lizee A, Hessler C et al. Systematic integration\nof biomedical knowledge prioritizes drugs for repurposing. Elife\n2017;6:e26726.\nLelong S, Zhou X, Afrasiabi C et al. BioThings SDK: a toolkit for build-\ning\nhigh-performance\ndata\nAPIs\nin\nbiomedical\nresearch.\nBioinformatics 2022;38:2077‚Äì9.\nMayers M, Tu R, Steinecke D et al. Design and application of a knowl-\nedge network for automatic prioritization of drug mechanisms.\nBioinformatics 2022;38:2880‚Äì91.\nMorris JH, Soman K, Akbas RE et al. The scalable precision medicine\nopen knowledge engine (SPOKE): a massive knowledge graph of bio-\nmedical information. Bioinformatics 2023;39(2):btad080.\nMungall CJ, McMurry JA, Ko¬®hler S et al. The monarch initiative: an in-\ntegrative data and analytic platform connecting phenotypes to geno-\ntypes across species. Nucleic Acids Res 2017;45:D712‚Äì22.\nNicholson DN, Greene CS. Constructing knowledge graphs and their\nbiomedical applications. Comput Struct Biotechnol J 2020;18:\n1414‚Äì28.\nRambla J, Baudis M, Ariosa R et al. Beacon v2 and Beacon networks: a\n‚Äòlingua franca‚Äô for federated data discovery in biomedical genomics,\nand beyond. Hum Mutat 2022;43:791‚Äì9.\nUnni DR, Moxon SAT, Bada M et al. Biolink Model: a universal schema\nfor knowledge graphs in clinical, biomedical, and translational sci-\nence. Clin Transl Sci 2022;15:1848‚Äì55.\nWood EC, Glen AK, Kvarfordt LG et al. RTX-KG2: a system for build-\ning a semantically standardized knowledge graph for translational\nbiomedicine. BMC Bioinformatics 2022;23:400.\nZaveri A, Dastgheib S, Wu C et al. smartAPI: towards a more intelligent\nnetwork of web APIs. In: Blomqvist, E., Maynard, D., Gangemi, A.,\net al. (eds) The Semantic Web. ESWC 2017. Lecture Notes in\nComputer Science, 2017, 154‚Äì69. https://link.springer.com/chapter/\n10.1007/978-3-319-58451-5_11#chapter-info.\n4\nCallaghan et al.\nDownloaded from https://academic.oup.com/bioinformatics/article/39/9/btad570/7273783 by guest on 11 January 2026'
 'Omar & Mohammed / Journal of Applied Science and Technology Trends Vol. 06, No. 02, pp. 342 ‚Äì357 (2025) \n \n356 \n \nFig. 7. Side effects of drugs that treat epilepsy \n \nExecuting this query returns drugs associated with epilepsy \nand their corresponding side effects. For example, the query \nidentifies Pregabalin as a treatment for epilepsy and retrieves \nmultiple side effects such as anxiety, arrhythmia, gastrointestinal \npain, and others. The visualization of the graph in Fig. 7, \nhighlights this pattern, where the drug node is linked to epilepsy \nvia TREATS and to several side effect nodes via CAUSES, \nproviding an interpretable biomedical knowledge structure. \nVI. DISCUSSION \nUnder \na \nunified \nevaluation \nprotocol \nreporting \nP@k/R@k/F1@k, Hits@k, and MRR at ùëò‚àà{1,5,10} , the \npipeline exhibits consistent end-to-end behavior across the three \ndifficulty tiers. The ùëò-ablation shows strong early precision (high \nP@1 and elevated MRR), while recall increases with larger ùëò, \nindicating that correct answers typically surface near the top yet \npersist deeper in the ranked slate. At the query-generation level, \nexact-match (EM) is high for simple questions (e.g., ‚àº96%) and \nlower for complex, multi-hop questions (e.g., ‚àº86.7%), which \naligns with increased compositionality. End-to-end latency \n(mean total ‚âà6.07\u2006s from question to final answer) remains within \ninteractive bounds on our local setup. Practically, a fully offline \ndeployment avoids internet connectivity, API keys, and external \ndata \ntransfer, \nstrengthening \nprivacy, \ncompliance, \nand \nreproducibility in biomedical contexts. \nSeveral challenges qualify these findings. First, the \ncomparability challenge: to our knowledge, no prior study \nevaluates iBKH using the same metric suite, making cross-paper \nnumeric comparisons not ‚Äúapples to apples.‚Äù We therefore \ninterpret the results as controlled evidence about this pipeline \nunder a single, consistent protocol rather than as a cross-study \nleaderboard. Beyond comparability, performance may vary with \nother biomedical knowledge graphs; the current iBKH snapshot \nis static, limiting real-time updates; and scaling to larger or \ndynamically refreshed graphs can introduce latency and \nconsistency trade-offs. \nError analysis shows that residual failures concentrate in \ncomplex, multi-hop queries where missing relation hops or brittle \nname-based matching (synonyms, abbreviations, homonyms) \nlead to partial answers. These observations motivate concept-\nlevel normalization via biomedical identifiers (e.g., UMLS, \nSNOMED, RxNorm) with synonym expansion, as well as \nschema-constrained query generation and path-guided decoding \nto better satisfy multi-hop constraints. Finally, correctness is \nassessed primarily at the answer level (Hits@k, MRR, \nprecision/recall) and does not yet include full semantic-\nequivalence checks across alternative Cypher queries, which may \noverlook cases where different queries yield the same correct \nresults. Complementing template EM with result-set equivalence \nchecks, expanding the metric suite (e.g., nDCG/MAP) with per-\nquestion 95% bootstrap confidence intervals, and supporting \nincremental graph updates with distributed storage are promising \nsteps to enhance robustness, scalability, and external validity. \nVII. CONCLUSION \nThis study introduced a hybrid biomedical question \nanswering framework that couples the LLaMA-3B language \nmodel with a Neo4j-based iBKH knowledge graph to enable the \nautomatic generation of executable Cypher queries and to deliver \ntransparent, evidence-grounded answers through justification \nsubgraphs. Evaluation in simple, medium, and complex queries \ndemonstrated consistently high performance, with strong \nprecision, recall, F1, Hits@k, and MRR values, while \nmaintaining low latency suitable for interactive biomedical \napplications. Beyond quantitative performance, the proposed \nsystem provides a reproducible and privacy-preserving solution \nby operating fully offline, a property of particular importance in \nsensitive clinical and research contexts. However, certain \nlimitations remain. The reliance on a static iBKH snapshot \nconstrains coverage and adaptability, recall is lower for complex \nmulti-hop reasoning, and the absence of canonical entity \nnormalization (e.g., UMLS, SNOMED, RxNorm) may reduce \nsemantic robustness. Future research will therefore focus on \nintegrating standardized biomedical entity normalization, \nenabling dynamic and incremental knowledge graph updates, and \nleveraging domain-adapted or fine-tuned biomedical LLMs. \nThese directions are expected to further strengthen the \nrobustness, scalability, and applicability of the framework in real-\nworld biomedical and clinical environments. \n \nREFERENCES \n[1] S. Bubeck, V. Chandrasekaran, R. Eldan, J. Gehrke, E. Horvitz, E. Kamar, \nP. Lee, Y. T. Lee, Y. Li, S. Lundberg, et al., ‚ÄúSparks of artificial general \nintelligence: \nEarly experiments with \ngpt-4,‚Äù arXiv preprint \narXiv:2303.12712, 2023. \n[2] J. Achiam, S. Adler, S. Agarwal, L. Ahmad, I. Akkaya, F. L. Aleman, D. \nAlmeida, J. Altenschmidt, S. Altman, S. Anadkat, et al., ‚ÄúGpt-4 \ntechnical report,‚Äù arXiv preprint arXiv:2303.08774, 2023. \n[3] Y. Hou, J. Yeung, H. Xu, C. Su, F. Wang, and R. Zhang, ‚ÄúFrom answers to \ninsights: unveiling the strengths and limitations of chatgpt and \nbiomedical knowledge graphs,‚Äù Research Square, pp. rs‚Äì3, 2023.'
 'Omar & Mohammed / Journal of Applied Science and Technology Trends Vol. 06, No. 02, pp. 342 ‚Äì357 (2025) \n \n343 \nfor visualizing and validating LLM outputs [16], and MedKA for \nKG-enhanced question answering [17]. \nTo address these challenges, several recent works have \nexplored the integration of large language models with \nbiomedical knowledge graphs (KGs). A biomedical KG is a \nstructured network that connects entities such as diseases, drugs, \nand symptoms using defined relationships [18, 19]. These graphs \nstore verified medical knowledge from trusted databases, \nallowing for more accurate and explainable responses [12]. KGs \nare especially useful in multi-step reasoning tasks, where finding \nan answer requires connecting different pieces of information \n[20]. These entities and relationships can be visually represented \nin a biomedical knowledge graph, as shown in Fig. 1, where \nnodes represent medical concepts such as drugs, diseases, \nsymptoms, and pathways, and edges denote their semantic \nrelationships. \n \nFig. 1. Overview of Biomedical Knowledge Graph Entities and Relationships \n \nOne example of a widely used biomedical KG is SPOKE, \nwhich includes millions of nodes and relationships from over 40 \nbiomedical databases [12]. Integrating an LLM with a KG allows \nthe strengths of both technologies to work together: the LLM \nprovides language understanding, and the KG provides \nstructured, factual knowledge [21, 22]. A common method is \nretrieval-augmented generation (RAG), where the LLM retrieves \ninformation from the KG and uses it to generate more accurate \nresponses [13, 23]. In more advanced setups, the LLM can even \ngenerate queries like Cypher to fetch specific data from the graph \n[24, 25]. Neo4j is a popular graph database that supports fast and \nflexible storage and querying of knowledge graphs using Cypher \n[21]. It is well-suited for biomedical applications because it \nallows easy exploration of complex medical relationships. Recent \nwork has shown that combining Neo4j with LLMs can lead to \nbetter accuracy, fewer hallucinations, and more explainable \nresults [24, 26]. \nDespite improvements, building a reliable hybrid system that \ncombines an LLM with a biomedical KG remains a technical \nchallenge. Some approaches require complex pipelines or large \ntraining datasets, while others rely on fine-tuning specific to a \nnarrow set of questions [27, 28]. There is still a need for systems \nthat are both accurate and easy to scale, especially in domains like \nmedicine where the cost of errors is high [22]. \nRecent advances in KG-augmented LLMs have improved \nperformance, yet biomedical QA continues to face three practical \ngaps: \n1. Traceability: LLM-only or text-retrieval-only pipelines \nrarely provide graph-grounded justifications; users lack the \nability to inspect the exact nodes and edges that support an \nanswer. \n2. Evaluation: Prior work often judges quality via surface-\nform checks (e.g., matching a Cypher template), which fails to \ncapture end-to-end answer correctness or ranking quality \nacross different difficulty levels. \n3. Deployment: Many solutions assume cloud resources or \ndomain-specific fine-tuning, yet biomedical contexts typically \ndemand a local, privacy-preserving system with low latency \nand reproducible behavior. \nTimestamp-aware execution and periodic KG refresh help \navoid deprecated or contraindicated links, making the tool better \nsuited for safety-critical clinical contexts (e.g., drug‚Äìdrug \ninteractions). \nTo address these limitations, Our work introduces a locally \ndeployable pipeline that translates biomedical questions into \nexecutable Cypher queries over a Neo4j knowledge graph. The \nsystem returns answers with supporting nodes and edges, and is'
 '8 | GigaScience , 2025, Vol. 14 \nTable 5. Ablation study of the KGT fr ame work under Code-Llama-13B \nROUGE (%) \nMethod \nGPT-4 Eval (%) \nBERTScore (%) \nRecall \nPrecision \nF1 score \nKGT (ours) \n92.4 \n97.7 \n87.4 \n87.7 \n86.8 \nWithout GSBI \n71.8 \n95.5 \n68.1 \n69.8 \n66.8 \nWithout QA&GSBI \n69.7 \n94.7 \n55.0 \n66.3 \n58.2 \nWithout QA&GSBI&SC \n24.7 \n77.4 \n14.8 \n12.3 \n12.2 \ncompared to the complete KGT, indicating that each of the 3 mod- \nules is vital for the final performance . Furthermore , such obser- \nvations confirm that our KGT can indeed le v er a ge knowledge to \nenhance the final performance of LLMs. \nImplementation settings \nOur knowledge gr a ph is quite large, with a complex schema, and \ntypicall y involv es input tokens within 1,300. Our experiment does \nnot r equir e fine-tuning, and the infer ence time is r elated to the \nmodel size and computational resources. For example, when us- \ning our method, KGT, with the Code-Llama-13B model on an 80 GB \nA100 GPU, it occupies 33 GB of VRAM. Without any acceleration \nfr ame works, the infer ence r equir es 4 passes, eac h taking ar ound \n20 seconds. \nCase studies \nDrug repositioning \nDrug r epositioning emer ges as a pr omising str ategy to acceler ate \nthe process of drug development. This approach involves identi- \nfying new therapeutic uses for existing drugs, thereby saving time \nand r esources typicall y r equir ed for bringing a ne w drug to market \n[ 45 ]. Our system is capable of investigating the potential reposi- \ntioning of carteolol for the treatment of hemangiomas . T he exam- \nple is shown in Supplementary Table S2 and r elational dia gr am is \nshown in Fig. 4 A. Utilizing the system‚Äôs knowledge gr a ph, a r ela- \ntional chain is delineated, illustrating that propranolol, another \ninhibitor of ADRB1, is effectiv el y emplo y ed in the treatment of \nhemangiomas . T he system harnesses this insight to formulate a \nhypothesis that carteolol, by virtue of its similar mechanism of \ninhibition, could be potentially repositioning for treating heman- \ngiomas [ 46 ]. This hypothesis would serve as a precursor to clin- \nical trials and r esearc h, potentiall y expediting the availability of \nan additional ther a peutic option for patients with hemangiomas. \nDrug resistance resear c h \nDrug resistance in cancer treatment poses a significant challenge \nin clinical oncology. Understanding the genetic basis of resistance \ncan lead to mor e effectiv e tr eatment str ategies and personalized \nmedicine a ppr oac hes. Researc h in drug r esistance involv es deter- \nmining why certain cancer-carrying mutated genes are not re- \nsponsive to specific drugs and finding ways to overcome this re- \nsistance [ 47 ]. Our system is capable of exploring drug resistance \nin cancer. The example is shown in Supplementary Table S3 , and \na relational diagram is shown in Fig. 4 B. The KG data indicate \nthat the ALK-p.L1196M m utation, whic h is associated with gas- \ntric cancer, has a known resistance to nalatinib [ 48 , 49 ]. The LLM \nprocesses this information and infers that due to this resistance, \nnalatinib might not be an effective medication for treating can- \ncers caused by the ALK-p.L1196M mutation. The case highlights \nthe critical importance of understanding specific gene‚Äìdrug in- \nteractions in drug resistance research. It demonstrates how cer- \ntain gene mutations could render a drug ineffective, which in turn \ncould guide oncologists in choosing alternative treatments or de- \nv eloping ne w drugs that can bypass or tar get the r esistance mec h- \nanisms. By accelerating the process of understanding drug resis- \ntance, these artificial intelligence‚Äìdriven systems can contribute \nto impr ov ed patient outcomes and the optimization of cancer \ntr eatment pr otocols. \nIndividualized treatment \nDetails \non \nindividualized \ntr eatment \nar e \npr ovided \nin \nSupplementary Case Studies A . It is important to note that this \nexample is included solely to illustrate the technical capabilities \nof the proposed method. The output generated in this example \nhas not been validated for clinical use, and further validation in \nclinical settings would be r equir ed befor e an y suc h a pplication. \nSelection and understanding of biomarkers \nDetails on selection and understanding of biomarkers are pro- \nvided in Supplementary Case Studies B . \nDiscussion \nIn this article, we introduce a novel framework KGT, which em- \nploys LLMs for reasoning on the schema of KGs, to enhance the \nreasoning abilities of LLMs in areas with missing domain data \nby utilizing domain-specific knowledge gr a phs, suc h as oncology \nknowledge gr a phs, ther eby addr essing the issue of factual hal- \nlucinations in LLMs. Our method excels in extr acting, v alidat- \ning, and refining factual knowledge throughout the LLMs‚Äô rea- \nsoning process. It seamlessly integrates with various LLMs, in- \ncluding open-source models like Code-Llama, and enhances the \ncapabilities of LLMs solely through prompt engineering and in- \ncontext learning without any fine-tuning. This grants it significant \ngeneralizability. \nWe possess an extensive oncology knowledge graph and have \nestablished a benchmark based on it to e v aluate the ca pabilities of \nvarious methods. When tested on PcQA using various open-source \nLLMs, the KGT fr ame work performs exceptionall y well, sur passing \nthe current best methods by 33%. This significant improvement \npositions our a ppr oac h as a pioneering benchmark in biomedical \nKGQA, setting a new standard that advances beyond previously \nestablished best pr actices. Additionall y, thr ough case studies, our \na ppr oac h has been shown to effectiv el y pr ovide ther a peutic plans, \ngener ate v aluable hypotheses for drug r epositioning, identify po- \ntential drug targets, and study drug resistance . T his underscores \nthe pr actical v alue of the KGT fr ame work in deliv ering insight- \nful contributions that aid in the de v elopment and optimization \nof treatment strategies. Each case study‚Äôs conclusions are further \nv alidated by e vidence fr om pr e viousl y published r esearc h pa pers, \nenhancing the credibility and impact of our findings. \nHo w e v er, it is important to note that the constructed QA \ndataset and the corresponding published subset of the SOKG \nDownloaded from https://academic.oup.com/gigascience/article/doi/10.1093/gigascience/giae082/7943459 by guest on 11 January 2026'
 'Omar & Mohammed / Journal of Applied Science and Technology Trends Vol. 06, No. 02, pp. 342 ‚Äì357 (2025) \n \n357 \n[4] Z. Ji, N. Lee, R. Frieske, T. Yu, D. Su, Y. Xu, E. Ishii, Y. J. Bang, A. Madotto, \nand P. Fung, ‚ÄúSurvey of hallucination in natural language generation,‚Äù \nACM Computing Surveys, vol. 55, no. 12, pp. 1‚Äì38, 2023. \n[5] C. Malaviya, S. Lee, S. Chen, E. Sieber, M. Yatskar, and D. Roth, ‚ÄúExpertqa: \nexpert-curated questions and attributed answers,‚Äù arXiv preprint \narXiv:2309.07852, 2023. \n[6] L. Pusch and T. O. Conrad, ‚ÄúCombining llms and knowledge graphs to reduce \nhallucinations \nin \nquestion \nanswering,‚Äù \narXiv \npreprint \narXiv:2409.04181, 2024. \n[7] H. Nori, N. King, S. M. McKinney, D. Carignan, and E. Horvitz, \n‚ÄúCapabilities of gpt-4 on medical challenge problems,‚Äù arXiv preprint \narXiv:2303.13375, 2023. \n[8] T. Sekar, Kushal, S. Shankar, S. Mohammed, and J. Fiaidhi, ‚ÄúInvestigations \non using evidence-based graphrag pipeline using llm tailored for usmle \nstyle questions,‚Äù medRxiv, pp. 2025‚Äì05, 2025. \n[9] S. Farquhar, J. Kossen, L. Kuhn, and Y. Gal, ‚ÄúDetecting hallucinations in \nlarge language models using semantic entropy,‚Äù Nature, vol. 630, no. \n8017, pp. 625‚Äì630, 2024. \n[10] E. Asgari, N. Monta√±a-Brown, M. Dubois, S. Khalil, J. Balloch, J. A. \nYeung, and D. Pimenta, ‚ÄúA framework to assess clinical safety and \nhallucination rates of llms for medical text summarisation,‚Äù npj Digital \nMedicine, vol. 8, no. 1, p. 274, 2025. \n[11] C. Su, Y. Hou, M. Zhou, S. Rajendran, J. R. Maasch, Z. Abedi, H. Zhang, \nZ. Bai, A. Cuturrufo, W. Guo, et al., ‚ÄúBiomedical discovery through the \nintegrative biomedical knowledge hub (ibkh),‚Äù Iscience, vol. 26, no. 4, \n2023. \n[12] J. H. Morris, K. Soman, R. E. Akbas, X. Zhou, B. Smith, E. C. Meng, C. C. \nHuang, G. Cerono, G. Schenk, A. Rizk-Jackson, et al., ‚ÄúThe scalable \nprecision medicine open knowledge engine (spoke): a massive \nknowledge graph of biomedical information,‚Äù Bioinformatics, vol. 39, \nno. 2, p. btad080, 2023. \n[13] K. Soman, P. W. Rose, J. H. Morris, R. E. Akbas, B. Smith, B. Peetoom, C. \nVillouta-Reyes, G. Cerono, Y. Shi, A. Rizk-Jackson, et al., ‚ÄúBiomedical \nknowledge graph-optimized prompt generation for large language \nmodels,‚Äù Bioinformatics, vol. 40, no. 9, p. btae560, 2024. \n[14] F. Frau, P. Loustalot, M. T√∂rnqvist, N. Temam, J. Cupe, M. Montmerle, and \nF. Aug√©, ‚ÄúConnecting electronic health records to a biomedical \nknowledge graph to link clinical phenotypes and molecular endotypes \nin atopic dermatitis,‚Äù Scientific Reports, vol. 15, no. 1, p. 3082, 2025. \n[15] Y. Gao, R. Li, E. Croxford, J. Caskey, B. W. Patterson, M. Churpek, T. \nMiller, D. Dligach, and M. Afshar, ‚ÄúLeveraging medical knowledge \ngraphs into large language models for diagnosis prediction: design and \napplication study,‚Äù Jmir AI, vol. 4, p. e58670, 2025. \n[16] Y. Yan, Y. Hou, Y. Xiao, R. Zhang, and Q. Wang, ‚ÄúKnownet: guided health \ninformation seeking from llms via knowledge graph integration,‚Äù IEEE \nTransactions on Visualization and Computer Graphics, 2024. \n[17] Y. Deng, S. Zhao, Y. Miao, J. Zhu, and J. Li, ‚ÄúMedka: a knowledge graph-\naugmented approach to improve factuality in medical large language \nmodels,‚Äù Journal of Biomedical Informatics, p. 104871, 2025. \n[18] L. Ehrlinger and W. W√∂√ü, ‚ÄúTowards a definition of knowledge graphs,‚Äù \nSEMANTiCS (Posters, Demos, SuCCESS), vol. 48, no. 1‚Äì4, p. 2, 2016. \n[19] E. Rajabi and S. Kafaie, ‚ÄúBuilding a disease knowledge graph,‚Äù in Caring \nis Sharing ‚Äì Exploiting the Value in Data for Health and Innovation, \npp. 701‚Äì705, IOS Press, 2023. \n[20] L. Guan, Y. Huang, and J. Liu, ‚ÄúBiomedical question answering via multi-\nlevel summarization on a local knowledge graph,‚Äù arXiv preprint \narXiv:2504.01309, 2025. \n[21] D. Steinigen, R. Teucher, T. H. Ruland, M. Rudat, N. Flores-Herr, P. \nFischer, N. Milosevic, C. Schymura, and A. Ziletti, ‚ÄúFact finder ‚Äì \nenhancing domain expertise of large language models by incorporating \nknowledge graphs,‚Äù arXiv preprint arXiv:2408.03010, 2024. \n[22] Y. Feng, L. Zhou, C. Ma, Y. Zheng, R. He, and Y. Li, ‚ÄúKnowledge graph‚Äì\nbased thought: a knowledge graph‚Äìenhanced llm framework for pan-\ncancer question answering,‚Äù GigaScience, vol. 14, p. giae082, 2025. \n[23] H. Luo, Z. Tang, S. Peng, Y. Guo, W. Zhang, C. Ma, G. Dong, M. Song, \nW. Lin, Y. Zhu, et al., ‚ÄúChatkbqa: a generate-then-retrieve framework \nfor knowledge base question answering with fine-tuned large language \nmodels,‚Äù arXiv preprint arXiv:2310.08975, 2023. \n[24] A. Tiwari, S. K. R. Malay, V. Yadav, M. Hashemi, and S. T. Madhusudhan, \n‚ÄúAuto-cypher: improving llms on cypher generation via llm-supervised \ngeneration-verification framework,‚Äù in Proceedings of the 2025 \nConference of the Nations of the Americas Chapter of the Association \nfor Computational Linguistics: Human Language Technologies \n(Volume 2: Short Papers), pp. 623‚Äì640, 2025. \n[25] R. Wang, Z. Zhang, L. Rossetto, F. Ruosch, and A. Bernstein, ‚ÄúNlqxform: \na language model-based question to sparql transformer,‚Äù arXiv preprint \narXiv:2311.07588, 2023. \n[26] M. R. Rezaei, R. S. Fard, J. L. Parker, R. G. Krishnan, and M. Lankarany, \n‚ÄúAgentic medical knowledge graphs enhance medical question \nanswering: bridging the gap between llms and evolving medical \nknowledge,‚Äù arXiv preprint arXiv:2502.13010, 2025. \n[27] Z. Dong, B. Peng, Y. Wang, J. Fu, X. Wang, Y. Shan, and X. Zhou, ‚ÄúEffiqa: \nefficient question-answering with strategic multi-model collaboration \non knowledge graphs,‚Äù arXiv preprint arXiv:2406.01238, 2024. \n[28] Y. Duan, Q. Zhou, Y. Li, C. Qin, Z. Wang, H. Kan, and J. Hu, ‚ÄúResearch \non a traditional chinese medicine case-based question-answering \nsystem integrating large language models and knowledge graphs,‚Äù \nFrontiers in Medicine, vol. 11, p. 1512329, 2025. \n[29] S. Mohammed, J. Fiaidhi, T. Sekar, K. Kushal, and S. Shankar, \n‚ÄúInvestigations on using evidence-based graphrag pipeline using llm \ntailored for answering usmle medical exam questions,‚Äù medRxiv, pp. \n2025‚Äì05, 2025. \n[30] H. Yang, J. Li, C. Zhang, A. P. Sierra, and B. Shen, ‚ÄúLarge language model‚Äì\ndriven knowledge graph construction in sepsis care using multicenter \nclinical databases: development and usability study,‚Äù Journal of \nMedical Internet Research, vol. 27, p. e65537, 2025. \n[31] K.-L. Hsieh, G. Plascencia-Villa, K.-H. Lin, G. Perry, X. Jiang, and Y. Kim, \n‚ÄúSynthesize heterogeneous biological knowledge via representation \nlearning for alzheimer‚Äôs disease drug repurposing,‚Äù Iscience, vol. 26, \nno. 1, 2023. \n[32] R. Angles and C. Gutierrez, ‚ÄúSurvey of graph database models,‚Äù ACM \nComputing Surveys (CSUR), vol. 40, no. 1, pp. 1‚Äì39, 2008. \n[33] B. Chicho and A. O. Mohammed, ‚ÄúAn empirical comparison of neo4j and \ntigergraph databases for network centrality,‚Äù Science Journal of \nUniversity of Zakho, vol. 11, no. 2, pp. 190‚Äì201, 2023. \n[34] I. Robinson, J. Webber, and E. Eifrem, Graph Databases: New \nOpportunities for Connected Data, O‚ÄôReilly Media, 2015. \n[35] A. Lysenko, I. A. RoznovƒÉ≈£, M. Saqi, A. Mazein, C. J. Rawlings, and C. \nAuffray, ‚ÄúRepresenting and querying disease networks using graph \ndatabases,‚Äù BioData Mining, vol. 9, no. 1, p. 23, 2016. \n[36] M. ≈†estak, M. Heriƒçko, T. W. Dru≈æovec, and M. Turkanoviƒá, ‚ÄúApplying k-\nvertex cardinality constraints on a neo4j graph database,‚Äù Future \nGeneration Computer Systems, vol. 115, pp. 459‚Äì474, 2021. \n[37] M. Desai, R. G. Mehta, and D. P. Rana, ‚ÄúAn empirical analysis to identify \nthe effect of indexing on influence detection using graph databases,‚Äù \nInternational Journal of Innovative Technology and Exploring \nEngineering, vol. 8, no. 9S, pp. 414‚Äì421, 2019. \n[38] S. Beis, S. Papadopoulos, and Y. Kompatsiaris, ‚ÄúBenchmarking graph \ndatabases on the problem of community detection,‚Äù in New Trends in \nDatabase and Information Systems II, pp. 3‚Äì14, Springer, 2015. \n[39] R. Wang, Z. Yang, W. Zhang, and X. Lin, ‚ÄúAn empirical study on recent \ngraph database systems,‚Äù in International Conference on Knowledge \nScience, Engineering and Management, pp. 328‚Äì340, Springer, 2020. \n[40] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, ‚ÄúBert: pre-training of \ndeep bidirectional transformers for language understanding,‚Äù in \nProceedings of the 2019 Conference of the North American Chapter of \nthe Association for Computational Linguistics: Human Language \nTechnologies, vol. 1, pp. 4171‚Äì4186, 2019. \n[41] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. \nNeelakantan, P. Shyam, G. Sastry, A. Askell, et al., ‚ÄúLanguage models \nare few-shot learners,‚Äù Advances in Neural Information Processing \nSystems, vol. 33, pp. 1877‚Äì1901, 2020. \n[42] J. Lee, W. Yoon, S. Kim, D. Kim, S. Kim, C. H. So, and J. Kang, ‚ÄúBiobert: \na pre-trained biomedical language representation model for biomedical \ntext mining,‚Äù Bioinformatics, vol. 36, no. 4, pp. 1234‚Äì1240, 2020. \n[43] E. Alsentzer, J. R. Murphy, W. Boag, W.-H. Weng, D. Jin, T. Naumann, \nand M. McDermott, ‚ÄúPublicly available clinical bert embeddings,‚Äù \narXiv preprint arXiv:1904.03323, 2019. \n[44] Z. He, S. Sunkara, X. Zang, Y. Xu, L. Liu, N. Wichers, G. Schubiner, R. \nLee, and J. Chen, ‚ÄúActionbert: leveraging user actions for semantic \nunderstanding of user interfaces,‚Äù in Proceedings of the AAAI \nConference on Artificial Intelligence, vol. 35, pp. 5931‚Äì5938, 2021.'
 '5 Discussion\nIntegration of existing data from multiple disparate sources is\na key step in assessing the state of current knowledge. There\nare many existing efforts to create biomedical knowledge\ngraphs by integrating locally downloaded data and standard-\nizing it using a common data model (Himmelstein et al. 2017;\nFecho et al. 2021; Mayers et al. 2022; Wood et al. 2022;\nMorris et al. 2023). These efforts result in centralized knowl-\nedge graphs of substantial size, often with millions of nodes\nand tens of millions of edges.\nBioThings Explorer offers a unique strategy for data inte-\ngration, focusing on creating a federated knowledge graph by\nsemantically annotating APIs. Rather than bringing all data\ninto a massive, centralized graph database, this federated de-\nsign instead allows knowledge to remain behind each resour-\nce‚Äôs API. Data are retrieved at query time by dynamically\nexecuting API calls and semantically parsing the results. This\narchitecture\nfunctionally\nseparates\ndata\ndissemination\n(through API creation) from data modeling and data integra-\ntion (through semantic annotations).\nThis approach has several advantages. First, by moving the\nrequirements for interoperability from implementation in\ncode to semantic API annotation, we significantly lower the\nbarrier to participation in our API ecosystem. Second, by sep-\narating these roles into distinct layers, we promote the overall\nmodularity of our system. These components can develop and\nevolve in parallel, and these two roles can even be undertaken\nby separate teams (e.g. one team semantically annotates an\nAPI that was created by another team). Third, this design\nfacilitates\nan\niterative\napproach\nto\nAPI\nannotation.\nDevelopers and API annotators can first provide a minimal\nset of API metadata, which can later be extended based on fu-\nture needs and use cases.\nThe federated design of BioThings Explorer also has some\nnotable\nlimitations.\nFirst,\nour\nOpenAPI\nextensions\nin\nSmartAPI to semantically annotate APIs only work on APIs\nthat follow the REST protocol and provide output in JSON\nFigure 1. A visualization of the meta-KG for BioThings Explorer. The nodes in this graph are the semantic types of biomedical entities that BioThings\nExplorer can retrieve associations between (limited to the top eight most common semantic types). The edges between nodes show what associations\nbetween biomedical entities exist in the semantic API network that is accessible through BioThings Explorer. The edge label shows the number of APIs\nthat can retrieve those types of associations, which is also represented by the edge width.\nFigure 2. Deconstruction of a query in BioThings Explorer. (A) A free-text\nrepresentation of a query that can be answered by BioThings Explorer.\n(B) The graph representation of the same query. The exact syntax of this\ngraph query is speciÔ¨Åed in the Translator Reasoner API standard\ndescribed in Fecho et al. (2022) and shown in Supplementary Fig. S2.\n(C) The deconstruction of the graph query into multiple API calls by\nconsulting the meta-KG in the SmartAPI registry.\nBioThings Explorer\n3\nDownloaded from https://academic.oup.com/bioinformatics/article/39/9/btad570/7273783 by guest on 11 January 2026']"
what Cypher do for biomedical data?,"['III. PRELIMINARIES This section outlines the fundamental concepts required to understand the proposed system. It introduces biomedical knowledge graphs, explains how Neo4j stores data in graph form, and describes the use of Cypher for querying. It also provides a brief overview of large language models (LLMs) and their role in interpreting natural language. A. Biomedical Knowledge Graphs Biomedical Knowledge Graphs (BKGs) provide a structured representation of complex biomedical information by modeling diverse medical entities, such as diseases, drugs, symptoms, and biological pathways, as interconnected nodes within a graph structure. The edges in these graphs represent the semantic relationships between these entities, including ‚Äôtreats‚Äô, ‚Äôcauses‚Äô, ‚Äôinteracts with‚Äô and many others, as illustrated in Fig 1. This form of representation enables the integration of heterogeneous biomedical data from a wide range of sources, including']","Cypher is used for querying biomedical knowledge graphs stored in Neo4j, enabling retrieval and manipulation of complex biomedical information represented as interconnected nodes and relationships.",single_hop_specifc_query_synthesizer,"Cypher is used in biomedical data to enable the translation of natural language biomedical questions into executable graph queries over a biomedical knowledge graph (such as the Integrative Biomedical Knowledge Hub, iBKH). Specifically, Cypher allows the system to:

1. **Query Complex Biomedical Knowledge Graphs:** Cypher queries are executed on Neo4j graph databases that store biomedical entities (e.g., diseases, drugs, symptoms, pathways) as nodes and their relationships (e.g., TREATS, HAS_SYMPTOM, CAUSES) as edges. This structure supports complex, multi-hop queries that reflect real biomedical reasoning.

2. **Retrieve Clinically Relevant Information:** By generating Cypher queries from natural language questions, the system can retrieve accurate and clinically plausible answers such as symptoms of diseases, side effects of drugs, drug interactions, and biological pathways involved.

3. **Support Multi-level Question Complexity:** Cypher queries can handle different levels of question difficulty:
   - Simple (single-hop) queries, e.g., ""What are the symptoms of Alzheimer‚Äôs?""
   - Medium complexity queries involving multiple relationships, e.g., ""What are the side effects of drugs used to treat asthma?""
   - Complex multi-step queries requiring reasoning across multiple biomedical concepts, e.g., ""What cholesterol medications cause side effects, and what are some of those effects?""

4. **Enable Transparency and Validation:** The system displays the generated Cypher query alongside the answer, allowing users to verify how their question was interpreted and how the answer was derived from the graph.

5. **Facilitate Efficient and Reliable Query Execution:** Cypher queries are executed efficiently on Neo4j with indexing and caching, ensuring fast response times even for complex biomedical queries. The system also handles errors in query generation by retrying or informing the user.

6. **Integrate with Large Language Models (LLMs):** The system uses LLaMA 3 to convert natural language questions into Cypher queries, leveraging Cypher‚Äôs expressive power to explore the biomedical knowledge graph accurately.

In summary, Cypher serves as the critical query language that bridges natural language biomedical questions and the structured biomedical knowledge graph, enabling precise, explainable, and efficient retrieval of biomedical information for research and clinical decision support.","[""Omar & Mohammed / Journal of Applied Science and Technology Trends Vol. 06, No. 02, pp. 342 ‚Äì357 (2025) \n \n352 \nThe dataset contains 60 questions divided into three difficulty \nlevels based on how complex the language is and how deep the \nbiomedical reasoning needs to be: \n‚Ä¢ Level 1: 25 simple questions focusing mostly on easy-to-\nunderstand facts, such as symptoms of certain diseases or \ndrugs used to treat specific conditions. \n‚Ä¢ Level 2: 20 medium-level questions that involve more \ndetailed relationships, such as drug interactions and \nSideEffect. \n‚Ä¢ Level 3: 15 hard questions requiring multi-step reasoning \nacross multiple biomedical concepts or biological \npathways, similar to the complexity found in real clinical \ncases. \nThe set of evaluation questions was designed to span a wide \nrange of common biomedical topics and to reflect clinically \nrelevant query types reported in prior literature. Each question is \npaired with its corresponding gold standard cypher query and \ncategorized by difficulty level, as summarized in Table V, where \nthree illustrative examples are shown. The questions were derived \nfrom publicly available biomedical QA benchmarks and adapted \nfrom established knowledge bases (e.g., iBKH schema relations), \nensuring both coverage and diversity across diseases, drugs, \nsymptoms, and pathways.\nTABLE V.  QUESTION DIFFICULTY LEVELS AND SAMPLE CYPHER QUERIES \nLevel \nDefinition \nExample \nCypher Query \n1 \nSingle-hop question using \none relationship \nWhat are the symptoms of \nAlzheimer? \nMATCH (d:Disease)-[:HAS_SYMPTOM]->(s:Symptom) \nWHERE toLower(d.name) CONTAINS 'alzheimer' \nRETURN s.name AS symptom \n \n2 \nQuestions requiring one \nrelationship \nWhat are the side effects of \ndrugs used to treat asthma? \nWITH toLower('asthma') AS disease_name \nMATCH (d:Disease)<-[:TREATS]-(dr:Drug) \nWHERE toLower(d.name) CONTAINS disease_name \nMATCH (dr:Drug)-[:CAUSES]->(se:SideEffect) \nRETURN DISTINCT se.name AS side_effect, dr.name AS drug \n \n3 \nQuestions requiring two or \nmore relationships \nWhat cholesterol medications \ncause side effects, and what are \nsome of those effects? \nWITH toLower('cholesterol') AS disease_name \nMATCH (d:Disease)<-[:TREATS]-(dr:Drug) \nWHERE toLower(d.name) CONTAINS disease_name \nMATCH (dr:Drug)-[:CAUSES]->(se:SideEffect) \nRETURN DISTINCT dr.name AS drug, se.name AS side_effect \n \n \n \nFig. 4. Graphical User Interface of the Biomedical Knowledge Explorer System \nB. Quantitative Evaluation \nThe performance of the proposed system is evaluated on the \niBKH knowledge graph using Exact Match, Precision@k, \nRecall@k, F1@k, Hits@k, MRR, and total response time across \nsimple, medium, and complex biomedical queries. These metrics \nwere measured for each difficulty level (simple, medium, and \ncomplex) to see how performance changes as questions become \nmore challenging.\n \nTABLE VI.  QUERY GENERATION EXACT MATCH BY DIFFICULTY LEVEL""
 'Omar & Mohammed / Journal of Applied Science and Technology Trends Vol. 06, No. 02, pp. 342 ‚Äì357 (2025) \n \n351 \nThis query searches for a disease node whose name contains \n‚Äôalzheimer‚Äô and follows HAS_SYMPTOM edges to list related \nsymptom names. The system then executes this cypher to retrieve \nanswers. The prompts (such as few-shot examples and schema \nhints) were carefully designed to help LLaMA 3 generate correct \nCypher queries. The model learns how to use the graph‚Äôs labels \nand relationships properly. For example, if a user asks, ‚ÄôWhich \ndrugs treat diabetes?‚Äô, LLaMA might respond with a suitable \nCypher query: \n \nMATCH (d:Drug)-[:TREATS]->(di:Disease) \nWHERE toLower(di.name) CONTAINS ""diabetes"" \nRETURN d.name  \n \nThis queries for drug nodes that have a TREATS edge to a \ndiabetes disease node. By leveraging LLaMA 3 in this way, our \nsystem can flexibly handle many phrasing variations without \nmanual mapping rules. \nD. Model Configuration & Decoding \nWe run a local LLaMA 3.2-3B model in GGUF format (llama-\n3.2-3b-instruct-q4_k_m.gguf) via llama.cpp, as shown in Table \nIV. \nTABLE IV.  MODEL RUNTIME AND DECODING SETTINGS \nRuntime settings \nDecoding settings \nn_ctx = 1024 \ntemperature = 0.2 \nn_threads = 12 \ntop_p = 0.95 \nn_gpu_layers = 33 \ntop_k = 40 \nn_batch = 512 \nrepeat_penalty = 1.1 \n \nmax_tokens = 80 \n \nseed = 42 \n \nE. Graph Subset and Versioning \nWe use an iBKH derived subgraph (‚âà65.8k nodes; ‚âà3.0M \nedges) spanning DRUG, DISEASE, SYMPTOM, PATHWAY. IDs are \nnormalized to CURIEs and duplicates collapsed across \nUMLS/DrugBank/DisGeNET/SIDER/KEGG. Each edge stores \nprovenance/licensing metadata (source, source_version, license, \nretrieved_at, \nevidence_pmids/urls) \nand, \nwhen \navailable, \ntemporal fields (valid_from, valid_to). We report coverage as \nthe \npercentage \nof \nevaluated \nquestions \nwhose \ngold \nentities/relations are present. \nF. Query Execution and Reliability \nAfter a Cypher query is generated, it is executed on the Neo4j \ndatabase through the official Neo4j Python driver, which \nmanages the secure connection and returns the results. Instead of \nrestricting the output with a fixed LIMIT (e.g., LIMIT 5), the \nsystem retrieves candidate results and evaluates them using \nstandardized retrieval metrics such as Hits@1, Hits@5, and \nHits@10. This approach ensures that the system remains \nresponsive while providing a fair assessment of ranking quality \nacross different cutoff levels, rather than depending solely on a \nfixed number of returned items. Neo4j‚Äôs indexing on key node \nproperties, such as name and primary identifiers, also helps speed \nup lookups as the knowledge graph grows. In cases where the \nlanguage model generates an incomplete or incorrect query, such \nas referencing nodes or relationships that do not exist, the system \ncatches the error and either retries with a simpler prompt or \ninforms the user. Together, these steps make sure that queries run \nquickly, return valid results, and keep the overall experience \nsmooth and reliable for biomedical question-answering. \nG. User Interface for Query Transparency \nThe system includes a lightweight, cross-platform graphical \nuser interface (GUI) implemented as a web application using the \nFlask framework in Python, with HTML and Bootstrap for \ninteractive visualization. The interface is designed to make the \nquestion-answering process transparent and accessible to users \nwithout technical expertise. It consists of three main panels: \n1. Input Panel: Where the user can enter a biomedical \nquestion in natural language. \n2. Query Panel: Which displays the Cypher query generated \nby the language model, allowing users to verify how their \nquestion was interpreted. \n3. Results Panel: Which presents the retrieved answers in a \nclear, readable format, accompanied by a brief natural \nlanguage explanation generated by the system. \nBy showing both the query and the answer, the GUI promotes \nuser trust and enables validation of the system‚Äôs reasoning \nprocess. The interface is lightweight enough to run smoothly on \nstandard desktop machines without additional dependencies, \nmaking it practical for local deployments in clinical or research \nsettings. Fig. 4 illustrates the overall layout. \nV. EXPERIMENTAL RESULTS \nIn this section, the proposed system is evaluated based on its \nability to translate natural-language biomedical questions into \nexecutable Cypher queries over the iBKH knowledge graph. The \nassessment is conducted end-to-end and focuses on three main \naspects: (i) the accuracy of query generation, measured by the \nproportion of correctly produced Cypher queries; (ii) system \nefficiency, quantified through total response time from question \nsubmission to final answer delivery, covering both query \ngeneration and execution; and (iii) the quality of retrieved \nanswers, evaluated using standardized information retrieval \nmetrics including Precision@k, Recall@k, F1@k, Hits@k, and \nMean Reciprocal Rank (MRR). For clarity, all these metrics are \nformally defined in the following subsections, with their \ncorresponding equations, and will be used consistently \nthroughout the results section. Together, these dimensions \nprovide a comprehensive view of both correctness and efficiency \nacross simple, medium, and complex biomedical queries. \nA. Description of the Experimental Data \nTo evaluate the proposed system, this work used a carefully \nselected set of biomedical questions designed to test how well the \nsystem understands natural language and converts it into Cypher \nqueries for a biomedical knowledge graph. \nTo evaluate the system, a benchmark of 60 biomedical \nquestions was constructed by the authors, guided by the schema \nand relations in iBKH. The questions were grouped into simple, \nmedium, and complex levels to assess performance across \ndifferent reasoning requirements. Gold-standard answers were \nmanually prepared for each question to enable quantitative \nevaluation using Exact Match, Precision@k, Recall@k, F1@k, \nHits@k, and MRR. The complete set of 60 questions is available \nat this link https://drive.google.com/drive/my-drive.'
 'Omar & Mohammed / Journal of Applied Science and Technology Trends Vol. 06, No. 02, pp. 342 ‚Äì357 (2025) \n \n355 \nFor the question ‚ÄôWhat are the symptoms of brain cancer?‚Äô \nThe system generated a Cypher query that correctly followed the \nHAS_SYMPTOM relationship from disease nodes to symptoms \nnodes, filtering by the specified disease name. The results \nretrieved included terms such as aphasia (Broca, Wernicke), \nanomia, agraphia, agnosia, amnesia (including retrograde \namnesia), olfaction disorders, and apnea symptoms consistent \nwith established neurological manifestations of brain tumors. In \nNeo4j, the data formed a clear center-and-spoke pattern, with \nbrain cancer at the center and its associated nodes of symptoms \nradiating outward, as shown in Fig. 5. \nCypher: \nWITH toLower(""brain cancer"") AS disease_name \nMATCH (d:Disease)-[:HAS_SYMPTOM]->(s:Symptom) \nWHERE toLower(d.name) CONTAINS disease_name \nRETURN DISTINCT s,d \nLIMIT 10; \n \n \nFig. 5. Graphical user interface displaying Cypher query and results for breast \ncancer treatment \n \n \nA second query, What are the side effects of insulin?, \nproduced a Cypher statement starting from the drug node for \ninsulin and traversing the CAUSES relationship to the connected \nSideEffect nodes. The resulting list included dizziness, diarrhea, \ncough, back pain, weakness, rash/dermatitis, and hypersensitivity \nside effects well documented in insulin safety profiles. In the \nvisual representation in Fig. 6, insulin appeared centrally \nconnected to these side effect nodes, further strengthening the \ncorrectness of the relationship mapping of the system. \n \nCypher: \nWITH toLower(""insulin"") AS drug_name  \nMATCH (d:Drug)-[:CAUSES]->(se:SideEffect)  \nWHERE toLower(d.name) CONTAINS drug_name  \nRETURN se,d \nLIMIT 20; \n \nFig. 6. Knowledge Graph Visualization of Insulin and Its Related Side Effects \n \n \nThese examples demonstrate the system‚Äôs ability to interpret \nplain English biomedical questions, generate correct Cypher \nqueries, and return results that are clinically plausible, easy to \ninterpret, and directly traceable to specific graph relationships. \nThis supports both the accuracy and the transparency of the \nproposed approach. \nTo illustrate how biomedical questions can be translated into \ngraph queries, consider the natural language question: ‚ÄôWhat are \nthe side effects of drugs that treat epilepsy?‚Äô. The following \nCypher query demonstrates how this question is mapped in the \nknowledge graph, where diseases are connected to drugs through \nthe TREATS relation and drugs are connected to side effects \nthrough the CAUSES relation. \n \nCypher: \nMATCH (d:Disease)<-[:TREATS]-(dr:Drug)  \nWHERE toLower(d.name) CONTAINS ""epilepsy"" \nMATCH (dr:Drug)-[:CAUSES]->(se:SideEffect) \nRETURN DISTINCT se,d,dr \nlimit 10'
 'Omar & Mohammed / Journal of Applied Science and Technology Trends Vol. 06, No. 02, pp. 342 ‚Äì357 (2025) \n \n350 \ndocumentation, indexes ‚Äúenable quicker and more efficient \npattern matching‚Äù by allowing the query planner to rapidly locate \nnodes by label and property. \nWith the schema in place, data was imported using Cypher‚Äôs \nLOAD CSV commands. For each vocabulary file, rows with \nnonnull primary IDs were processed: the code used MERGE to \ncreate (or match existing) nodes with the given label and primary \nproperty, and then SET the remaining attributes from the CSV \ncolumns. For example, in importing disease_vocab.csv, each row \nproduced a node (:Disease primary: <id>) with properties name, \ndo_id, kegg_id, and umls_cui set from the row (empty strings \nwere turned into null). Similarly, drug_vocab.csv rows produced \ndrug nodes with properties for DrugBank, KEGG, PharmGKB \nIDs, etc. This approach follows best practice: using MERGE on \nthe unique ID ensures that nodes are not duplicated during \nmultiple passes. \nAfter all nodes were created, the relationship CSVs were \nloaded. Each row in those files was matched to the corresponding \nsource and target nodes by their primary IDs, and conditional \nlogic was used to create edges. For example, the disease-\nsymptom file (Di_Sy_res.csv) was processed by matching a \ndisease node and a symptom node for each row, then executing \nMERGE (d)-[r:HAS_SYMPTOM]-(s) if the present column is \nnonzero; the edge was given a present property and a source \nproperty from the CSV. The disease-disease file (di_di_res.csv) \nwas processed by matching disease nodes d1 and d2:If is_a = 1, \na (:Disease)-[:IS_A]->(:Disease) edge was merged; if Resemble \n= 1, a (:Disease)-[:RESEMBLES]->(:Disease) edge was merged. \nDrug-disease relationships were handled similarly: the script \nmatched a Drug node and a Disease node for each row of \nD_Di_res.csv, then for each flag that was 1, it merged the \ncorresponding edge label (such as TREATS, PALLIATES, \nASSOCIATED_WITH, etc.) from the drug to the disease. Drug‚Äì\nside-effect rows produced (:Drug)-[:CAUSES]->(:SideEffect) \nedges with the source noted, and drug‚Äìdrug rows produced either \nINTERACTS_WITH or RESEMBLES edges between matched \ndrug pairs. Finally, the drug-pathway and disease-pathway files \neach produced:ASSOCIATED_WITH edges linking drugs or \ndiseases to pathway nodes. \nIn this graph model, most relationships are directional (for \nexample, a drug TREATS a disease; a disease does not TREAT \na drug). This follows common practice in biomedical KGs. The \nsame relations (such as RESEMBLES or INTERACTS_WITH) \nare inherently symmetric, but were stored as two directed edges \nor one undirected edge depending on implementation. All \nrelationship creation steps used Cypher‚Äôs MERGE so that \nrepeated loads or out-of-order imports did not create duplicate \nedges. \nThis study used a static KG snapshot for reproducibility, but \nthe system also supports incremental updates through the Neo4j \nMERGE and batch import functions. A summary of the different \ntypes of nodes and relationships is provided, together with their \ncounts, descriptions, and examples in Table III.\n \nTABLE III.  DETAILED SUMMARY OF KNOWLEDGE GRAPH COMPONENTS IN IBKH SUBSET \nComponent Type \nEntity/Relationship \nCount \nDescription \nExamples \n \nDisease \n19,236 \nMedical conditions and disorders \nDiabetes, Hypertension, Cancer \n \nDrug \n37,997 \nPharmaceutical compounds \nAspirin, Insulin, Amoxicillin \nNodes \nSymptom \n1,361 \nClinical signs and symptoms \nHeadache, Fever, Nausea \n \nSideEffect \n4,251 \nNegative outcomes of drugs \nNausea, Drowsiness \n \nPathway \n2,983 \nBiological processes and pathways \nApoptosis, Glycolysis \n \nASSOCIATED_WITH \n101,534 \nGeneral associations (disease‚Äì\npathway, drug‚Äìpathway, etc.) \nDiabetes ASSOCIATED_WITH Pathway \n \nCAUSES \n145,321 \nDrug-SideEffect relationships \nAspirin CAUSES Gastric_Bleeding \n \nHAS_SYMPTOM \n3,357 \nDisease-symptom links \nCOVID-19 HAS_SYMPTOM Fever \n \nHAS_BIOMARKER \n412 \nBiomarkers linked to disease \nPSA HAS_BIOMARKER Prostate_Cancer \nRelationships \nINHIBITS_CELL_GROWTH \n1,508 \nDrugs inhibiting cell growth \nChemo INHIBITS_CELL_GROWTH Tumor \n \nINTERACTS_WITH \n2,682,142 \nDrug-drug interactions \nAspirin INTERACTS_WITH Warfarin \n \nIS_A \n10,529 \nSubtype hierarchy \nFlu IS_A Viral_Infection \n \nPALLIATES \n388 \nDrug palliates disease \nMorphine PALLIATES Cancer \n \nPREVENTS_SUPPRESSES \n859 \nPreventive links \nVaccine PREVENTS_SUPPRESSES Measles \n \nRESEMBLES \n7,000 \nSimilarity relationships \nDrugA RESEMBLES DrugB \n \nTREATMENT_THERAPY \n44,852 \nTherapy relationships \nRadiotherapy TREATMENT_THERAPY Tumor \n \nTREATS \n5,491 \nDrug-disease links \nInsulin TREATS Diabetes \n \nALLEVIATES_REDUCES \n‚àº180,000 \nSymptom relief \nParacetamol ALLEVIATES_REDUCES Fever \nTotal \nNodes \n65,828 \nTotal biomedical entities \n‚Äî \n \nRelationships \n3,004,166 \nTotal knowledge graph links \n‚Äî \nC. Natural Language to Cypher Query Translation \nA key feature of the system is its ability to accept questions \nwritten in plain English and automatically generate the \ncorresponding Cypher queries. This is accomplished using \nMeta‚Äôs LLaMA 3 large language model, which runs entirely on \na local machine through the open-source llama.cpp framework. \nRunning the model locally ensures low-latency execution and \nkeeps sensitive queries within the user‚Äôs environment. \nTo generate a Cypher query, LLaMA 3 is prompted with \nexamples of natural language questions along with their correct \nCypher translations. The prompt also includes instructions on \nhow to navigate the structure of the graph schema. When a user \nenters a question (e.g., ‚ÄôWhat are the symptoms of Alzheimer‚Äôs \ndisease?‚Äô), the system inserts it into the prompt and asks LLaMA \n3 to produce a corresponding query. For example, the model may \nreturn: \nMATCH (d:Disease)-[:HAS_SYMPTOM]->(s:Symptom) \nWHERE toLower(d.name) CONTAINS ""alzheimer"" \nRETURN s.name'
 'D 734 \nNucleic Acids Research , 2025, Vol. 53, Database issue \nuses the whole genome / proteome background; however, it \nis recommended that users provide a custom background \nlist representing a more realistic universe of genes / proteins \ndetected by their assay ( 35 ). Alternatively, the entire sorted \ndataset can be analyzed using gene set enrichment analysis to \ndetermine whether the distribution of genes at the top, bottom \nor both ends deviates significantly from what would be ex- \npected by chance. STRING supports both of these enrichment \nmethods. The most recent update to STRING introduces vari- \nous improvements, including a new false discovery rate (FDR) \ncorrection, enhanced filtering options and a new graphical vi- \nsualization. \nFDR correction \nIn its analyses, STRING performs a test for each path- \nway (‚Äòterm‚Äô) within a given pathway collection (‚Äòcategory‚Äô). \nLike many other enrichment tools, STRING employs the \nBenjamini‚ÄìHochberg correction ( 36 ) to adjust for multiple \nhypothesis testing. This statistical correction is essential for \nmanaging the FDR effectively. However, the larger the term \ncount and the greater the diversity of the terms, the higher \nthe required correction and the lower the chance of exceed- \ning the alpha level (significance threshold) for a given term. \nOne method used to address this issue involves restricting the \nanalysis to a subset of terms, such as ‚ÄòGO slims‚Äô that focus \non broad, high-level terms with a simplified ontology struc- \nture ( 16 ). However, this approach has some disadvantages: it \nmight exclude terms particularly relevant to a tested gene set, \nit may not cover all functions by design and it is mainly ap- \nplicable to hierarchical classification systems such as the Gene \nOntology. \nSTRING utilizes several hierarchical ontologies, such as \nthe Brenda Tissue Ontology, Disease Ontology, Human Phe- \nnotype Ontology and the aforementioned Gene Ontology; \nthey typically contain many more leaf-ward nodes than nodes \ncloser to the root. The parental nodes must contain all the \ngenes included in the child terms, creating an imbalance in \nthe ontology with many more smaller terms and fewer larger \nterms. Removing smaller terms can substantially increase sta- \ntistical power . However , simply eliminating all small terms \ncould negatively impact discoverability, especially for small \nquery gene sets where the smaller terms might be the most rel- \nevant for biological interpretation. The ontologies are usually \nlarge, with > 10 000 terms (and therefore tests) in the Biolog- \nical Process branch of the Gene Ontology tree. Such strong \ncorrections have the effect that for larger query gene sets, or \nfor backgrounds that cover only part of the proteome, it might \nnot be statistically possible to detect enrichments for a subset \nof the terms; these can therefore be removed from considera- \ntion. \nIn its new version, STRING takes a more flexible \napproach‚Äîby only testing terms that have a priori any statis- \ntical potential for enrichment. This decision is based on sev- \neral parameters: the size of the term, the size of the query set, \nthe background size and the number of tests conducted. By \ncalculating the smallest and largest term sizes that could the- \noretically be enriched given the FDR correction, STRING can \ndetermine which terms are viable for inclusion in the anal- \nysis. Terms outside these bounds are excluded from testing. \nThis methodical exclusion significantly enhances the statisti- \ncal power of the enrichment analysis, without omitting any \nterms that could be enriched. For smaller query sets, this strat- \negy will maintain all terms, as even the smallest tested sets \n(term size = 2) might still be theoretically enriched. How- \never , for larger , less specific sets or for tests against a cus- \ntom smaller background, this approach markedly improves \nthe statistical power, enabling STRING to perform more fo- \ncused analyses without the drawbacks of increased false posi- \ntive rates or omitting potentially biologically interesting terms. \nAlthough this method leverages the characteristics of ontolo- \ngies, it does not require a hierarchy (parent‚Äìchild relationships \nbetween the terms) to function and can be applied to any \npathway / term collection with an imbalance of small terms, \nsuch as PMID-derived gene sets and STRING neighborhood \nclusters. \nTerm filtering \nOne key insight from our online user surveys is that the of- \nten overwhelming number of enrichment results can make it \nchallenging to discern the most relevant enriched terms. We \nhave thus enhanced the display of enrichment results, allow- \ning users to filter and sort them based on (i) FDR, (ii) strength, \n(iii) signal, (iv) term size and (v) term similarity. \nThe newest addition to our filtering options are the ‚Äòterm \nsimilarity‚Äô and ‚Äòsignal‚Äô filters. The ‚Äòterm similarity‚Äô filter uses \nthe Jaccard index to measure the similarity between the gene \nsets of terms within the same category. The method proceeds \nby sorting the terms according to their P -values, prioritizing \nthose with the strongest statistical significance. The algorithm \nthen iterates through the list of terms; any term that exhibits \na similarity exceeding a predetermined, user-defined threshold \nrelative to terms already included in the results is subsequently \nexcluded. This approach requires no additional information \nbeyond what is already available in the dataset and can be \nuniformly applied across all categories of terms. The ‚Äòsignal‚Äô \nfilter is defined as a weighted harmonic mean between the ra- \ntio of observed to expected gene occurrences in an enriched \nterm and its ‚àílog(FDR), respectively. This filter balances the \nimpact of both metrics and provides a more intuitive ordering \nof enriched terms and has been implemented as the default \nsorting criterion. \nThese filters are designed to exclude potentially less relevant \nresults, such as terms that are marginally significant, small or \nshow significant overlap in terms of gene content. This en- \nhancement streamlines the search for relevant observations, \nhelping users focus on the most meaningful results. \nGraphical visualization \nIn response to user feedback requesting more intuitive, graph- \nical visualizations of the detected enrichments, STRING now \nhas adopted an interactive dot plot as a primary display \nmethod for enrichment analysis outcomes. We chose the dot \nplot for its clarity in representing enriched functions across \nthree dimensions: (i) enrichment signal along the X -axis, (ii) \nthe FDR indicated through the color coding of the dots and \n(iii) the term‚Äôs protein count in the network represented by the \nsize of each dot. The terms (listed on the Y -axis) are ranked \nby their enrichment signal or, if grouped by similarity, by the \nenrichment signal of their group first (Figure 2 ). This visual- \nization reflects all filter settings chosen by the user to manage \nthe volume of results displayed. These plots can be addition- \nally customized in the interface and the resulting graphs can be \ndownloaded as PNG for immediate use or as SVG for further \nmodifications. \nDownloaded from https://academic.oup.com/nar/article/53/D1/D730/7903368 by guest on 11 January 2026'
 'Omar & Mohammed / Journal of Applied Science and Technology Trends Vol. 06, No. 02, pp. 342 ‚Äì357 (2025) \n \n353 \nDifficulty Level \nTotal Questions \nCorrect Queries \nCypher Exact Match (EM) (%) \nSimple \n25 \n24 \n96% \nMedium \n20 \n19 \n95% \nComplex \n15 \n13 \n86.7% \navg \n \n \n92.6% \n \nTo better understand the quality of the responses returned by \nthe system, this work examined real examples. One of the test \nquestions was ""What drugs are used to treat breast cancer?"" As \nshown in Fig. 4, the system was able to understand the question, \ngenerate the correct Cypher query using the TREATS \nrelationship, and retrieve the right information from the \nbiomedical knowledge graph. It returned a list of five drugs, \nincluding Leuprolide, Goserelin, and Trastuzumab, which are \nknown treatments for breast cancer. This result shows that the \nsystem is able to connect the question to the right part of the graph \nand provide accurate and helpful answers, even for medical \nquestions that involve specific treatments. \n1) \nExact Match of Query Generation \nAs shown in Table VI, how often the system generated the \ncorrect query for each difficulty level. Here, accuracy is defined \nas the percentage of questions for which the system‚Äôs generated \nCypher query matched the expected query. It is calculated using \nthe Eq. (1)     \n \nExact Match (%) = Correct Queries\nTotal Questions √ó 100 \n(1) \n \n \nThese findings highlight the purpose of this experiment to test \nwhether the framework can reliably map natural language to \nCypher across varying levels of complexity. The graceful drop \nfrom 96% on simple to 86.7% on complex queries indicates that \nthe system is robust for straightforward questions but still \nchallenged by multi-hop reasoning. This points to clear \nopportunities for improvement, such as synonym expansion, \nconstrained decoding, or enhanced error handling for multi-step \nqueries. \n2) \nLatency \nTable VII reports the average latency per difficulty level and \ndecomposes it into query generation and Neo4j execution. The \ntotal response time is computed as in Eq. (2). Execution time is \neffectively constant across all levels ( ‚âà0.04‚Äì0.05  s), so \nvariation in total latency is dominated by query generation. As \ndifficulty increases, the mean total time rises from 5.12  s \n(simple) to 5.75 s (medium) and 7.35 s (complex). Dispersion \n(Std. Dev.) grows with task complexity 0.72 s (simple), 0.32 s \n(medium), 2.09 s (complex) reflecting more variable planning \nand reasoning needed to assemble correct Cypher for harder \nquestions. Pooled over all questions, the overall mean is 6.07 s \nwith an overall SD of 1.38 s, keeping latencies within single-digit \nseconds and practical for interactive, real-world use. \n \nùëátotal = ùëágen + ùëáexec   \n(2)     \nStandard Deviation (SD). Unless otherwise stated, SD is the \nunbiased sample standard deviation computed over per-question \ntotal times within each difficulty group ùê∫ with ùëõùê∫ questions: \n \nùëá‚Äæùê∫= 1\nùëõùê∫\n‚àëùëátotal\n(ùëñ)\nùëõùê∫\nùëñ=1\n,\nùë†ùê∫= ‚àö\n1\n\u2009ùëõùê∫‚àí1\u2009 ‚àë\nùëõùê∫\nùëñ=1\n(ùëátotal\n(ùëñ) ‚àíùëá‚Äæùê∫)\n2\u2009.\n \n \n(3)   \n   \n    (4)    \n \nOverall values are computed by pooling all questions across \nlevels, with ùëÅ= ‚àëùëõùê∫\nùê∫\n: \n \nùëá‚Äæoverall = 1\nùëÅ‚àëùëátotal\n(ùëñ)\nùëÅ\nùëñ=1\n,\nùë†overall = ‚àö\n1\n\u2009ùëÅ‚àí1\u2009 ‚àë\nùëÅ\nùëñ=1\n(ùëátotal\n(ùëñ) ‚àíùëá‚Äæoverall)\n2\u2009.\n \n \n(5)   \n   \n       \n(6) \n \n \nTABLE VII.  AVERAGE LATENCY (S) BY DIFFICULTY LEVEL (SD = STANDARD \nDEVIATION) \nDifficulty \nLevel \nQuery \nGeneration (s) \nQuery \nExecution (s) \nTotal \nTime (s) \nStd. \nDev. (s) \nSimple \n5.09 \n0.03 \n5.12 \n0.72 \nMedium \n5.69 \n0.01 \n5.75 \n0.32 \nComplex \n6.94 \n0.4 \n7.35 \n2.09 \nOverall \n5.9 \n0.15 \n6.07 \n1.38 \n \n \n \n \n \n3) \nAnswer-Level Evaluation Metrics \nAs shown in Table VIII, the quality of the returned items is \nevaluated using five standardized metrics: Precision@k, \nRecall@k, F1@k, Hits@k, and MRR. Precision@k measures the \nproportion of correct answers among the top-ùëò retrieved items \nEq. (7), while Recall@k quantifies the fraction of gold-standard \nanswers covered within the top-ùëò results Eq. (8). F1@k combines \nboth aspects through the harmonic mean Eq. (9). Hits@k reports \nwhether at least one correct answer appears in the top-ùëò Eq. \n(10, 11), and MRR captures how early the first correct answer is \nretrieved in the ranking Eq. (12). Together, these metrics provide \na comprehensive view of both the correctness and completeness \nof retrieval, as well as the ranking quality across simple, medium \nand complex queries. \n \nPrecision@ùëò= |{\u2009relevant ‚à©retrieved@ùëò\u2009}|\nùëò\n \n(7)   \n   \nPrecision@k measures the fraction of the top-ùëò retrieved \nitems that are correct (i.e., appear in the gold set). For example,'
 'Omar & Mohammed / Journal of Applied Science and Technology Trends Vol. 06, No. 02, pp. 342 ‚Äì357 (2025) \n \n346 \nscientific literature, clinical records, genomic databases, and \nexperimental findings [19, 31]. \nSuch integration creates a comprehensive biomedical \nknowledge base that supports advanced analytics and discovery. \nFor example, biomedical knowledge graphs can reveal previously \nunknown relationships (e.g., between drugs and diseases) and \nhelp prioritize potential biomarkers for complex conditions. The \nIntegrative Biomedical Knowledge Hub (iBKH) is one such \nlarge-scale graph that consolidates diverse biomedical resources \ninto a unified hub, enabling discovery at scale [11]. Beyond \niBKH, large biomedical knowledge graphs such as SPOKE \nfurther illustrate how graph integration accelerates research and \nsupports precision-medicine use cases [12]. Overall, these graphs \nserve as foundational resources for data-driven and personalized \nmedicine. These knowledge graphs serve as foundational \nresources for precision medicine, where treatment can be tailored \nto the individual‚Äôs biological profile, improving outcomes and \nminimizing side effects [19, 31]. \n \nTABLE I.  SUMMARY OF RELATED RESEARCH ON BIOMEDICAL KGS AND QUESTION ANSWERING \nRef. \nYear \nData/Graph \nMethod \nBaselines \nKey Metric \nLimitation \n[11] \n2023 \niBKH (18 biomedical \nsources, Neo4j) \nIntegrative KG + Graph \nlearning; drug repurposing \ncase \nKnown drugs, \nAlzheimer‚Äôs \nstudy \nPredictive performance \n(drug repurposing) \nLimited to Alzheimer‚Äôs case \nstudy; scalability and updates not \ndetailed \n[19] \n2023 \nDisease KG (9,400 \nnodes, 45,000 relations \nin Neo4j) \nCypher queries for disease‚Äì\ndrug‚Äìsymptom reasoning \nCross-referential \ndisease DB \nNew relation inference; \ncomplex query \nanswering \nLimited to single domain; lacks \nlarge-scale evaluation \n[3] \n2023 \nBKGs vs. GPT-3.5/4 \nComparative QA study: \nLLMs vs. KGs \nGPT-3.5, GPT-4, \nKG reasoning \nAccuracy, Novelty, \nReasoning \nGPT-4 inconsistent in \nreasoning/citations; KG less fluent \nbut more reliable \n[13] \n2024 \nSPOKE KG + \nLLaMA2, GPT-3.5, \nGPT-4 \nKG-optimized retrieval for \nLLMs (RAG) \nOther retrieval \nmethods \nAccuracy, token \nreduction >50% \nFocus on retrieval optimization, \nnot KG construction \n[23] \n2024 \nBenchmark KB datasets \nGenerate-then-retrieve \n(LLM ‚Üí logical form ‚Üí \nKB retrieval) \nPrior KBQA \nmethods \nAccuracy (highest to \ndate) \nRisk of error in logical form \ngeneration \n[6] \n2024 \nPrimeKG \nLLM + KG hybrid, Cypher \nquery validation, RAG \nGPT-4 Turbo, \nLLaMA 3 \nAccuracy, \nExplainability \nDependent on KG coverage; \ncomputationally intensive \n[22] \n2025 \nPan-cancer KG (PcQA \ndataset) \nKG-enhanced reasoning \n(subgraph selection) \nSOTA KGQA \nmethods \nOutperformed SOTA on \nPcQA \nLimited to pan-cancer focus; no \nfine-tuning explored \n[26] \n2025 \nDynamic Medical KG + \nNeo4j \nLLM agents + multi-hop \nreasoning \nMEDQA, \nMedMCQA \nbaselines \nAccuracy, \nInterpretability \nHigh system complexity; requires \ncontinuous updating \n[24] \n2025 \nSynthCypher dataset \n(Neo4j + synthetic \nqueries) \nLLM-supervised Cypher \ngeneration and verification \nSPIDER \nbenchmark \nCypher accuracy 40% \nSynthetic dataset may not capture \nall real-world cases \n[29] \n2025 \nUMLS KG + Neo4j \nHybrid GraphRAG \nLLM-only QA \nAccuracy, Citation \nfidelity \nMore complex pipeline; relies on \nexternal vector store \n[30] \n2025 \nClinical data (10k \npatients, 1,894 nodes, \nNeo4j) \nKG construction using \nGPT-4 for entity/relation \nextraction \nOther KG \nconstruction \nmethods \nHigh F1-scores \nFocus only on sepsis; limited \ngeneralization \n[20] \n2025 \nLocal KG from \nbiomedical documents \nMulti-level summarization \nover KG for QA \nRAG baselines \nQA accuracy, reasoning \nTested mainly on document QA; \nscalability not proven \n \nB. Neo4j Graph Database \nTo manage the complexity and large size of biomedical \nknowledge graphs, specialized graph databases are needed. \nNeo4j is one of the most popular graph databases designed to \nstore and query data structured as nodes (entities) and \nrelationships (edges), both of which can have descriptive \nproperties [32, 33]. It uses the property graph model, which \nmakes it easy to represent complex, connected biomedical data \nsuch as drug-gene interactions or disease pathways. Neo4j‚Äôs \nCypher query language is especially advantageous because it \nallows users to write expressive and efficient queries to explore \nmulti-step connections in the data [34]. \nNeo4j works well for biomedical data because it can quickly \nrun complicated queries over highly interconnected datasets. This \nis important in biology and medicine, where relationships \nbetween entities like proteins, diseases, and drugs are often \ncomplex and layered. Studies have shown that Neo4j handles \nlarge biomedical graphs efficiently, making it a favorite among \nresearchers and industry users alike [33, 35, 36]. Its indexing and \ncaching mechanisms also help speed up query processing and \ndata retrieval [37]. \nMoreover, \nNeo4j \nintegrates \nsmoothly \nwith \nmany \nprogramming languages and analytics tools, which makes it \neasier to build interactive biomedical applications and clinical \ndecision support systems that can turn complex graph data into \nuseful insights [38, 39]. \nC. Large Language Models (LLMs) in Biomedical Question \nAnswering \nLarge Language Models (LLMs) are powerful AI systems \ntrained on vast amounts of text data. They learn the structure and \npatterns of language, enabling them to understand questions, \ngenerate responses, summarize information, and perform other \ncomplex language tasks. Well-known models such as LLaMA \nand GPT-3 have greatly advanced the field of natural language \nprocessing by showing strong performance across many tasks \n[40, 41].'
 'Omar & Mohammed / Journal of Applied Science and Technology Trends Vol. 06, No. 02, pp. 342 ‚Äì357 (2025) \n \n343 \nfor visualizing and validating LLM outputs [16], and MedKA for \nKG-enhanced question answering [17]. \nTo address these challenges, several recent works have \nexplored the integration of large language models with \nbiomedical knowledge graphs (KGs). A biomedical KG is a \nstructured network that connects entities such as diseases, drugs, \nand symptoms using defined relationships [18, 19]. These graphs \nstore verified medical knowledge from trusted databases, \nallowing for more accurate and explainable responses [12]. KGs \nare especially useful in multi-step reasoning tasks, where finding \nan answer requires connecting different pieces of information \n[20]. These entities and relationships can be visually represented \nin a biomedical knowledge graph, as shown in Fig. 1, where \nnodes represent medical concepts such as drugs, diseases, \nsymptoms, and pathways, and edges denote their semantic \nrelationships. \n \nFig. 1. Overview of Biomedical Knowledge Graph Entities and Relationships \n \nOne example of a widely used biomedical KG is SPOKE, \nwhich includes millions of nodes and relationships from over 40 \nbiomedical databases [12]. Integrating an LLM with a KG allows \nthe strengths of both technologies to work together: the LLM \nprovides language understanding, and the KG provides \nstructured, factual knowledge [21, 22]. A common method is \nretrieval-augmented generation (RAG), where the LLM retrieves \ninformation from the KG and uses it to generate more accurate \nresponses [13, 23]. In more advanced setups, the LLM can even \ngenerate queries like Cypher to fetch specific data from the graph \n[24, 25]. Neo4j is a popular graph database that supports fast and \nflexible storage and querying of knowledge graphs using Cypher \n[21]. It is well-suited for biomedical applications because it \nallows easy exploration of complex medical relationships. Recent \nwork has shown that combining Neo4j with LLMs can lead to \nbetter accuracy, fewer hallucinations, and more explainable \nresults [24, 26]. \nDespite improvements, building a reliable hybrid system that \ncombines an LLM with a biomedical KG remains a technical \nchallenge. Some approaches require complex pipelines or large \ntraining datasets, while others rely on fine-tuning specific to a \nnarrow set of questions [27, 28]. There is still a need for systems \nthat are both accurate and easy to scale, especially in domains like \nmedicine where the cost of errors is high [22]. \nRecent advances in KG-augmented LLMs have improved \nperformance, yet biomedical QA continues to face three practical \ngaps: \n1. Traceability: LLM-only or text-retrieval-only pipelines \nrarely provide graph-grounded justifications; users lack the \nability to inspect the exact nodes and edges that support an \nanswer. \n2. Evaluation: Prior work often judges quality via surface-\nform checks (e.g., matching a Cypher template), which fails to \ncapture end-to-end answer correctness or ranking quality \nacross different difficulty levels. \n3. Deployment: Many solutions assume cloud resources or \ndomain-specific fine-tuning, yet biomedical contexts typically \ndemand a local, privacy-preserving system with low latency \nand reproducible behavior. \nTimestamp-aware execution and periodic KG refresh help \navoid deprecated or contraindicated links, making the tool better \nsuited for safety-critical clinical contexts (e.g., drug‚Äìdrug \ninteractions). \nTo address these limitations, Our work introduces a locally \ndeployable pipeline that translates biomedical questions into \nexecutable Cypher queries over a Neo4j knowledge graph. The \nsystem returns answers with supporting nodes and edges, and is']"
Wht is the role of the Journal of Applied Science and Technology Trends in biomedical data analysis?,"['Omar & Mohammed / Journal of Applied Science and Technology Trends Vol. 06, No. 02, pp. 342 ‚Äì357 (2025) 348 drugs, diseases, symptoms) and execution returns structured data (tuples) relevant to the question. Step 5. Answer Synthesis: The structured tuples flow to Answer Synthesis, which aggregates and formats them into a concise raw answer. This raw answer is sent back to LLaMA 3 to optionally refine the phrasing while preserving the retrieved facts. Step 6. Result Presentation: LLaMA 3 produces the final answer, which the interface displays together with the executed Cypher query and an optional preview of the returned rows, improving transparency and trust. The pipeline couples LLM-based language understanding (LLaMA 3) with a schema-grounded Neo4j knowledge graph. The Cypher Query Gen refines the query formulation, Query Execution retrieves evidence and Answer Synthesis converts structured results into readable outputs that produce answers that are accurate, interpretable, and easy to audit directly from the displayed query and evidence.']","The Journal of Applied Science and Technology Trends publishes research such as the work by Omar & Mohammed, which details a pipeline that combines LLM-based language understanding (LLaMA 3) with a schema-grounded Neo4j knowledge graph to produce accurate, interpretable, and auditable answers in biomedical data analysis.",single_hop_specifc_query_synthesizer,"The **Journal of Applied Science and Technology Trends** plays a significant role in advancing biomedical data analysis by publishing research that focuses on the integration and application of large language models (LLMs), biomedical knowledge graphs (KGs), and retrieval-augmented generation (RAG) techniques to improve biomedical question answering and data retrieval systems. Specifically, the journal features studies that:

1. **Develop Hybrid Systems Combining LLMs and Biomedical Knowledge Graphs:**  
   The journal presents work on systems that translate natural language biomedical questions into structured graph queries (e.g., Cypher queries) executed on biomedical knowledge graphs stored in graph databases like Neo4j. This integration enhances the accuracy, explainability, and traceability of biomedical data retrieval.

2. **Address Challenges in Biomedical QA and Data Integration:**  
   Articles discuss overcoming issues such as hallucinations in LLM-generated responses by grounding answers in verified biomedical knowledge graphs, improving semantic interoperability among diverse biomedical APIs, and enabling multi-step reasoning across complex biomedical concepts.

3. **Introduce Federated Knowledge Retrieval Approaches:**  
   The journal highlights frameworks like BioThings Explorer and BTE-RAG that federate multiple biomedical APIs and knowledge sources to create virtual, federated knowledge graphs. These approaches allow dynamic, up-to-date data retrieval without maintaining a large centralized graph, improving scalability and data freshness.

4. **Evaluate and Benchmark Biomedical QA Systems:**  
   Research published includes quantitative evaluations of biomedical question-answering systems across varying difficulty levels, demonstrating improvements in accuracy, precision, and response quality when combining LLMs with knowledge graphs.

5. **Promote Locally Deployable, Privacy-Preserving Systems:**  
   The journal emphasizes the development of systems that can be run locally (e.g., using LLaMA 3 models) to ensure data privacy, reproducibility, and low latency, which are critical in sensitive biomedical and clinical contexts.

6. **Support Clinical and Translational Applications:**  
   The journal covers applications of these technologies in clinical decision-making, personalized treatment, drug repurposing, and medical question answering, thereby bridging the gap between computational methods and real-world biomedical practice.

In summary, the **Journal of Applied Science and Technology Trends** contributes to biomedical data analysis by disseminating innovative methodologies and systems that integrate advanced computational models with structured biomedical knowledge, enhancing the accuracy, interpretability, and practical utility of biomedical data retrieval and analysis tools.","['Databases and ontologies\nBioThings Explorer: a query engine for a federated\nknowledge graph of biomedical APIs\nJackson Callaghan\n1,‚Ä†, Colleen H. Xu\n1,‚Ä†, Jiwen Xin1,‚Ä†, Marco Alvarado Cano1,\nAnders Riutta\n2, Eric Zhou1, Rohan Juneja1, Yao Yao1, Madhumita Narayan1,\nKristina Hanspers2, Ayushi Agrawal\n2, Alexander R. Pico2, Chunlei Wu\n1,*, Andrew I. Su\n1,*\n1Department of Integrative Structural and Computational Biology, The Scripps Research Institute, La Jolla, CA 92037, United States\n2Data Science and Biotechnology, Gladstone Institutes, University of California, San Francisco, CA 94158, United States\n*Corresponding authors. Department of Integrative Structural and Computational Biology, The Scripps Research Institute, La Jolla, CA 92037, United States.\nE-mails: cwu@scripps.edu (C.W.) and asu@scripps.edu (A.I.S.)\n‚Ä†These authors contributed equally to this work.\nAssociate Editor: Peter N. Robinson\nAbstract\nSummary: Knowledge graphs are an increasingly common data structure for representing biomedical information. These knowledge graphs can\neasily represent heterogeneous types of information, and many algorithms and tools exist for querying and analyzing graphs. Biomedical knowl-\nedge graphs have been used in a variety of applications, including drug repurposing, identiÔ¨Åcation of drug targets, prediction of drug side effects,\nand clinical decision support. Typically, knowledge graphs are constructed by centralization and integration of data from multiple disparate sour-\nces. Here, we describe BioThings Explorer, an application that can query a virtual, federated knowledge graph derived from the aggregated infor-\nmation in a network of biomedical web services. BioThings Explorer leverages semantically precise annotations of the inputs and outputs for\neach resource, and automates the chaining of web service calls to execute multi-step graph queries. Because there is no large, centralized\nknowledge graph to maintain, BioThings Explorer is distributed as a lightweight application that dynamically retrieves information at query time.\nAvailability and implementation: More information can be found at https://explorer.biothings.io and code is available at https://github.com/bio\nthings/biothings_explorer.\n1 Introduction\nWhile downloadable files are the most common way to share\nbiomedical data, application programming interfaces (APIs)\nare another popular and powerful mechanism for data dis-\nsemination. Accessing data through APIs has many comple-\nmentary advantages relative to downloading local copies of\ndata. APIs typically allow users to query for specific subsets\nof the data that are of interest. API queries are often highly\nindexed, leading to efficient data retrieval. Finally, API access\nallows for easy incorporation of the most up-to-date data into\nother computational applications or workflows.\nWhile APIs offer many advantages in terms of data acces-\nsibility, these advantages do not immediately translate into\nefficient data integration. APIs generally follow some com-\nmon architectures and protocols [e.g. representational state\ntransfer (REST), output in JavaScript Object Notation\n(JSON)], but alignment at this technical level does not guar-\nantee either syntactic or semantic interoperability. For ex-\nample, APIs can use different identifiers for the same gene,\ndifferent data structures to represent gene attributes, and\ndifferent terms to describe the relationships between bio-\nmedical entities.\nThere have been some efforts to define and enforce seman-\ntic and syntactic standards to achieve data interoperability.\nExamples of this approach include the Beacon API from the\nGA4GH consortium (Rambla et al. 2022) and the DAS speci-\nfication for sharing annotations of genomic features (Dowell\net al. 2001). These efforts rely on the active participation of\nAPI developers in adopting a community standard for their\nAPI endpoints.\nHere, we explore the use of semantically precise API anno-\ntations as a complementary approach to achieving API inter-\noperability. We divided this work into two components. First,\nwe created an extension of the widely used OpenAPI annota-\ntion standard (http://spec.openapis.org/oas/v3.1.0) to seman-\ntically describe the APIs‚Äô inputs and outputs, and a registry to\norganize these API annotations. Second, we created an appli-\ncation called BioThings Explorer to consume the API meta-\ndata and to execute multi-hop graph queries that span\nmultiple APIs. Together, this architecture allows users to\nquery a large, federated knowledge graph based on an inter-\nconnected network of biomedical APIs. This federated design\noffers a unique approach for creating knowledge graphs that\nis complementary to the more common strategy of centraliza-\ntion and local data integration.\nReceived: 18 April 2023; Revised: 18 August 2023; Editorial Decision: 11 September 2023; Accepted: 12 September 2023\nV\nC The Author(s) 2023. Published by Oxford University Press.\nThis is an Open Access article distributed under the terms of the Creative Commons Attribution License (https://creativecommons.org/licenses/by/4.0/), which\npermits unrestricted reuse, distribution, and reproduction in any medium, provided the original work is properly cited.\nBioinformatics, 2023, 39(9), btad570\nhttps://doi.org/10.1093/bioinformatics/btad570\nAdvance Access Publication Date: 14 September 2023\nApplications Note\nDownloaded from https://academic.oup.com/bioinformatics/article/39/9/btad570/7273783 by guest on 11 January 2026'
 'Omar & Mohammed / Journal of Applied Science and Technology Trends Vol. 06, No. 02, pp. 342 ‚Äì357 (2025) \n \n355 \nFor the question ‚ÄôWhat are the symptoms of brain cancer?‚Äô \nThe system generated a Cypher query that correctly followed the \nHAS_SYMPTOM relationship from disease nodes to symptoms \nnodes, filtering by the specified disease name. The results \nretrieved included terms such as aphasia (Broca, Wernicke), \nanomia, agraphia, agnosia, amnesia (including retrograde \namnesia), olfaction disorders, and apnea symptoms consistent \nwith established neurological manifestations of brain tumors. In \nNeo4j, the data formed a clear center-and-spoke pattern, with \nbrain cancer at the center and its associated nodes of symptoms \nradiating outward, as shown in Fig. 5. \nCypher: \nWITH toLower(""brain cancer"") AS disease_name \nMATCH (d:Disease)-[:HAS_SYMPTOM]->(s:Symptom) \nWHERE toLower(d.name) CONTAINS disease_name \nRETURN DISTINCT s,d \nLIMIT 10; \n \n \nFig. 5. Graphical user interface displaying Cypher query and results for breast \ncancer treatment \n \n \nA second query, What are the side effects of insulin?, \nproduced a Cypher statement starting from the drug node for \ninsulin and traversing the CAUSES relationship to the connected \nSideEffect nodes. The resulting list included dizziness, diarrhea, \ncough, back pain, weakness, rash/dermatitis, and hypersensitivity \nside effects well documented in insulin safety profiles. In the \nvisual representation in Fig. 6, insulin appeared centrally \nconnected to these side effect nodes, further strengthening the \ncorrectness of the relationship mapping of the system. \n \nCypher: \nWITH toLower(""insulin"") AS drug_name  \nMATCH (d:Drug)-[:CAUSES]->(se:SideEffect)  \nWHERE toLower(d.name) CONTAINS drug_name  \nRETURN se,d \nLIMIT 20; \n \nFig. 6. Knowledge Graph Visualization of Insulin and Its Related Side Effects \n \n \nThese examples demonstrate the system‚Äôs ability to interpret \nplain English biomedical questions, generate correct Cypher \nqueries, and return results that are clinically plausible, easy to \ninterpret, and directly traceable to specific graph relationships. \nThis supports both the accuracy and the transparency of the \nproposed approach. \nTo illustrate how biomedical questions can be translated into \ngraph queries, consider the natural language question: ‚ÄôWhat are \nthe side effects of drugs that treat epilepsy?‚Äô. The following \nCypher query demonstrates how this question is mapped in the \nknowledge graph, where diseases are connected to drugs through \nthe TREATS relation and drugs are connected to side effects \nthrough the CAUSES relation. \n \nCypher: \nMATCH (d:Disease)<-[:TREATS]-(dr:Drug)  \nWHERE toLower(d.name) CONTAINS ""epilepsy"" \nMATCH (dr:Drug)-[:CAUSES]->(se:SideEffect) \nRETURN DISTINCT se,d,dr \nlimit 10'
 ""integrates over 40 publicly available biomedical knowledge \nsources across separate domains, such as genes, proteins, \ndrugs, compounds, and diseases, along with their known \nrelationships.32 Two studies used textbooks, such as Harri\xad\nson's Principles of Internal Medicine, while 3 others utilized \nelectronic health record (EHR) data. Additionally, Zakka \net al. added over 500 markdown files from MDCalc to \nimprove clinical calculation capabilities in LLM.28 Two stud\xad\nies employed real-time online browsing to search academic \nsites, such as PubMed and UpToDate. The amount of \nretrieval resources varied across studies, ranging from a small \ndataset specific to 6 osteoarthritis guidelines to a large dataset \nof EHR data from 7 hospitals.\nRetrieval strategies\nIdentified retrieval strategies were grouped based on the RAG \nstages: pre-retrieval, retrieval, and post-retrieval. Figure 3 \npresents an example of how RAG is applied and lists identi\xad\nfied strategies within each stage.\nIn the pre-retrieval stage, 50% of studies (n ¬º 10) reported \nstrategies, such as query rewriting, document preprocessing, \nand assessing the necessity of retrieval. Zakka et al. simplified \nqueries by rephrasing text into search terms that are better \nsuited for website browsing,28 while Wang et al. focused on \ntechniques such as correcting errors, expanding abbrevia\xad\ntions, and matching synonyms in user queries.30 Soman et al. \nextracted disease entities in queries and retrieved correspond\xad\ning nodes from a knowledge graph.33 Document preprocess\xad\ning involved removing non-textual elements from PMC \npapers (eg, figures, references, and author disclosures),30\nextracted tables from PDFs using pdfplumber, structured the \ncontent with pydantic for seamless integration.25 In addition \nto query modification and document preprocessing, Jeong \net al. fine-tuned a model to determine whether retrieval was \nnecessary for a given query.34\nDuring the data retrieval stage, 85% of studies (n ¬º 17) \nreported strategies regarding indexing, aligning queries with \ndocuments, and ranking retrieval chunks. Chunking methods \nranged from fixed-size chunks35 to recursive splits.36 Embed\xad\nding models such as Text-embedding-ada-002,24,28‚Äì30,36,37\nMiniLM, and PubMedBERT33 were commonly used to con\xad\nvert sentences into vectors. Cosine similarity was the primary \nmetric for measuring query-document alignment. Two stud\xad\nies adopted Maximal Marginal Relevance for search and \nhighlighted its improved performance over similarity-based \nmethods.24,35 A domain-specific retriever, MedCPT, was \nused in one study.34 Another study used the multi-vector \nretriever that leveraged summarized document sections to \nidentify the original content for final answer generation.25\nThe retrieval cutoff parameters varied widely, with probabil\xad\nity thresholds up to 0.83 and the number of retrieved chunks \nranging from 3 to 90.28,36,38 Vector databases like FAISS and \nChroma were frequently reported, and LangChain was \nwidely used for document processing and retrieval.23,25,35,38\nIn the subgroup analysis, 12 studies used simple data retrieval \nstrategies (OR 1.30, 95% CI [1.16, 1.45]), while 5 studies \nused complex data retrieval strategies (OR 1.30, 95% CI \n[1.07, 1.24]), with no statistically significant difference \nobserved between the 2 approaches.\nIn the post-retrieval stage, 65% of studies (n ¬º 13) imple\xad\nmented specific strategies to refine outputs. Murugan et al. \ntailored prompts by providing clear context, defining roles \n(eg, distinguishing between healthcare providers and patients \nto deliver appropriately detailed information), and incorpo\xad\nrating relevant citations from retrieval sources such as the \nClinical \nPharmacogenetics \nImplementation \nConsortium \nguidelines and Food and Drug Administration (FDA) label\xad\ning.24 Soman et al. utilized prompt engineering to integrate \naccurate knowledge sources and statistical evidence, such as \nP-values and z-scores, from the SPOKE knowledge graph \ninto their outputs.33 Wang et al. outlined a detailed process \nin the post-retrieval stage using prompt engineering, which \ninvolved decomposing retrieved text into individual claims, \nverifying each claim with external knowledge sources, \nFigure 3. An example of using RAG in clinical applications, with identified strategies in 3 stages: pre-retrieval, retrieval, and post-retrieval. The user query \nseeks recommended medications for symptomatic peripheral artery disease without increased bleeding risk. The system retrieves evidence from clinical \nguidelines, processes it through embeddings and a vector database, and outputs a response, including rivaroxaban with low-dose aspirin, as recommended \nby the retrieved guideline information. In this example, GPT-4 suggested a dose of 75 mg Aspirin, but the common low-dose Aspirin is 81 mg.\nJournal of the American Medical Informatics Association, 2025, Vol. 32, No. 4                                                                                                      \n609 \nDownloaded from https://academic.oup.com/jamia/article/32/4/605/7954485 by guest on 11 January 2026""
 'Omar & Mohammed / Journal of Applied Science and Technology Trends Vol. 06, No. 02, pp. 342 ‚Äì357 (2025) \n \n347 \nIn biomedical research and clinical settings, LLMs help \ntranslate natural language questions from doctors, researchers, or \npatients into precise, structured queries that can be executed on \nbiomedical knowledge graphs and databases. This makes it easier \nto retrieve detailed biomedical information like drug interactions, \ngene-disease associations, and symptom descriptions [42, 43]. \nDespite their power, LLMs can sometimes generate incorrect \nor fabricated responses, a phenomenon known as hallucination, \nwhich poses risks in sensitive biomedical contexts. These \nhallucinations occur because the models generate plausible \nanswers based on patterns learned from data rather than verified \nknowledge. To mitigate this, researchers integrate LLMs with \nbiomedical knowledge graphs to ground answers in factual data, \nsignificantly improving accuracy and reducing misinformation \n[4]. Further improvements come from fine-tuning LLMs on \nbiomedical corpora and carefully engineering prompts, which \nenhance their reliability and relevance in medical question \nanswering. \nAdditionally, combining LLMs with knowledge graphs and \nreasoning techniques is an active area of research that promises \nto increase the interpretability and trustworthiness of AI systems \nin biomedicine. These advances are critical for developing tools \nthat assist clinical decision-making and accelerate biomedical \ndiscoveries [43, 44]. \nIV. METHODS AND MATERIALS \nThis section describes the methodology used to build a \nbiomedical question-answer system. The proposed method \nconsists of three main stages; First, a biomedical knowledge \ngraph is constructed in the data ingestion phase, using structured \ndata sources (e.g., diseases, drugs, symptoms). Second, a \nlanguage model (LLaMA 3) interprets the user‚Äôs question \nwritten in English in the user interaction phase and converts it \ninto a Cypher query. Third, a graphical user interface allows \nusers to type questions and view the results interactively. \nA. System Architecture \nThe proposed framework is organized as a step-by-step \npipeline that integrates a local large language model (LLM) with \na biomedical knowledge graph stored in Neo4j. The overall \nworkflow is illustrated in Fig. 2. Each module performs a \nspecific function, and together they ensure that the system \ndelivers accurate, reliable, and explainable answers. \n \n \nFig. 2. Workflow of the LLaMA 3 and Neo4j-Based QA System \n \nStep 1. User Input (Flask Web Interface): A user submits \na biomedical question in natural language through a Flask-based \nweb interface. The same interface will later display the answer, \nthe executed query, and a compact preview of the retrieved rows. \nStep 2. LLM Processing and Initial Cypher: The text query \nis forwarded to a local LLaMA 3 module, which interprets the \nintent and drafts an initial Cypher pattern suitable for querying \nthe graph. \nStep 3. Cypher Query Generation: The Cypher Query Gen \nblock receives the initial pattern from LLaMA 3, canonicalizes \nand completes it (projection, DISTINCT, filters), and returns a \nfinalized Cypher query to the model. \nStep 4. Query execution on Neo4j: LLaMA 3 passes the \nfinalized query to the Query execution component (inside the \n‚ÄôAnswer modules‚Äô box), which runs it against the Neo4j \nKnowledge Graph. KG supplies knowledge graph data (e.g.,'
 'Joy et. al \n1 \n \nFederated Knowledge Retrieval Elevates Large Language Model Performance on \nBiomedical Benchmarks \nJanet Joy1,2, Andrew I. Su1,2 \n1Department of Integrative Structural and Computational Biology, Scripps Research, La \nJolla, CA, USA. \n2Correspondence: Janet Joy (jjoy@scripps.edu) and Andrew I. Su (asu@scripps.edu) \nMain Figures: 4 \nSupplemental Figures: 11 \nSupplemental Table: 1 \nAbstract \n \nBackground \nLarge language models (LLMs) have significantly advanced natural language \nprocessing in biomedical research, however, their reliance on implicit, statistical \nrepresentations often results in factual inaccuracies or hallucinations, posing significant \nconcerns in high-stakes biomedical contexts. \nResults \nTo overcome these limitations, we developed BTE-RAG, a retrieval-augmented \ngeneration framework that integrates the reasoning capabilities of advanced language \nmodels with explicit mechanistic evidence sourced from BioThings Explorer, an API \nfederation of more than sixty authoritative biomedical knowledge sources. We \nsystematically evaluated BTE-RAG in comparison to traditional LLM-only methods \nacross three benchmark datasets that we created from DrugMechDB. These datasets \nspecifically targeted gene-centric mechanisms (798 questions), metabolite effects (201 \nquestions), and drug‚Äìbiological process relationships (842 questions). On the gene-\ncentric task, BTE-RAG increased accuracy from 51% to 75.8% for GPT-4o mini and \nfrom 69.8% to 78.6% for GPT-4o. In metabolite-focused questions, the proportion of \nresponses with cosine similarity scores of at least 0.90 rose by 82% for GPT-4o mini \nand 77% for GPT-4o. While overall accuracy was consistent in the drug‚Äìbiological \nprocess benchmark, the retrieval method enhanced response concordance, producing a \ngreater than 10% increase in high-agreement answers (from 129 to 144) using GPT-4o. \nConclusion \nFederated knowledge retrieval provides transparent improvements in accuracy for large \nlanguage models, establishing BTE-RAG as a valuable and practical tool for \nmechanistic exploration and translational biomedical research. \n.\nCC-BY 4.0 International license\navailable under a\n(which was not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made \nThe copyright holder for this preprint\nthis version posted August 2, 2025. \n; \nhttps://doi.org/10.1101/2025.08.01.668022\ndoi: \nbioRxiv preprint'
 ""Omar & Mohammed / Journal of Applied Science and Technology Trends Vol. 06, No. 02, pp. 342 ‚Äì357 (2025) \n \n352 \nThe dataset contains 60 questions divided into three difficulty \nlevels based on how complex the language is and how deep the \nbiomedical reasoning needs to be: \n‚Ä¢ Level 1: 25 simple questions focusing mostly on easy-to-\nunderstand facts, such as symptoms of certain diseases or \ndrugs used to treat specific conditions. \n‚Ä¢ Level 2: 20 medium-level questions that involve more \ndetailed relationships, such as drug interactions and \nSideEffect. \n‚Ä¢ Level 3: 15 hard questions requiring multi-step reasoning \nacross multiple biomedical concepts or biological \npathways, similar to the complexity found in real clinical \ncases. \nThe set of evaluation questions was designed to span a wide \nrange of common biomedical topics and to reflect clinically \nrelevant query types reported in prior literature. Each question is \npaired with its corresponding gold standard cypher query and \ncategorized by difficulty level, as summarized in Table V, where \nthree illustrative examples are shown. The questions were derived \nfrom publicly available biomedical QA benchmarks and adapted \nfrom established knowledge bases (e.g., iBKH schema relations), \nensuring both coverage and diversity across diseases, drugs, \nsymptoms, and pathways.\nTABLE V.  QUESTION DIFFICULTY LEVELS AND SAMPLE CYPHER QUERIES \nLevel \nDefinition \nExample \nCypher Query \n1 \nSingle-hop question using \none relationship \nWhat are the symptoms of \nAlzheimer? \nMATCH (d:Disease)-[:HAS_SYMPTOM]->(s:Symptom) \nWHERE toLower(d.name) CONTAINS 'alzheimer' \nRETURN s.name AS symptom \n \n2 \nQuestions requiring one \nrelationship \nWhat are the side effects of \ndrugs used to treat asthma? \nWITH toLower('asthma') AS disease_name \nMATCH (d:Disease)<-[:TREATS]-(dr:Drug) \nWHERE toLower(d.name) CONTAINS disease_name \nMATCH (dr:Drug)-[:CAUSES]->(se:SideEffect) \nRETURN DISTINCT se.name AS side_effect, dr.name AS drug \n \n3 \nQuestions requiring two or \nmore relationships \nWhat cholesterol medications \ncause side effects, and what are \nsome of those effects? \nWITH toLower('cholesterol') AS disease_name \nMATCH (d:Disease)<-[:TREATS]-(dr:Drug) \nWHERE toLower(d.name) CONTAINS disease_name \nMATCH (dr:Drug)-[:CAUSES]->(se:SideEffect) \nRETURN DISTINCT dr.name AS drug, se.name AS side_effect \n \n \n \nFig. 4. Graphical User Interface of the Biomedical Knowledge Explorer System \nB. Quantitative Evaluation \nThe performance of the proposed system is evaluated on the \niBKH knowledge graph using Exact Match, Precision@k, \nRecall@k, F1@k, Hits@k, MRR, and total response time across \nsimple, medium, and complex biomedical queries. These metrics \nwere measured for each difficulty level (simple, medium, and \ncomplex) to see how performance changes as questions become \nmore challenging.\n \nTABLE VI.  QUERY GENERATION EXACT MATCH BY DIFFICULTY LEVEL""
 'format. Second, because the entire federated KG is never in-\nstantiated in a single place, reasoning and scoring methods\nthat rely on having the entire knowledge graph in memory\ncannot be used with BioThings Explorer.\nIn sum, we believe that knowledge graphs enable many ex-\nciting use cases in biomedical research (Nicholson and Greene\n2020), and that BioThings Explorer is complementary to\nexisting approaches for assembling knowledge graphs, offer-\ning powerful and unique capabilities for both scientific data\nanalysts and tool developers.\nSupplementary data\nSupplementary data are available at Bioinformatics online.\nConflict of interest\nNone declared.\nFunding\nSupport for this work was provided by the National Center\nfor Advancing Translational Sciences, National Institutes of\nHealth, through the Biomedical Data Translator program,\nawards OT2TR003427 and OT2TR003445.\nData availability\nBioThings Explorer is implemented as a NodeJS application.\nThe primary repository for the BioThings Explorer project is\nat https://github.com/biothings/biothings_explorer, which in\nturn links to and incorporates other repositories as sub-\nmodules. All code is released under the Apache 2.0 open-\nsource software license.\nReferences\nCilibrasi RL, Vitanyi PMB. The google similarity distance. IEEE Trans\nKnowl Data Eng 2007;19:370‚Äì83.\nDavis AP, Wiegers TC, Johnson RJ et al. Comparative toxicogenomics\ndatabase\n(CTD):\nupdate\n2023.\nNucleic\nAcids\nRes\n2023;51:\nD1257‚Äì62.\nDowell RD, Jokerst RM, Day A et al. The distributed annotation system.\nBMC Bioinformatics 2001;2:7.\nFecho K, Bizon C, Miller F et al. A biomedical knowledge graph system\nto propose mechanistic hypotheses for real-world environmental\nhealth observations: cohort study and informatics application. JMIR\nMed Inform 2021;9:e26714.\nFecho K, Thessen AE, Baranzini SE et al. Progress toward a universal\nbiomedical data translator. Clin Transl Sci 2022;15:1838‚Äì47.\nHimmelstein DS, Lizee A, Hessler C et al. Systematic integration\nof biomedical knowledge prioritizes drugs for repurposing. Elife\n2017;6:e26726.\nLelong S, Zhou X, Afrasiabi C et al. BioThings SDK: a toolkit for build-\ning\nhigh-performance\ndata\nAPIs\nin\nbiomedical\nresearch.\nBioinformatics 2022;38:2077‚Äì9.\nMayers M, Tu R, Steinecke D et al. Design and application of a knowl-\nedge network for automatic prioritization of drug mechanisms.\nBioinformatics 2022;38:2880‚Äì91.\nMorris JH, Soman K, Akbas RE et al. The scalable precision medicine\nopen knowledge engine (SPOKE): a massive knowledge graph of bio-\nmedical information. Bioinformatics 2023;39(2):btad080.\nMungall CJ, McMurry JA, Ko¬®hler S et al. The monarch initiative: an in-\ntegrative data and analytic platform connecting phenotypes to geno-\ntypes across species. Nucleic Acids Res 2017;45:D712‚Äì22.\nNicholson DN, Greene CS. Constructing knowledge graphs and their\nbiomedical applications. Comput Struct Biotechnol J 2020;18:\n1414‚Äì28.\nRambla J, Baudis M, Ariosa R et al. Beacon v2 and Beacon networks: a\n‚Äòlingua franca‚Äô for federated data discovery in biomedical genomics,\nand beyond. Hum Mutat 2022;43:791‚Äì9.\nUnni DR, Moxon SAT, Bada M et al. Biolink Model: a universal schema\nfor knowledge graphs in clinical, biomedical, and translational sci-\nence. Clin Transl Sci 2022;15:1848‚Äì55.\nWood EC, Glen AK, Kvarfordt LG et al. RTX-KG2: a system for build-\ning a semantically standardized knowledge graph for translational\nbiomedicine. BMC Bioinformatics 2022;23:400.\nZaveri A, Dastgheib S, Wu C et al. smartAPI: towards a more intelligent\nnetwork of web APIs. In: Blomqvist, E., Maynard, D., Gangemi, A.,\net al. (eds) The Semantic Web. ESWC 2017. Lecture Notes in\nComputer Science, 2017, 154‚Äì69. https://link.springer.com/chapter/\n10.1007/978-3-319-58451-5_11#chapter-info.\n4\nCallaghan et al.\nDownloaded from https://academic.oup.com/bioinformatics/article/39/9/btad570/7273783 by guest on 11 January 2026'
 'Omar & Mohammed / Journal of Applied Science and Technology Trends Vol. 06, No. 02, pp. 342 ‚Äì357 (2025) \n \n343 \nfor visualizing and validating LLM outputs [16], and MedKA for \nKG-enhanced question answering [17]. \nTo address these challenges, several recent works have \nexplored the integration of large language models with \nbiomedical knowledge graphs (KGs). A biomedical KG is a \nstructured network that connects entities such as diseases, drugs, \nand symptoms using defined relationships [18, 19]. These graphs \nstore verified medical knowledge from trusted databases, \nallowing for more accurate and explainable responses [12]. KGs \nare especially useful in multi-step reasoning tasks, where finding \nan answer requires connecting different pieces of information \n[20]. These entities and relationships can be visually represented \nin a biomedical knowledge graph, as shown in Fig. 1, where \nnodes represent medical concepts such as drugs, diseases, \nsymptoms, and pathways, and edges denote their semantic \nrelationships. \n \nFig. 1. Overview of Biomedical Knowledge Graph Entities and Relationships \n \nOne example of a widely used biomedical KG is SPOKE, \nwhich includes millions of nodes and relationships from over 40 \nbiomedical databases [12]. Integrating an LLM with a KG allows \nthe strengths of both technologies to work together: the LLM \nprovides language understanding, and the KG provides \nstructured, factual knowledge [21, 22]. A common method is \nretrieval-augmented generation (RAG), where the LLM retrieves \ninformation from the KG and uses it to generate more accurate \nresponses [13, 23]. In more advanced setups, the LLM can even \ngenerate queries like Cypher to fetch specific data from the graph \n[24, 25]. Neo4j is a popular graph database that supports fast and \nflexible storage and querying of knowledge graphs using Cypher \n[21]. It is well-suited for biomedical applications because it \nallows easy exploration of complex medical relationships. Recent \nwork has shown that combining Neo4j with LLMs can lead to \nbetter accuracy, fewer hallucinations, and more explainable \nresults [24, 26]. \nDespite improvements, building a reliable hybrid system that \ncombines an LLM with a biomedical KG remains a technical \nchallenge. Some approaches require complex pipelines or large \ntraining datasets, while others rely on fine-tuning specific to a \nnarrow set of questions [27, 28]. There is still a need for systems \nthat are both accurate and easy to scale, especially in domains like \nmedicine where the cost of errors is high [22]. \nRecent advances in KG-augmented LLMs have improved \nperformance, yet biomedical QA continues to face three practical \ngaps: \n1. Traceability: LLM-only or text-retrieval-only pipelines \nrarely provide graph-grounded justifications; users lack the \nability to inspect the exact nodes and edges that support an \nanswer. \n2. Evaluation: Prior work often judges quality via surface-\nform checks (e.g., matching a Cypher template), which fails to \ncapture end-to-end answer correctness or ranking quality \nacross different difficulty levels. \n3. Deployment: Many solutions assume cloud resources or \ndomain-specific fine-tuning, yet biomedical contexts typically \ndemand a local, privacy-preserving system with low latency \nand reproducible behavior. \nTimestamp-aware execution and periodic KG refresh help \navoid deprecated or contraindicated links, making the tool better \nsuited for safety-critical clinical contexts (e.g., drug‚Äìdrug \ninteractions). \nTo address these limitations, Our work introduces a locally \ndeployable pipeline that translates biomedical questions into \nexecutable Cypher queries over a Neo4j knowledge graph. The \nsystem returns answers with supporting nodes and edges, and is'
 'Clinical applications of RAG\nRAG techniques have been applied across a broad range of \nmedical specialties, as shown in Table 1. These applications \ninclude clinical decision-making and medical question- \nanswering. In clinical decision making, RAG has supported \npersonalized treatment,23,24 emergency triage,25 and disease \nmanagement.26,27 For medical question-answering, RAG‚Äôs \ncapability has been explored to address complex treatment \nguidelines questions,28 as well as queries focused on specific \nareas, such as head and neck surgery-related questions,29 and \npatient questions regarding diabetes.30 In the subgroup anal\xad\nysis, 13 studies focused on clinical decision-making (OR \n1.46, 95% CI [1.16, 1.71]) and 7 studies focused on medical \nquestion-answering (OR 1.32, 95% CI [1.08, 1.63]), with no \nstatistically significant difference observed between these 2 \ngroups.\nBaseline LLMs\nThe baseline LLMs varied across studies, with GPT-4 being \nthe most common, used in 14 studies, (OR: 1.58, 95% CI: \n1.21-2.04). GPT-3.5, used in 6 studies, showed an OR of \n1.43 (95% CI: 1.06-1.93). Llama2 was applied in 5 studies \n(OR: 1.25, 95% CI: 1.08-1.44).\nRetrieval sources\nRetrieval sources were categorized as pre-stored documents \nand real-time online browsing. Regarding pre-stored docu\xad\nments, 6 studies used clinical guidelines, such as the Emer\xad\ngency Severity Index (ESI) Ver.3 Field Triage.25 Five studies \nused academic articles from sources like PubMed abstracts or \nfull texts, or document sets such as the Radiographics Top 10 \nReading List on Gastrointestinal Imaging.31 Three studies \nused specialized knowledge bases, including ChatENT, \nOncoKB, and RareDis Corpus, while one study employed a \ngeneral biomedical knowledge graph (Scalable Precision \nMedicine Open Knowledge Engine [SPOKE]). SPOKE \nTable 1. Distribution of RAG applications by medical specialty and task \ntype.\nMedical Specialty\nFrequency\nInternal medicine\n4\nGeneral medicine\n3\nOncology\n3\nEmergency medicine\n2\nGastroenterology\n2\nOtolaryngology\n1\nHepatology\n1\nRare diseases\n1\nOrthopedics\n1\nNeurology\n1\nOphthalmology\n1\nTask\nFrequency (%)\nClinical decision-making\n13 (65%)\nMedical question-answering\n7 (35%)\nFigure 2. A forest plot showing the odds ratio (OR) of total impacts of the RAG-enhanced system compared with the baseline LLM system in clinical \ntasks. The left panel lists individual studies (e.g., Zakka et al., Murugan et al., Long et al.) along with their corresponding ORs and 95% confidence \nintervals (CIs). The right panel visualizes these ORs, with square markers proportional to study weights and horizontal lines representing CIs. The overall \npooled OR is 1.35 (95% CI: 1.19‚Äì1.53), represented by a diamond at the bottom.\n608                                                                                                      \nJournal of the American Medical Informatics Association, 2025, Vol. 32, No. 4 \nDownloaded from https://academic.oup.com/jamia/article/32/4/605/7954485 by guest on 11 January 2026'
 'Omar & Mohammed / Journal of Applied Science and Technology Trends Vol. 06, No. 02, pp. 342 ‚Äì357 (2025) \n \n349 \nentities such as Diseases and Drugs are uploaded much faster, \ngenerally under 2 seconds. \nTABLE II.  DATA UPLOAD TIMES FOR DIFFERENT ENTITY AND RELATIONSHIP \nTYPES IN NEO4J \nEntity / Relationship Type \nUpload Time (seconds) \nDisease \n0.81 \nDrugs \n1.08 \nSymptoms \n0.06 \nSide Effects \n0.14 \nPathways \n0.08 \nDisease-Disease Relationships \n30.97 \nDrug-Disease Relationships \n30.28 \nDrug-SideEffect Relationships \n5.24 \nDrug-Drug Relationships \n190.09 \nDrug-Pathway Relationships \n0.14 \nDisease-Pathway Relationships \n0.06 \nDisease-Symptom Relationships \n0.12 \n \nFig. 3, presents a vertical bar chart that visually compares \nthese upload times across the different entity and relationship \ntypes. The chart clearly shows the significant difference in upload \nduration between nodes and edges, emphasizing the higher cost \nof ingesting complex relationships in the graph. \n3) \nExperimental Environment \nIn this study, the proposed biomedical question answering \nsystem was evaluated using a locally hosted environment. All \nexperiments were conducted on a Windows 11 Pro (64-bit) \nsystem equipped with an Intel Core i5-10500H processor running \nat 2.50 GHz (12 logical CPUs), 24 GB of RAM, and an NVIDIA \nGeForce GTX 1650 GPU with Max-Q Design. The Neo4j graph \ndatabase (v4.4.5) was managed through Neo4j Desktop (v1.6.2), \nand the LLaMA 3B language model was executed locally using \noptimized configurations suitable for this hardware setup. \n \nEach Cypher query generated by the system was executed \nmultiple times to calculate an average response time, ensuring \nconsistency across varying levels of question difficulty. The \nknowledge graph was constructed using the iBKH dataset, and \ndata loading and system performance were carefully monitored \nto maintain stability during testing. This experimental setup \nprovides a reliable and reproducible environment for \nbenchmarking the effectiveness and responsiveness of the hybrid \nQA system. \n \nFig. 3. Upload times for various biomedical entities and relationships in Neo4j. \n \nWe run LLaMA 3 locally (not via cloud APIs) to satisfy \nbiomedical privacy/governance (no data leaves the host) and to \nmaximize reproducibility (fixed GGUF checkpoint, pinned \nllama.cpp commit, controlled seeds settings, constant hardware). \nLocal execution yields predictable cost availability and stable \nlatency (no network jitter) and lets us enforce executable Cypher \ngrounding with per edge provenance and timestamp aware \nexecution. \n4) \nKnowledge Graph(KG) Construction \nThe Neo4j graph database was used as the backend to store \nand query the KG, and it is a graph database designed for highly \nconnected data. Before loading data, uniqueness constraints were \ncreated on the primary property for each node label (Disease, \nDrug, Symptom, SideE ffect, Pathway). This enforces that each \nprimary ID appears only once, preventing duplicate entities. For \nefficient lookups in queries, a search index was created based on \nthe name property of each node label. As noted in the Neo4j']"
How is the UMLS term utilized in the iBKH dataset for biomedical entities?,"['B. Dataset and Knowledge Graph Construction 1) Dataset The integrated Biomedical Knowledge Hub (iBKH), a large biomedical knowledge base, forms the first level of the system and integrates information from various curated high-quality biomedical databases. This implies that the data set includes various types of entities, such as diseases, symptoms, drugs, biological pathways, etc. This study used the representative subset of the iBKH dataset, which contained 65828 biomedical entities. These entities are semantically interconnected through a total of 3004166 relationships, thus creating a rich knowledge graph. The iBKH dataset was originally introduced in [11], and it is freely available at (https://github.com/wcm-wanglab/iBKH). This dataset is the core semantic foundation upon which this study is built. The knowledge graph is populated from multiple tabular sources (CSV files), each listing entities or relationships. The main input files and their contents are as follows: ‚Ä¢ Disease vocabulary(disease_vocab.csv): Contains columns such as primary (a unique disease ID), name, do_id (Disease Ontology ID), kegg_id, and umls_cui (UMLS Concept ID). Each row represents a disease node with external identifiers. ‚Ä¢ Drug vocabulary (drug_vocab.csv): Includes primary (unique drug ID), name, drugbank_id, kegg_id, pharmgkb_id, umls_cui, mesh_id, iDISK_id and CID (PubChem ID). Each row defines a drug node with standard database identifiers. ‚Ä¢ Symptom vocabulary (symptom_vocab.csv): Contains primary (unique symptom ID), name, mesh_id, umls_cui and iDISK_id. Each row defines a symptom node. ‚Ä¢ Side effect vocabulary (side_effect_vocab.csv): Includes primary (unique side-effect ID) and name. Each row represents a side-effect node (with UMLS ID when available). ‚Ä¢ Pathway vocabulary (pathway_vocab.csv): Contains primary (unique pathway ID), name, reactome_id, go_id, and kegg_id. Each row defines a biological pathway node. Relationship files (each row typically contains two entity IDs and one or more boolean flags or codes) include: ‚Ä¢ Disease‚ÄìSymptom links (Di_Sy_res.csv): Rows include Disease and Symptom IDs, a presence flag (1 or 0) and a data source. If Present = 1, a HAS_SYMPTOM edge is created from the disease to the symptom, with properties for presence and source. ‚Ä¢ Disease‚ÄìDisease links (di_di_res.csv): Rows include Disease_1 and Disease_2 IDs with binary flags for is_a and Resemble. If is_a = 1, an (IS_A) edge is created (Disease_1 ‚Üí Disease_2); if Resemble = 1, a RESEMBLES edge is created. The source field is used for provenance. ‚Ä¢ Drug‚ÄìDisease links (D_Di_res.csv): Includes Drug and Disease IDs with several binary flags. If a flag equals 1, a corresponding edge is created: o TREATS (Treats = 1) o PALLIATES (Palliates = 1) o ASSOCIATED_WITH (Associate = 1) o ALLEVIATES_REDUCES (alleviates = 1) o TREATMENT_THERAPY (treatment/therapy = 1) o INHIBITS_CELL_GROWTH (inhibits cell growth = 1) o HAS_BIOMARKER (biomarkers = 1) o PREVENTS_SUPPRESSES (prevents/suppresses = 1) o ROLE_IN_PATHOGENESIS (role in disease pathogenesis = 1) ‚Ä¢ Drug‚ÄìSideEffect links (D_SE_res.csv): Contains Drug and SideEffect IDs with a Source column. Each row creates a CAUSES edge from the drug to the side effect, with source as an edge property. ‚Ä¢ Drug‚ÄìDrug interactions (D_D_res.csv): Rows include Drug_1 and Drug_2 IDs with flags for Interaction and Resemble. If Interaction = 1, an INTERACTS_WITH edge is created (bidirectional). If Resemble = 1, a RESEMBLES edge is added. ‚Ä¢ Drug‚ÄìPathway links (D_Pwy_res.csv): Includes Drug ID and Pathway ID. Each row generates an ASSOCIATED_WITH edge from the drug to the pathway. ‚Ä¢ Disease‚ÄìPathway links (Di_Pwy_res.csv): Contains Disease ID and Pathway ID. Each row creates an ASSOCIATED_WITH edge from the disease to the pathway. 2) Data Upload Performance The time required to upload different types of entities and relationships into the Neo4j biomedical knowledge graph, measured in seconds. These measurements reflect both the size and complexity of the data being processed. As shown in Table II, the longest upload time is for Drug- Drug Relationships, which takes approximately 190 seconds due to the large number of edges (over 3 million). Following this, Disease-Disease and Drug-Disease Relationships also require considerable time for loading. On the other hand, individual']","In the iBKH dataset, the UMLS Concept Unique Identifier (UMLS CUI) is used as an external identifier for multiple biomedical entities. Specifically, it appears as a column in the disease vocabulary, drug vocabulary, and symptom vocabulary files, linking each entity to its corresponding UMLS concept.",single_hop_specifc_query_synthesizer,"The UMLS term (specifically the UMLS Concept Unique Identifier, or UMLS CUI) is utilized in the iBKH dataset as a standardized biomedical identifier to normalize and represent biomedical entities across different vocabularies. In the iBKH dataset:

- Each biomedical entity type (such as diseases, drugs, symptoms, and side effects) includes a column for the UMLS CUI in its respective vocabulary CSV file (e.g., disease_vocab.csv, drug_vocab.csv, symptom_vocab.csv, side_effect_vocab.csv).
- The UMLS CUI serves as a canonical identifier that helps unify and link entities from multiple external biomedical databases and ontologies, ensuring semantic consistency and reducing duplication.
- During the construction of the iBKH knowledge graph, nodes representing biomedical entities are created with properties that include the UMLS CUI alongside other database-specific IDs (e.g., DrugBank ID, KEGG ID, MeSH ID).
- This normalization facilitates accurate matching and integration of entities from diverse sources, supporting reliable multi-hop queries and reasoning over the graph.
- The use of UMLS CUIs also aids in addressing challenges such as synonymy, abbreviation, and homonymy by providing a common reference point for concept-level normalization.

In summary, the UMLS term in the iBKH dataset is employed as a key identifier to normalize biomedical entities across multiple data sources, enabling consistent integration, disambiguation, and effective querying within the Neo4j-based biomedical knowledge graph.","[""Omar & Mohammed / Journal of Applied Science and Technology Trends Vol. 06, No. 02, pp. 342 ‚Äì357 (2025) \n \n352 \nThe dataset contains 60 questions divided into three difficulty \nlevels based on how complex the language is and how deep the \nbiomedical reasoning needs to be: \n‚Ä¢ Level 1: 25 simple questions focusing mostly on easy-to-\nunderstand facts, such as symptoms of certain diseases or \ndrugs used to treat specific conditions. \n‚Ä¢ Level 2: 20 medium-level questions that involve more \ndetailed relationships, such as drug interactions and \nSideEffect. \n‚Ä¢ Level 3: 15 hard questions requiring multi-step reasoning \nacross multiple biomedical concepts or biological \npathways, similar to the complexity found in real clinical \ncases. \nThe set of evaluation questions was designed to span a wide \nrange of common biomedical topics and to reflect clinically \nrelevant query types reported in prior literature. Each question is \npaired with its corresponding gold standard cypher query and \ncategorized by difficulty level, as summarized in Table V, where \nthree illustrative examples are shown. The questions were derived \nfrom publicly available biomedical QA benchmarks and adapted \nfrom established knowledge bases (e.g., iBKH schema relations), \nensuring both coverage and diversity across diseases, drugs, \nsymptoms, and pathways.\nTABLE V.  QUESTION DIFFICULTY LEVELS AND SAMPLE CYPHER QUERIES \nLevel \nDefinition \nExample \nCypher Query \n1 \nSingle-hop question using \none relationship \nWhat are the symptoms of \nAlzheimer? \nMATCH (d:Disease)-[:HAS_SYMPTOM]->(s:Symptom) \nWHERE toLower(d.name) CONTAINS 'alzheimer' \nRETURN s.name AS symptom \n \n2 \nQuestions requiring one \nrelationship \nWhat are the side effects of \ndrugs used to treat asthma? \nWITH toLower('asthma') AS disease_name \nMATCH (d:Disease)<-[:TREATS]-(dr:Drug) \nWHERE toLower(d.name) CONTAINS disease_name \nMATCH (dr:Drug)-[:CAUSES]->(se:SideEffect) \nRETURN DISTINCT se.name AS side_effect, dr.name AS drug \n \n3 \nQuestions requiring two or \nmore relationships \nWhat cholesterol medications \ncause side effects, and what are \nsome of those effects? \nWITH toLower('cholesterol') AS disease_name \nMATCH (d:Disease)<-[:TREATS]-(dr:Drug) \nWHERE toLower(d.name) CONTAINS disease_name \nMATCH (dr:Drug)-[:CAUSES]->(se:SideEffect) \nRETURN DISTINCT dr.name AS drug, se.name AS side_effect \n \n \n \nFig. 4. Graphical User Interface of the Biomedical Knowledge Explorer System \nB. Quantitative Evaluation \nThe performance of the proposed system is evaluated on the \niBKH knowledge graph using Exact Match, Precision@k, \nRecall@k, F1@k, Hits@k, MRR, and total response time across \nsimple, medium, and complex biomedical queries. These metrics \nwere measured for each difficulty level (simple, medium, and \ncomplex) to see how performance changes as questions become \nmore challenging.\n \nTABLE VI.  QUERY GENERATION EXACT MATCH BY DIFFICULTY LEVEL""
 'D 734 \nNucleic Acids Research , 2025, Vol. 53, Database issue \nuses the whole genome / proteome background; however, it \nis recommended that users provide a custom background \nlist representing a more realistic universe of genes / proteins \ndetected by their assay ( 35 ). Alternatively, the entire sorted \ndataset can be analyzed using gene set enrichment analysis to \ndetermine whether the distribution of genes at the top, bottom \nor both ends deviates significantly from what would be ex- \npected by chance. STRING supports both of these enrichment \nmethods. The most recent update to STRING introduces vari- \nous improvements, including a new false discovery rate (FDR) \ncorrection, enhanced filtering options and a new graphical vi- \nsualization. \nFDR correction \nIn its analyses, STRING performs a test for each path- \nway (‚Äòterm‚Äô) within a given pathway collection (‚Äòcategory‚Äô). \nLike many other enrichment tools, STRING employs the \nBenjamini‚ÄìHochberg correction ( 36 ) to adjust for multiple \nhypothesis testing. This statistical correction is essential for \nmanaging the FDR effectively. However, the larger the term \ncount and the greater the diversity of the terms, the higher \nthe required correction and the lower the chance of exceed- \ning the alpha level (significance threshold) for a given term. \nOne method used to address this issue involves restricting the \nanalysis to a subset of terms, such as ‚ÄòGO slims‚Äô that focus \non broad, high-level terms with a simplified ontology struc- \nture ( 16 ). However, this approach has some disadvantages: it \nmight exclude terms particularly relevant to a tested gene set, \nit may not cover all functions by design and it is mainly ap- \nplicable to hierarchical classification systems such as the Gene \nOntology. \nSTRING utilizes several hierarchical ontologies, such as \nthe Brenda Tissue Ontology, Disease Ontology, Human Phe- \nnotype Ontology and the aforementioned Gene Ontology; \nthey typically contain many more leaf-ward nodes than nodes \ncloser to the root. The parental nodes must contain all the \ngenes included in the child terms, creating an imbalance in \nthe ontology with many more smaller terms and fewer larger \nterms. Removing smaller terms can substantially increase sta- \ntistical power . However , simply eliminating all small terms \ncould negatively impact discoverability, especially for small \nquery gene sets where the smaller terms might be the most rel- \nevant for biological interpretation. The ontologies are usually \nlarge, with > 10 000 terms (and therefore tests) in the Biolog- \nical Process branch of the Gene Ontology tree. Such strong \ncorrections have the effect that for larger query gene sets, or \nfor backgrounds that cover only part of the proteome, it might \nnot be statistically possible to detect enrichments for a subset \nof the terms; these can therefore be removed from considera- \ntion. \nIn its new version, STRING takes a more flexible \napproach‚Äîby only testing terms that have a priori any statis- \ntical potential for enrichment. This decision is based on sev- \neral parameters: the size of the term, the size of the query set, \nthe background size and the number of tests conducted. By \ncalculating the smallest and largest term sizes that could the- \noretically be enriched given the FDR correction, STRING can \ndetermine which terms are viable for inclusion in the anal- \nysis. Terms outside these bounds are excluded from testing. \nThis methodical exclusion significantly enhances the statisti- \ncal power of the enrichment analysis, without omitting any \nterms that could be enriched. For smaller query sets, this strat- \negy will maintain all terms, as even the smallest tested sets \n(term size = 2) might still be theoretically enriched. How- \never , for larger , less specific sets or for tests against a cus- \ntom smaller background, this approach markedly improves \nthe statistical power, enabling STRING to perform more fo- \ncused analyses without the drawbacks of increased false posi- \ntive rates or omitting potentially biologically interesting terms. \nAlthough this method leverages the characteristics of ontolo- \ngies, it does not require a hierarchy (parent‚Äìchild relationships \nbetween the terms) to function and can be applied to any \npathway / term collection with an imbalance of small terms, \nsuch as PMID-derived gene sets and STRING neighborhood \nclusters. \nTerm filtering \nOne key insight from our online user surveys is that the of- \nten overwhelming number of enrichment results can make it \nchallenging to discern the most relevant enriched terms. We \nhave thus enhanced the display of enrichment results, allow- \ning users to filter and sort them based on (i) FDR, (ii) strength, \n(iii) signal, (iv) term size and (v) term similarity. \nThe newest addition to our filtering options are the ‚Äòterm \nsimilarity‚Äô and ‚Äòsignal‚Äô filters. The ‚Äòterm similarity‚Äô filter uses \nthe Jaccard index to measure the similarity between the gene \nsets of terms within the same category. The method proceeds \nby sorting the terms according to their P -values, prioritizing \nthose with the strongest statistical significance. The algorithm \nthen iterates through the list of terms; any term that exhibits \na similarity exceeding a predetermined, user-defined threshold \nrelative to terms already included in the results is subsequently \nexcluded. This approach requires no additional information \nbeyond what is already available in the dataset and can be \nuniformly applied across all categories of terms. The ‚Äòsignal‚Äô \nfilter is defined as a weighted harmonic mean between the ra- \ntio of observed to expected gene occurrences in an enriched \nterm and its ‚àílog(FDR), respectively. This filter balances the \nimpact of both metrics and provides a more intuitive ordering \nof enriched terms and has been implemented as the default \nsorting criterion. \nThese filters are designed to exclude potentially less relevant \nresults, such as terms that are marginally significant, small or \nshow significant overlap in terms of gene content. This en- \nhancement streamlines the search for relevant observations, \nhelping users focus on the most meaningful results. \nGraphical visualization \nIn response to user feedback requesting more intuitive, graph- \nical visualizations of the detected enrichments, STRING now \nhas adopted an interactive dot plot as a primary display \nmethod for enrichment analysis outcomes. We chose the dot \nplot for its clarity in representing enriched functions across \nthree dimensions: (i) enrichment signal along the X -axis, (ii) \nthe FDR indicated through the color coding of the dots and \n(iii) the term‚Äôs protein count in the network represented by the \nsize of each dot. The terms (listed on the Y -axis) are ranked \nby their enrichment signal or, if grouped by similarity, by the \nenrichment signal of their group first (Figure 2 ). This visual- \nization reflects all filter settings chosen by the user to manage \nthe volume of results displayed. These plots can be addition- \nally customized in the interface and the resulting graphs can be \ndownloaded as PNG for immediate use or as SVG for further \nmodifications. \nDownloaded from https://academic.oup.com/nar/article/53/D1/D730/7903368 by guest on 11 January 2026'
 'Omar & Mohammed / Journal of Applied Science and Technology Trends Vol. 06, No. 02, pp. 342 ‚Äì357 (2025) \n \n351 \nThis query searches for a disease node whose name contains \n‚Äôalzheimer‚Äô and follows HAS_SYMPTOM edges to list related \nsymptom names. The system then executes this cypher to retrieve \nanswers. The prompts (such as few-shot examples and schema \nhints) were carefully designed to help LLaMA 3 generate correct \nCypher queries. The model learns how to use the graph‚Äôs labels \nand relationships properly. For example, if a user asks, ‚ÄôWhich \ndrugs treat diabetes?‚Äô, LLaMA might respond with a suitable \nCypher query: \n \nMATCH (d:Drug)-[:TREATS]->(di:Disease) \nWHERE toLower(di.name) CONTAINS ""diabetes"" \nRETURN d.name  \n \nThis queries for drug nodes that have a TREATS edge to a \ndiabetes disease node. By leveraging LLaMA 3 in this way, our \nsystem can flexibly handle many phrasing variations without \nmanual mapping rules. \nD. Model Configuration & Decoding \nWe run a local LLaMA 3.2-3B model in GGUF format (llama-\n3.2-3b-instruct-q4_k_m.gguf) via llama.cpp, as shown in Table \nIV. \nTABLE IV.  MODEL RUNTIME AND DECODING SETTINGS \nRuntime settings \nDecoding settings \nn_ctx = 1024 \ntemperature = 0.2 \nn_threads = 12 \ntop_p = 0.95 \nn_gpu_layers = 33 \ntop_k = 40 \nn_batch = 512 \nrepeat_penalty = 1.1 \n \nmax_tokens = 80 \n \nseed = 42 \n \nE. Graph Subset and Versioning \nWe use an iBKH derived subgraph (‚âà65.8k nodes; ‚âà3.0M \nedges) spanning DRUG, DISEASE, SYMPTOM, PATHWAY. IDs are \nnormalized to CURIEs and duplicates collapsed across \nUMLS/DrugBank/DisGeNET/SIDER/KEGG. Each edge stores \nprovenance/licensing metadata (source, source_version, license, \nretrieved_at, \nevidence_pmids/urls) \nand, \nwhen \navailable, \ntemporal fields (valid_from, valid_to). We report coverage as \nthe \npercentage \nof \nevaluated \nquestions \nwhose \ngold \nentities/relations are present. \nF. Query Execution and Reliability \nAfter a Cypher query is generated, it is executed on the Neo4j \ndatabase through the official Neo4j Python driver, which \nmanages the secure connection and returns the results. Instead of \nrestricting the output with a fixed LIMIT (e.g., LIMIT 5), the \nsystem retrieves candidate results and evaluates them using \nstandardized retrieval metrics such as Hits@1, Hits@5, and \nHits@10. This approach ensures that the system remains \nresponsive while providing a fair assessment of ranking quality \nacross different cutoff levels, rather than depending solely on a \nfixed number of returned items. Neo4j‚Äôs indexing on key node \nproperties, such as name and primary identifiers, also helps speed \nup lookups as the knowledge graph grows. In cases where the \nlanguage model generates an incomplete or incorrect query, such \nas referencing nodes or relationships that do not exist, the system \ncatches the error and either retries with a simpler prompt or \ninforms the user. Together, these steps make sure that queries run \nquickly, return valid results, and keep the overall experience \nsmooth and reliable for biomedical question-answering. \nG. User Interface for Query Transparency \nThe system includes a lightweight, cross-platform graphical \nuser interface (GUI) implemented as a web application using the \nFlask framework in Python, with HTML and Bootstrap for \ninteractive visualization. The interface is designed to make the \nquestion-answering process transparent and accessible to users \nwithout technical expertise. It consists of three main panels: \n1. Input Panel: Where the user can enter a biomedical \nquestion in natural language. \n2. Query Panel: Which displays the Cypher query generated \nby the language model, allowing users to verify how their \nquestion was interpreted. \n3. Results Panel: Which presents the retrieved answers in a \nclear, readable format, accompanied by a brief natural \nlanguage explanation generated by the system. \nBy showing both the query and the answer, the GUI promotes \nuser trust and enables validation of the system‚Äôs reasoning \nprocess. The interface is lightweight enough to run smoothly on \nstandard desktop machines without additional dependencies, \nmaking it practical for local deployments in clinical or research \nsettings. Fig. 4 illustrates the overall layout. \nV. EXPERIMENTAL RESULTS \nIn this section, the proposed system is evaluated based on its \nability to translate natural-language biomedical questions into \nexecutable Cypher queries over the iBKH knowledge graph. The \nassessment is conducted end-to-end and focuses on three main \naspects: (i) the accuracy of query generation, measured by the \nproportion of correctly produced Cypher queries; (ii) system \nefficiency, quantified through total response time from question \nsubmission to final answer delivery, covering both query \ngeneration and execution; and (iii) the quality of retrieved \nanswers, evaluated using standardized information retrieval \nmetrics including Precision@k, Recall@k, F1@k, Hits@k, and \nMean Reciprocal Rank (MRR). For clarity, all these metrics are \nformally defined in the following subsections, with their \ncorresponding equations, and will be used consistently \nthroughout the results section. Together, these dimensions \nprovide a comprehensive view of both correctness and efficiency \nacross simple, medium, and complex biomedical queries. \nA. Description of the Experimental Data \nTo evaluate the proposed system, this work used a carefully \nselected set of biomedical questions designed to test how well the \nsystem understands natural language and converts it into Cypher \nqueries for a biomedical knowledge graph. \nTo evaluate the system, a benchmark of 60 biomedical \nquestions was constructed by the authors, guided by the schema \nand relations in iBKH. The questions were grouped into simple, \nmedium, and complex levels to assess performance across \ndifferent reasoning requirements. Gold-standard answers were \nmanually prepared for each question to enable quantitative \nevaluation using Exact Match, Precision@k, Recall@k, F1@k, \nHits@k, and MRR. The complete set of 60 questions is available \nat this link https://drive.google.com/drive/my-drive.'
 'Omar & Mohammed / Journal of Applied Science and Technology Trends Vol. 06, No. 02, pp. 342 ‚Äì357 (2025) \n \n348 \ndrugs, diseases, symptoms) and execution returns structured data \n(tuples) relevant to the question. \nStep 5. Answer Synthesis: The structured tuples flow to \nAnswer Synthesis, which aggregates and formats them into a \nconcise raw answer. This raw answer is sent back to LLaMA 3 \nto optionally refine the phrasing while preserving the retrieved \nfacts. \nStep 6. Result Presentation: LLaMA 3 produces the final \nanswer, which the interface displays together with the executed \nCypher query and an optional preview of the returned rows, \nimproving transparency and trust. \nThe pipeline couples LLM-based language understanding \n(LLaMA 3) with a schema-grounded Neo4j knowledge graph. \nThe Cypher Query Gen refines the query formulation, Query \nExecution retrieves evidence and Answer Synthesis converts \nstructured results into readable outputs that produce answers that \nare accurate, interpretable, and easy to audit directly from the \ndisplayed query and evidence. \nB. Dataset and Knowledge Graph Construction \n1) \nDataset \nThe integrated Biomedical Knowledge Hub (iBKH), a large \nbiomedical knowledge base, forms the first level of the system \nand integrates information from various curated high-quality \nbiomedical databases. This implies that the data set includes \nvarious types of entities, such as diseases, symptoms, drugs, \nbiological pathways, etc. This study used the representative \nsubset of the iBKH dataset, which contained 65828 biomedical \nentities. These entities are semantically interconnected through a \ntotal of 3004166 relationships, thus creating a rich knowledge \ngraph. The iBKH dataset was originally introduced in [11], and it \nis freely available at (https://github.com/wcm-wanglab/iBKH). \nThis dataset is the core semantic foundation upon which this \nstudy is built. The knowledge graph is populated from multiple \ntabular sources (CSV files), each listing entities or relationships. \nThe main input files and their contents are as follows: \n‚Ä¢ Disease \nvocabulary(disease_vocab.csv): \nContains \ncolumns such as primary (a unique disease ID), name, \ndo_id (Disease Ontology ID), kegg_id, and umls_cui \n(UMLS Concept ID). Each row represents a disease node \nwith external identifiers. \n‚Ä¢ Drug vocabulary (drug_vocab.csv): Includes primary \n(unique drug ID), name, drugbank_id, kegg_id, \npharmgkb_id, umls_cui, mesh_id, iDISK_id and CID \n(PubChem ID). Each row defines a drug node with \nstandard database identifiers. \n‚Ä¢ Symptom vocabulary (symptom_vocab.csv): Contains \nprimary (unique symptom ID), name, mesh_id, umls_cui \nand iDISK_id. Each row defines a symptom node. \n‚Ä¢ Side \neffect \nvocabulary \n(side_effect_vocab.csv): \nIncludes primary (unique side-effect ID) and name. Each \nrow represents a side-effect node (with UMLS ID when \navailable). \n‚Ä¢ Pathway vocabulary (pathway_vocab.csv): Contains \nprimary (unique pathway ID), name, reactome_id, go_id, \nand kegg_id. Each row defines a biological pathway \nnode. \nRelationship files (each row typically contains two entity IDs \nand one or more boolean flags or codes) include: \n‚Ä¢ Disease‚ÄìSymptom links (Di_Sy_res.csv): Rows include \nDisease and Symptom IDs, a presence flag (1 or 0) and a \ndata source. If Present = 1, a HAS_SYMPTOM edge is \ncreated from the disease to the symptom, with properties \nfor presence and source. \n‚Ä¢ Disease‚ÄìDisease links (di_di_res.csv): Rows include \nDisease_1 and Disease_2 IDs with binary flags for is_a \nand Resemble. If is_a = 1, an (IS_A) edge is created \n(Disease_1 ‚Üí Disease_2); if Resemble = 1, a \nRESEMBLES edge is created. The source field is used \nfor provenance. \n‚Ä¢ Drug‚ÄìDisease links (D_Di_res.csv): Includes Drug and \nDisease IDs with several binary flags. If a flag equals 1, \na corresponding edge is created: \no \nTREATS (Treats = 1) \no \nPALLIATES (Palliates = 1) \no \nASSOCIATED_WITH (Associate = 1) \no \nALLEVIATES_REDUCES (alleviates = 1) \no \nTREATMENT_THERAPY (treatment/therapy = \n1) \no \nINHIBITS_CELL_GROWTH \n(inhibits \ncell \ngrowth = 1) \no \nHAS_BIOMARKER (biomarkers = 1) \no \nPREVENTS_SUPPRESSES \n(prevents/suppresses = 1) \no \nROLE_IN_PATHOGENESIS (role in disease \npathogenesis = 1) \n‚Ä¢ Drug‚ÄìSideEffect links (D_SE_res.csv): Contains Drug \nand SideEffect IDs with a Source column. Each row \ncreates a CAUSES edge from the drug to the side effect, \nwith source as an edge property. \n‚Ä¢ Drug‚ÄìDrug interactions (D_D_res.csv): Rows include \nDrug_1 and Drug_2 IDs with flags for Interaction and \nResemble. If Interaction = 1, an INTERACTS_WITH \nedge is created (bidirectional). If Resemble = 1, a \nRESEMBLES edge is added. \n‚Ä¢ Drug‚ÄìPathway links (D_Pwy_res.csv): Includes Drug \nID and Pathway ID. Each row generates an \nASSOCIATED_WITH edge from the drug to the \npathway. \n‚Ä¢ Disease‚ÄìPathway links (Di_Pwy_res.csv): Contains \nDisease ID and Pathway ID. Each row creates an \nASSOCIATED_WITH edge from the disease to the \npathway. \n2) \nData Upload Performance \nThe time required to upload different types of entities and \nrelationships into the Neo4j biomedical knowledge graph, \nmeasured in seconds. These measurements reflect both the size \nand complexity of the data being processed. \nAs shown in Table II, the longest upload time is for Drug-\nDrug Relationships, which takes approximately 190 seconds due \nto the large number of edges (over 3 million). Following this, \nDisease-Disease and Drug-Disease Relationships also require \nconsiderable time for loading. On the other hand, individual'
 'Omar & Mohammed / Journal of Applied Science and Technology Trends Vol. 06, No. 02, pp. 342 ‚Äì357 (2025) \n \n356 \n \nFig. 7. Side effects of drugs that treat epilepsy \n \nExecuting this query returns drugs associated with epilepsy \nand their corresponding side effects. For example, the query \nidentifies Pregabalin as a treatment for epilepsy and retrieves \nmultiple side effects such as anxiety, arrhythmia, gastrointestinal \npain, and others. The visualization of the graph in Fig. 7, \nhighlights this pattern, where the drug node is linked to epilepsy \nvia TREATS and to several side effect nodes via CAUSES, \nproviding an interpretable biomedical knowledge structure. \nVI. DISCUSSION \nUnder \na \nunified \nevaluation \nprotocol \nreporting \nP@k/R@k/F1@k, Hits@k, and MRR at ùëò‚àà{1,5,10} , the \npipeline exhibits consistent end-to-end behavior across the three \ndifficulty tiers. The ùëò-ablation shows strong early precision (high \nP@1 and elevated MRR), while recall increases with larger ùëò, \nindicating that correct answers typically surface near the top yet \npersist deeper in the ranked slate. At the query-generation level, \nexact-match (EM) is high for simple questions (e.g., ‚àº96%) and \nlower for complex, multi-hop questions (e.g., ‚àº86.7%), which \naligns with increased compositionality. End-to-end latency \n(mean total ‚âà6.07\u2006s from question to final answer) remains within \ninteractive bounds on our local setup. Practically, a fully offline \ndeployment avoids internet connectivity, API keys, and external \ndata \ntransfer, \nstrengthening \nprivacy, \ncompliance, \nand \nreproducibility in biomedical contexts. \nSeveral challenges qualify these findings. First, the \ncomparability challenge: to our knowledge, no prior study \nevaluates iBKH using the same metric suite, making cross-paper \nnumeric comparisons not ‚Äúapples to apples.‚Äù We therefore \ninterpret the results as controlled evidence about this pipeline \nunder a single, consistent protocol rather than as a cross-study \nleaderboard. Beyond comparability, performance may vary with \nother biomedical knowledge graphs; the current iBKH snapshot \nis static, limiting real-time updates; and scaling to larger or \ndynamically refreshed graphs can introduce latency and \nconsistency trade-offs. \nError analysis shows that residual failures concentrate in \ncomplex, multi-hop queries where missing relation hops or brittle \nname-based matching (synonyms, abbreviations, homonyms) \nlead to partial answers. These observations motivate concept-\nlevel normalization via biomedical identifiers (e.g., UMLS, \nSNOMED, RxNorm) with synonym expansion, as well as \nschema-constrained query generation and path-guided decoding \nto better satisfy multi-hop constraints. Finally, correctness is \nassessed primarily at the answer level (Hits@k, MRR, \nprecision/recall) and does not yet include full semantic-\nequivalence checks across alternative Cypher queries, which may \noverlook cases where different queries yield the same correct \nresults. Complementing template EM with result-set equivalence \nchecks, expanding the metric suite (e.g., nDCG/MAP) with per-\nquestion 95% bootstrap confidence intervals, and supporting \nincremental graph updates with distributed storage are promising \nsteps to enhance robustness, scalability, and external validity. \nVII. CONCLUSION \nThis study introduced a hybrid biomedical question \nanswering framework that couples the LLaMA-3B language \nmodel with a Neo4j-based iBKH knowledge graph to enable the \nautomatic generation of executable Cypher queries and to deliver \ntransparent, evidence-grounded answers through justification \nsubgraphs. Evaluation in simple, medium, and complex queries \ndemonstrated consistently high performance, with strong \nprecision, recall, F1, Hits@k, and MRR values, while \nmaintaining low latency suitable for interactive biomedical \napplications. Beyond quantitative performance, the proposed \nsystem provides a reproducible and privacy-preserving solution \nby operating fully offline, a property of particular importance in \nsensitive clinical and research contexts. However, certain \nlimitations remain. The reliance on a static iBKH snapshot \nconstrains coverage and adaptability, recall is lower for complex \nmulti-hop reasoning, and the absence of canonical entity \nnormalization (e.g., UMLS, SNOMED, RxNorm) may reduce \nsemantic robustness. Future research will therefore focus on \nintegrating standardized biomedical entity normalization, \nenabling dynamic and incremental knowledge graph updates, and \nleveraging domain-adapted or fine-tuned biomedical LLMs. \nThese directions are expected to further strengthen the \nrobustness, scalability, and applicability of the framework in real-\nworld biomedical and clinical environments. \n \nREFERENCES \n[1] S. Bubeck, V. Chandrasekaran, R. Eldan, J. Gehrke, E. Horvitz, E. Kamar, \nP. Lee, Y. T. Lee, Y. Li, S. Lundberg, et al., ‚ÄúSparks of artificial general \nintelligence: \nEarly experiments with \ngpt-4,‚Äù arXiv preprint \narXiv:2303.12712, 2023. \n[2] J. Achiam, S. Adler, S. Agarwal, L. Ahmad, I. Akkaya, F. L. Aleman, D. \nAlmeida, J. Altenschmidt, S. Altman, S. Anadkat, et al., ‚ÄúGpt-4 \ntechnical report,‚Äù arXiv preprint arXiv:2303.08774, 2023. \n[3] Y. Hou, J. Yeung, H. Xu, C. Su, F. Wang, and R. Zhang, ‚ÄúFrom answers to \ninsights: unveiling the strengths and limitations of chatgpt and \nbiomedical knowledge graphs,‚Äù Research Square, pp. rs‚Äì3, 2023.'
 'Omar & Mohammed / Journal of Applied Science and Technology Trends Vol. 06, No. 02, pp. 342 ‚Äì357 (2025) \n \n346 \nscientific literature, clinical records, genomic databases, and \nexperimental findings [19, 31]. \nSuch integration creates a comprehensive biomedical \nknowledge base that supports advanced analytics and discovery. \nFor example, biomedical knowledge graphs can reveal previously \nunknown relationships (e.g., between drugs and diseases) and \nhelp prioritize potential biomarkers for complex conditions. The \nIntegrative Biomedical Knowledge Hub (iBKH) is one such \nlarge-scale graph that consolidates diverse biomedical resources \ninto a unified hub, enabling discovery at scale [11]. Beyond \niBKH, large biomedical knowledge graphs such as SPOKE \nfurther illustrate how graph integration accelerates research and \nsupports precision-medicine use cases [12]. Overall, these graphs \nserve as foundational resources for data-driven and personalized \nmedicine. These knowledge graphs serve as foundational \nresources for precision medicine, where treatment can be tailored \nto the individual‚Äôs biological profile, improving outcomes and \nminimizing side effects [19, 31]. \n \nTABLE I.  SUMMARY OF RELATED RESEARCH ON BIOMEDICAL KGS AND QUESTION ANSWERING \nRef. \nYear \nData/Graph \nMethod \nBaselines \nKey Metric \nLimitation \n[11] \n2023 \niBKH (18 biomedical \nsources, Neo4j) \nIntegrative KG + Graph \nlearning; drug repurposing \ncase \nKnown drugs, \nAlzheimer‚Äôs \nstudy \nPredictive performance \n(drug repurposing) \nLimited to Alzheimer‚Äôs case \nstudy; scalability and updates not \ndetailed \n[19] \n2023 \nDisease KG (9,400 \nnodes, 45,000 relations \nin Neo4j) \nCypher queries for disease‚Äì\ndrug‚Äìsymptom reasoning \nCross-referential \ndisease DB \nNew relation inference; \ncomplex query \nanswering \nLimited to single domain; lacks \nlarge-scale evaluation \n[3] \n2023 \nBKGs vs. GPT-3.5/4 \nComparative QA study: \nLLMs vs. KGs \nGPT-3.5, GPT-4, \nKG reasoning \nAccuracy, Novelty, \nReasoning \nGPT-4 inconsistent in \nreasoning/citations; KG less fluent \nbut more reliable \n[13] \n2024 \nSPOKE KG + \nLLaMA2, GPT-3.5, \nGPT-4 \nKG-optimized retrieval for \nLLMs (RAG) \nOther retrieval \nmethods \nAccuracy, token \nreduction >50% \nFocus on retrieval optimization, \nnot KG construction \n[23] \n2024 \nBenchmark KB datasets \nGenerate-then-retrieve \n(LLM ‚Üí logical form ‚Üí \nKB retrieval) \nPrior KBQA \nmethods \nAccuracy (highest to \ndate) \nRisk of error in logical form \ngeneration \n[6] \n2024 \nPrimeKG \nLLM + KG hybrid, Cypher \nquery validation, RAG \nGPT-4 Turbo, \nLLaMA 3 \nAccuracy, \nExplainability \nDependent on KG coverage; \ncomputationally intensive \n[22] \n2025 \nPan-cancer KG (PcQA \ndataset) \nKG-enhanced reasoning \n(subgraph selection) \nSOTA KGQA \nmethods \nOutperformed SOTA on \nPcQA \nLimited to pan-cancer focus; no \nfine-tuning explored \n[26] \n2025 \nDynamic Medical KG + \nNeo4j \nLLM agents + multi-hop \nreasoning \nMEDQA, \nMedMCQA \nbaselines \nAccuracy, \nInterpretability \nHigh system complexity; requires \ncontinuous updating \n[24] \n2025 \nSynthCypher dataset \n(Neo4j + synthetic \nqueries) \nLLM-supervised Cypher \ngeneration and verification \nSPIDER \nbenchmark \nCypher accuracy 40% \nSynthetic dataset may not capture \nall real-world cases \n[29] \n2025 \nUMLS KG + Neo4j \nHybrid GraphRAG \nLLM-only QA \nAccuracy, Citation \nfidelity \nMore complex pipeline; relies on \nexternal vector store \n[30] \n2025 \nClinical data (10k \npatients, 1,894 nodes, \nNeo4j) \nKG construction using \nGPT-4 for entity/relation \nextraction \nOther KG \nconstruction \nmethods \nHigh F1-scores \nFocus only on sepsis; limited \ngeneralization \n[20] \n2025 \nLocal KG from \nbiomedical documents \nMulti-level summarization \nover KG for QA \nRAG baselines \nQA accuracy, reasoning \nTested mainly on document QA; \nscalability not proven \n \nB. Neo4j Graph Database \nTo manage the complexity and large size of biomedical \nknowledge graphs, specialized graph databases are needed. \nNeo4j is one of the most popular graph databases designed to \nstore and query data structured as nodes (entities) and \nrelationships (edges), both of which can have descriptive \nproperties [32, 33]. It uses the property graph model, which \nmakes it easy to represent complex, connected biomedical data \nsuch as drug-gene interactions or disease pathways. Neo4j‚Äôs \nCypher query language is especially advantageous because it \nallows users to write expressive and efficient queries to explore \nmulti-step connections in the data [34]. \nNeo4j works well for biomedical data because it can quickly \nrun complicated queries over highly interconnected datasets. This \nis important in biology and medicine, where relationships \nbetween entities like proteins, diseases, and drugs are often \ncomplex and layered. Studies have shown that Neo4j handles \nlarge biomedical graphs efficiently, making it a favorite among \nresearchers and industry users alike [33, 35, 36]. Its indexing and \ncaching mechanisms also help speed up query processing and \ndata retrieval [37]. \nMoreover, \nNeo4j \nintegrates \nsmoothly \nwith \nmany \nprogramming languages and analytics tools, which makes it \neasier to build interactive biomedical applications and clinical \ndecision support systems that can turn complex graph data into \nuseful insights [38, 39]. \nC. Large Language Models (LLMs) in Biomedical Question \nAnswering \nLarge Language Models (LLMs) are powerful AI systems \ntrained on vast amounts of text data. They learn the structure and \npatterns of language, enabling them to understand questions, \ngenerate responses, summarize information, and perform other \ncomplex language tasks. Well-known models such as LLaMA \nand GPT-3 have greatly advanced the field of natural language \nprocessing by showing strong performance across many tasks \n[40, 41].'
 'Omar & Mohammed / Journal of Applied Science and Technology Trends Vol. 06, No. 02, pp. 342 ‚Äì357 (2025) \n \n350 \ndocumentation, indexes ‚Äúenable quicker and more efficient \npattern matching‚Äù by allowing the query planner to rapidly locate \nnodes by label and property. \nWith the schema in place, data was imported using Cypher‚Äôs \nLOAD CSV commands. For each vocabulary file, rows with \nnonnull primary IDs were processed: the code used MERGE to \ncreate (or match existing) nodes with the given label and primary \nproperty, and then SET the remaining attributes from the CSV \ncolumns. For example, in importing disease_vocab.csv, each row \nproduced a node (:Disease primary: <id>) with properties name, \ndo_id, kegg_id, and umls_cui set from the row (empty strings \nwere turned into null). Similarly, drug_vocab.csv rows produced \ndrug nodes with properties for DrugBank, KEGG, PharmGKB \nIDs, etc. This approach follows best practice: using MERGE on \nthe unique ID ensures that nodes are not duplicated during \nmultiple passes. \nAfter all nodes were created, the relationship CSVs were \nloaded. Each row in those files was matched to the corresponding \nsource and target nodes by their primary IDs, and conditional \nlogic was used to create edges. For example, the disease-\nsymptom file (Di_Sy_res.csv) was processed by matching a \ndisease node and a symptom node for each row, then executing \nMERGE (d)-[r:HAS_SYMPTOM]-(s) if the present column is \nnonzero; the edge was given a present property and a source \nproperty from the CSV. The disease-disease file (di_di_res.csv) \nwas processed by matching disease nodes d1 and d2:If is_a = 1, \na (:Disease)-[:IS_A]->(:Disease) edge was merged; if Resemble \n= 1, a (:Disease)-[:RESEMBLES]->(:Disease) edge was merged. \nDrug-disease relationships were handled similarly: the script \nmatched a Drug node and a Disease node for each row of \nD_Di_res.csv, then for each flag that was 1, it merged the \ncorresponding edge label (such as TREATS, PALLIATES, \nASSOCIATED_WITH, etc.) from the drug to the disease. Drug‚Äì\nside-effect rows produced (:Drug)-[:CAUSES]->(:SideEffect) \nedges with the source noted, and drug‚Äìdrug rows produced either \nINTERACTS_WITH or RESEMBLES edges between matched \ndrug pairs. Finally, the drug-pathway and disease-pathway files \neach produced:ASSOCIATED_WITH edges linking drugs or \ndiseases to pathway nodes. \nIn this graph model, most relationships are directional (for \nexample, a drug TREATS a disease; a disease does not TREAT \na drug). This follows common practice in biomedical KGs. The \nsame relations (such as RESEMBLES or INTERACTS_WITH) \nare inherently symmetric, but were stored as two directed edges \nor one undirected edge depending on implementation. All \nrelationship creation steps used Cypher‚Äôs MERGE so that \nrepeated loads or out-of-order imports did not create duplicate \nedges. \nThis study used a static KG snapshot for reproducibility, but \nthe system also supports incremental updates through the Neo4j \nMERGE and batch import functions. A summary of the different \ntypes of nodes and relationships is provided, together with their \ncounts, descriptions, and examples in Table III.\n \nTABLE III.  DETAILED SUMMARY OF KNOWLEDGE GRAPH COMPONENTS IN IBKH SUBSET \nComponent Type \nEntity/Relationship \nCount \nDescription \nExamples \n \nDisease \n19,236 \nMedical conditions and disorders \nDiabetes, Hypertension, Cancer \n \nDrug \n37,997 \nPharmaceutical compounds \nAspirin, Insulin, Amoxicillin \nNodes \nSymptom \n1,361 \nClinical signs and symptoms \nHeadache, Fever, Nausea \n \nSideEffect \n4,251 \nNegative outcomes of drugs \nNausea, Drowsiness \n \nPathway \n2,983 \nBiological processes and pathways \nApoptosis, Glycolysis \n \nASSOCIATED_WITH \n101,534 \nGeneral associations (disease‚Äì\npathway, drug‚Äìpathway, etc.) \nDiabetes ASSOCIATED_WITH Pathway \n \nCAUSES \n145,321 \nDrug-SideEffect relationships \nAspirin CAUSES Gastric_Bleeding \n \nHAS_SYMPTOM \n3,357 \nDisease-symptom links \nCOVID-19 HAS_SYMPTOM Fever \n \nHAS_BIOMARKER \n412 \nBiomarkers linked to disease \nPSA HAS_BIOMARKER Prostate_Cancer \nRelationships \nINHIBITS_CELL_GROWTH \n1,508 \nDrugs inhibiting cell growth \nChemo INHIBITS_CELL_GROWTH Tumor \n \nINTERACTS_WITH \n2,682,142 \nDrug-drug interactions \nAspirin INTERACTS_WITH Warfarin \n \nIS_A \n10,529 \nSubtype hierarchy \nFlu IS_A Viral_Infection \n \nPALLIATES \n388 \nDrug palliates disease \nMorphine PALLIATES Cancer \n \nPREVENTS_SUPPRESSES \n859 \nPreventive links \nVaccine PREVENTS_SUPPRESSES Measles \n \nRESEMBLES \n7,000 \nSimilarity relationships \nDrugA RESEMBLES DrugB \n \nTREATMENT_THERAPY \n44,852 \nTherapy relationships \nRadiotherapy TREATMENT_THERAPY Tumor \n \nTREATS \n5,491 \nDrug-disease links \nInsulin TREATS Diabetes \n \nALLEVIATES_REDUCES \n‚àº180,000 \nSymptom relief \nParacetamol ALLEVIATES_REDUCES Fever \nTotal \nNodes \n65,828 \nTotal biomedical entities \n‚Äî \n \nRelationships \n3,004,166 \nTotal knowledge graph links \n‚Äî \nC. Natural Language to Cypher Query Translation \nA key feature of the system is its ability to accept questions \nwritten in plain English and automatically generate the \ncorresponding Cypher queries. This is accomplished using \nMeta‚Äôs LLaMA 3 large language model, which runs entirely on \na local machine through the open-source llama.cpp framework. \nRunning the model locally ensures low-latency execution and \nkeeps sensitive queries within the user‚Äôs environment. \nTo generate a Cypher query, LLaMA 3 is prompted with \nexamples of natural language questions along with their correct \nCypher translations. The prompt also includes instructions on \nhow to navigate the structure of the graph schema. When a user \nenters a question (e.g., ‚ÄôWhat are the symptoms of Alzheimer‚Äôs \ndisease?‚Äô), the system inserts it into the prompt and asks LLaMA \n3 to produce a corresponding query. For example, the model may \nreturn: \nMATCH (d:Disease)-[:HAS_SYMPTOM]->(s:Symptom) \nWHERE toLower(d.name) CONTAINS ""alzheimer"" \nRETURN s.name'
 ""integrates over 40 publicly available biomedical knowledge \nsources across separate domains, such as genes, proteins, \ndrugs, compounds, and diseases, along with their known \nrelationships.32 Two studies used textbooks, such as Harri\xad\nson's Principles of Internal Medicine, while 3 others utilized \nelectronic health record (EHR) data. Additionally, Zakka \net al. added over 500 markdown files from MDCalc to \nimprove clinical calculation capabilities in LLM.28 Two stud\xad\nies employed real-time online browsing to search academic \nsites, such as PubMed and UpToDate. The amount of \nretrieval resources varied across studies, ranging from a small \ndataset specific to 6 osteoarthritis guidelines to a large dataset \nof EHR data from 7 hospitals.\nRetrieval strategies\nIdentified retrieval strategies were grouped based on the RAG \nstages: pre-retrieval, retrieval, and post-retrieval. Figure 3 \npresents an example of how RAG is applied and lists identi\xad\nfied strategies within each stage.\nIn the pre-retrieval stage, 50% of studies (n ¬º 10) reported \nstrategies, such as query rewriting, document preprocessing, \nand assessing the necessity of retrieval. Zakka et al. simplified \nqueries by rephrasing text into search terms that are better \nsuited for website browsing,28 while Wang et al. focused on \ntechniques such as correcting errors, expanding abbrevia\xad\ntions, and matching synonyms in user queries.30 Soman et al. \nextracted disease entities in queries and retrieved correspond\xad\ning nodes from a knowledge graph.33 Document preprocess\xad\ning involved removing non-textual elements from PMC \npapers (eg, figures, references, and author disclosures),30\nextracted tables from PDFs using pdfplumber, structured the \ncontent with pydantic for seamless integration.25 In addition \nto query modification and document preprocessing, Jeong \net al. fine-tuned a model to determine whether retrieval was \nnecessary for a given query.34\nDuring the data retrieval stage, 85% of studies (n ¬º 17) \nreported strategies regarding indexing, aligning queries with \ndocuments, and ranking retrieval chunks. Chunking methods \nranged from fixed-size chunks35 to recursive splits.36 Embed\xad\nding models such as Text-embedding-ada-002,24,28‚Äì30,36,37\nMiniLM, and PubMedBERT33 were commonly used to con\xad\nvert sentences into vectors. Cosine similarity was the primary \nmetric for measuring query-document alignment. Two stud\xad\nies adopted Maximal Marginal Relevance for search and \nhighlighted its improved performance over similarity-based \nmethods.24,35 A domain-specific retriever, MedCPT, was \nused in one study.34 Another study used the multi-vector \nretriever that leveraged summarized document sections to \nidentify the original content for final answer generation.25\nThe retrieval cutoff parameters varied widely, with probabil\xad\nity thresholds up to 0.83 and the number of retrieved chunks \nranging from 3 to 90.28,36,38 Vector databases like FAISS and \nChroma were frequently reported, and LangChain was \nwidely used for document processing and retrieval.23,25,35,38\nIn the subgroup analysis, 12 studies used simple data retrieval \nstrategies (OR 1.30, 95% CI [1.16, 1.45]), while 5 studies \nused complex data retrieval strategies (OR 1.30, 95% CI \n[1.07, 1.24]), with no statistically significant difference \nobserved between the 2 approaches.\nIn the post-retrieval stage, 65% of studies (n ¬º 13) imple\xad\nmented specific strategies to refine outputs. Murugan et al. \ntailored prompts by providing clear context, defining roles \n(eg, distinguishing between healthcare providers and patients \nto deliver appropriately detailed information), and incorpo\xad\nrating relevant citations from retrieval sources such as the \nClinical \nPharmacogenetics \nImplementation \nConsortium \nguidelines and Food and Drug Administration (FDA) label\xad\ning.24 Soman et al. utilized prompt engineering to integrate \naccurate knowledge sources and statistical evidence, such as \nP-values and z-scores, from the SPOKE knowledge graph \ninto their outputs.33 Wang et al. outlined a detailed process \nin the post-retrieval stage using prompt engineering, which \ninvolved decomposing retrieved text into individual claims, \nverifying each claim with external knowledge sources, \nFigure 3. An example of using RAG in clinical applications, with identified strategies in 3 stages: pre-retrieval, retrieval, and post-retrieval. The user query \nseeks recommended medications for symptomatic peripheral artery disease without increased bleeding risk. The system retrieves evidence from clinical \nguidelines, processes it through embeddings and a vector database, and outputs a response, including rivaroxaban with low-dose aspirin, as recommended \nby the retrieved guideline information. In this example, GPT-4 suggested a dose of 75 mg Aspirin, but the common low-dose Aspirin is 81 mg.\nJournal of the American Medical Informatics Association, 2025, Vol. 32, No. 4                                                                                                      \n609 \nDownloaded from https://academic.oup.com/jamia/article/32/4/605/7954485 by guest on 11 January 2026""
 'Omar & Mohammed / Journal of Applied Science and Technology Trends Vol. 06, No. 02, pp. 342 ‚Äì357 (2025) \n \n345 \nrecord-high accuracy, while open-source models achieved \nimpressive gains through prompt optimization. \nFeng et al.[22] developed the Knowledge Graph-based \nThought (KGT) framework that integrated LLMs with a pan-\ncancer knowledge graph for biomedical question answering. \nKGT was designed to reason on the knowledge graph schema and \nidentify optimal subgraphs to use for directing accurate answer \ngeneration, all without fine-tuning the LLMs. The framework is \nbenchmarked against a new dataset (PcQA) designed specifically \nfor pan-cancer KGQA tasks and has outperformed all existing \nstate-of-the-art approaches by a rather large margin. KGT‚Äôs \npracticality in biomedical issues was highlighted through case \nstudies for drug repositioning, drug resistance, and biomarker \ndiscovery. Their approach exhibited robust adaptability among \nvarious LLMs. \nRezaei et al.[26] developed AMG-RAG, a dynamic \nframework that utilizes autonomous LLM agents with medical \nsearch tools in the continuous construction and real-time updating \nof Medical Knowledge Graphs (MKGs). Their system \nincorporated confidence scoring and multi-hop reasoning to \nimprove accuracy and interpretability in medical question \nanswering. AMG-RAG outperformed size models on both very \nhard MEDQA benchmarks and more accessible MedMCQA \nones, proving that it could conduct efficient reasoning based on \ncurrent structured medical knowledge. They also used Neo4j to \nmanage the knowledge graphs while adding external searches to \nensure the latest data. \nTiwari et al.[24] presented Auto-Cypher, a recent automated \npipeline for producing high-quality synthetic data for training \nLLMs by mapping natural language to Cypher queries for graph \ndatabases like Neo4j. The pipeline deployed the novelty of LLM-\nas-database-filler to synthesize Neo4j databases for the execution \nof generated queries to ensure their correctness. A sizable dataset \ncalled SynthCypher was created, spanning multiple domains and \ncomplex queries, leading to a 40% improvement in LLM \nperformance on Cypher generation. The datasets were used to \nfine-tune open-source models such as LLaMA, Mistral, and \nQwen, and the SPIDER benchmark was adapted for evaluation \npurposes. \nMohammed et al.[29] proposed a hybridized GraphRAG \nframework combining Neo4j-based UMLS knowledge graphs \nwith a vector store for medical textbooks to create an improved \nU.S.M.L.E.-style clinical question-answering approach. The \nproject integrated symbolic reasoning from knowledge graphs \nwith semantic retrieval performed on text embeddings to enhance \nrelevance and accuracy via adaptive re-ranking and query \nexpansion. The system had the answers produced by GPT-4o-\nMini, with different prompting strategies encouraging evidence-\nbased and traceable responses grounded in verified medical \nknowledge. Experiments showed that the hybrid approach \nimproved factual accuracy and citation fidelity as compared to \nthe L.L.M.-only approach, enhancing transparency and \nreliability. It is shown that binding both structured and \nunstructured medical knowledge sources could aid in \nameliorating hallucinations and hence improve clinical \ntrustworthiness in AI-driven medical QA. \nYang et al.[30] articulated sepsis knowledge graph was \ncrafted by combining multicenter clinical data from over 10,000 \npatients with the help of GPT-4 for entity recognition and \nrelationship extraction. Real-world data were collected from \nthree hospitals and integrated with clinical guidelines and \ndatabases from the public domain. The knowledge graph \ncontained 1,894 nodes and 2,021 relationships pertaining to \ndiseases, symptoms, biomarkers, treatments, and complications. \nGPT outperformed other models in every resolution on sepsis-\nspecific datasets to obtain high F1-score results. The constructed \ngraph highlighted complex interactions in sepsis for assisting \nclinical decision-making and was implemented on Neo4j. \nGuan et al.[20] proposed a novel method for constructing a \nlocal knowledge graph from retrieved biomedical documents by \nextracting propositional claims. They carried out layer wise \nsummarization on this graph to capture multi-document \nrelationships and provide comprehensive contextual information \nto a language model for question-answering purposes. The \nmethod resolved issues in multi-document biomedical QA, such \nas noise cancellation and efficient context usage. They then tested \ntheir method on several benchmarks for biomedical question \nanswering, achieving performance at least comparable to, if not \nbetter than, existing retrieval-augmented generation (RAG) \nbaselines. The study established enhanced reasoning and answer \naccuracy of the model achieved through structured graph \nsummarization.   \nPrevious studies have improved biomedical QA using KGs \nand LLMs, but important gaps remain. Most systems lack \ntransparent, graph-based justifications, rely on limited evaluation \nmethods, or depend on cloud resources that reduce privacy and \nreproducibility. Our framework addresses these gaps by \nproviding visible Cypher queries with evidence subgraphs, \napplying comprehensive performance metrics across difficulty \nlevels, and ensuring fully local, privacy-preserving deployment. \nTable I summarizes key previous studies on biomedical \nknowledge graphs and question answering, outlining their \nmethods, datasets, and main limitations. \nIII. PRELIMINARIES \nThis section outlines the fundamental concepts required to \nunderstand the proposed system. It introduces biomedical \nknowledge graphs, explains how Neo4j stores data in graph \nform, and describes the use of Cypher for querying. It also \nprovides a brief overview of large language models (LLMs) and \ntheir role in interpreting natural language. \n \n \nA. Biomedical Knowledge Graphs \nBiomedical Knowledge Graphs (BKGs) provide a structured \nrepresentation of complex biomedical information by modeling \ndiverse medical entities, such as diseases, drugs, symptoms, and \nbiological pathways, as interconnected nodes within a graph \nstructure. The edges in these graphs represent the semantic \nrelationships between these entities, including ‚Äôtreats‚Äô, ‚Äôcauses‚Äô, \n‚Äôinteracts with‚Äô and many others, as illustrated in Fig 1. This \nform of representation enables the integration of heterogeneous \nbiomedical data from a wide range of sources, including']"
How does the KGT framework leverage biomedical knowledge graphs to enhance large language models' reasoning capabilities and improve biomedical question answering performance?,"['<1-hop>\n\n8 | GigaScience , 2025, Vol. 14 Table 5. Ablation study of the KGT fr ame work under Code-Llama-13B ROUGE (%) Method GPT-4 Eval (%) BERTScore (%) Recall Precision F1 score KGT (ours) 92.4 97.7 87.4 87.7 86.8 Without GSBI 71.8 95.5 68.1 69.8 66.8 Without QA&GSBI 69.7 94.7 55.0 66.3 58.2 Without QA&GSBI&SC 24.7 77.4 14.8 12.3 12.2 compared to the complete KGT, indicating that each of the 3 mod- ules is vital for the final performance . Furthermore , such obser- vations confirm that our KGT can indeed le v er a ge knowledge to enhance the final performance of LLMs. Implementation settings Our knowledge gr a ph is quite large, with a complex schema, and typicall y involv es input tokens within 1,300. Our experiment does not r equir e fine-tuning, and the infer ence time is r elated to the model size and computational resources. For example, when us- ing our method, KGT, with the Code-Llama-13B model on an 80 GB A100 GPU, it occupies 33 GB of VRAM. Without any acceleration fr ame works, the infer ence r equir es 4 passes, eac h taking ar ound 20 seconds. Case studies Drug repositioning Drug r epositioning emer ges as a pr omising str ategy to acceler ate the process of drug development. This approach involves identi- fying new therapeutic uses for existing drugs, thereby saving time and r esources typicall y r equir ed for bringing a ne w drug to market [ 45 ]. Our system is capable of investigating the potential reposi- tioning of carteolol for the treatment of hemangiomas . T he exam- ple is shown in Supplementary Table S2 and r elational dia gr am is shown in Fig. 4 A. Utilizing the system‚Äôs knowledge gr a ph, a r ela- tional chain is delineated, illustrating that propranolol, another inhibitor of ADRB1, is effectiv el y emplo y ed in the treatment of hemangiomas . T he system harnesses this insight to formulate a hypothesis that carteolol, by virtue of its similar mechanism of inhibition, could be potentially repositioning for treating heman- giomas [ 46 ]. This hypothesis would serve as a precursor to clin- ical trials and r esearc h, potentiall y expediting the availability of an additional ther a peutic option for patients with hemangiomas. Drug resistance resear c h Drug resistance in cancer treatment poses a significant challenge in clinical oncology. Understanding the genetic basis of resistance can lead to mor e effectiv e tr eatment str ategies and personalized medicine a ppr oac hes. Researc h in drug r esistance involv es deter- mining why certain cancer-carrying mutated genes are not re- sponsive to specific drugs and finding ways to overcome this re- sistance [ 47 ]. Our system is capable of exploring drug resistance in cancer. The example is shown in Supplementary Table S3 , and a relational diagram is shown in Fig. 4 B. The KG data indicate that the ALK-p.L1196M m utation, whic h is associated with gas- tric cancer, has a known resistance to nalatinib [ 48 , 49 ]. The LLM processes this information and infers that due to this resistance, nalatinib might not be an effective medication for treating can- cers caused by the ALK-p.L1196M mutation. The case highlights the critical importance of understanding specific gene‚Äìdrug in- teractions in drug resistance research. It demonstrates how cer- tain gene mutations could render a drug ineffective, which in turn could guide oncologists in choosing alternative treatments or de- v eloping ne w drugs that can bypass or tar get the r esistance mec h- anisms. By accelerating the process of understanding drug resis- tance, these artificial intelligence‚Äìdriven systems can contribute to impr ov ed patient outcomes and the optimization of cancer tr eatment pr otocols.'
 '<2-hop>\n\nIndividualized treatment Details on individualized tr eatment ar e pr ovided in Supplementary Case Studies A . It is important to note that this example is included solely to illustrate the technical capabilities of the proposed method. The output generated in this example has not been validated for clinical use, and further validation in clinical settings would be r equir ed befor e an y suc h a pplication. Selection and understanding of biomarkers Details on selection and understanding of biomarkers are pro- vided in Supplementary Case Studies B . Discussion In this article, we introduce a novel framework KGT, which em- ploys LLMs for reasoning on the schema of KGs, to enhance the reasoning abilities of LLMs in areas with missing domain data by utilizing domain-specific knowledge gr a phs, suc h as oncology knowledge gr a phs, ther eby addr essing the issue of factual hal- lucinations in LLMs. Our method excels in extr acting, v alidat- ing, and refining factual knowledge throughout the LLMs‚Äô rea- soning process. It seamlessly integrates with various LLMs, in- cluding open-source models like Code-Llama, and enhances the capabilities of LLMs solely through prompt engineering and in- context learning without any fine-tuning. This grants it significant generalizability. We possess an extensive oncology knowledge graph and have established a benchmark based on it to e v aluate the ca pabilities of various methods. When tested on PcQA using various open-source LLMs, the KGT fr ame work performs exceptionall y well, sur passing the current best methods by 33%. This significant improvement positions our a ppr oac h as a pioneering benchmark in biomedical KGQA, setting a new standard that advances beyond previously established best pr actices. Additionall y, thr ough case studies, our a ppr oac h has been shown to effectiv el y pr ovide ther a peutic plans, gener ate v aluable hypotheses for drug r epositioning, identify po- tential drug targets, and study drug resistance . T his underscores the pr actical v alue of the KGT fr ame work in deliv ering insight- ful contributions that aid in the de v elopment and optimization of treatment strategies. Each case study‚Äôs conclusions are further v alidated by e vidence fr om pr e viousl y published r esearc h pa pers, enhancing the credibility and impact of our findings. Ho w e v er, it is important to note that the constructed QA dataset and the corresponding published subset of the SOKG Downloaded from https://academic.oup.com/gigascience/article/doi/10.1093/gigascience/giae082/7943459 by guest on 11 January 2026'
 '<3-hop>\n\nOmar & Mohammed / Journal of Applied Science and Technology Trends Vol. 06, No. 02, pp. 342 ‚Äì357 (2025) 345 record-high accuracy, while open-source models achieved impressive gains through prompt optimization. Feng et al.[22] developed the Knowledge Graph-based Thought (KGT) framework that integrated LLMs with a pan- cancer knowledge graph for biomedical question answering. KGT was designed to reason on the knowledge graph schema and identify optimal subgraphs to use for directing accurate answer generation, all without fine-tuning the LLMs. The framework is benchmarked against a new dataset (PcQA) designed specifically for pan-cancer KGQA tasks and has outperformed all existing state-of-the-art approaches by a rather large margin. KGT‚Äôs practicality in biomedical issues was highlighted through case studies for drug repositioning, drug resistance, and biomarker discovery. Their approach exhibited robust adaptability among various LLMs. Rezaei et al.[26] developed AMG-RAG, a dynamic framework that utilizes autonomous LLM agents with medical search tools in the continuous construction and real-time updating of Medical Knowledge Graphs (MKGs). Their system incorporated confidence scoring and multi-hop reasoning to improve accuracy and interpretability in medical question answering. AMG-RAG outperformed size models on both very hard MEDQA benchmarks and more accessible MedMCQA ones, proving that it could conduct efficient reasoning based on current structured medical knowledge. They also used Neo4j to manage the knowledge graphs while adding external searches to ensure the latest data. Tiwari et al.[24] presented Auto-Cypher, a recent automated pipeline for producing high-quality synthetic data for training LLMs by mapping natural language to Cypher queries for graph databases like Neo4j. The pipeline deployed the novelty of LLM- as-database-filler to synthesize Neo4j databases for the execution of generated queries to ensure their correctness. A sizable dataset called SynthCypher was created, spanning multiple domains and complex queries, leading to a 40% improvement in LLM performance on Cypher generation. The datasets were used to fine-tune open-source models such as LLaMA, Mistral, and Qwen, and the SPIDER benchmark was adapted for evaluation purposes. Mohammed et al.[29] proposed a hybridized GraphRAG framework combining Neo4j-based UMLS knowledge graphs with a vector store for medical textbooks to create an improved U.S.M.L.E.-style clinical question-answering approach. The project integrated symbolic reasoning from knowledge graphs with semantic retrieval performed on text embeddings to enhance relevance and accuracy via adaptive re-ranking and query expansion. The system had the answers produced by GPT-4o- Mini, with different prompting strategies encouraging evidence- based and traceable responses grounded in verified medical knowledge. Experiments showed that the hybrid approach improved factual accuracy and citation fidelity as compared to the L.L.M.-only approach, enhancing transparency and reliability. It is shown that binding both structured and unstructured medical knowledge sources could aid in ameliorating hallucinations and hence improve clinical trustworthiness in AI-driven medical QA. Yang et al.[30] articulated sepsis knowledge graph was crafted by combining multicenter clinical data from over 10,000 patients with the help of GPT-4 for entity recognition and relationship extraction. Real-world data were collected from three hospitals and integrated with clinical guidelines and databases from the public domain. The knowledge graph contained 1,894 nodes and 2,021 relationships pertaining to diseases, symptoms, biomarkers, treatments, and complications. GPT outperformed other models in every resolution on sepsis- specific datasets to obtain high F1-score results. The constructed graph highlighted complex interactions in sepsis for assisting clinical decision-making and was implemented on Neo4j. Guan et al.[20] proposed a novel method for constructing a local knowledge graph from retrieved biomedical documents by extracting propositional claims. They carried out layer wise summarization on this graph to capture multi-document relationships and provide comprehensive contextual information to a language model for question-answering purposes. The method resolved issues in multi-document biomedical QA, such as noise cancellation and efficient context usage. They then tested their method on several benchmarks for biomedical question answering, achieving performance at least comparable to, if not better than, existing retrieval-augmented generation (RAG) baselines. The study established enhanced reasoning and answer accuracy of the model achieved through structured graph summarization. Previous studies have improved biomedical QA using KGs and LLMs, but important gaps remain. Most systems lack transparent, graph-based justifications, rely on limited evaluation methods, or depend on cloud resources that reduce privacy and reproducibility. Our framework addresses these gaps by providing visible Cypher queries with evidence subgraphs, applying comprehensive performance metrics across difficulty levels, and ensuring fully local, privacy-preserving deployment. Table I summarizes key previous studies on biomedical knowledge graphs and question answering, outlining their methods, datasets, and main limitations.'
 '<4-hop>\n\nIII. PRELIMINARIES This section outlines the fundamental concepts required to understand the proposed system. It introduces biomedical knowledge graphs, explains how Neo4j stores data in graph form, and describes the use of Cypher for querying. It also provides a brief overview of large language models (LLMs) and their role in interpreting natural language. A. Biomedical Knowledge Graphs Biomedical Knowledge Graphs (BKGs) provide a structured representation of complex biomedical information by modeling diverse medical entities, such as diseases, drugs, symptoms, and biological pathways, as interconnected nodes within a graph structure. The edges in these graphs represent the semantic relationships between these entities, including ‚Äôtreats‚Äô, ‚Äôcauses‚Äô, ‚Äôinteracts with‚Äô and many others, as illustrated in Fig 1. This form of representation enables the integration of heterogeneous biomedical data from a wide range of sources, including']","The KGT framework enhances large language models' (LLMs) reasoning capabilities by employing biomedical knowledge graphs (KGs) to provide structured domain-specific knowledge. It reasons on the schema of these KGs to identify optimal subgraphs that guide accurate answer generation without requiring fine-tuning of the LLMs. This approach addresses the issue of factual hallucinations by extracting, validating, and refining factual knowledge during the reasoning process. The framework integrates seamlessly with various LLMs, including open-source models like Code-Llama, and improves performance solely through prompt engineering and in-context learning. Evaluations on a pan-cancer question answering benchmark (PcQA) demonstrate that KGT surpasses existing state-of-the-art methods by 33%, establishing a new standard in biomedical KGQA. Additionally, case studies show KGT's practical value in generating therapeutic plans, drug repositioning hypotheses, identifying drug resistance mechanisms, and biomarker discovery, all supported by evidence from prior research. Thus, by combining LLMs with biomedical knowledge graphs, KGT significantly improves the accuracy, interpretability, and efficiency of biomedical question answering.",multi_hop_abstract_query_synthesizer,"The KGT (Knowledge Graph-based Thought) framework enhances large language models' (LLMs) reasoning capabilities and improves biomedical question answering performance by tightly integrating LLMs with biomedical knowledge graphs (KGs), specifically a pan-cancer knowledge graph. The key ways KGT leverages biomedical knowledge graphs are as follows:

1. **Reasoning on Knowledge Graph Schema and Subgraph Selection**:  
   KGT is designed to reason explicitly over the structure of the biomedical knowledge graph. It identifies optimal subgraphs within the larger KG that are most relevant to the question at hand. This targeted subgraph selection guides the LLM to generate more accurate and factually grounded answers without requiring fine-tuning of the LLM itself.

2. **Use of Verifiable and Structured Knowledge**:  
   By grounding the LLM‚Äôs responses in the structured, verified information contained in the biomedical KG, KGT significantly reduces factual hallucinations common in LLM-only approaches. The KG provides a reliable source of multi-hop biomedical relationships (e.g., drug‚Äìcancer associations, biomarkers, genetic mechanisms) that the LLM can leverage for reasoning.

3. **Flexible Integration with Various LLMs**:  
   The framework is architected to be adaptable and compatible with multiple open-source LLMs, enhancing its utility and robustness across different model backbones.

4. **Benchmarking with a Domain-Specific Dataset (PcQA)**:  
   KGT was evaluated on a newly developed pan-cancer question answering dataset (PcQA), demonstrating superior performance over existing state-of-the-art KGQA methods. This shows that the framework effectively improves answer accuracy and reasoning in complex biomedical domains.

5. **Practical Biomedical Applications**:  
   Case studies in drug repositioning, drug resistance prediction, and biomarker discovery illustrate KGT‚Äôs ability to support real-world biomedical challenges by uncovering novel insights through KG-enhanced reasoning.

6. **No Fine-Tuning Required**:  
   Unlike some approaches that require extensive fine-tuning of LLMs on domain-specific data, KGT achieves improved reasoning and accuracy by leveraging the KG structure and subgraph retrieval, making it more efficient and easier to deploy.

In summary, the KGT framework enhances LLM reasoning and biomedical question answering by combining the natural language understanding power of LLMs with the structured, multi-relational, and verifiable knowledge encoded in biomedical knowledge graphs. This integration enables accurate, interpretable, and scalable biomedical QA without the need for fine-tuning, while also supporting complex multi-hop reasoning tasks grounded in real biomedical data.","['Omar & Mohammed / Journal of Applied Science and Technology Trends Vol. 06, No. 02, pp. 342 ‚Äì357 (2025) \n \n345 \nrecord-high accuracy, while open-source models achieved \nimpressive gains through prompt optimization. \nFeng et al.[22] developed the Knowledge Graph-based \nThought (KGT) framework that integrated LLMs with a pan-\ncancer knowledge graph for biomedical question answering. \nKGT was designed to reason on the knowledge graph schema and \nidentify optimal subgraphs to use for directing accurate answer \ngeneration, all without fine-tuning the LLMs. The framework is \nbenchmarked against a new dataset (PcQA) designed specifically \nfor pan-cancer KGQA tasks and has outperformed all existing \nstate-of-the-art approaches by a rather large margin. KGT‚Äôs \npracticality in biomedical issues was highlighted through case \nstudies for drug repositioning, drug resistance, and biomarker \ndiscovery. Their approach exhibited robust adaptability among \nvarious LLMs. \nRezaei et al.[26] developed AMG-RAG, a dynamic \nframework that utilizes autonomous LLM agents with medical \nsearch tools in the continuous construction and real-time updating \nof Medical Knowledge Graphs (MKGs). Their system \nincorporated confidence scoring and multi-hop reasoning to \nimprove accuracy and interpretability in medical question \nanswering. AMG-RAG outperformed size models on both very \nhard MEDQA benchmarks and more accessible MedMCQA \nones, proving that it could conduct efficient reasoning based on \ncurrent structured medical knowledge. They also used Neo4j to \nmanage the knowledge graphs while adding external searches to \nensure the latest data. \nTiwari et al.[24] presented Auto-Cypher, a recent automated \npipeline for producing high-quality synthetic data for training \nLLMs by mapping natural language to Cypher queries for graph \ndatabases like Neo4j. The pipeline deployed the novelty of LLM-\nas-database-filler to synthesize Neo4j databases for the execution \nof generated queries to ensure their correctness. A sizable dataset \ncalled SynthCypher was created, spanning multiple domains and \ncomplex queries, leading to a 40% improvement in LLM \nperformance on Cypher generation. The datasets were used to \nfine-tune open-source models such as LLaMA, Mistral, and \nQwen, and the SPIDER benchmark was adapted for evaluation \npurposes. \nMohammed et al.[29] proposed a hybridized GraphRAG \nframework combining Neo4j-based UMLS knowledge graphs \nwith a vector store for medical textbooks to create an improved \nU.S.M.L.E.-style clinical question-answering approach. The \nproject integrated symbolic reasoning from knowledge graphs \nwith semantic retrieval performed on text embeddings to enhance \nrelevance and accuracy via adaptive re-ranking and query \nexpansion. The system had the answers produced by GPT-4o-\nMini, with different prompting strategies encouraging evidence-\nbased and traceable responses grounded in verified medical \nknowledge. Experiments showed that the hybrid approach \nimproved factual accuracy and citation fidelity as compared to \nthe L.L.M.-only approach, enhancing transparency and \nreliability. It is shown that binding both structured and \nunstructured medical knowledge sources could aid in \nameliorating hallucinations and hence improve clinical \ntrustworthiness in AI-driven medical QA. \nYang et al.[30] articulated sepsis knowledge graph was \ncrafted by combining multicenter clinical data from over 10,000 \npatients with the help of GPT-4 for entity recognition and \nrelationship extraction. Real-world data were collected from \nthree hospitals and integrated with clinical guidelines and \ndatabases from the public domain. The knowledge graph \ncontained 1,894 nodes and 2,021 relationships pertaining to \ndiseases, symptoms, biomarkers, treatments, and complications. \nGPT outperformed other models in every resolution on sepsis-\nspecific datasets to obtain high F1-score results. The constructed \ngraph highlighted complex interactions in sepsis for assisting \nclinical decision-making and was implemented on Neo4j. \nGuan et al.[20] proposed a novel method for constructing a \nlocal knowledge graph from retrieved biomedical documents by \nextracting propositional claims. They carried out layer wise \nsummarization on this graph to capture multi-document \nrelationships and provide comprehensive contextual information \nto a language model for question-answering purposes. The \nmethod resolved issues in multi-document biomedical QA, such \nas noise cancellation and efficient context usage. They then tested \ntheir method on several benchmarks for biomedical question \nanswering, achieving performance at least comparable to, if not \nbetter than, existing retrieval-augmented generation (RAG) \nbaselines. The study established enhanced reasoning and answer \naccuracy of the model achieved through structured graph \nsummarization.   \nPrevious studies have improved biomedical QA using KGs \nand LLMs, but important gaps remain. Most systems lack \ntransparent, graph-based justifications, rely on limited evaluation \nmethods, or depend on cloud resources that reduce privacy and \nreproducibility. Our framework addresses these gaps by \nproviding visible Cypher queries with evidence subgraphs, \napplying comprehensive performance metrics across difficulty \nlevels, and ensuring fully local, privacy-preserving deployment. \nTable I summarizes key previous studies on biomedical \nknowledge graphs and question answering, outlining their \nmethods, datasets, and main limitations. \nIII. PRELIMINARIES \nThis section outlines the fundamental concepts required to \nunderstand the proposed system. It introduces biomedical \nknowledge graphs, explains how Neo4j stores data in graph \nform, and describes the use of Cypher for querying. It also \nprovides a brief overview of large language models (LLMs) and \ntheir role in interpreting natural language. \n \n \nA. Biomedical Knowledge Graphs \nBiomedical Knowledge Graphs (BKGs) provide a structured \nrepresentation of complex biomedical information by modeling \ndiverse medical entities, such as diseases, drugs, symptoms, and \nbiological pathways, as interconnected nodes within a graph \nstructure. The edges in these graphs represent the semantic \nrelationships between these entities, including ‚Äôtreats‚Äô, ‚Äôcauses‚Äô, \n‚Äôinteracts with‚Äô and many others, as illustrated in Fig 1. This \nform of representation enables the integration of heterogeneous \nbiomedical data from a wide range of sources, including'
 'Omar & Mohammed / Journal of Applied Science and Technology Trends Vol. 06, No. 02, pp. 342 ‚Äì357 (2025) \n \n344 \nevaluated using Exact Match, Precision, Recall, F1, Hits@k, \nMRR, and latency across simple, medium, and complex question \nsets. Unlike prior template-based methods, our approach enables \ntraceable, outcome-level validation. In summary, the main \ncontributions of this work are as follows: \n‚Ä¢ Hybrid LLM to Cypher QA: A system that translates \nnatural language questions into accurate, executable \nCypher over a Neo4j drug, disease, and symptom KG. \n‚Ä¢ Prompt-driven query generation: Schema,  entity \naware prompting that reliably maps diverse biomedical \nquestions to graph queries. \n‚Ä¢ Evidence transparency: Along with each answer, we \nsurface the generated Cypher and the supporting \njustification subgraph (nodes, edges) plus a brief natural \nlanguage rationale. \n‚Ä¢ Answer-level evaluation: End-to-end assessment using \nExact Match, F1, Precision/Recall, Hits@k, MRR and \nlatency across simple, medium and complex tiers. \n‚Ä¢ Local, reproducible deployment: On-prem LLaMA 3 \ninference (no cloud dependency) suitable for biomedical \nsettings requiring low latency and strong data control. \n \nThe remainder of this paper is organized as follows: Section \n2 reviews related work on biomedical knowledge graphs and \nLLM-based QA systems. Section 3 provides background on \nknowledge graphs, large language models, and question \nanswering frameworks. Section 4 details the proposed \nmethodology, \nincluding \nsystem \narchitecture, \ndataset \nconstruction, and query translation. Section 5 presents the \nexperimental results through both quantitative metrics and \nqualitative case studies. Section 6 discusses the findings, analyzes \nlimitations, and compares performance against baseline models. \nFinally, Section 7 concludes this paper and outlines directions for \nfuture work. \nII. RELATED WORK \nRecently, studies have concentrated on the integration of \nclinical and medical knowledge graphs (LLM) to improve the \nanswer to medical questions. Researchers have derived several \nbiomedical KGs using Neo4j and incorporated the application of \nLLMs like LLaMA and GPT to convert natural language \nquestions into graph queries. Improvements in answer \ncorrectness, reduction of hallucination errors, one-to-many \nrelationships, and support for complex reasoning were the \nobjectives of these efforts. Some frameworks also adopted \nretrieval methods to ground responses in secure data. \nSu et al.[11] developed an integrative Biomedical Knowledge \nHub (iBKH), a huge biomedical knowledge graph that comprises \n18 of the very best data sources. The deployment of the iBKH in \nNeo4j allows for a user-friendly web portal to allow fast and \ninteractive knowledge retrieval. The system implemented \nadvanced graph learning techniques to enable the discovery of \nbiomedical knowledge, illustrated by an example of repurposing \nin silico drugs for Alzheimer‚Äôs disease. iBKH achieved \npromising predictive performance for known drugs and proposed \npossible new drug candidates. \nRajabi and Kafaie[19] proposed a disease knowledge graph \nusing a cross-referential disease database comprising diseases, \nsymptoms, and drugs interconnected with relationships. They \ntransferred the data into Neo4j to create a graph of 9,400 nodes \nand 45,000 relationships representing the semantic links between \nmedical concepts. Applying Cypher queries enabled answering \ncomplex medical questions regarding identifying drugs that may \ncause certain diseases; it was demonstrated that the graph inferred \nnew relationships not explicitly existing in the original data. The \nconclusion was that disease knowledge graphs sped up clinical \ndiscovery and contributed to understanding complex medical \nrelationships. \nHou et al.[3] assessed and contrasted ChatGPT (both GPT-\n3.5 and GPT-4) and the biomedical knowledge graphs (BKGs) \nconcerning their ability to answer biomedical questions, generate \nnew knowledge, and reason. Their datasets were focused on \ndietary supplements and drugs, while evaluation criteria entailed \naccuracy, novelty, and reasoning ability. The results indicate that \nwhile GPT-4 surpassed GPT-3.5 and BKGs in knowledge \nprovision, it proved inconsistent with regard to citations and \nreasoning. Compared to them, BKGs scored higher in accuracy \nand reliability, especially in discovering novel links as well as \nwithin structured reasoning. \nSoman et al.[13] presented a novel framework called KG-\nRAG that integrates a large biomedical knowledge graph \n(SPOKE) with LLaMA 2, GPT-3.5, and GPT-4 (LLMs) to \nproduce accurate biomedical text. They optimized the retrieval of \nrelevant graph context to cut over 50% tokens without losing \naccuracy. It aided LLMs in performing better on biomedical \nquestion answering with very high accuracy boosts, especially in \nthe case of LLaMA 2. They compared KG-RAG to other retrieval \nmethods and indicated its comparatively more robust and \nefficient results. The framework produced reliable evidence-\nbased answers grounded in biomedical knowledge. \nLuo et al.[23] created ChatKBQA, a new framework with a \nquestion-and-answer approach over knowledge bases that first \ngenerates logical forms with the help of fine-tuned LLMs and \nthen retrieves the relevant entities and relations. This generate-\nthen-retrieve approach is supposed to handle a couple of issues \nwith the earlier methods concerning tedious retrieval and error \npropagation. They fine-tuned open-source LLMs like LLaMA 2 \nto change natural-language questions into logical forms with high \naccuracy. The retrieval phase uses unsupervised phrase-level \nsemantic matching in a way that enhances the alignment of \nentities and relations. Experiments on benchmark datasets \nindicate ChatKBQA to be superior to its predecessors, with the \nhighest accuracy to date. \nPusch and Conrad[6] conducted work under a hybrid \napproach conflating LLMs and biomedical Knowledge Graphs \n(KGs) to suppress hallucination errors in question-answering. \nThey proposed query-checking algorithms for validating, \ncorrecting, and executing the KG Cypher queries that LLMs \ngenerated, thereby attaining accurate and understandable \nanswers. The system used retrieval-augmented generation (RAG) \nto ground answers within KG data. The methodology was \nvalidated on a biomedical KG called PrimeKG using 50 \nbenchmark questions, assessing models like GPT-4 Turbo and \nLLaMA 3. Commercially available GPT-4 Turbo obtained'
 'Vol. 06, No. 02, pp. 342 ‚Äì357 (2025) \nISSN: 2708-0757 \n \nJOURNAL OF APPLIED SCIENCE AND TECHNOLOGY TRENDS \n \nwww.jastt.org  \n \n                                                          342 \ndoi: 10.38094/jastt62404  \nA Hybrid LLM‚ÄìKnowledge Graph Framework for Accurate \nBiomedical Question Answering \n \nHavraz Y. Omar1,¬≤*, Abdulhakeem O. Mohammed¬≥  \n \n¬πDepartment of Information Technology, Technical College of Duhok, Duhok Polytechnic University, Duhok, Kurdistan Region, \nIraq. havraz.omar@dpu.edu.krd  \n¬≤ Department of Information Technology, Technical College of Informatics ‚Äì Akre, Akre University for Applied Sciences, Akre, \nKurdistan Region, Iraq. \n¬≥ Department of Computer Science, College of Science, University of Zakho, Zakho, Kurdistan Region, Iraq. \na.mohammed@uoz.edu.krd \n \n*Correspondence: havraz.omar@dpu.edu.krd \n \nAbstract \nBiomedical question answering requires accurate and interpretable systems; however, existing approaches often face challenges such as \nlanguage model hallucinations and limited reasoning when relying solely on standalone knowledge graphs. To address these limitations, \nthis study proposes a hybrid framework that integrates the LLaMA-3B language model with a Neo4j-based drug‚Äìdisease‚Äìsymptom \nknowledge graph. The system translates natural language questions into executable Cypher queries, operates on an iBKH-derived graph \ncomprising over 65,000 entities and 3 million relationships, and returns answers with supporting evidence through a transparent interface. \nExperiments conducted on 60 biomedical questions across three levels of difficulty demonstrate the robustness of the approach: 96% \nexact match for simple queries, 95% for medium queries, and 86.7% for complex queries. Overall, the system achieves Precision@5 of \n96.1%, Recall@5 of 89.0%, F1@5 of 91.0%, Hits@k of 96.1%, and an MRR of 94.4%, while maintaining an average response time of \nonly 6.07 seconds. These results indicate that the system retrieves nearly all relevant answers, ranks them correctly, and delivers them \nwith latency low enough for interactive use. Moreover, unlike cloud-based APIs such as ChatGPT, which require internet connectivity \nand external data transmission, the proposed framework operates fully offline, ensuring privacy, reproducibility, and compliance with \nbiomedical data governance. Overall, this pipeline provides an accurate, efficient, and privacy-preserving solution for biomedical question \nanswering, making it a practical alternative to cloud-dependent approaches in sensitive healthcare contexts. \n \nKeywords: Knowledge Graph, LLM, Question Answering, Neo4j, Biomedical Informatics, Healthcare AI, LLaMA 3. \n \nReceived: August 14th, 2025 / Revised: October 10th, 2025 / Accepted: October 16th, 2025 / Online: October 20th, 2025 \n \nI. INTRODUCTION \nAnswering questions in the biomedical field is a difficult task \ndue to the complexity of medical knowledge and the need for \nprecision. In recent years, large language models  (LLMs) like \nLLaMA, GPT-4 have made progress in understanding and \ngenerating human-like responses to medical questions  [1,  2]. \nThese models can process large amounts of information and \nrespond in natural language, which makes them helpful in \nhealthcare settings [3]. However, they often struggle to provide \naccurate answers when dealing with specialized biomedical \ncontent [4, 5]. \nOne major issue with LLMs is a problem called hallucination, \nwhere the model generates information that sounds right but is \nactually incorrect or unsupported [6]. In medical applications, \nthis can be dangerous, as healthcare professionals rely on precise \nand trustworthy information  [7]. Therefore, researchers are \nexploring ways to combine LLMs with structured sources of \nknowledge to improve their reliability [8]. \nLLM-only systems in biomedicine still hallucinate and are \nhard to verify, limiting safe use [9, 10]. Biomedical knowledge \ngraphs (BKGs) such as iBKH and SPOKE curate multi-source \nfacts and enable multi-hop reasoning, yet they neither interpret \nfree text nor generate answers [11, 12]. Recent hybrids (KG-\naware RAG) improve grounding but often lack explicit path-level \njustifications and robust end-to-end answer evaluation [13, 14]. \nRecent studies have increasingly integrated Knowledge \nGraphs (KGs) with Large Language Models (LLMs) to improve \nfactual accuracy, reasoning, and reduce hallucinations. Notable \nexamples include DR.KNOWS, which combines UMLS-based \nKGs with LLMs for better diagnostic reasoning [15], KnowNet'
 'Omar & Mohammed / Journal of Applied Science and Technology Trends Vol. 06, No. 02, pp. 342 ‚Äì357 (2025) \n \n346 \nscientific literature, clinical records, genomic databases, and \nexperimental findings [19, 31]. \nSuch integration creates a comprehensive biomedical \nknowledge base that supports advanced analytics and discovery. \nFor example, biomedical knowledge graphs can reveal previously \nunknown relationships (e.g., between drugs and diseases) and \nhelp prioritize potential biomarkers for complex conditions. The \nIntegrative Biomedical Knowledge Hub (iBKH) is one such \nlarge-scale graph that consolidates diverse biomedical resources \ninto a unified hub, enabling discovery at scale [11]. Beyond \niBKH, large biomedical knowledge graphs such as SPOKE \nfurther illustrate how graph integration accelerates research and \nsupports precision-medicine use cases [12]. Overall, these graphs \nserve as foundational resources for data-driven and personalized \nmedicine. These knowledge graphs serve as foundational \nresources for precision medicine, where treatment can be tailored \nto the individual‚Äôs biological profile, improving outcomes and \nminimizing side effects [19, 31]. \n \nTABLE I.  SUMMARY OF RELATED RESEARCH ON BIOMEDICAL KGS AND QUESTION ANSWERING \nRef. \nYear \nData/Graph \nMethod \nBaselines \nKey Metric \nLimitation \n[11] \n2023 \niBKH (18 biomedical \nsources, Neo4j) \nIntegrative KG + Graph \nlearning; drug repurposing \ncase \nKnown drugs, \nAlzheimer‚Äôs \nstudy \nPredictive performance \n(drug repurposing) \nLimited to Alzheimer‚Äôs case \nstudy; scalability and updates not \ndetailed \n[19] \n2023 \nDisease KG (9,400 \nnodes, 45,000 relations \nin Neo4j) \nCypher queries for disease‚Äì\ndrug‚Äìsymptom reasoning \nCross-referential \ndisease DB \nNew relation inference; \ncomplex query \nanswering \nLimited to single domain; lacks \nlarge-scale evaluation \n[3] \n2023 \nBKGs vs. GPT-3.5/4 \nComparative QA study: \nLLMs vs. KGs \nGPT-3.5, GPT-4, \nKG reasoning \nAccuracy, Novelty, \nReasoning \nGPT-4 inconsistent in \nreasoning/citations; KG less fluent \nbut more reliable \n[13] \n2024 \nSPOKE KG + \nLLaMA2, GPT-3.5, \nGPT-4 \nKG-optimized retrieval for \nLLMs (RAG) \nOther retrieval \nmethods \nAccuracy, token \nreduction >50% \nFocus on retrieval optimization, \nnot KG construction \n[23] \n2024 \nBenchmark KB datasets \nGenerate-then-retrieve \n(LLM ‚Üí logical form ‚Üí \nKB retrieval) \nPrior KBQA \nmethods \nAccuracy (highest to \ndate) \nRisk of error in logical form \ngeneration \n[6] \n2024 \nPrimeKG \nLLM + KG hybrid, Cypher \nquery validation, RAG \nGPT-4 Turbo, \nLLaMA 3 \nAccuracy, \nExplainability \nDependent on KG coverage; \ncomputationally intensive \n[22] \n2025 \nPan-cancer KG (PcQA \ndataset) \nKG-enhanced reasoning \n(subgraph selection) \nSOTA KGQA \nmethods \nOutperformed SOTA on \nPcQA \nLimited to pan-cancer focus; no \nfine-tuning explored \n[26] \n2025 \nDynamic Medical KG + \nNeo4j \nLLM agents + multi-hop \nreasoning \nMEDQA, \nMedMCQA \nbaselines \nAccuracy, \nInterpretability \nHigh system complexity; requires \ncontinuous updating \n[24] \n2025 \nSynthCypher dataset \n(Neo4j + synthetic \nqueries) \nLLM-supervised Cypher \ngeneration and verification \nSPIDER \nbenchmark \nCypher accuracy 40% \nSynthetic dataset may not capture \nall real-world cases \n[29] \n2025 \nUMLS KG + Neo4j \nHybrid GraphRAG \nLLM-only QA \nAccuracy, Citation \nfidelity \nMore complex pipeline; relies on \nexternal vector store \n[30] \n2025 \nClinical data (10k \npatients, 1,894 nodes, \nNeo4j) \nKG construction using \nGPT-4 for entity/relation \nextraction \nOther KG \nconstruction \nmethods \nHigh F1-scores \nFocus only on sepsis; limited \ngeneralization \n[20] \n2025 \nLocal KG from \nbiomedical documents \nMulti-level summarization \nover KG for QA \nRAG baselines \nQA accuracy, reasoning \nTested mainly on document QA; \nscalability not proven \n \nB. Neo4j Graph Database \nTo manage the complexity and large size of biomedical \nknowledge graphs, specialized graph databases are needed. \nNeo4j is one of the most popular graph databases designed to \nstore and query data structured as nodes (entities) and \nrelationships (edges), both of which can have descriptive \nproperties [32, 33]. It uses the property graph model, which \nmakes it easy to represent complex, connected biomedical data \nsuch as drug-gene interactions or disease pathways. Neo4j‚Äôs \nCypher query language is especially advantageous because it \nallows users to write expressive and efficient queries to explore \nmulti-step connections in the data [34]. \nNeo4j works well for biomedical data because it can quickly \nrun complicated queries over highly interconnected datasets. This \nis important in biology and medicine, where relationships \nbetween entities like proteins, diseases, and drugs are often \ncomplex and layered. Studies have shown that Neo4j handles \nlarge biomedical graphs efficiently, making it a favorite among \nresearchers and industry users alike [33, 35, 36]. Its indexing and \ncaching mechanisms also help speed up query processing and \ndata retrieval [37]. \nMoreover, \nNeo4j \nintegrates \nsmoothly \nwith \nmany \nprogramming languages and analytics tools, which makes it \neasier to build interactive biomedical applications and clinical \ndecision support systems that can turn complex graph data into \nuseful insights [38, 39]. \nC. Large Language Models (LLMs) in Biomedical Question \nAnswering \nLarge Language Models (LLMs) are powerful AI systems \ntrained on vast amounts of text data. They learn the structure and \npatterns of language, enabling them to understand questions, \ngenerate responses, summarize information, and perform other \ncomplex language tasks. Well-known models such as LLaMA \nand GPT-3 have greatly advanced the field of natural language \nprocessing by showing strong performance across many tasks \n[40, 41].'
 'Geesa Daluwatumulle, Rupika Wijesinghe, and Ruvan\nWeerasinghe. 2023. In silico drug repurposing us-\ning knowledge graph embeddings for alzheimer‚Äôs\ndisease. In Proceedings of the 9th International Con-\nference on Bioinformatics Research and Applications,\nICBRA ‚Äô22, page 61‚Äì66, New York, NY, USA. As-\nsociation for Computing Machinery.\nPreetha Datta, Fedor Vitiugin, Anastasiia Chizhikova,\nand Nitin Sawhney. 2024. Construction of hyper-\nrelational knowledge graphs using pre-trained large\nlanguage models. arXiv preprint arXiv:2403.11786.\nStefan Dernbach, Khushbu Agarwal, Alejandro Zu-\nniga, Michael Henry, and Sutanay Choudhury. 2024.\nGlam: Fine-tuning large language models for domain\nknowledge graph alignment via neighborhood par-\ntitioning and generative subgraph encoding. arXiv\npreprint arXiv:2402.06764.\nTianqing Fang,\nHongming Zhang,\nWeiqi Wang,\nYangqiu Song, and Bin He. 2021. Discos: bridg-\ning the gap between discourse knowledge and com-\nmonsense knowledge. In Proceedings of the Web\nConference 2021, pages 2648‚Äì2659.\nYingjie Feng, Xiaoyin Xu, Yueting Zhuang, and Min\nZhang. 2023.\nLarge language models improve\nalzheimer‚Äôs disease diagnosis using multi-modality\ndata. In 2023 IEEE International Conference on\nMedical Artificial Intelligence (MedAI), pages 61‚Äì66.\nIEEE.\nJiuzhou Han, Nigel Collier, Wray Buntine, and Ehsan\nShareghi. 2023. Pive: Prompting with iterative verifi-\ncation improving graph-based generative capability\nof llms. arXiv preprint arXiv:2305.12392.\nDan Hendrycks, Collin Burns, Steven Basart, Andy Zou,\nMantas Mazeika, Dawn Song, and Jacob Steinhardt.\n2021. Measuring massive multitask language under-\nstanding. In International Conference on Learning\nRepresentations.\nDaniel Scott Himmelstein, Antoine Lizee, Christine\nHessler, Leo Brueggeman, Sabrina L Chen, Dexter\nHadley, Ari Green, Pouya Khankhanian, and Ser-\ngio E Baranzini. 2017. Systematic integration of\nbiomedical knowledge prioritizes drugs for repurpos-\ning. Elife, 6:e26726.\nKang-Lin Hsieh, German Plascencia-Villa, Ko-Hong\nLin, George Perry, Xiaoqian Jiang, and Yejin Kim.\n2023. Synthesize heterogeneous biological knowl-\nedge via representation learning for alzheimer‚Äôs dis-\nease drug repurposing. iScience, 26(1):105678.\nEdward J Hu, Phillip Wallis, Zeyuan Allen-Zhu,\nYuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen,\net al. 2021. Lora: Low-rank adaptation of large lan-\nguage models. In International Conference on Learn-\ning Representations.\nJinhao Jiang, Kun Zhou, Zican Dong, Keming Ye,\nWayne Xin Zhao, and Ji-Rong Wen. 2023. Structgpt:\nA general framework for large language model to\nreason over structured data. In Proceedings of the\n2023 Conference on Empirical Methods in Natural\nLanguage Processing, pages 9237‚Äì9251.\nDi Jin, Eileen Pan, Nassim Oufattole, Wei-Hung Weng,\nHanyi Fang, and Peter Szolovits. 2021. What disease\ndoes this patient have? a large-scale open domain\nquestion answering dataset from medical exams. Ap-\nplied Sciences, 11(14):6421.\nQiao Jin, Bhuwan Dhingra, Zhengping Liu, William W\nCohen, and Xinghua Lu. 2019. Pubmedqa: A dataset\nfor biomedical research question answering. arXiv\npreprint arXiv:1909.06146.\nNikhil Kandpal, Haikang Deng, Adam Roberts, Eric\nWallace, and Colin Raffel. 2023. Large language\nmodels struggle to learn long-tail knowledge. In In-\nternational Conference on Machine Learning, pages\n15696‚Äì15707. PMLR.\nJiho Kim, Yeonsu Kwon, Yohan Jo, and Edward Choi.\n2023. Kg-gpt: A general framework for reasoning\non knowledge graphs using large language models.\nIn Findings of the Association for Computational\nLinguistics: EMNLP 2023, pages 9410‚Äì9421.\nYanis Labrak, Adrien Bazoge, Emmanuel Morin, Pierre-\nAntoine Gourraud, Mickael Rouvier, and Richard\nDufour. 2024.\nBiomistral: A collection of open-\nsource pretrained large language models for medical\ndomains. arXiv preprint arXiv:2402.10373.\nDawei Li, Yanran Li, Jiayi Zhang, Ke Li, Chen Wei,\nJianwei Cui, and Bin Wang. 2022. C3kg: A chi-\nnese commonsense conversation knowledge graph.\nIn Findings of the Association for Computational\nLinguistics: ACL 2022, pages 1369‚Äì1383.\nDawei Li, Yaxuan Li, Dheeraj Mekala, Shuyao\nLi, Xueqi Wang, William Hogan, Jingbo Shang,\net al. 2023a.\nDail:\nData augmentation for in-\ncontext learning via self-paraphrase. arXiv preprint\narXiv:2311.03319.\nDawei Li, Zhen Tan, Tianlong Chen, and Huan Liu.\n2024. Contextualization distillation from large lan-\nguage model for knowledge graph completion. arXiv\npreprint arXiv:2402.01729.\nDawei Li, Hengyuan Zhang, Yanran Li, and Shiping\nYang. 2023b. Multi-level contrastive learning for\nscript-based character understanding. In Proceedings\nof the 2023 Conference on Empirical Methods in\nNatural Language Processing, pages 5995‚Äì6013.\nRumeng Li, Xun Wang, and Hong Yu. 2023c. Two\ndirections for clinical data generation with large lan-\nguage models: Data-to-label and label-to-data. In\nProceedings of the 2023 Conference on Empirical\nMethods in Natural Language Processing, pages\n7129‚Äì7143.\nYucheng Li, Bo Dong, Frank Guerin, and Chenghua Lin.\n2023d. Compressing context to enhance inference\nefficiency of large language models. In Proceedings'
 'Omar & Mohammed / Journal of Applied Science and Technology Trends Vol. 06, No. 02, pp. 342 ‚Äì357 (2025) \n \n343 \nfor visualizing and validating LLM outputs [16], and MedKA for \nKG-enhanced question answering [17]. \nTo address these challenges, several recent works have \nexplored the integration of large language models with \nbiomedical knowledge graphs (KGs). A biomedical KG is a \nstructured network that connects entities such as diseases, drugs, \nand symptoms using defined relationships [18, 19]. These graphs \nstore verified medical knowledge from trusted databases, \nallowing for more accurate and explainable responses [12]. KGs \nare especially useful in multi-step reasoning tasks, where finding \nan answer requires connecting different pieces of information \n[20]. These entities and relationships can be visually represented \nin a biomedical knowledge graph, as shown in Fig. 1, where \nnodes represent medical concepts such as drugs, diseases, \nsymptoms, and pathways, and edges denote their semantic \nrelationships. \n \nFig. 1. Overview of Biomedical Knowledge Graph Entities and Relationships \n \nOne example of a widely used biomedical KG is SPOKE, \nwhich includes millions of nodes and relationships from over 40 \nbiomedical databases [12]. Integrating an LLM with a KG allows \nthe strengths of both technologies to work together: the LLM \nprovides language understanding, and the KG provides \nstructured, factual knowledge [21, 22]. A common method is \nretrieval-augmented generation (RAG), where the LLM retrieves \ninformation from the KG and uses it to generate more accurate \nresponses [13, 23]. In more advanced setups, the LLM can even \ngenerate queries like Cypher to fetch specific data from the graph \n[24, 25]. Neo4j is a popular graph database that supports fast and \nflexible storage and querying of knowledge graphs using Cypher \n[21]. It is well-suited for biomedical applications because it \nallows easy exploration of complex medical relationships. Recent \nwork has shown that combining Neo4j with LLMs can lead to \nbetter accuracy, fewer hallucinations, and more explainable \nresults [24, 26]. \nDespite improvements, building a reliable hybrid system that \ncombines an LLM with a biomedical KG remains a technical \nchallenge. Some approaches require complex pipelines or large \ntraining datasets, while others rely on fine-tuning specific to a \nnarrow set of questions [27, 28]. There is still a need for systems \nthat are both accurate and easy to scale, especially in domains like \nmedicine where the cost of errors is high [22]. \nRecent advances in KG-augmented LLMs have improved \nperformance, yet biomedical QA continues to face three practical \ngaps: \n1. Traceability: LLM-only or text-retrieval-only pipelines \nrarely provide graph-grounded justifications; users lack the \nability to inspect the exact nodes and edges that support an \nanswer. \n2. Evaluation: Prior work often judges quality via surface-\nform checks (e.g., matching a Cypher template), which fails to \ncapture end-to-end answer correctness or ranking quality \nacross different difficulty levels. \n3. Deployment: Many solutions assume cloud resources or \ndomain-specific fine-tuning, yet biomedical contexts typically \ndemand a local, privacy-preserving system with low latency \nand reproducible behavior. \nTimestamp-aware execution and periodic KG refresh help \navoid deprecated or contraindicated links, making the tool better \nsuited for safety-critical clinical contexts (e.g., drug‚Äìdrug \ninteractions). \nTo address these limitations, Our work introduces a locally \ndeployable pipeline that translates biomedical questions into \nexecutable Cypher queries over a Neo4j knowledge graph. The \nsystem returns answers with supporting nodes and edges, and is'
 'GigaScience , 2025, 14 , 1‚Äì12 \nDOI: 10.1093/gigascience/giae082 \nRESEARCH \nKno wledge gr aph‚Äìbased thought: a kno wledge \ngraph‚Äìenhanced LLM framework for pan-cancer \nquestion ans w ering \nYichun Feng \n1 ,2 ,‚Ä° , Lu Zhou 2 ,‚Ä° , Chao Ma 3 ,‚Ä° , Yikai Zheng 2 , Ruikun He 4 ,5 , * , and Yixue Li 1 ,2 , * \n1 Hangzhou Institute for Advanced Study, University of Chinese Academy of Sciences, 310024 Hangzhou, China \n2 Guangzhou National Laboratory, Guangzhou International Bio Island, 510005 Guangzhou, China \n3 Smartquerier Gene Technology (Shanghai) Co ., Ltd. , 200100 Shanghai, China \n4 BYHEALTH Institute of Nutrition & Health, 510663 Guangzhou, China \n5 Shanghai Institute of Nutrition and Health, Chinese Academy of Sciences Shanghai, 200030 Shanghai, China \n‚àóCorr espondence addr ess . Ruikun He , BYHEALTH Institute of Nutrition & Health, 510663 Guangzhou, China. E-mail: herk@by-health.com ; Yixue Li, Guangzhou \nNational Laboratory, No. 9 XingDaoHuanBei Road, Guangzhou International Bio Island, 510005 Guangzhou. E-mail: yxli@sibs.ac.cn .\n‚Ä° Contributed equally. \nAbstract \nBac kgr ound: In recent years, large language models (LLMs) have shown promise in various domains, notably in biomedical sciences. \nHowev er, their r eal-world application is often limited by issues like erroneous outputs and hallucinatory responses. \nResults: We developed the knowledge graph‚Äìbased thought (KGT) framework, an innov ati v e solution that integrates LLMs with knowl- \nedge graphs (KGs) to impr ov e their initial r esponses by utilizing v erifia b le information from KGs, thus significantl y r educing factual \nerr ors in r easoning. The KGT fr amew ork demonstr ates str ong adapta bility and performs well acr oss v arious open-source LLMs. No- \nta b l y, KGT can facilitate the discov er y of new uses for existing drugs through potential drug‚Äìcancer associations and can assist in \npr edicting r esistance by anal yzing r elev ant biomarkers and genetic mechanisms. To ev aluate the kno wledge graph question ans wer- \ning task within biomedicine , w e utilize a pan-cancer knowledge graph to develop a pan-cancer question answering benchmark, named \npan-cancer question answering. \nConclusions: The KGT fr amew ork substantiall y impr ov es the accuracy and utility of LLMs in the biomedical field. This study serves \nas a proof of concept, demonstrating its exceptional performance in biomedical question answering. \nKe yw ords: pan-cancer knowledge graph, large language model, knowledge graph question answering, prompt engineering \nKe y P oints: \nr We introduce a framework combining large language \nmodels (LLMs) with knowledge gr a phs (KGs) to impr ov e \nfactual accuracy in LLM reasoning. \nr Our system is a flexible arc hitectur e that seamlessly in- \ntegr ates v arious LLMs. \nr Utilizing a pan-cancer knowledge gr a ph, we hav e pr o- \nposed the first knowledge gr a ph question answering \nbenchmark in the field of biomedicine. \nr Case studies r e v eal our method enhanced LLMs in ad- \ndressing biomedical challenges such as drug reposition- \ning, r esistance r esearc h, individualized tr eatment, and \nbiomarker analysis. \nr The method performs favor abl y in comparison to exist- \ning methods. \nIntroduction \nWith the increasing prominence of large language models (LLMs) \nin the field of artificial intelligence, the advent of influential mod- \nels such as ChatGPT [ 1 ] and Llama [ 2 ] consequently catalyze the \nde v elopment of a wide array of applications in biomedicine and \nhealth care. Ho w ever, LLMs still face the challenge of factual hal- \nlucination, wher e they gener ate incorr ect statements due to lim- \nited inherent knowledge [ 3 ]. Factual hallucination presents a sig- \nnificant challenge for the practical use of LLMs, especially in real- \nworld scenarios where factual accuracy is crucial. Consequently, \nthere is a growing focus on addressing factual hallucinations in \nLLMs within the field of natural language processing (NLP) [ 4 , 5 ]. \nLLMs often struggle to ca ptur e and access factual knowledge, \nprimarily due to 3 aspects: the inability to comprehend ques- \ntions due to the lack of contextual information, the insufficient \nknowledge to generate accurate ans wers , and the incapacity to \nrecall specific facts [ 6 ]. Consequently, researchers consider the \nfine-tuning technique as a solution to address these issues. For \nexample, MedAlpaca [ 7 ] builds upon medical data to fine-tune \nStanford Alpaca for applications related to medical question \nans wering and dialogue . ChatDoctor [ 8 ] is designed to simulate \na conversation between a doctor and a patient by fine-tuning \nRecei v ed: May 27, 2024. Re vised: A ugust 14, 2024. Accepted: October 2, 2024 \n¬© The Author(s) 2025. Published by Oxford Uni v ersity Pr ess GigaScience. This is an Open Access article distributed under the terms of the Cr eati v e Commons \nAttribution License ( https://cr eati v ecommons.org/licenses/by/4.0/ ), which permits unrestricted reuse, distribution, and reproduction in any medium, provided \nthe original work is pr operl y cited. \nDownloaded from https://academic.oup.com/gigascience/article/doi/10.1093/gigascience/giae082/7943459 by guest on 11 January 2026']"
"What evaluation methods and performance metrics have been used to compare RAG-enhanced LLMs with baseline LLMs in clinical tasks, and how do these methods reflect the accuracy and usefulness of the models?","['<1-hop>\n\nLimitations This study was limited to peer-reviewed publications avail\xad able in biomedical databases (eg, PubMed, Embase), exclud\xad ing preprint articles from repositories like ArXiv. Additionally, only studies in English language were included, which might have excluded relevant studies in other lan\xad guages. We did not include sources such as IEEE Xplore or Google Scholar, which might have additional relevant stud\xad ies. However, our focus was on biomedicine, and we priori\xad tized databases specifically tailored to biomedical research to maintain the relevance and quality of the included studies. Furthermore, we used free-text searches in the databases, which activated automatic mapping to Medical Subject Headings (MeSH) and Emtree terms, improving retrieval accuracy. However, the limitations of automatic term map\xad ping cannot be ignored, as it may introduce variability if the underlying algorithms change. To address this, we have docu\xad mented all identified papers from our search. The title, publi\xad cation year, PMID, PUI, and database source for each study are provided in File S3. Conclusion We conducted a systematic literature review of studies exploring the use of RAG and LLM in clinical tasks. RAG implementation showed a 1.35 odds ratio increase in per\xad formance compared to baseline LLMs. To improve perform\xad ance and transparency in future studies, we developed guidelines for improving clinical RAG applications based on current research findings. Future research could focus on these 3 directions: (1) system-level enhancement: the combi\xad nation of RAG and agent, (2) knowledge-level enhancements: deep integration of knowledge into LLM, and (3) integration- level enhancements: integrating RAG systems within EHRs. Author contributions Siru Liu (Conceptualization, Data curation, Formal analysis, Investigation, Methodology, Software, Visualization, Writing ‚Äì original draft), Allison B. McCoy (Conceptualization, Writ\xad ing ‚Äì review & editing), Adam Wright (Conceptualization, Writing ‚Äì review & editing). Supplementary material Supplementary material is available at Journal of the Ameri\xad can Medical Informatics Association online.'
 '<2-hop>\n\nwas asked about medications for peripheral artery disease patients without increased bleeding risk, it initially omitted low-dose rivaroxaban. After integrating retrieved text from the 2024 American College of Cardiology / American Heart Association Guideline for the Management of Lower Extrem\xad ity Peripheral Artery Disease,10 the model correctly recom\xad mended rivaroxaban. Several guidelines exist for evaluating Artificial Intelligence (AI) applications and LLMs in healthcare, including DECIDE-AI (Developmental and Exploratory Clinical Inves\xad tigations of DEcision support systems driven by Artificial Intelligence),11 CLAIM (Checklist for Artificial Intelligence in Medical Imaging),12 and CONSORT-AI (Consolidated Standards of Reporting Trials-AI).13 In addition, Tam et al. introduced QUEST, a framework specifically for human eval\xad uation of LLMs in healthcare.14 However, these guidelines do not cover RAG applications in clinical settings, emphasiz\xad ing the need for a more specific guideline. Despite the promise of RAG in improving LLM performance in clinical settings, there is limited understanding of its overall effectiveness comparing with the baseline LLM, adoption in clinical domains, and optimal strategies for its development in biomedical applications. The aim of this study is to synthesize findings from recent research of RAG and LLM in biomedicine and provide clinical development guidelines to improve effec\xad tiveness as well as transparency in future research. Materials and methods Study design We conducted a systematic literature review. The report was created in adherence to the Preferred Reporting Items for Sys\xad tematic Reviews and Meta-Analyses 2020 analysis checklist (File S1).15 We also performed a meta-analysis following the Cochrane Handbook guidelines.16 Screening papers We searched in 3 databases (PubMed, Embase, and Psy\xad cINFO), using terms related to ‚Äúretrieval augmented gener\xad ation‚Äù and ‚Äúlarge language model.‚Äù PubMed and Embase were selected based on recommendations from the Cochrane Handbook, as they are widely recognized for their extensive coverage of biomedical literature.17 PsycINFO was included to capture articles from the psychological and behavioral sci\xad ences. To maintain a high standard of quality and reliability, we focused on peer-reviewed articles and excluded preprints. The specific search terms used for each database are provided in File S2. Given that ChatGPT was released on November 30, 2022, we set the publication filter to search papers pub\xad lished in 2023 and 2024. The search was performed on December 12, 2024. The inclusion criteria were: (1) the study must compare baseline LLM performance with RAG per\xad formance and (2) the study must address a biomedical ques\xad tion. The exclusion criteria were: (1) literature reviews, editorial comments, or viewpoint papers, (2) studies focusing on LLMs in languages other than English, or (3) studies cen\xad tered on a multi-agent system without a focus on RAG. SL screened titles and abstracts, then conducted a full-text review of papers meeting the criteria. Data extraction For each included study, we extracted the following informa\xad tion: author, title, publication year, journal, clinical task, and specialty. Regarding RAG techniques, we gathered details about the baseline LLM, retrieval sources, and strategies used in the pre-retrieval, retrieval, and post-retrieval stages. For evaluation, we extracted the evaluation method (human, automated, or a combination of both), the number of evalua\xad tors, the evaluation dataset, and the evaluation metrics.'
 '<3-hop>\n\nMeta-analysis Effect size was defined as a metric quantifying the relation\xad ship between variables, including both direction and magni\xad tude.18 For each included study, we calculated the effect size between baseline LLM performance and RAG-enhanced LLM performance. The outcomes focused on the perform\xad ance of generation results, such as accuracy and usefulness. Metrics related to the retrieval process, cost, or speed were not included as outcomes in the meta-analysis. For continu\xad ous outcomes, we used Cohen‚Äôs d, standardized between- group mean difference (SMD), calculated as the difference in means divided by the pooled standard deviation. The stand\xad ard error (SE) of SMD was calculated using the following for\xad mula (1), where n1 and n2 represent the sample sizes of each group.19 For dichotomous measurements, we calculated the log-odds ratio, obtained by transforming the odds ratio (OR) with the natural logarithm, and the associated SE was calcu\xad lated using formula (2), where a, b, c, and d represent the number of successful and failed events in the baseline LLM and RAG-enhanced LLM approaches. For studies reporting multiple outcomes, we used the overall outcome to calculate effect size. If no overall outcome was reported, we averaged the effect sizes of all reported outcomes. We excluded out\xad comes with a sample size of less than 30 to avoid small- sample bias. SESMD ¬º ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi n1 √æ n2 n1n2 √æ SMD2 2 n1 √æ n2 √∞ √û s (1) SElogOR ¬º ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi 1 a √æ 1 b √æ 1 c √æ 1 d r (2) We developed a random-effect meta-analysis model, because of the variability in RAG architectures and evalua\xad tion datasets among the included studies. The random-effect model was used when individual study effects likely con\xad tained additional sources of variance beyond sampling error. Between-study heterogeneity was assessed using Higgins & Thompson‚Äôs I2 statistic, where 25% indicated low heteroge\xad neity, 50% moderate, and 75% substantial.20 We conducted subgroup analyses to explore performance variations across different factors. First, we analyzed the influence of the baseline LLM, referring to the foundation model (eg, GPT-4 or Llama2) that provides the core architec\xad ture for the system. Second, we examined data retrieval strat\xad egies, categorizing them as simple or complex. Simple strategies included fixed-length chunking and basic similarity search, and we performed a subgroup analysis to compare these with complex retrieval strategies. Third, we analyzed differences based on evaluation methods, distinguishing between human evaluations, such as Likert scale ratings for helpfulness and accuracy, and automatic evaluation metrics, including ROUGE-1 and BLEU. Finally, we conducted a sub\xad group analysis based on the type of task, classifying studies 606 Journal of the American Medical Informatics Association, 2025, Vol. 32, No. 4 Downloaded from https://academic.oup.com/jamia/article/32/4/605/7954485 by guest on 11 January 2026']","Studies comparing RAG-enhanced LLMs with baseline LLMs in clinical tasks have employed various evaluation methods, including human evaluations such as Likert scale ratings for helpfulness and accuracy, and automated evaluation metrics like ROUGE-1 and BLEU. Several guidelines exist for evaluating AI applications and LLMs in healthcare, including DECIDE-AI, CLAIM, CONSORT-AI, and the QUEST framework for human evaluation of LLMs, though these do not specifically cover RAG applications, highlighting the need for more tailored guidelines. Performance metrics focused on generation results such as accuracy and usefulness were analyzed in a meta-analysis using effect size measures like Cohen‚Äôs d and log-odds ratio. The meta-analysis excluded metrics related to retrieval process, cost, or speed, concentrating on the quality of generated outputs. Subgroup analyses further explored variations in performance based on baseline LLM models, retrieval strategies, evaluation methods, and task types, demonstrating a comprehensive approach to assessing RAG and LLM performance in biomedical applications.",multi_hop_abstract_query_synthesizer,"The evaluation methods and performance metrics used to compare Retrieval-Augmented Generation (RAG)-enhanced Large Language Models (LLMs) with baseline LLMs in clinical tasks, as described in the provided context, include the following:

### Evaluation Methods

1. **Human Evaluation:**
   - Used in 9 studies.
   - Typically involved physicians or domain experts as evaluators (average of 4 evaluators, range 1 to 10).
   - Evaluators assessed model outputs on criteria such as accuracy, helpfulness, completeness, understandability, safety, and adherence to clinical guidelines.
   - Some evaluations included patient perspectives (e.g., diabetic patients assessing understandability).
   - Human evaluation often used Likert scale ratings to quantify subjective assessments.
   - Human evaluation showed a higher odds ratio (OR) of 1.65 (95% CI: 1.36-2.03) favoring RAG-enhanced LLMs over baseline LLMs.

2. **Automated Evaluation:**
   - Used in 8 studies.
   - Metrics included text similarity measures comparing generated responses to expert-provided answers.
   - Common automated metrics: ROUGE (ROUGE-1, ROUGE-2, ROUGE-L), BLEU, METEOR, BERTScore.
   - Also included accuracy-related metrics such as match rates, correct treatment percentages, AUC, AUPRC, and F1 scores.
   - Automated evaluation showed a lower but still significant OR of 1.20 (95% CI: 1.1-1.41).
   
3. **Mixed Evaluation:**
   - 3 studies used a combination of human and automated evaluation methods.

4. **Evaluation Datasets:**
   - Varied across studies, including:
     - Self-curated datasets (e.g., ClinicalQA benchmark with 314 questions, diabetes-related questions).
     - Public benchmark datasets (e.g., US board exam practice questions, MedMCQA, LiveQA, MedicationQA).
     - Simulated clinical cases and real Electronic Health Record (EHR) data.
   - Dataset sizes ranged from small (7 questions) to very large (up to 194,000 questions).

5. **Meta-Analysis Approach:**
   - Effect sizes were calculated using odds ratios (OR) for dichotomous outcomes and standardized mean differences (Cohen‚Äôs d) for continuous outcomes.
   - A random-effects meta-analysis model was used to account for variability across studies.
   - Subgroup analyses were performed based on evaluation method (human vs automated), baseline LLM type, retrieval strategy complexity, and clinical task type.

### Performance Metrics

1. **Accuracy:**
   - Most frequently reported metric.
   - Included direct correctness measures such as:
     - Likert scale ratings of accuracy.
     - Match rates with gold standard answers.
     - Correct treatment percentages.
     - Statistical metrics like AUC, AUPRC, F1 score.
     - Text similarity metrics (ROUGE, BLEU, METEOR, BERTScore).

2. **Completeness:**
   - Assessed whether responses included all necessary information.
   - Typically measured via Likert scales in human evaluations.

3. **User Perception:**
   - Subjective feedback on understandability, helpfulness, and whether responses met user intent.
   - Collected from healthcare providers and patients.

4. **Safety:**
   - Evaluated potential harm, adversarial safety, and risk management.
   - Included assessments of hallucinations (nonfactual or conflicting information).
   - Some studies used adversarial prompting to test system robustness.

5. **Citation Accuracy:**
   - Measured the correctness of references provided by the model.
   - Valid citations pointed to established publications, guidelines, or research.

6. **Bias and Language Quality:**
   - Evaluated clarity, neutrality, and empathy in responses.

7. **Retrieval-Specific Metrics:**
   - Some studies also evaluated retrieval process metrics such as recall, retrieval accuracy, and relevance of retrieved information.

### Reflection on Accuracy and Usefulness

- The combination of human and automated evaluations provides a comprehensive assessment of both factual correctness and practical usefulness.
- Human evaluations capture nuanced aspects like clinical relevance, safety, and user satisfaction that automated metrics may miss.
- Automated metrics offer scalable, objective measures of textual similarity and correctness.
- The statistically significant improvement in odds ratios (overall pooled OR of 1.35) indicates that RAG-enhanced LLMs outperform baseline LLMs in clinical tasks.
- Subgroup analyses showed that human evaluations tend to report larger improvements, reflecting the importance of expert judgment in assessing clinical utility.
- Safety and hallucination assessments ensure that improvements in accuracy do not come at the cost of generating harmful or misleading information.
- The use of diverse datasets and tasks ensures that evaluation results are robust across different clinical scenarios.

---

**Summary:**  
RAG-enhanced LLMs in clinical tasks have been evaluated using a mix of human expert assessments and automated text similarity and accuracy metrics. These evaluations focus on accuracy, completeness, safety, user perception, and citation correctness. Meta-analytic methods quantify performance improvements, showing statistically significant gains over baseline LLMs. The evaluation methods collectively reflect both the factual accuracy and practical usefulness of the models in clinical settings.","['was asked about medications for peripheral artery disease \npatients without increased bleeding risk, it initially omitted \nlow-dose rivaroxaban. After integrating retrieved text from \nthe 2024 American College of Cardiology / American Heart \nAssociation Guideline for the Management of Lower Extrem\xad\nity Peripheral Artery Disease,10 the model correctly recom\xad\nmended rivaroxaban.\nSeveral guidelines exist for evaluating Artificial Intelligence \n(AI) applications and LLMs in healthcare, including \nDECIDE-AI (Developmental and Exploratory Clinical Inves\xad\ntigations of DEcision support systems driven by Artificial \nIntelligence),11 CLAIM (Checklist for Artificial Intelligence in \nMedical Imaging),12\nand CONSORT-AI (Consolidated \nStandards of Reporting Trials-AI).13 In addition, Tam et al. \nintroduced QUEST, a framework specifically for human eval\xad\nuation of LLMs in healthcare.14 However, these guidelines \ndo not cover RAG applications in clinical settings, emphasiz\xad\ning the need for a more specific guideline.\nDespite the promise of RAG in improving LLM performance \nin clinical settings, there is limited understanding of its overall \neffectiveness comparing with the baseline LLM, adoption in \nclinical domains, and optimal strategies for its development in \nbiomedical applications. The aim of this study is to synthesize \nfindings from recent research of RAG and LLM in biomedicine \nand provide clinical development guidelines to improve effec\xad\ntiveness as well as transparency in future research.\nMaterials and methods\nStudy design\nWe conducted a systematic literature review. The report was \ncreated in adherence to the Preferred Reporting Items for Sys\xad\ntematic Reviews and Meta-Analyses 2020 analysis checklist \n(File S1).15 We also performed a meta-analysis following the \nCochrane Handbook guidelines.16\nScreening papers\nWe searched in 3 databases (PubMed, Embase, and Psy\xad\ncINFO), using terms related to ‚Äúretrieval augmented gener\xad\nation‚Äù and ‚Äúlarge language model.‚Äù PubMed and Embase \nwere selected based on recommendations from the Cochrane \nHandbook, as they are widely recognized for their extensive \ncoverage of biomedical literature.17 PsycINFO was included \nto capture articles from the psychological and behavioral sci\xad\nences. To maintain a high standard of quality and reliability, \nwe focused on peer-reviewed articles and excluded preprints. \nThe specific search terms used for each database are provided \nin File S2. Given that ChatGPT was released on November \n30, 2022, we set the publication filter to search papers pub\xad\nlished in 2023 and 2024. The search was performed on \nDecember 12, 2024. The inclusion criteria were: (1) the study \nmust compare baseline LLM performance with RAG per\xad\nformance and (2) the study must address a biomedical ques\xad\ntion. The exclusion criteria were: (1) literature reviews, \neditorial comments, or viewpoint papers, (2) studies focusing \non LLMs in languages other than English, or (3) studies cen\xad\ntered on a multi-agent system without a focus on RAG. SL \nscreened titles and abstracts, then conducted a full-text \nreview of papers meeting the criteria.\nData extraction\nFor each included study, we extracted the following informa\xad\ntion: author, title, publication year, journal, clinical task, and \nspecialty. Regarding RAG techniques, we gathered details \nabout the baseline LLM, retrieval sources, and strategies used \nin the pre-retrieval, retrieval, and post-retrieval stages. For \nevaluation, we extracted the evaluation method (human, \nautomated, or a combination of both), the number of evalua\xad\ntors, the evaluation dataset, and the evaluation metrics.\nMeta-analysis\nEffect size was defined as a metric quantifying the relation\xad\nship between variables, including both direction and magni\xad\ntude.18 For each included study, we calculated the effect size \nbetween baseline LLM performance and RAG-enhanced \nLLM performance. The outcomes focused on the perform\xad\nance of generation results, such as accuracy and usefulness. \nMetrics related to the retrieval process, cost, or speed were \nnot included as outcomes in the meta-analysis. For continu\xad\nous outcomes, we used Cohen‚Äôs d, standardized between- \ngroup mean difference (SMD), calculated as the difference in \nmeans divided by the pooled standard deviation. The stand\xad\nard error (SE) of SMD was calculated using the following for\xad\nmula (1), where n1 and n2 represent the sample sizes of each \ngroup.19 For dichotomous measurements, we calculated the \nlog-odds ratio, obtained by transforming the odds ratio (OR) \nwith the natural logarithm, and the associated SE was calcu\xad\nlated using formula (2), where a, b, c, and d represent the \nnumber of successful and failed events in the baseline LLM \nand RAG-enhanced LLM approaches. For studies reporting \nmultiple outcomes, we used the overall outcome to calculate \neffect size. If no overall outcome was reported, we averaged \nthe effect sizes of all reported outcomes. We excluded out\xad\ncomes with a sample size of less than 30 to avoid small- \nsample bias. \nSESMD ¬º\nffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi\nn1 √æ n2\nn1n2\n√æ\nSMD2\n2 n1 √æ n2\n√∞\n√û\ns\n(1) \nSElogOR ¬º\nffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi\n1\na √æ 1\nb √æ 1\nc √æ 1\nd\nr\n(2) \nWe developed a random-effect meta-analysis model, \nbecause of the variability in RAG architectures and evalua\xad\ntion datasets among the included studies. The random-effect \nmodel was used when individual study effects likely con\xad\ntained additional sources of variance beyond sampling error. \nBetween-study heterogeneity was assessed using Higgins & \nThompson‚Äôs I2 statistic, where 25% indicated low heteroge\xad\nneity, 50% moderate, and 75% substantial.20\nWe conducted subgroup analyses to explore performance \nvariations across different factors. First, we analyzed the \ninfluence of the baseline LLM, referring to the foundation \nmodel (eg, GPT-4 or Llama2) that provides the core architec\xad\nture for the system. Second, we examined data retrieval strat\xad\negies, categorizing them as simple or complex. Simple \nstrategies included fixed-length chunking and basic similarity \nsearch, and we performed a subgroup analysis to compare \nthese with complex retrieval strategies. Third, we analyzed \ndifferences based on evaluation methods, distinguishing \nbetween human evaluations, such as Likert scale ratings for \nhelpfulness and accuracy, and automatic evaluation metrics, \nincluding ROUGE-1 and BLEU. Finally, we conducted a sub\xad\ngroup analysis based on the type of task, classifying studies \n606                                                                                                      \nJournal of the American Medical Informatics Association, 2025, Vol. 32, No. 4 \nDownloaded from https://academic.oup.com/jamia/article/32/4/605/7954485 by guest on 11 January 2026'
 'conducting safety checks by applying 24 predefined rules to \nensure ethical and factual accuracy, and summarizing the \nresults.30 Glicksberg et al. developed an ensemble model that \ncombined structured and unstructured data to predict hospi\xad\ntal admission probabilities. These predicted probabilities, \nalong with similar historical cases, were incorporated into the \nprompt to enhance the performance of LLM.37 Chen et al. \nused Chain-of-Thought (CoT) prompting to improve LLM \nreasoning capabilities.39 Kresevic et al. customized prompts \nto help the model interpret structured guidelines, combined \nwith few-shot learning using 54 question-answer pairs.27\nJeong et al. fine-tuned LLMs to assess the relevance of \nretrieved evidence, ensure all statements were evidence-based, \nand confirm that the response effectively addressed the \nquery.34\nEvaluation\nNine studies used human evaluation, 8 relied on automated \nevaluation (eg, similarity comparisons between generated \nsentences and original answers), and 3 used a mix of both. \nOutcomes from human evaluation showed an overall OR of \n1.65 (95% CI: 1.36-2.03), while automatic evaluation \nresulted in an OR of 1.20 (95% CI: 1.1-1.41). The differen\xad\nces between the 2 were statistically significant (P < .01). \nThere were 4 human evaluators on average, with the range \nspanning from 1 to 10. Most human evaluators were \nphysicians from relevant specialties according to the \nstudy focus. In one case, 3 diabetic patients were involved in \nevaluating the understandability of diabetes-related patient \nqueries.30\nTwelve studies used self-curated datasets focused on \nresearch tasks. Examples included the ClinicalQA bench\xad\nmark, which comprised 314 open-ended questions about \ntreatment guidelines and clinical calculations generated by \nphysicians,28 and 43 diabetes-related questions sourced from \nthe National Institute of Diabetes and Digestive and Kidney \nDiseases website.30 Simulated cases from medical examina\xad\ntions were also utilized.25\nThree studies used EHR \ndata.35,37,40 Six studies used public benchmark datasets, such \nUS board exam practice questions, MedMCQA29,34 and \nlongform question-answering benchmarks (eg, LiveQA, Med\xad\nicationQA).34 The self-curated datasets averaged 76 ques\xad\ntions, ranging from 7 to 314. The length of public benchmark \ndatasets varied significantly, from 102 questions in the \nLiveQA dataset28 to 194 000 questions in the MedMCQA \ndataset.34\nMost studies reported evaluation metrics for the final \nresponse generation, while 4 (25%) also included specific \nmetrics to evaluate the retrieval process. For instance, 1 study \nmeasured recall in context retrieval,24 another evaluated \nretrieval accuracy,33 and a fine-tuned LLM was developed to \nassess the relevance of retrieved information to the user‚Äôs \nquery.34 Additionally, 1 study evaluated the accuracy of \nusing LLMs to extract text from figures and tables during \ndocument preprocessing.27 The final evaluation metrics \nfocused on the generated responses, consistent with those \nused in LLM-only systems. These metrics could be catego\xad\nrized as accuracy, completeness, user perception, safety, hal\xad\nlucination, citation, bias, and language. Accuracy was the \nmost frequently reported metric, covering Likert scale rat\xad\nings, match rates, correct treatment percentages,9 AUC, \nAUPRC, and F1 scores, as well as text similarity metrics like \nROUGE \n(ROUGE-1, \nROUGE-2, \nROUGE-L), \nBLEU, \nMETEOR, and BERTScore,21\nwhich compared LLM- \ngenerated responses to expert-provided answers. Complete\xad\nness metrics assessed whether responses included all neces\xad\nsary information, typically using Likert scales. User \nperception captured subjective feedback from both healthcare \nproviders and patients on understandability, helpfulness, and \nwhether responses met user intent, usually using Likert scales. \nSafety metrics focused both on user-related and system- \nrelated aspects. These metrics assessed potential harm, adver\xad\nsarial safety, and risk management,24 ensuring that outputs \nwere free of harmful content or risks. Scientific validity and \nadherence to evidence were also evaluated.29 One study used \nadversarial prompting, defined as intentionally adding harm\xad\nful directives to a prompt, to evaluate the safety of the RAG \nsystem.28 Hallucinations were primarily identified through \nmanual review, with definitions varying across studies. Some \nstudies defined hallucinations as nonfactual information, \nwhile one study added 2 other types of hallucinations: input- \nconflicting (content deviating from user-provided input) and \ncontextual-conflicting (content conflicting with previously \ngenerated information).27,41 Citation metrics measured the \naccuracy of provided references, with valid references consid\xad\nered those that pointed to established publications, guide\xad\nlines, or research. Bias and language were evaluated for \nclarity and neutrality, ensuring responses were unbiased and \nempathetic to patient concerns.24\nDiscussion\nThis study presents a systematic review of current research \non RAG for clinical tasks. Overall, RAG implementation \nincreased outcomes by 1.35 times compared to baseline \nLLM. We analyzed clinical tasks, baseline LLMs, retrieval \nsources and strategies, as well as evaluation methods. Despite \nthe potential benefits of RAG systems, there remains room \nfor improvement. Building on our literature review, we devel\xad\noped GUIDE-RAG (Guidelines for Unified Implementation \nand Development of Enhanced LLM Applications with RAG \nin Clinical Settings) for future clinical applications using \nRAG (Figure 4).\nGUIDE-RAG:\n1) Define clear clinical tasks and evaluation datasets. \nFuture research should clearly define clinical tasks and \nquestions to maximize the effectiveness of RAGs. Ambi\xad\nguity in questions can hinder performance, particularly \nin less powerful LLMs, making it challenging to achieve \nsignificant improvements in responses generation, even \nwith improved knowledge selection.42 For example, one \nstudy in the review constructed the evaluation dataset \nusing 30 case reports on rare diseases from PubMed.38\nThe authors did not report human performance on the \nself-build dataset. The questions themselves might have \nbeen inherently challenging or ambiguous. As expected, \nthe reported performance showed modest improvement, \nwith an OR of 1.31. \n2) Identify appropriate external resources for specific clini\xad\ncal tasks. \nThe first step in developing a RAG-based clinical system \nis to identify external resources that fill the knowledge \ngaps of the baseline LLM in relation to specific clinical \ntasks. The external knowledge should complement the \nLLM‚Äôs existing capabilities to effectively address task \n610                                                                                                      \nJournal of the American Medical Informatics Association, 2025, Vol. 32, No. 4 \nDownloaded from https://academic.oup.com/jamia/article/32/4/605/7954485 by guest on 11 January 2026'
 'Clinical applications of RAG\nRAG techniques have been applied across a broad range of \nmedical specialties, as shown in Table 1. These applications \ninclude clinical decision-making and medical question- \nanswering. In clinical decision making, RAG has supported \npersonalized treatment,23,24 emergency triage,25 and disease \nmanagement.26,27 For medical question-answering, RAG‚Äôs \ncapability has been explored to address complex treatment \nguidelines questions,28 as well as queries focused on specific \nareas, such as head and neck surgery-related questions,29 and \npatient questions regarding diabetes.30 In the subgroup anal\xad\nysis, 13 studies focused on clinical decision-making (OR \n1.46, 95% CI [1.16, 1.71]) and 7 studies focused on medical \nquestion-answering (OR 1.32, 95% CI [1.08, 1.63]), with no \nstatistically significant difference observed between these 2 \ngroups.\nBaseline LLMs\nThe baseline LLMs varied across studies, with GPT-4 being \nthe most common, used in 14 studies, (OR: 1.58, 95% CI: \n1.21-2.04). GPT-3.5, used in 6 studies, showed an OR of \n1.43 (95% CI: 1.06-1.93). Llama2 was applied in 5 studies \n(OR: 1.25, 95% CI: 1.08-1.44).\nRetrieval sources\nRetrieval sources were categorized as pre-stored documents \nand real-time online browsing. Regarding pre-stored docu\xad\nments, 6 studies used clinical guidelines, such as the Emer\xad\ngency Severity Index (ESI) Ver.3 Field Triage.25 Five studies \nused academic articles from sources like PubMed abstracts or \nfull texts, or document sets such as the Radiographics Top 10 \nReading List on Gastrointestinal Imaging.31 Three studies \nused specialized knowledge bases, including ChatENT, \nOncoKB, and RareDis Corpus, while one study employed a \ngeneral biomedical knowledge graph (Scalable Precision \nMedicine Open Knowledge Engine [SPOKE]). SPOKE \nTable 1. Distribution of RAG applications by medical specialty and task \ntype.\nMedical Specialty\nFrequency\nInternal medicine\n4\nGeneral medicine\n3\nOncology\n3\nEmergency medicine\n2\nGastroenterology\n2\nOtolaryngology\n1\nHepatology\n1\nRare diseases\n1\nOrthopedics\n1\nNeurology\n1\nOphthalmology\n1\nTask\nFrequency (%)\nClinical decision-making\n13 (65%)\nMedical question-answering\n7 (35%)\nFigure 2. A forest plot showing the odds ratio (OR) of total impacts of the RAG-enhanced system compared with the baseline LLM system in clinical \ntasks. The left panel lists individual studies (e.g., Zakka et al., Murugan et al., Long et al.) along with their corresponding ORs and 95% confidence \nintervals (CIs). The right panel visualizes these ORs, with square markers proportional to study weights and horizontal lines representing CIs. The overall \npooled OR is 1.35 (95% CI: 1.19‚Äì1.53), represented by a diamond at the bottom.\n608                                                                                                      \nJournal of the American Medical Informatics Association, 2025, Vol. 32, No. 4 \nDownloaded from https://academic.oup.com/jamia/article/32/4/605/7954485 by guest on 11 January 2026'
 ""Omar & Mohammed / Journal of Applied Science and Technology Trends Vol. 06, No. 02, pp. 342 ‚Äì357 (2025) \n \n352 \nThe dataset contains 60 questions divided into three difficulty \nlevels based on how complex the language is and how deep the \nbiomedical reasoning needs to be: \n‚Ä¢ Level 1: 25 simple questions focusing mostly on easy-to-\nunderstand facts, such as symptoms of certain diseases or \ndrugs used to treat specific conditions. \n‚Ä¢ Level 2: 20 medium-level questions that involve more \ndetailed relationships, such as drug interactions and \nSideEffect. \n‚Ä¢ Level 3: 15 hard questions requiring multi-step reasoning \nacross multiple biomedical concepts or biological \npathways, similar to the complexity found in real clinical \ncases. \nThe set of evaluation questions was designed to span a wide \nrange of common biomedical topics and to reflect clinically \nrelevant query types reported in prior literature. Each question is \npaired with its corresponding gold standard cypher query and \ncategorized by difficulty level, as summarized in Table V, where \nthree illustrative examples are shown. The questions were derived \nfrom publicly available biomedical QA benchmarks and adapted \nfrom established knowledge bases (e.g., iBKH schema relations), \nensuring both coverage and diversity across diseases, drugs, \nsymptoms, and pathways.\nTABLE V.  QUESTION DIFFICULTY LEVELS AND SAMPLE CYPHER QUERIES \nLevel \nDefinition \nExample \nCypher Query \n1 \nSingle-hop question using \none relationship \nWhat are the symptoms of \nAlzheimer? \nMATCH (d:Disease)-[:HAS_SYMPTOM]->(s:Symptom) \nWHERE toLower(d.name) CONTAINS 'alzheimer' \nRETURN s.name AS symptom \n \n2 \nQuestions requiring one \nrelationship \nWhat are the side effects of \ndrugs used to treat asthma? \nWITH toLower('asthma') AS disease_name \nMATCH (d:Disease)<-[:TREATS]-(dr:Drug) \nWHERE toLower(d.name) CONTAINS disease_name \nMATCH (dr:Drug)-[:CAUSES]->(se:SideEffect) \nRETURN DISTINCT se.name AS side_effect, dr.name AS drug \n \n3 \nQuestions requiring two or \nmore relationships \nWhat cholesterol medications \ncause side effects, and what are \nsome of those effects? \nWITH toLower('cholesterol') AS disease_name \nMATCH (d:Disease)<-[:TREATS]-(dr:Drug) \nWHERE toLower(d.name) CONTAINS disease_name \nMATCH (dr:Drug)-[:CAUSES]->(se:SideEffect) \nRETURN DISTINCT dr.name AS drug, se.name AS side_effect \n \n \n \nFig. 4. Graphical User Interface of the Biomedical Knowledge Explorer System \nB. Quantitative Evaluation \nThe performance of the proposed system is evaluated on the \niBKH knowledge graph using Exact Match, Precision@k, \nRecall@k, F1@k, Hits@k, MRR, and total response time across \nsimple, medium, and complex biomedical queries. These metrics \nwere measured for each difficulty level (simple, medium, and \ncomplex) to see how performance changes as questions become \nmore challenging.\n \nTABLE VI.  QUERY GENERATION EXACT MATCH BY DIFFICULTY LEVEL""
 'Research and Applications\nImproving large language model applications in \nbiomedicine with retrieval-augmented generation: a \nsystematic review, meta-analysis, and clinical \ndevelopment guidelines\nSiru Liu, PhD\x03,1,2, Allison B. McCoy \n, PhD1, Adam Wright \n, PhD1,3 \n1Department of Biomedical Informatics, Vanderbilt University Medical Center, Nashville, TN 37212, United States, 2Department of Computer \nScience, Vanderbilt University, Nashville, TN 37212, United States, 3Department of Medicine, Vanderbilt University Medical Center, \nNashville, TN 37212, United States\n\x03Corresponding author: Siru Liu, PhD, Department of Biomedical Informatics, Vanderbilt University Medical Center, 2525 West End Ave #1475, Nashville, \nTN 37212, United States (siru.liu@vumc.org)\nAbstract\nObjective: The objectives of this study are to synthesize findings from recent research of retrieval-augmented generation (RAG) and large lan\xad\nguage models (LLMs) in biomedicine and provide clinical development guidelines to improve effectiveness.\nMaterials and Methods: We conducted a systematic literature review and a meta-analysis. The report was created in adherence to the Pre\xad\nferred Reporting Items for Systematic Reviews and Meta-Analyses 2020 analysis. Searches were performed in 3 databases (PubMed, Embase, \nPsycINFO) using terms related to ‚Äúretrieval augmented generation‚Äù and ‚Äúlarge language model,‚Äù for articles published in 2023 and 2024. We \nselected studies that compared baseline LLM performance with RAG performance. We developed a random-effect meta-analysis model, using \nodds ratio as the effect size.\nResults: Among 335 studies, 20 were included in this literature review. The pooled effect size was 1.35, with a 95% confidence interval of \n1.19-1.53, indicating a statistically significant effect (P ¬º .001). We reported clinical tasks, baseline LLMs, retrieval sources and strategies, as \nwell as evaluation methods.\nDiscussion: Building on our literature review, we developed Guidelines for Unified Implementation and Development of Enhanced LLM Appli\xad\ncations with RAG in Clinical Settings to inform clinical applications using RAG.\nConclusion: Overall, RAG implementation showed a 1.35 odds ratio increase in performance compared to baseline LLMs. Future research \nshould focus on (1) system-level enhancement: the combination of RAG and agent, (2) knowledge-level enhancement: deep integration of \nknowledge into LLM, and (3) integration-level enhancement: integrating RAG systems within electronic health records.\nKey words: large language model; retrieval augmented generation; systematic review; meta-analysis. \nIntroduction\nLarge language models (LLMs) have reported remarkable \nperformance in question-answering, summarization, and text \ngeneration.1 Given this, researchers have explored its poten\xad\ntial in biomedical areas.2 For example, several studies \nreported the ability of using LLM to answer patient mes\xad\nsages,3 to analyze alert logic in clinical decision support,4 and \nto make discharge summaries more readable to patients.5\nHowever, several challenges remain.\nLLMs are trained on fixed datasets, which restrict their \nknowledge to information available up to the training cut-off \ndate. For example, GPT-4o‚Äôs training data only includes \ninformation up to October 2023, making it unable to \nrespond accurately to findings that emerged afterward. LLM \ntraining datasets are also generally broad and lack the specif\xad\nicity required for biomedical applications. Finally, not all \nsources used to train the LLMs are reliable and trustworthy. \nTo address these limitations, researchers have performed \nfine-tuning and retrieval-augmented generation (RAG) tech\xad\nniques. Fine-tuning can adapt LLMs to specific domains, but \nit is resource-intensive and does not allow for real-time \nupdates. In contrast, RAG maintains the original LLM archi\xad\ntecture while incorporating relevant context directly into \nqueries, offering more flexibility and control. In addition, \nRAG‚Äôs unique advantage in biomedical applications lies in its \nability to adapt to dynamic environments by delivering up-to- \ndate information and efficiently integrating external knowl\xad\nedge sources with high interpretability.6\nAnother limitation of using LLMs directly is the risk of hal\xad\nlucination, where the model generates incorrect or fabricated \ninformation.7 To mitigate such issues, researchers have pro\xad\nposed RAG as a solution that integrates up-to-date, relevant \ninformation, enhancing both the accuracy and reliability of \nLLM generated responses.8,9 For example, when ChatGPT \nReceived: November 19, 2024; Revised: December 17, 2024; Editorial Decision: January 2, 2025; Accepted: January 3, 2025 \n¬© The Author(s) 2025. Published by Oxford University Press on behalf of the American Medical Informatics Association.  \nThis is an Open Access article distributed under the terms of the Creative Commons Attribution-NonCommercial License (https://creativecommons.org/licenses/ \nby-nc/4.0/), which permits non-commercial re-use, distribution, and reproduction in any medium, provided the original work is properly cited. For commercial \nre-use, please contact reprints@oup.com for reprints and translation rights for reprints. All other permissions can be obtained through our RightsLink service via \nthe Permissions link on the article page on our site‚Äîfor further information please contact journals.permissions@oup.com. \nJournal of the American Medical Informatics Association, 2025, 32(4), 605‚Äì615 \nhttps://doi.org/10.1093/jamia/ocaf008 \nAdvance access publication 15 January 2025 \nResearch and Applications \nDownloaded from https://academic.oup.com/jamia/article/32/4/605/7954485 by guest on 11 January 2026'
 'offer global information based on user query, offering \nbetter performance than na√Øve RAG on the traditional \nvector databases.50\n6) Implement few-shot learning with CoT for complex clin\xad\nical reasoning. \nFew-shot learning has been shown to enhance LLMs‚Äô \nreasoning capabilities by teaching specific reasoning that \nmay not have been included in their original training. \nSimilarly, CoT techniques can improve complex reason\xad\ning in clinical tasks.51,52 Researchers should generate \nhigh-quality examples and incorporate CoT strategies \ninto the final query to refine specialized reasoning. \n7) Evaluate all stages of the RAG system development. \nMost current studies focus only on final performance, \noverlooking the importance of evaluating each stage of \ndevelopment. It is crucial to formally assess and report \nperformance at the pre-retrieval, retrieval, and post- \nretrieval stages. Evaluating the knowledge boundaries of \nthe baseline LLM, potential conflicts between the LLM \nand external knowledge, and the accuracy and coverage \nof retrieved information helps ensure replicability and \ntransparency. This level of evaluation enables other \nresearchers to understand why a RAG system works (or \ndoes not) and facilitates reproducibility. \n8) Consider cost and time constraints. \nAdvanced retrieval strategies can improve performance \nbut often increase processing time and computational \ncosts. For example, graph-based RAG requires substan\xad\ntial resources for developing knowledge graphs, and \nresponses from global summaries may take longer than \nwith na√Øve RAG methods.50 Another example is to fine- \ntune LLMs to evaluate the needs and the quality of \nretrieval.34 In terms of computational cost, this process \nis expensive, especially when scaling the method to \nlarger datasets or deploying it in a real-time system. \nAlso, a set of extra processes will make the whole speed \nslow. The long response time might have a nonignorable \nnegative impact in situations that need a quick answer, \nespecially common in clinical settings. Researchers \nshould balance performance improvements with time \nand cost considerations. Only 1 study in our review, \nwhich focused on gastrointestinal radiology diagnosis \nbased on imaging descriptions, compared the cost and \nresponse time between LLMs and LLMs with RAG.31\nThe mean response time was 29.8 s for LLM with RAG \nvs 15.7s for LLM alone, with costs of $0.15 and $0.02 \nper case, respectively. Another study used EHR to pre\xad\ndict cognitive decline only reported cost, with LLM: \n$4.49; RAG: $12.51. Another study that used EHR \ndata to predict cognitive decline reported costs of $4.49 \nfor LLM alone and $12.51 for LLM with RAG.53\n9) Establish monitoring and knowledge update strategies. \nAn important concept in AI applications in healthcare, \nalgorithmovigilance, which defined as ‚Äúscientific meth\xad\nods and activities relating to the evaluation, monitoring, \nunderstanding, and prevention of adverse effects of \nalgorithms in health care,‚Äù 54 should also be considered \nin the RAG applications. Researchers need to develop \nlong-term monitoring strategies for the RAG system per\xad\nformance, especially in clinical applications. In addition, \ncurrent studies use fixed external datasets. Researchers \nshould update external knowledge sources as latest \ninformation becomes available. Clear strategies for \nupdating knowledge should be defined, specifying when \nand how updates will occur. \nFor future studies, the first direction could be the system- \nlevel enhancement, the combination of RAG and LLM- \npowered agents. LLM-powered agents are AI systems that \nuse LLMs with complex reasoning and planning capabilities, \nmemory management, interactive capabilities with the envi\xad\nronment, and actions to execute tasks.55,56 Recent research \npoints to the emerging trend of combination of RAG and \nLLM-powered agents, where agents can assist in planning \nand decision making for complex tasks, rather than simple \nretrieval.57 For example, clinicians and patients have diverse \ninformation access needs, some needing to analyze text from \na knowledge base, others needing to incorporate structured \ndata from an EHR. RAG will eventually only become one of \nthe methods for agents to access information. Moreover, \nfuture research could focus on the usage of internal and \nexternal functions and tools, long-term and short-term mem\xad\nory module, self-learning module. For example, a study \ndeveloped an agent to answer questions related to rare dis\xad\neases by expanding beyond RAG with additional tool func\xad\ntions, such as querying phenotypes and performing web \nsearches. This approach improved the overall correctness \nfrom 0.48 to 0.75 compared to the GPT-4 baseline LLM.58\nThe second future direction could focus on the knowledge- \nlevel enhancement: deep integration of external knowledge \ninto LLM. LLM exhibits the knowledge boundaries. RAG \napproaches retrieving external knowledge and then integrates \nit into LLMs in the forms of prompts for the final generation \nto enhance the capabilities of LLMs in perceiving knowledge \nboundaries.59 However, the integration of external knowl\xad\nedge into LLM reasoning is typically limited to providing the \nretrieved data as additional context for the LLM‚Äôs query dur\xad\ning generation. This approach keeps retrieval and generation \nloosely connected, and the LLM‚Äôs output can still be influ\xad\nenced by its inherent knowledge boundaries or by noise in the \nretrieved text, leading to incorrect answers. Additionally, \nwhen the external knowledge source is EHR data, this \nenhancement becomes even more important. Current EHR \ndata is organized in a ‚Äúproblem-oriented medical record‚Äù \n(POMR) format, which collects and displays information in a \nstructured manner.60 LLMs excel in free-form contexts, and \ntheir ability to perform clinical tasks depends on access to \nunstructured text that provides a comprehensive view of the \npatient. Achieving this within the structured POMR format \nin modern EHR systems poses a significant challenge.61\nTherefore, investigating how to realize the deep integration \nof external knowledge with LLM reasoning is an important \ndirection for future research in clinical applications.\nThe final direction is the integration-level enhancement, \nfocusing on integrating RAG systems within EHRs. Current \nresearch has primarily focused on development and testing \noutside of EHR systems. To seamlessly provide support for \nhealthcare providers and patients, future efforts should pri\xad\noritize embedding RAG systems into EHR interfaces. This \nrequires collaboration with EHR vendors to ensure the neces\xad\nsary infrastructure is available. Researchers also can facilitate \nthis integration using data exchange frameworks, such as \nSMART on FHIR.62\n612                                                                                                      \nJournal of the American Medical Informatics Association, 2025, Vol. 32, No. 4 \nDownloaded from https://academic.oup.com/jamia/article/32/4/605/7954485 by guest on 11 January 2026'
 'Limitations\nThis study was limited to peer-reviewed publications avail\xad\nable in biomedical databases (eg, PubMed, Embase), exclud\xad\ning \npreprint \narticles \nfrom \nrepositories \nlike \nArXiv. \nAdditionally, only studies in English language were included, \nwhich might have excluded relevant studies in other lan\xad\nguages. We did not include sources such as IEEE Xplore or \nGoogle Scholar, which might have additional relevant stud\xad\nies. However, our focus was on biomedicine, and we priori\xad\ntized databases specifically tailored to biomedical research to \nmaintain the relevance and quality of the included studies. \nFurthermore, we used free-text searches in the databases, \nwhich activated automatic mapping to Medical Subject \nHeadings (MeSH) and Emtree terms, improving retrieval \naccuracy. However, the limitations of automatic term map\xad\nping cannot be ignored, as it may introduce variability if the \nunderlying algorithms change. To address this, we have docu\xad\nmented all identified papers from our search. The title, publi\xad\ncation year, PMID, PUI, and database source for each study \nare provided in File S3.\nConclusion\nWe conducted a systematic literature review of studies \nexploring the use of RAG and LLM in clinical tasks. RAG \nimplementation showed a 1.35 odds ratio increase in per\xad\nformance compared to baseline LLMs. To improve perform\xad\nance and transparency in future studies, we developed \nguidelines for improving clinical RAG applications based on \ncurrent research findings. Future research could focus on \nthese 3 directions: (1) system-level enhancement: the combi\xad\nnation of RAG and agent, (2) knowledge-level enhancements: \ndeep integration of knowledge into LLM, and (3) integration- \nlevel enhancements: integrating RAG systems within EHRs.\nAuthor contributions\nSiru Liu (Conceptualization, Data curation, Formal analysis, \nInvestigation, Methodology, Software, Visualization, Writing \n‚Äì original draft), Allison B. McCoy (Conceptualization, Writ\xad\ning ‚Äì review & editing), Adam Wright (Conceptualization, \nWriting ‚Äì review & editing).\nSupplementary material\nSupplementary material is available at Journal of the Ameri\xad\ncan Medical Informatics Association online.\nFunding\nThis work was supported by National Institutes of Health \ngrants: R00LM014097-02 and R01LM013995-01.\nConflicts of interest\nThe authors do not have conflicts of interest related to this \nstudy.\nData availability\nThe characteristics and outcomes for each included study \nwere reported in the File S2.\nReferences\n01. Raiaan MAK, Mukta MSH, Fatema K, et al. A review on large lan\xad\nguage models: architectures, applications, taxonomies, open issues \nand challenges. IEEE Access. 2024;12:26839-26874. https://doi. \norg/10.1109/ACCESS.2024.3365742\n02. Thirunavukarasu AJ, Ting DSJ, Elangovan K, et al. Large language \nmodels in medicine. Nat Med. 2023;29:1930-1940. https://doi. \norg/10.1038/s41591-023-02448-8\n03. Ayers JW, Poliak A, Dredze M, et al. Comparing physician and arti\xad\nficial intelligence Chatbot responses to patient questions posted to a \npublic social media forum. JAMA Intern Med. 2023;183:589-596. \nhttps://doi.org/10.1001/jamainternmed.2023.1838\n04. Liu S, Wright AP, Patterson BL, et al. Using AI-generated sugges\xad\ntions from ChatGPT to optimize clinical decision support. J Am \nMed Inform Assoc. 2023;30:1237-1245. https://doi.org/10.1093/ \njamia/ocad072\n05. Zaretsky J, Kim JM, Baskharoun S, et al. Generative artificial intel\xad\nligence to transform inpatient discharge summaries to patient- \nfriendly language and format. JAMA Netw Open. 2024;7: \ne240357. https://doi.org/10.1001/jamanetworkopen.2024.0357\n06. Gao Y, Xiong Y, Gao X, et al. Retrieval-augmented generation for \nlarge language models: a survey. 2023, preprint: not peer reviewed. \nhttps://arxiv.org/abs/2312.10997\n07. Xu Z, Jain S, Kankanhalli M. Hallucination is inevitable: an innate \nlimitation of large language models. January 22, 2024, preprint: \nnot peer reviewed. https://arxiv.org/abs/2401.11817\n08. Shuster K, Poff S, Chen M, et al. Retrieval augmentation reduces \nhallucination in conversation. Findings of the Association for Com\xad\nputational Linguistics, Findings of ACL: EMNLP 2021. Associa\xad\ntion for Computational Linguistics (ACL); 2021:3784-3803.\n09. Ayala  O, Bechard P. Reducing hallucination in structured outputs \nvia Retrieval-Augmented Generation. Proceedings of the 2024 \nConference of the North American Chapter of the Association for \nComputational Linguistics: Human Language Technologies. \nAssociation for Computational Linguistics; 2024;228-38. https:// \ndoi.org/10.18653/v1/2024.naacl-industry.19\n10. Gornik HL, Aronow HD, Goodney PP, et al. 2024 ACC/AHA/ \nAACVPR/APMA/ABC/SCAI/SVM/SVN/SVS/SIR/VESS guideline \nfor the management of lower extremity peripheral artery disease: a \nreport of the American College of Cardiology/American Heart \nAssociation Joint Committee on Clinical Practice Guidelines. Cir\xad\nculation. 2024;149:e1313-e1410. https://doi.org/10.1161/CIR. \n0000000000001251\n11. Vasey B, Nagendran M, Campbell B, et al. Reporting guideline for \nthe early stage clinical evaluation of decision support systems \ndriven by artificial intelligence: DECIDE-AI. BMJ. 2022;377: \ne070904. https://doi.org/10.1136/bmj-2022-070904\n12. Mongan J, Moy L, Kahn CE. Checklist for Artificial Intelligence in \nMedical Imaging (CLAIM): a guide for authors and reviewers. \nRadiol Artif Intell. 2020;2:e200029. https://doi.org/10.1148/ryai. \n2020200029\n13. Martindale APL, Ng B, Ngai V, et al. Concordance of randomised \ncontrolled trials for artificial intelligence interventions with the \nCONSORT-AI \nreporting \nguidelines. \nNat \nCommun. \n2024;15:6376-6311. https://doi.org/10.1038/s41467-024-45355-3\n14. Tam TYC, Sivarajkumar S, Kapoor S, et al. A framework for \nhuman evaluation of large language models in healthcare derived \nfrom literature review. NPJ Digit Med. 2024;7:258. https://doi. \norg/10.1038/s41746-024-01258-7\n15. Shamseer L, Moher D, Clarke M, et al. Preferred reporting items \nfor systematic review and meta-analysis protocols (PRISMA-p) \n2015: elaboration and explanation. BMJ. 2015;350:g7647. \nhttps://doi.org/10.1136/bmj.g7647\n16. Higgins JPT, Thomas J, Chandler J, et al. (editors). Cochrane Hand\xad\nbook for Systematic Reviews of Interventions version 6.5 (updated \nAugust 2024). Cochrane, 2024. Available from www.training. \ncochrane.org/handbook. Date accessed December 11, 2024.\nJournal of the American Medical Informatics Association, 2025, Vol. 32, No. 4                                                                                                      \n613 \nDownloaded from https://academic.oup.com/jamia/article/32/4/605/7954485 by guest on 11 January 2026'
 'Omar & Mohammed / Journal of Applied Science and Technology Trends Vol. 06, No. 02, pp. 342 ‚Äì357 (2025) \n \n344 \nevaluated using Exact Match, Precision, Recall, F1, Hits@k, \nMRR, and latency across simple, medium, and complex question \nsets. Unlike prior template-based methods, our approach enables \ntraceable, outcome-level validation. In summary, the main \ncontributions of this work are as follows: \n‚Ä¢ Hybrid LLM to Cypher QA: A system that translates \nnatural language questions into accurate, executable \nCypher over a Neo4j drug, disease, and symptom KG. \n‚Ä¢ Prompt-driven query generation: Schema,  entity \naware prompting that reliably maps diverse biomedical \nquestions to graph queries. \n‚Ä¢ Evidence transparency: Along with each answer, we \nsurface the generated Cypher and the supporting \njustification subgraph (nodes, edges) plus a brief natural \nlanguage rationale. \n‚Ä¢ Answer-level evaluation: End-to-end assessment using \nExact Match, F1, Precision/Recall, Hits@k, MRR and \nlatency across simple, medium and complex tiers. \n‚Ä¢ Local, reproducible deployment: On-prem LLaMA 3 \ninference (no cloud dependency) suitable for biomedical \nsettings requiring low latency and strong data control. \n \nThe remainder of this paper is organized as follows: Section \n2 reviews related work on biomedical knowledge graphs and \nLLM-based QA systems. Section 3 provides background on \nknowledge graphs, large language models, and question \nanswering frameworks. Section 4 details the proposed \nmethodology, \nincluding \nsystem \narchitecture, \ndataset \nconstruction, and query translation. Section 5 presents the \nexperimental results through both quantitative metrics and \nqualitative case studies. Section 6 discusses the findings, analyzes \nlimitations, and compares performance against baseline models. \nFinally, Section 7 concludes this paper and outlines directions for \nfuture work. \nII. RELATED WORK \nRecently, studies have concentrated on the integration of \nclinical and medical knowledge graphs (LLM) to improve the \nanswer to medical questions. Researchers have derived several \nbiomedical KGs using Neo4j and incorporated the application of \nLLMs like LLaMA and GPT to convert natural language \nquestions into graph queries. Improvements in answer \ncorrectness, reduction of hallucination errors, one-to-many \nrelationships, and support for complex reasoning were the \nobjectives of these efforts. Some frameworks also adopted \nretrieval methods to ground responses in secure data. \nSu et al.[11] developed an integrative Biomedical Knowledge \nHub (iBKH), a huge biomedical knowledge graph that comprises \n18 of the very best data sources. The deployment of the iBKH in \nNeo4j allows for a user-friendly web portal to allow fast and \ninteractive knowledge retrieval. The system implemented \nadvanced graph learning techniques to enable the discovery of \nbiomedical knowledge, illustrated by an example of repurposing \nin silico drugs for Alzheimer‚Äôs disease. iBKH achieved \npromising predictive performance for known drugs and proposed \npossible new drug candidates. \nRajabi and Kafaie[19] proposed a disease knowledge graph \nusing a cross-referential disease database comprising diseases, \nsymptoms, and drugs interconnected with relationships. They \ntransferred the data into Neo4j to create a graph of 9,400 nodes \nand 45,000 relationships representing the semantic links between \nmedical concepts. Applying Cypher queries enabled answering \ncomplex medical questions regarding identifying drugs that may \ncause certain diseases; it was demonstrated that the graph inferred \nnew relationships not explicitly existing in the original data. The \nconclusion was that disease knowledge graphs sped up clinical \ndiscovery and contributed to understanding complex medical \nrelationships. \nHou et al.[3] assessed and contrasted ChatGPT (both GPT-\n3.5 and GPT-4) and the biomedical knowledge graphs (BKGs) \nconcerning their ability to answer biomedical questions, generate \nnew knowledge, and reason. Their datasets were focused on \ndietary supplements and drugs, while evaluation criteria entailed \naccuracy, novelty, and reasoning ability. The results indicate that \nwhile GPT-4 surpassed GPT-3.5 and BKGs in knowledge \nprovision, it proved inconsistent with regard to citations and \nreasoning. Compared to them, BKGs scored higher in accuracy \nand reliability, especially in discovering novel links as well as \nwithin structured reasoning. \nSoman et al.[13] presented a novel framework called KG-\nRAG that integrates a large biomedical knowledge graph \n(SPOKE) with LLaMA 2, GPT-3.5, and GPT-4 (LLMs) to \nproduce accurate biomedical text. They optimized the retrieval of \nrelevant graph context to cut over 50% tokens without losing \naccuracy. It aided LLMs in performing better on biomedical \nquestion answering with very high accuracy boosts, especially in \nthe case of LLaMA 2. They compared KG-RAG to other retrieval \nmethods and indicated its comparatively more robust and \nefficient results. The framework produced reliable evidence-\nbased answers grounded in biomedical knowledge. \nLuo et al.[23] created ChatKBQA, a new framework with a \nquestion-and-answer approach over knowledge bases that first \ngenerates logical forms with the help of fine-tuned LLMs and \nthen retrieves the relevant entities and relations. This generate-\nthen-retrieve approach is supposed to handle a couple of issues \nwith the earlier methods concerning tedious retrieval and error \npropagation. They fine-tuned open-source LLMs like LLaMA 2 \nto change natural-language questions into logical forms with high \naccuracy. The retrieval phase uses unsupervised phrase-level \nsemantic matching in a way that enhances the alignment of \nentities and relations. Experiments on benchmark datasets \nindicate ChatKBQA to be superior to its predecessors, with the \nhighest accuracy to date. \nPusch and Conrad[6] conducted work under a hybrid \napproach conflating LLMs and biomedical Knowledge Graphs \n(KGs) to suppress hallucination errors in question-answering. \nThey proposed query-checking algorithms for validating, \ncorrecting, and executing the KG Cypher queries that LLMs \ngenerated, thereby attaining accurate and understandable \nanswers. The system used retrieval-augmented generation (RAG) \nto ground answers within KG data. The methodology was \nvalidated on a biomedical KG called PrimeKG using 50 \nbenchmark questions, assessing models like GPT-4 Turbo and \nLLaMA 3. Commercially available GPT-4 Turbo obtained']"
How does the system leverage cypher query generation and multi-hop reasoning with confidence scoring to ensure accurate and reliable biomedical question answering over the iBKH knowledge graph?,"['<1-hop>\n\nOmar & Mohammed / Journal of Applied Science and Technology Trends Vol. 06, No. 02, pp. 342 ‚Äì357 (2025) 351 This query searches for a disease node whose name contains ‚Äôalzheimer‚Äô and follows HAS_SYMPTOM edges to list related symptom names. The system then executes this cypher to retrieve answers. The prompts (such as few-shot examples and schema hints) were carefully designed to help LLaMA 3 generate correct Cypher queries. The model learns how to use the graph‚Äôs labels and relationships properly. For example, if a user asks, ‚ÄôWhich drugs treat diabetes?‚Äô, LLaMA might respond with a suitable Cypher query: MATCH (d:Drug)-[:TREATS]->(di:Disease) WHERE toLower(di.name) CONTAINS ""diabetes"" RETURN d.name This queries for drug nodes that have a TREATS edge to a diabetes disease node. By leveraging LLaMA 3 in this way, our system can flexibly handle many phrasing variations without manual mapping rules. D. Model Configuration & Decoding We run a local LLaMA 3.2-3B model in GGUF format (llama- 3.2-3b-instruct-q4_k_m.gguf) via llama.cpp, as shown in Table IV. TABLE IV. MODEL RUNTIME AND DECODING SETTINGS Runtime settings Decoding settings n_ctx = 1024 temperature = 0.2 n_threads = 12 top_p = 0.95 n_gpu_layers = 33 top_k = 40 n_batch = 512 repeat_penalty = 1.1 max_tokens = 80 seed = 42 E. Graph Subset and Versioning We use an iBKH derived subgraph (‚âà65.8k nodes; ‚âà3.0M edges) spanning DRUG, DISEASE, SYMPTOM, PATHWAY. IDs are normalized to CURIEs and duplicates collapsed across UMLS/DrugBank/DisGeNET/SIDER/KEGG. Each edge stores provenance/licensing metadata (source, source_version, license, retrieved_at, evidence_pmids/urls) and, when available, temporal fields (valid_from, valid_to). We report coverage as the percentage of evaluated questions whose gold entities/relations are present. F. Query Execution and Reliability After a Cypher query is generated, it is executed on the Neo4j database through the official Neo4j Python driver, which manages the secure connection and returns the results. Instead of restricting the output with a fixed LIMIT (e.g., LIMIT 5), the system retrieves candidate results and evaluates them using standardized retrieval metrics such as Hits@1, Hits@5, and Hits@10. This approach ensures that the system remains responsive while providing a fair assessment of ranking quality across different cutoff levels, rather than depending solely on a fixed number of returned items. Neo4j‚Äôs indexing on key node properties, such as name and primary identifiers, also helps speed up lookups as the knowledge graph grows. In cases where the language model generates an incomplete or incorrect query, such as referencing nodes or relationships that do not exist, the system catches the error and either retries with a simpler prompt or informs the user. Together, these steps make sure that queries run quickly, return valid results, and keep the overall experience smooth and reliable for biomedical question-answering. G. User Interface for Query Transparency The system includes a lightweight, cross-platform graphical user interface (GUI) implemented as a web application using the Flask framework in Python, with HTML and Bootstrap for interactive visualization. The interface is designed to make the question-answering process transparent and accessible to users without technical expertise. It consists of three main panels: 1. Input Panel: Where the user can enter a biomedical question in natural language. 2. Query Panel: Which displays the Cypher query generated by the language model, allowing users to verify how their question was interpreted. 3. Results Panel: Which presents the retrieved answers in a clear, readable format, accompanied by a brief natural language explanation generated by the system. By showing both the query and the answer, the GUI promotes user trust and enables validation of the system‚Äôs reasoning process. The interface is lightweight enough to run smoothly on standard desktop machines without additional dependencies, making it practical for local deployments in clinical or research settings. Fig. 4 illustrates the overall layout.'
 '<2-hop>\n\nV. EXPERIMENTAL RESULTS In this section, the proposed system is evaluated based on its ability to translate natural-language biomedical questions into executable Cypher queries over the iBKH knowledge graph. The assessment is conducted end-to-end and focuses on three main aspects: (i) the accuracy of query generation, measured by the proportion of correctly produced Cypher queries; (ii) system efficiency, quantified through total response time from question submission to final answer delivery, covering both query generation and execution; and (iii) the quality of retrieved answers, evaluated using standardized information retrieval metrics including Precision@k, Recall@k, F1@k, Hits@k, and Mean Reciprocal Rank (MRR). For clarity, all these metrics are formally defined in the following subsections, with their corresponding equations, and will be used consistently throughout the results section. Together, these dimensions provide a comprehensive view of both correctness and efficiency across simple, medium, and complex biomedical queries. A. Description of the Experimental Data To evaluate the proposed system, this work used a carefully selected set of biomedical questions designed to test how well the system understands natural language and converts it into Cypher queries for a biomedical knowledge graph. To evaluate the system, a benchmark of 60 biomedical questions was constructed by the authors, guided by the schema and relations in iBKH. The questions were grouped into simple, medium, and complex levels to assess performance across different reasoning requirements. Gold-standard answers were manually prepared for each question to enable quantitative evaluation using Exact Match, Precision@k, Recall@k, F1@k, Hits@k, and MRR. The complete set of 60 questions is available at this link https://drive.google.com/drive/my-drive.'
 '<3-hop>\n\nOmar & Mohammed / Journal of Applied Science and Technology Trends Vol. 06, No. 02, pp. 342 ‚Äì357 (2025) 345 record-high accuracy, while open-source models achieved impressive gains through prompt optimization. Feng et al.[22] developed the Knowledge Graph-based Thought (KGT) framework that integrated LLMs with a pan- cancer knowledge graph for biomedical question answering. KGT was designed to reason on the knowledge graph schema and identify optimal subgraphs to use for directing accurate answer generation, all without fine-tuning the LLMs. The framework is benchmarked against a new dataset (PcQA) designed specifically for pan-cancer KGQA tasks and has outperformed all existing state-of-the-art approaches by a rather large margin. KGT‚Äôs practicality in biomedical issues was highlighted through case studies for drug repositioning, drug resistance, and biomarker discovery. Their approach exhibited robust adaptability among various LLMs. Rezaei et al.[26] developed AMG-RAG, a dynamic framework that utilizes autonomous LLM agents with medical search tools in the continuous construction and real-time updating of Medical Knowledge Graphs (MKGs). Their system incorporated confidence scoring and multi-hop reasoning to improve accuracy and interpretability in medical question answering. AMG-RAG outperformed size models on both very hard MEDQA benchmarks and more accessible MedMCQA ones, proving that it could conduct efficient reasoning based on current structured medical knowledge. They also used Neo4j to manage the knowledge graphs while adding external searches to ensure the latest data. Tiwari et al.[24] presented Auto-Cypher, a recent automated pipeline for producing high-quality synthetic data for training LLMs by mapping natural language to Cypher queries for graph databases like Neo4j. The pipeline deployed the novelty of LLM- as-database-filler to synthesize Neo4j databases for the execution of generated queries to ensure their correctness. A sizable dataset called SynthCypher was created, spanning multiple domains and complex queries, leading to a 40% improvement in LLM performance on Cypher generation. The datasets were used to fine-tune open-source models such as LLaMA, Mistral, and Qwen, and the SPIDER benchmark was adapted for evaluation purposes. Mohammed et al.[29] proposed a hybridized GraphRAG framework combining Neo4j-based UMLS knowledge graphs with a vector store for medical textbooks to create an improved U.S.M.L.E.-style clinical question-answering approach. The project integrated symbolic reasoning from knowledge graphs with semantic retrieval performed on text embeddings to enhance relevance and accuracy via adaptive re-ranking and query expansion. The system had the answers produced by GPT-4o- Mini, with different prompting strategies encouraging evidence- based and traceable responses grounded in verified medical knowledge. Experiments showed that the hybrid approach improved factual accuracy and citation fidelity as compared to the L.L.M.-only approach, enhancing transparency and reliability. It is shown that binding both structured and unstructured medical knowledge sources could aid in ameliorating hallucinations and hence improve clinical trustworthiness in AI-driven medical QA. Yang et al.[30] articulated sepsis knowledge graph was crafted by combining multicenter clinical data from over 10,000 patients with the help of GPT-4 for entity recognition and relationship extraction. Real-world data were collected from three hospitals and integrated with clinical guidelines and databases from the public domain. The knowledge graph contained 1,894 nodes and 2,021 relationships pertaining to diseases, symptoms, biomarkers, treatments, and complications. GPT outperformed other models in every resolution on sepsis- specific datasets to obtain high F1-score results. The constructed graph highlighted complex interactions in sepsis for assisting clinical decision-making and was implemented on Neo4j. Guan et al.[20] proposed a novel method for constructing a local knowledge graph from retrieved biomedical documents by extracting propositional claims. They carried out layer wise summarization on this graph to capture multi-document relationships and provide comprehensive contextual information to a language model for question-answering purposes. The method resolved issues in multi-document biomedical QA, such as noise cancellation and efficient context usage. They then tested their method on several benchmarks for biomedical question answering, achieving performance at least comparable to, if not better than, existing retrieval-augmented generation (RAG) baselines. The study established enhanced reasoning and answer accuracy of the model achieved through structured graph summarization. Previous studies have improved biomedical QA using KGs and LLMs, but important gaps remain. Most systems lack transparent, graph-based justifications, rely on limited evaluation methods, or depend on cloud resources that reduce privacy and reproducibility. Our framework addresses these gaps by providing visible Cypher queries with evidence subgraphs, applying comprehensive performance metrics across difficulty levels, and ensuring fully local, privacy-preserving deployment. Table I summarizes key previous studies on biomedical knowledge graphs and question answering, outlining their methods, datasets, and main limitations.']","The system uses LLaMA 3 to generate Cypher queries by learning to utilize the graph's labels and relationships properly, enabling flexible handling of various natural language biomedical questions without manual mapping rules. After generating a Cypher query, it is executed on the Neo4j database, which manages secure connections and returns results efficiently. The system evaluates retrieved answers using standardized metrics such as Hits@k and Mean Reciprocal Rank (MRR) to assess ranking quality rather than relying on fixed limits, ensuring responsiveness and fairness. Additionally, the system catches errors from incomplete or incorrect queries and retries with simpler prompts or informs the user, maintaining reliability. Multi-hop reasoning and confidence scoring are incorporated as described in related frameworks like AMG-RAG, which utilize autonomous LLM agents and medical search tools to improve accuracy and interpretability in medical question answering. This combination of transparent Cypher query generation, multi-hop reasoning, and confidence scoring over the iBKH knowledge graph enables accurate, efficient, and trustworthy biomedical question answering.",multi_hop_abstract_query_synthesizer,"The system leverages Cypher query generation and multi-hop reasoning with confidence scoring to ensure accurate and reliable biomedical question answering over the iBKH knowledge graph through the following integrated mechanisms:

1. **Hybrid LLM-to-Cypher Query Generation**  
   - The system uses the LLaMA-3B large language model (LLM) to translate natural language biomedical questions into executable Cypher queries tailored for the Neo4j-based iBKH knowledge graph.  
   - Prompt-driven query generation incorporates schema and entity-aware prompting, enabling the LLM to reliably map diverse biomedical questions into precise graph queries. This approach allows flexible handling of various question phrasings without manual rule crafting.  
   - The generated Cypher queries are designed to reflect the graph schema accurately, including nodes (e.g., Drug, Disease, Symptom) and relationships (e.g., TREATS, CAUSES), supporting both simple and complex multi-hop queries.

2. **Multi-hop Reasoning via Cypher Queries**  
   - The system supports multi-hop queries that traverse multiple relationships and nodes to answer complex biomedical questions requiring compositional reasoning (e.g., drugs that treat a disease and their side effects).  
   - Multi-hop queries are constructed by chaining multiple MATCH clauses in Cypher, enabling the system to explore paths across the graph that connect different biomedical concepts.  
   - This capability is critical for answering complex questions that involve multiple biomedical entities and relations, as demonstrated by the system‚Äôs ability to handle queries with two or more relationships.

3. **Confidence Scoring and Evaluation Metrics**  
   - After executing the generated Cypher queries on the Neo4j database, the system retrieves candidate answers and evaluates them using standardized information retrieval metrics such as Precision@k, Recall@k, F1@k, Hits@k, and Mean Reciprocal Rank (MRR).  
   - These metrics provide quantitative confidence scores that reflect the accuracy and ranking quality of the retrieved answers, ensuring that the system surfaces the most relevant and correct responses near the top of the result list.  
   - The system‚Äôs evaluation shows high exact match rates (‚àº96% for simple, ‚àº95% for medium, and ‚àº86.7% for complex queries), indicating reliable query generation and answer retrieval even for multi-hop reasoning tasks.

4. **Error Handling and Query Validation**  
   - The system detects and manages errors arising from incomplete or incorrect Cypher queries generated by the LLM, such as referencing non-existent nodes or relationships.  
   - In such cases, it either retries query generation with simpler prompts or informs the user, maintaining robustness and reliability in the question-answering process.

5. **Evidence Transparency and Justification Subgraphs**  
   - Alongside each answer, the system provides the generated Cypher query and a supporting justification subgraph (the relevant nodes and edges from the knowledge graph) as evidence.  
   - This transparency allows users to verify the reasoning path and trust the correctness of the answers, which is especially important in sensitive biomedical contexts.

6. **Local, Privacy-Preserving Deployment**  
   - The entire pipeline operates fully offline, avoiding reliance on cloud services or external APIs, which enhances privacy, reproducibility, and compliance with biomedical data governance.  
   - This local deployment ensures low latency (average response time ~6.07 seconds) suitable for interactive use in clinical and research environments.

7. **Normalization and Schema-Constrained Query Generation (Future Directions)**  
   - The system recognizes challenges in multi-hop queries due to missing relation hops or brittle name-based matching and plans to incorporate biomedical entity normalization (e.g., UMLS, SNOMED, RxNorm) and schema-constrained query generation to improve semantic robustness and multi-hop reasoning accuracy.

**In summary**, by combining LLaMA-3B‚Äôs natural language understanding with schema-aware Cypher query generation, executing multi-hop graph traversals, and applying rigorous confidence scoring and error handling, the system achieves accurate, interpretable, and reliable biomedical question answering over the iBKH knowledge graph. The inclusion of transparent evidence subgraphs and a fully offline architecture further strengthens trustworthiness and practical applicability in biomedical domains.","['Omar & Mohammed / Journal of Applied Science and Technology Trends Vol. 06, No. 02, pp. 342 ‚Äì357 (2025) \n \n356 \n \nFig. 7. Side effects of drugs that treat epilepsy \n \nExecuting this query returns drugs associated with epilepsy \nand their corresponding side effects. For example, the query \nidentifies Pregabalin as a treatment for epilepsy and retrieves \nmultiple side effects such as anxiety, arrhythmia, gastrointestinal \npain, and others. The visualization of the graph in Fig. 7, \nhighlights this pattern, where the drug node is linked to epilepsy \nvia TREATS and to several side effect nodes via CAUSES, \nproviding an interpretable biomedical knowledge structure. \nVI. DISCUSSION \nUnder \na \nunified \nevaluation \nprotocol \nreporting \nP@k/R@k/F1@k, Hits@k, and MRR at ùëò‚àà{1,5,10} , the \npipeline exhibits consistent end-to-end behavior across the three \ndifficulty tiers. The ùëò-ablation shows strong early precision (high \nP@1 and elevated MRR), while recall increases with larger ùëò, \nindicating that correct answers typically surface near the top yet \npersist deeper in the ranked slate. At the query-generation level, \nexact-match (EM) is high for simple questions (e.g., ‚àº96%) and \nlower for complex, multi-hop questions (e.g., ‚àº86.7%), which \naligns with increased compositionality. End-to-end latency \n(mean total ‚âà6.07\u2006s from question to final answer) remains within \ninteractive bounds on our local setup. Practically, a fully offline \ndeployment avoids internet connectivity, API keys, and external \ndata \ntransfer, \nstrengthening \nprivacy, \ncompliance, \nand \nreproducibility in biomedical contexts. \nSeveral challenges qualify these findings. First, the \ncomparability challenge: to our knowledge, no prior study \nevaluates iBKH using the same metric suite, making cross-paper \nnumeric comparisons not ‚Äúapples to apples.‚Äù We therefore \ninterpret the results as controlled evidence about this pipeline \nunder a single, consistent protocol rather than as a cross-study \nleaderboard. Beyond comparability, performance may vary with \nother biomedical knowledge graphs; the current iBKH snapshot \nis static, limiting real-time updates; and scaling to larger or \ndynamically refreshed graphs can introduce latency and \nconsistency trade-offs. \nError analysis shows that residual failures concentrate in \ncomplex, multi-hop queries where missing relation hops or brittle \nname-based matching (synonyms, abbreviations, homonyms) \nlead to partial answers. These observations motivate concept-\nlevel normalization via biomedical identifiers (e.g., UMLS, \nSNOMED, RxNorm) with synonym expansion, as well as \nschema-constrained query generation and path-guided decoding \nto better satisfy multi-hop constraints. Finally, correctness is \nassessed primarily at the answer level (Hits@k, MRR, \nprecision/recall) and does not yet include full semantic-\nequivalence checks across alternative Cypher queries, which may \noverlook cases where different queries yield the same correct \nresults. Complementing template EM with result-set equivalence \nchecks, expanding the metric suite (e.g., nDCG/MAP) with per-\nquestion 95% bootstrap confidence intervals, and supporting \nincremental graph updates with distributed storage are promising \nsteps to enhance robustness, scalability, and external validity. \nVII. CONCLUSION \nThis study introduced a hybrid biomedical question \nanswering framework that couples the LLaMA-3B language \nmodel with a Neo4j-based iBKH knowledge graph to enable the \nautomatic generation of executable Cypher queries and to deliver \ntransparent, evidence-grounded answers through justification \nsubgraphs. Evaluation in simple, medium, and complex queries \ndemonstrated consistently high performance, with strong \nprecision, recall, F1, Hits@k, and MRR values, while \nmaintaining low latency suitable for interactive biomedical \napplications. Beyond quantitative performance, the proposed \nsystem provides a reproducible and privacy-preserving solution \nby operating fully offline, a property of particular importance in \nsensitive clinical and research contexts. However, certain \nlimitations remain. The reliance on a static iBKH snapshot \nconstrains coverage and adaptability, recall is lower for complex \nmulti-hop reasoning, and the absence of canonical entity \nnormalization (e.g., UMLS, SNOMED, RxNorm) may reduce \nsemantic robustness. Future research will therefore focus on \nintegrating standardized biomedical entity normalization, \nenabling dynamic and incremental knowledge graph updates, and \nleveraging domain-adapted or fine-tuned biomedical LLMs. \nThese directions are expected to further strengthen the \nrobustness, scalability, and applicability of the framework in real-\nworld biomedical and clinical environments. \n \nREFERENCES \n[1] S. Bubeck, V. Chandrasekaran, R. Eldan, J. Gehrke, E. Horvitz, E. Kamar, \nP. Lee, Y. T. Lee, Y. Li, S. Lundberg, et al., ‚ÄúSparks of artificial general \nintelligence: \nEarly experiments with \ngpt-4,‚Äù arXiv preprint \narXiv:2303.12712, 2023. \n[2] J. Achiam, S. Adler, S. Agarwal, L. Ahmad, I. Akkaya, F. L. Aleman, D. \nAlmeida, J. Altenschmidt, S. Altman, S. Anadkat, et al., ‚ÄúGpt-4 \ntechnical report,‚Äù arXiv preprint arXiv:2303.08774, 2023. \n[3] Y. Hou, J. Yeung, H. Xu, C. Su, F. Wang, and R. Zhang, ‚ÄúFrom answers to \ninsights: unveiling the strengths and limitations of chatgpt and \nbiomedical knowledge graphs,‚Äù Research Square, pp. rs‚Äì3, 2023.'
 'Vol. 06, No. 02, pp. 342 ‚Äì357 (2025) \nISSN: 2708-0757 \n \nJOURNAL OF APPLIED SCIENCE AND TECHNOLOGY TRENDS \n \nwww.jastt.org  \n \n                                                          342 \ndoi: 10.38094/jastt62404  \nA Hybrid LLM‚ÄìKnowledge Graph Framework for Accurate \nBiomedical Question Answering \n \nHavraz Y. Omar1,¬≤*, Abdulhakeem O. Mohammed¬≥  \n \n¬πDepartment of Information Technology, Technical College of Duhok, Duhok Polytechnic University, Duhok, Kurdistan Region, \nIraq. havraz.omar@dpu.edu.krd  \n¬≤ Department of Information Technology, Technical College of Informatics ‚Äì Akre, Akre University for Applied Sciences, Akre, \nKurdistan Region, Iraq. \n¬≥ Department of Computer Science, College of Science, University of Zakho, Zakho, Kurdistan Region, Iraq. \na.mohammed@uoz.edu.krd \n \n*Correspondence: havraz.omar@dpu.edu.krd \n \nAbstract \nBiomedical question answering requires accurate and interpretable systems; however, existing approaches often face challenges such as \nlanguage model hallucinations and limited reasoning when relying solely on standalone knowledge graphs. To address these limitations, \nthis study proposes a hybrid framework that integrates the LLaMA-3B language model with a Neo4j-based drug‚Äìdisease‚Äìsymptom \nknowledge graph. The system translates natural language questions into executable Cypher queries, operates on an iBKH-derived graph \ncomprising over 65,000 entities and 3 million relationships, and returns answers with supporting evidence through a transparent interface. \nExperiments conducted on 60 biomedical questions across three levels of difficulty demonstrate the robustness of the approach: 96% \nexact match for simple queries, 95% for medium queries, and 86.7% for complex queries. Overall, the system achieves Precision@5 of \n96.1%, Recall@5 of 89.0%, F1@5 of 91.0%, Hits@k of 96.1%, and an MRR of 94.4%, while maintaining an average response time of \nonly 6.07 seconds. These results indicate that the system retrieves nearly all relevant answers, ranks them correctly, and delivers them \nwith latency low enough for interactive use. Moreover, unlike cloud-based APIs such as ChatGPT, which require internet connectivity \nand external data transmission, the proposed framework operates fully offline, ensuring privacy, reproducibility, and compliance with \nbiomedical data governance. Overall, this pipeline provides an accurate, efficient, and privacy-preserving solution for biomedical question \nanswering, making it a practical alternative to cloud-dependent approaches in sensitive healthcare contexts. \n \nKeywords: Knowledge Graph, LLM, Question Answering, Neo4j, Biomedical Informatics, Healthcare AI, LLaMA 3. \n \nReceived: August 14th, 2025 / Revised: October 10th, 2025 / Accepted: October 16th, 2025 / Online: October 20th, 2025 \n \nI. INTRODUCTION \nAnswering questions in the biomedical field is a difficult task \ndue to the complexity of medical knowledge and the need for \nprecision. In recent years, large language models  (LLMs) like \nLLaMA, GPT-4 have made progress in understanding and \ngenerating human-like responses to medical questions  [1,  2]. \nThese models can process large amounts of information and \nrespond in natural language, which makes them helpful in \nhealthcare settings [3]. However, they often struggle to provide \naccurate answers when dealing with specialized biomedical \ncontent [4, 5]. \nOne major issue with LLMs is a problem called hallucination, \nwhere the model generates information that sounds right but is \nactually incorrect or unsupported [6]. In medical applications, \nthis can be dangerous, as healthcare professionals rely on precise \nand trustworthy information  [7]. Therefore, researchers are \nexploring ways to combine LLMs with structured sources of \nknowledge to improve their reliability [8]. \nLLM-only systems in biomedicine still hallucinate and are \nhard to verify, limiting safe use [9, 10]. Biomedical knowledge \ngraphs (BKGs) such as iBKH and SPOKE curate multi-source \nfacts and enable multi-hop reasoning, yet they neither interpret \nfree text nor generate answers [11, 12]. Recent hybrids (KG-\naware RAG) improve grounding but often lack explicit path-level \njustifications and robust end-to-end answer evaluation [13, 14]. \nRecent studies have increasingly integrated Knowledge \nGraphs (KGs) with Large Language Models (LLMs) to improve \nfactual accuracy, reasoning, and reduce hallucinations. Notable \nexamples include DR.KNOWS, which combines UMLS-based \nKGs with LLMs for better diagnostic reasoning [15], KnowNet'
 'Omar & Mohammed / Journal of Applied Science and Technology Trends Vol. 06, No. 02, pp. 342 ‚Äì357 (2025) \n \n344 \nevaluated using Exact Match, Precision, Recall, F1, Hits@k, \nMRR, and latency across simple, medium, and complex question \nsets. Unlike prior template-based methods, our approach enables \ntraceable, outcome-level validation. In summary, the main \ncontributions of this work are as follows: \n‚Ä¢ Hybrid LLM to Cypher QA: A system that translates \nnatural language questions into accurate, executable \nCypher over a Neo4j drug, disease, and symptom KG. \n‚Ä¢ Prompt-driven query generation: Schema,  entity \naware prompting that reliably maps diverse biomedical \nquestions to graph queries. \n‚Ä¢ Evidence transparency: Along with each answer, we \nsurface the generated Cypher and the supporting \njustification subgraph (nodes, edges) plus a brief natural \nlanguage rationale. \n‚Ä¢ Answer-level evaluation: End-to-end assessment using \nExact Match, F1, Precision/Recall, Hits@k, MRR and \nlatency across simple, medium and complex tiers. \n‚Ä¢ Local, reproducible deployment: On-prem LLaMA 3 \ninference (no cloud dependency) suitable for biomedical \nsettings requiring low latency and strong data control. \n \nThe remainder of this paper is organized as follows: Section \n2 reviews related work on biomedical knowledge graphs and \nLLM-based QA systems. Section 3 provides background on \nknowledge graphs, large language models, and question \nanswering frameworks. Section 4 details the proposed \nmethodology, \nincluding \nsystem \narchitecture, \ndataset \nconstruction, and query translation. Section 5 presents the \nexperimental results through both quantitative metrics and \nqualitative case studies. Section 6 discusses the findings, analyzes \nlimitations, and compares performance against baseline models. \nFinally, Section 7 concludes this paper and outlines directions for \nfuture work. \nII. RELATED WORK \nRecently, studies have concentrated on the integration of \nclinical and medical knowledge graphs (LLM) to improve the \nanswer to medical questions. Researchers have derived several \nbiomedical KGs using Neo4j and incorporated the application of \nLLMs like LLaMA and GPT to convert natural language \nquestions into graph queries. Improvements in answer \ncorrectness, reduction of hallucination errors, one-to-many \nrelationships, and support for complex reasoning were the \nobjectives of these efforts. Some frameworks also adopted \nretrieval methods to ground responses in secure data. \nSu et al.[11] developed an integrative Biomedical Knowledge \nHub (iBKH), a huge biomedical knowledge graph that comprises \n18 of the very best data sources. The deployment of the iBKH in \nNeo4j allows for a user-friendly web portal to allow fast and \ninteractive knowledge retrieval. The system implemented \nadvanced graph learning techniques to enable the discovery of \nbiomedical knowledge, illustrated by an example of repurposing \nin silico drugs for Alzheimer‚Äôs disease. iBKH achieved \npromising predictive performance for known drugs and proposed \npossible new drug candidates. \nRajabi and Kafaie[19] proposed a disease knowledge graph \nusing a cross-referential disease database comprising diseases, \nsymptoms, and drugs interconnected with relationships. They \ntransferred the data into Neo4j to create a graph of 9,400 nodes \nand 45,000 relationships representing the semantic links between \nmedical concepts. Applying Cypher queries enabled answering \ncomplex medical questions regarding identifying drugs that may \ncause certain diseases; it was demonstrated that the graph inferred \nnew relationships not explicitly existing in the original data. The \nconclusion was that disease knowledge graphs sped up clinical \ndiscovery and contributed to understanding complex medical \nrelationships. \nHou et al.[3] assessed and contrasted ChatGPT (both GPT-\n3.5 and GPT-4) and the biomedical knowledge graphs (BKGs) \nconcerning their ability to answer biomedical questions, generate \nnew knowledge, and reason. Their datasets were focused on \ndietary supplements and drugs, while evaluation criteria entailed \naccuracy, novelty, and reasoning ability. The results indicate that \nwhile GPT-4 surpassed GPT-3.5 and BKGs in knowledge \nprovision, it proved inconsistent with regard to citations and \nreasoning. Compared to them, BKGs scored higher in accuracy \nand reliability, especially in discovering novel links as well as \nwithin structured reasoning. \nSoman et al.[13] presented a novel framework called KG-\nRAG that integrates a large biomedical knowledge graph \n(SPOKE) with LLaMA 2, GPT-3.5, and GPT-4 (LLMs) to \nproduce accurate biomedical text. They optimized the retrieval of \nrelevant graph context to cut over 50% tokens without losing \naccuracy. It aided LLMs in performing better on biomedical \nquestion answering with very high accuracy boosts, especially in \nthe case of LLaMA 2. They compared KG-RAG to other retrieval \nmethods and indicated its comparatively more robust and \nefficient results. The framework produced reliable evidence-\nbased answers grounded in biomedical knowledge. \nLuo et al.[23] created ChatKBQA, a new framework with a \nquestion-and-answer approach over knowledge bases that first \ngenerates logical forms with the help of fine-tuned LLMs and \nthen retrieves the relevant entities and relations. This generate-\nthen-retrieve approach is supposed to handle a couple of issues \nwith the earlier methods concerning tedious retrieval and error \npropagation. They fine-tuned open-source LLMs like LLaMA 2 \nto change natural-language questions into logical forms with high \naccuracy. The retrieval phase uses unsupervised phrase-level \nsemantic matching in a way that enhances the alignment of \nentities and relations. Experiments on benchmark datasets \nindicate ChatKBQA to be superior to its predecessors, with the \nhighest accuracy to date. \nPusch and Conrad[6] conducted work under a hybrid \napproach conflating LLMs and biomedical Knowledge Graphs \n(KGs) to suppress hallucination errors in question-answering. \nThey proposed query-checking algorithms for validating, \ncorrecting, and executing the KG Cypher queries that LLMs \ngenerated, thereby attaining accurate and understandable \nanswers. The system used retrieval-augmented generation (RAG) \nto ground answers within KG data. The methodology was \nvalidated on a biomedical KG called PrimeKG using 50 \nbenchmark questions, assessing models like GPT-4 Turbo and \nLLaMA 3. Commercially available GPT-4 Turbo obtained'
 'Omar & Mohammed / Journal of Applied Science and Technology Trends Vol. 06, No. 02, pp. 342 ‚Äì357 (2025) \n \n351 \nThis query searches for a disease node whose name contains \n‚Äôalzheimer‚Äô and follows HAS_SYMPTOM edges to list related \nsymptom names. The system then executes this cypher to retrieve \nanswers. The prompts (such as few-shot examples and schema \nhints) were carefully designed to help LLaMA 3 generate correct \nCypher queries. The model learns how to use the graph‚Äôs labels \nand relationships properly. For example, if a user asks, ‚ÄôWhich \ndrugs treat diabetes?‚Äô, LLaMA might respond with a suitable \nCypher query: \n \nMATCH (d:Drug)-[:TREATS]->(di:Disease) \nWHERE toLower(di.name) CONTAINS ""diabetes"" \nRETURN d.name  \n \nThis queries for drug nodes that have a TREATS edge to a \ndiabetes disease node. By leveraging LLaMA 3 in this way, our \nsystem can flexibly handle many phrasing variations without \nmanual mapping rules. \nD. Model Configuration & Decoding \nWe run a local LLaMA 3.2-3B model in GGUF format (llama-\n3.2-3b-instruct-q4_k_m.gguf) via llama.cpp, as shown in Table \nIV. \nTABLE IV.  MODEL RUNTIME AND DECODING SETTINGS \nRuntime settings \nDecoding settings \nn_ctx = 1024 \ntemperature = 0.2 \nn_threads = 12 \ntop_p = 0.95 \nn_gpu_layers = 33 \ntop_k = 40 \nn_batch = 512 \nrepeat_penalty = 1.1 \n \nmax_tokens = 80 \n \nseed = 42 \n \nE. Graph Subset and Versioning \nWe use an iBKH derived subgraph (‚âà65.8k nodes; ‚âà3.0M \nedges) spanning DRUG, DISEASE, SYMPTOM, PATHWAY. IDs are \nnormalized to CURIEs and duplicates collapsed across \nUMLS/DrugBank/DisGeNET/SIDER/KEGG. Each edge stores \nprovenance/licensing metadata (source, source_version, license, \nretrieved_at, \nevidence_pmids/urls) \nand, \nwhen \navailable, \ntemporal fields (valid_from, valid_to). We report coverage as \nthe \npercentage \nof \nevaluated \nquestions \nwhose \ngold \nentities/relations are present. \nF. Query Execution and Reliability \nAfter a Cypher query is generated, it is executed on the Neo4j \ndatabase through the official Neo4j Python driver, which \nmanages the secure connection and returns the results. Instead of \nrestricting the output with a fixed LIMIT (e.g., LIMIT 5), the \nsystem retrieves candidate results and evaluates them using \nstandardized retrieval metrics such as Hits@1, Hits@5, and \nHits@10. This approach ensures that the system remains \nresponsive while providing a fair assessment of ranking quality \nacross different cutoff levels, rather than depending solely on a \nfixed number of returned items. Neo4j‚Äôs indexing on key node \nproperties, such as name and primary identifiers, also helps speed \nup lookups as the knowledge graph grows. In cases where the \nlanguage model generates an incomplete or incorrect query, such \nas referencing nodes or relationships that do not exist, the system \ncatches the error and either retries with a simpler prompt or \ninforms the user. Together, these steps make sure that queries run \nquickly, return valid results, and keep the overall experience \nsmooth and reliable for biomedical question-answering. \nG. User Interface for Query Transparency \nThe system includes a lightweight, cross-platform graphical \nuser interface (GUI) implemented as a web application using the \nFlask framework in Python, with HTML and Bootstrap for \ninteractive visualization. The interface is designed to make the \nquestion-answering process transparent and accessible to users \nwithout technical expertise. It consists of three main panels: \n1. Input Panel: Where the user can enter a biomedical \nquestion in natural language. \n2. Query Panel: Which displays the Cypher query generated \nby the language model, allowing users to verify how their \nquestion was interpreted. \n3. Results Panel: Which presents the retrieved answers in a \nclear, readable format, accompanied by a brief natural \nlanguage explanation generated by the system. \nBy showing both the query and the answer, the GUI promotes \nuser trust and enables validation of the system‚Äôs reasoning \nprocess. The interface is lightweight enough to run smoothly on \nstandard desktop machines without additional dependencies, \nmaking it practical for local deployments in clinical or research \nsettings. Fig. 4 illustrates the overall layout. \nV. EXPERIMENTAL RESULTS \nIn this section, the proposed system is evaluated based on its \nability to translate natural-language biomedical questions into \nexecutable Cypher queries over the iBKH knowledge graph. The \nassessment is conducted end-to-end and focuses on three main \naspects: (i) the accuracy of query generation, measured by the \nproportion of correctly produced Cypher queries; (ii) system \nefficiency, quantified through total response time from question \nsubmission to final answer delivery, covering both query \ngeneration and execution; and (iii) the quality of retrieved \nanswers, evaluated using standardized information retrieval \nmetrics including Precision@k, Recall@k, F1@k, Hits@k, and \nMean Reciprocal Rank (MRR). For clarity, all these metrics are \nformally defined in the following subsections, with their \ncorresponding equations, and will be used consistently \nthroughout the results section. Together, these dimensions \nprovide a comprehensive view of both correctness and efficiency \nacross simple, medium, and complex biomedical queries. \nA. Description of the Experimental Data \nTo evaluate the proposed system, this work used a carefully \nselected set of biomedical questions designed to test how well the \nsystem understands natural language and converts it into Cypher \nqueries for a biomedical knowledge graph. \nTo evaluate the system, a benchmark of 60 biomedical \nquestions was constructed by the authors, guided by the schema \nand relations in iBKH. The questions were grouped into simple, \nmedium, and complex levels to assess performance across \ndifferent reasoning requirements. Gold-standard answers were \nmanually prepared for each question to enable quantitative \nevaluation using Exact Match, Precision@k, Recall@k, F1@k, \nHits@k, and MRR. The complete set of 60 questions is available \nat this link https://drive.google.com/drive/my-drive.'
 ""Omar & Mohammed / Journal of Applied Science and Technology Trends Vol. 06, No. 02, pp. 342 ‚Äì357 (2025) \n \n352 \nThe dataset contains 60 questions divided into three difficulty \nlevels based on how complex the language is and how deep the \nbiomedical reasoning needs to be: \n‚Ä¢ Level 1: 25 simple questions focusing mostly on easy-to-\nunderstand facts, such as symptoms of certain diseases or \ndrugs used to treat specific conditions. \n‚Ä¢ Level 2: 20 medium-level questions that involve more \ndetailed relationships, such as drug interactions and \nSideEffect. \n‚Ä¢ Level 3: 15 hard questions requiring multi-step reasoning \nacross multiple biomedical concepts or biological \npathways, similar to the complexity found in real clinical \ncases. \nThe set of evaluation questions was designed to span a wide \nrange of common biomedical topics and to reflect clinically \nrelevant query types reported in prior literature. Each question is \npaired with its corresponding gold standard cypher query and \ncategorized by difficulty level, as summarized in Table V, where \nthree illustrative examples are shown. The questions were derived \nfrom publicly available biomedical QA benchmarks and adapted \nfrom established knowledge bases (e.g., iBKH schema relations), \nensuring both coverage and diversity across diseases, drugs, \nsymptoms, and pathways.\nTABLE V.  QUESTION DIFFICULTY LEVELS AND SAMPLE CYPHER QUERIES \nLevel \nDefinition \nExample \nCypher Query \n1 \nSingle-hop question using \none relationship \nWhat are the symptoms of \nAlzheimer? \nMATCH (d:Disease)-[:HAS_SYMPTOM]->(s:Symptom) \nWHERE toLower(d.name) CONTAINS 'alzheimer' \nRETURN s.name AS symptom \n \n2 \nQuestions requiring one \nrelationship \nWhat are the side effects of \ndrugs used to treat asthma? \nWITH toLower('asthma') AS disease_name \nMATCH (d:Disease)<-[:TREATS]-(dr:Drug) \nWHERE toLower(d.name) CONTAINS disease_name \nMATCH (dr:Drug)-[:CAUSES]->(se:SideEffect) \nRETURN DISTINCT se.name AS side_effect, dr.name AS drug \n \n3 \nQuestions requiring two or \nmore relationships \nWhat cholesterol medications \ncause side effects, and what are \nsome of those effects? \nWITH toLower('cholesterol') AS disease_name \nMATCH (d:Disease)<-[:TREATS]-(dr:Drug) \nWHERE toLower(d.name) CONTAINS disease_name \nMATCH (dr:Drug)-[:CAUSES]->(se:SideEffect) \nRETURN DISTINCT dr.name AS drug, se.name AS side_effect \n \n \n \nFig. 4. Graphical User Interface of the Biomedical Knowledge Explorer System \nB. Quantitative Evaluation \nThe performance of the proposed system is evaluated on the \niBKH knowledge graph using Exact Match, Precision@k, \nRecall@k, F1@k, Hits@k, MRR, and total response time across \nsimple, medium, and complex biomedical queries. These metrics \nwere measured for each difficulty level (simple, medium, and \ncomplex) to see how performance changes as questions become \nmore challenging.\n \nTABLE VI.  QUERY GENERATION EXACT MATCH BY DIFFICULTY LEVEL""
 'Omar & Mohammed / Journal of Applied Science and Technology Trends Vol. 06, No. 02, pp. 342 ‚Äì357 (2025) \n \n345 \nrecord-high accuracy, while open-source models achieved \nimpressive gains through prompt optimization. \nFeng et al.[22] developed the Knowledge Graph-based \nThought (KGT) framework that integrated LLMs with a pan-\ncancer knowledge graph for biomedical question answering. \nKGT was designed to reason on the knowledge graph schema and \nidentify optimal subgraphs to use for directing accurate answer \ngeneration, all without fine-tuning the LLMs. The framework is \nbenchmarked against a new dataset (PcQA) designed specifically \nfor pan-cancer KGQA tasks and has outperformed all existing \nstate-of-the-art approaches by a rather large margin. KGT‚Äôs \npracticality in biomedical issues was highlighted through case \nstudies for drug repositioning, drug resistance, and biomarker \ndiscovery. Their approach exhibited robust adaptability among \nvarious LLMs. \nRezaei et al.[26] developed AMG-RAG, a dynamic \nframework that utilizes autonomous LLM agents with medical \nsearch tools in the continuous construction and real-time updating \nof Medical Knowledge Graphs (MKGs). Their system \nincorporated confidence scoring and multi-hop reasoning to \nimprove accuracy and interpretability in medical question \nanswering. AMG-RAG outperformed size models on both very \nhard MEDQA benchmarks and more accessible MedMCQA \nones, proving that it could conduct efficient reasoning based on \ncurrent structured medical knowledge. They also used Neo4j to \nmanage the knowledge graphs while adding external searches to \nensure the latest data. \nTiwari et al.[24] presented Auto-Cypher, a recent automated \npipeline for producing high-quality synthetic data for training \nLLMs by mapping natural language to Cypher queries for graph \ndatabases like Neo4j. The pipeline deployed the novelty of LLM-\nas-database-filler to synthesize Neo4j databases for the execution \nof generated queries to ensure their correctness. A sizable dataset \ncalled SynthCypher was created, spanning multiple domains and \ncomplex queries, leading to a 40% improvement in LLM \nperformance on Cypher generation. The datasets were used to \nfine-tune open-source models such as LLaMA, Mistral, and \nQwen, and the SPIDER benchmark was adapted for evaluation \npurposes. \nMohammed et al.[29] proposed a hybridized GraphRAG \nframework combining Neo4j-based UMLS knowledge graphs \nwith a vector store for medical textbooks to create an improved \nU.S.M.L.E.-style clinical question-answering approach. The \nproject integrated symbolic reasoning from knowledge graphs \nwith semantic retrieval performed on text embeddings to enhance \nrelevance and accuracy via adaptive re-ranking and query \nexpansion. The system had the answers produced by GPT-4o-\nMini, with different prompting strategies encouraging evidence-\nbased and traceable responses grounded in verified medical \nknowledge. Experiments showed that the hybrid approach \nimproved factual accuracy and citation fidelity as compared to \nthe L.L.M.-only approach, enhancing transparency and \nreliability. It is shown that binding both structured and \nunstructured medical knowledge sources could aid in \nameliorating hallucinations and hence improve clinical \ntrustworthiness in AI-driven medical QA. \nYang et al.[30] articulated sepsis knowledge graph was \ncrafted by combining multicenter clinical data from over 10,000 \npatients with the help of GPT-4 for entity recognition and \nrelationship extraction. Real-world data were collected from \nthree hospitals and integrated with clinical guidelines and \ndatabases from the public domain. The knowledge graph \ncontained 1,894 nodes and 2,021 relationships pertaining to \ndiseases, symptoms, biomarkers, treatments, and complications. \nGPT outperformed other models in every resolution on sepsis-\nspecific datasets to obtain high F1-score results. The constructed \ngraph highlighted complex interactions in sepsis for assisting \nclinical decision-making and was implemented on Neo4j. \nGuan et al.[20] proposed a novel method for constructing a \nlocal knowledge graph from retrieved biomedical documents by \nextracting propositional claims. They carried out layer wise \nsummarization on this graph to capture multi-document \nrelationships and provide comprehensive contextual information \nto a language model for question-answering purposes. The \nmethod resolved issues in multi-document biomedical QA, such \nas noise cancellation and efficient context usage. They then tested \ntheir method on several benchmarks for biomedical question \nanswering, achieving performance at least comparable to, if not \nbetter than, existing retrieval-augmented generation (RAG) \nbaselines. The study established enhanced reasoning and answer \naccuracy of the model achieved through structured graph \nsummarization.   \nPrevious studies have improved biomedical QA using KGs \nand LLMs, but important gaps remain. Most systems lack \ntransparent, graph-based justifications, rely on limited evaluation \nmethods, or depend on cloud resources that reduce privacy and \nreproducibility. Our framework addresses these gaps by \nproviding visible Cypher queries with evidence subgraphs, \napplying comprehensive performance metrics across difficulty \nlevels, and ensuring fully local, privacy-preserving deployment. \nTable I summarizes key previous studies on biomedical \nknowledge graphs and question answering, outlining their \nmethods, datasets, and main limitations. \nIII. PRELIMINARIES \nThis section outlines the fundamental concepts required to \nunderstand the proposed system. It introduces biomedical \nknowledge graphs, explains how Neo4j stores data in graph \nform, and describes the use of Cypher for querying. It also \nprovides a brief overview of large language models (LLMs) and \ntheir role in interpreting natural language. \n \n \nA. Biomedical Knowledge Graphs \nBiomedical Knowledge Graphs (BKGs) provide a structured \nrepresentation of complex biomedical information by modeling \ndiverse medical entities, such as diseases, drugs, symptoms, and \nbiological pathways, as interconnected nodes within a graph \nstructure. The edges in these graphs represent the semantic \nrelationships between these entities, including ‚Äôtreats‚Äô, ‚Äôcauses‚Äô, \n‚Äôinteracts with‚Äô and many others, as illustrated in Fig 1. This \nform of representation enables the integration of heterogeneous \nbiomedical data from a wide range of sources, including']"
How does LLaMA 3 utilize the Integrated Biomedical Knowledge Hub (iBKH) dataset to improve the accuracy and transparency of biomedical question answering in the described system?,"['<1-hop>\n\nOmar & Mohammed / Journal of Applied Science and Technology Trends Vol. 06, No. 02, pp. 342 ‚Äì357 (2025) 348 drugs, diseases, symptoms) and execution returns structured data (tuples) relevant to the question. Step 5. Answer Synthesis: The structured tuples flow to Answer Synthesis, which aggregates and formats them into a concise raw answer. This raw answer is sent back to LLaMA 3 to optionally refine the phrasing while preserving the retrieved facts. Step 6. Result Presentation: LLaMA 3 produces the final answer, which the interface displays together with the executed Cypher query and an optional preview of the returned rows, improving transparency and trust. The pipeline couples LLM-based language understanding (LLaMA 3) with a schema-grounded Neo4j knowledge graph. The Cypher Query Gen refines the query formulation, Query Execution retrieves evidence and Answer Synthesis converts structured results into readable outputs that produce answers that are accurate, interpretable, and easy to audit directly from the displayed query and evidence.'
 '<2-hop>\n\nB. Dataset and Knowledge Graph Construction 1) Dataset The integrated Biomedical Knowledge Hub (iBKH), a large biomedical knowledge base, forms the first level of the system and integrates information from various curated high-quality biomedical databases. This implies that the data set includes various types of entities, such as diseases, symptoms, drugs, biological pathways, etc. This study used the representative subset of the iBKH dataset, which contained 65828 biomedical entities. These entities are semantically interconnected through a total of 3004166 relationships, thus creating a rich knowledge graph. The iBKH dataset was originally introduced in [11], and it is freely available at (https://github.com/wcm-wanglab/iBKH). This dataset is the core semantic foundation upon which this study is built. The knowledge graph is populated from multiple tabular sources (CSV files), each listing entities or relationships. The main input files and their contents are as follows: ‚Ä¢ Disease vocabulary(disease_vocab.csv): Contains columns such as primary (a unique disease ID), name, do_id (Disease Ontology ID), kegg_id, and umls_cui (UMLS Concept ID). Each row represents a disease node with external identifiers. ‚Ä¢ Drug vocabulary (drug_vocab.csv): Includes primary (unique drug ID), name, drugbank_id, kegg_id, pharmgkb_id, umls_cui, mesh_id, iDISK_id and CID (PubChem ID). Each row defines a drug node with standard database identifiers. ‚Ä¢ Symptom vocabulary (symptom_vocab.csv): Contains primary (unique symptom ID), name, mesh_id, umls_cui and iDISK_id. Each row defines a symptom node. ‚Ä¢ Side effect vocabulary (side_effect_vocab.csv): Includes primary (unique side-effect ID) and name. Each row represents a side-effect node (with UMLS ID when available). ‚Ä¢ Pathway vocabulary (pathway_vocab.csv): Contains primary (unique pathway ID), name, reactome_id, go_id, and kegg_id. Each row defines a biological pathway node. Relationship files (each row typically contains two entity IDs and one or more boolean flags or codes) include: ‚Ä¢ Disease‚ÄìSymptom links (Di_Sy_res.csv): Rows include Disease and Symptom IDs, a presence flag (1 or 0) and a data source. If Present = 1, a HAS_SYMPTOM edge is created from the disease to the symptom, with properties for presence and source. ‚Ä¢ Disease‚ÄìDisease links (di_di_res.csv): Rows include Disease_1 and Disease_2 IDs with binary flags for is_a and Resemble. If is_a = 1, an (IS_A) edge is created (Disease_1 ‚Üí Disease_2); if Resemble = 1, a RESEMBLES edge is created. The source field is used for provenance. ‚Ä¢ Drug‚ÄìDisease links (D_Di_res.csv): Includes Drug and Disease IDs with several binary flags. If a flag equals 1, a corresponding edge is created: o TREATS (Treats = 1) o PALLIATES (Palliates = 1) o ASSOCIATED_WITH (Associate = 1) o ALLEVIATES_REDUCES (alleviates = 1) o TREATMENT_THERAPY (treatment/therapy = 1) o INHIBITS_CELL_GROWTH (inhibits cell growth = 1) o HAS_BIOMARKER (biomarkers = 1) o PREVENTS_SUPPRESSES (prevents/suppresses = 1) o ROLE_IN_PATHOGENESIS (role in disease pathogenesis = 1) ‚Ä¢ Drug‚ÄìSideEffect links (D_SE_res.csv): Contains Drug and SideEffect IDs with a Source column. Each row creates a CAUSES edge from the drug to the side effect, with source as an edge property. ‚Ä¢ Drug‚ÄìDrug interactions (D_D_res.csv): Rows include Drug_1 and Drug_2 IDs with flags for Interaction and Resemble. If Interaction = 1, an INTERACTS_WITH edge is created (bidirectional). If Resemble = 1, a RESEMBLES edge is added. ‚Ä¢ Drug‚ÄìPathway links (D_Pwy_res.csv): Includes Drug ID and Pathway ID. Each row generates an ASSOCIATED_WITH edge from the drug to the pathway. ‚Ä¢ Disease‚ÄìPathway links (Di_Pwy_res.csv): Contains Disease ID and Pathway ID. Each row creates an ASSOCIATED_WITH edge from the disease to the pathway. 2) Data Upload Performance The time required to upload different types of entities and relationships into the Neo4j biomedical knowledge graph, measured in seconds. These measurements reflect both the size and complexity of the data being processed. As shown in Table II, the longest upload time is for Drug- Drug Relationships, which takes approximately 190 seconds due to the large number of edges (over 3 million). Following this, Disease-Disease and Drug-Disease Relationships also require considerable time for loading. On the other hand, individual']","LLaMA 3 is integrated into a biomedical question answering pipeline that leverages the Integrated Biomedical Knowledge Hub (iBKH) dataset as its core semantic foundation. The iBKH dataset contains 65,828 biomedical entities interconnected by over 3 million relationships, forming a rich knowledge graph with entities such as diseases, symptoms, drugs, and biological pathways. In the system, structured data relevant to a biomedical question is retrieved from this knowledge graph using Cypher queries refined by the Query Generation component. The retrieved structured tuples are then aggregated and formatted into a concise raw answer, which is sent to LLaMA 3. LLaMA 3 optionally refines the phrasing of this answer while preserving the retrieved facts, producing the final answer. This final output is presented alongside the executed Cypher query and an optional preview of the returned rows, enhancing transparency and trust. Thus, LLaMA 3 couples large language model-based understanding with the schema-grounded iBKH knowledge graph to produce answers that are accurate, interpretable, and easy to audit.",multi_hop_abstract_query_synthesizer,"LLaMA 3 utilizes the Integrated Biomedical Knowledge Hub (iBKH) dataset to improve the accuracy and transparency of biomedical question answering through a hybrid framework that tightly integrates the language model with a Neo4j-based biomedical knowledge graph derived from iBKH. The key ways this integration enhances the system are as follows:

1. **Rich, Structured Knowledge Base from iBKH**:  
   - The iBKH dataset provides a large-scale, comprehensive biomedical knowledge graph containing approximately 65,828 entities (including drugs, diseases, symptoms, pathways) and about 3 million relationships.  
   - This graph is constructed by integrating multiple high-quality biomedical databases, with entities normalized to standard identifiers (e.g., UMLS, DrugBank, KEGG) and relationships annotated with provenance and temporal metadata.  
   - This rich, multi-relational graph serves as a factual, structured foundation that grounds the system‚Äôs answers in verified biomedical knowledge.

2. **Translation of Natural Language to Executable Cypher Queries**:  
   - LLaMA 3 is employed locally to interpret users‚Äô natural language biomedical questions and translate them into precise, executable Cypher queries that run against the iBKH-derived Neo4j graph.  
   - The system uses prompt-driven, schema- and entity-aware prompting to guide LLaMA 3 in generating accurate Cypher queries that reflect the graph‚Äôs structure and semantics.  
   - This approach allows flexible handling of diverse question phrasings without relying on brittle template-based methods.

3. **Evidence Transparency and Explainability**:  
   - Alongside each answer, the system displays the exact Cypher query generated by LLaMA 3 and the supporting subgraph (nodes and edges) retrieved from the iBKH graph.  
   - A brief natural language rationale is also provided, explaining how the answer was derived from the graph data.  
   - This transparency enables users to validate and audit the system‚Äôs reasoning process, addressing the common issue of hallucinations in LLM-only biomedical QA systems.

4. **Robustness and Accuracy through Graph Grounding**:  
   - By grounding answers in the iBKH knowledge graph, the system significantly reduces hallucination errors typical of standalone LLMs, improving factual correctness.  
   - The system achieves high accuracy metrics (e.g., 96% exact match on simple queries, 95% on medium, and 86.7% on complex queries) and strong retrieval performance (Precision@5 of 96.1%, Recall@5 of 89.0%, F1@5 of 91.0%, Hits@k of 96.1%, MRR of 94.4%).  
   - The use of provenance and temporal metadata in iBKH also supports timestamp-aware query execution, ensuring up-to-date and clinically relevant answers.

5. **Local, Privacy-Preserving Deployment**:  
   - The entire system, including LLaMA 3 inference and Neo4j querying over the iBKH graph, runs locally without cloud dependencies.  
   - This setup ensures data privacy, reproducibility, and compliance with biomedical data governance, which is critical in sensitive healthcare environments.

6. **Comprehensive Evaluation and User Interface**:  
   - The system is evaluated end-to-end on a benchmark of 60 biomedical questions of varying complexity, using multiple metrics that assess both query generation correctness and answer retrieval quality.  
   - A user-friendly web interface built with Flask displays the input question, generated Cypher query, retrieved answers, and supporting evidence, promoting user trust and facilitating interactive exploration.

**In summary**, LLaMA 3 leverages the iBKH dataset by converting natural language questions into precise graph queries executed on a richly annotated biomedical knowledge graph. This hybrid approach grounds answers in verified data, provides transparent evidence for each response, and achieves high accuracy and efficiency, all within a local, privacy-conscious deployment suitable for biomedical applications.","['Omar & Mohammed / Journal of Applied Science and Technology Trends Vol. 06, No. 02, pp. 342 ‚Äì357 (2025) \n \n344 \nevaluated using Exact Match, Precision, Recall, F1, Hits@k, \nMRR, and latency across simple, medium, and complex question \nsets. Unlike prior template-based methods, our approach enables \ntraceable, outcome-level validation. In summary, the main \ncontributions of this work are as follows: \n‚Ä¢ Hybrid LLM to Cypher QA: A system that translates \nnatural language questions into accurate, executable \nCypher over a Neo4j drug, disease, and symptom KG. \n‚Ä¢ Prompt-driven query generation: Schema,  entity \naware prompting that reliably maps diverse biomedical \nquestions to graph queries. \n‚Ä¢ Evidence transparency: Along with each answer, we \nsurface the generated Cypher and the supporting \njustification subgraph (nodes, edges) plus a brief natural \nlanguage rationale. \n‚Ä¢ Answer-level evaluation: End-to-end assessment using \nExact Match, F1, Precision/Recall, Hits@k, MRR and \nlatency across simple, medium and complex tiers. \n‚Ä¢ Local, reproducible deployment: On-prem LLaMA 3 \ninference (no cloud dependency) suitable for biomedical \nsettings requiring low latency and strong data control. \n \nThe remainder of this paper is organized as follows: Section \n2 reviews related work on biomedical knowledge graphs and \nLLM-based QA systems. Section 3 provides background on \nknowledge graphs, large language models, and question \nanswering frameworks. Section 4 details the proposed \nmethodology, \nincluding \nsystem \narchitecture, \ndataset \nconstruction, and query translation. Section 5 presents the \nexperimental results through both quantitative metrics and \nqualitative case studies. Section 6 discusses the findings, analyzes \nlimitations, and compares performance against baseline models. \nFinally, Section 7 concludes this paper and outlines directions for \nfuture work. \nII. RELATED WORK \nRecently, studies have concentrated on the integration of \nclinical and medical knowledge graphs (LLM) to improve the \nanswer to medical questions. Researchers have derived several \nbiomedical KGs using Neo4j and incorporated the application of \nLLMs like LLaMA and GPT to convert natural language \nquestions into graph queries. Improvements in answer \ncorrectness, reduction of hallucination errors, one-to-many \nrelationships, and support for complex reasoning were the \nobjectives of these efforts. Some frameworks also adopted \nretrieval methods to ground responses in secure data. \nSu et al.[11] developed an integrative Biomedical Knowledge \nHub (iBKH), a huge biomedical knowledge graph that comprises \n18 of the very best data sources. The deployment of the iBKH in \nNeo4j allows for a user-friendly web portal to allow fast and \ninteractive knowledge retrieval. The system implemented \nadvanced graph learning techniques to enable the discovery of \nbiomedical knowledge, illustrated by an example of repurposing \nin silico drugs for Alzheimer‚Äôs disease. iBKH achieved \npromising predictive performance for known drugs and proposed \npossible new drug candidates. \nRajabi and Kafaie[19] proposed a disease knowledge graph \nusing a cross-referential disease database comprising diseases, \nsymptoms, and drugs interconnected with relationships. They \ntransferred the data into Neo4j to create a graph of 9,400 nodes \nand 45,000 relationships representing the semantic links between \nmedical concepts. Applying Cypher queries enabled answering \ncomplex medical questions regarding identifying drugs that may \ncause certain diseases; it was demonstrated that the graph inferred \nnew relationships not explicitly existing in the original data. The \nconclusion was that disease knowledge graphs sped up clinical \ndiscovery and contributed to understanding complex medical \nrelationships. \nHou et al.[3] assessed and contrasted ChatGPT (both GPT-\n3.5 and GPT-4) and the biomedical knowledge graphs (BKGs) \nconcerning their ability to answer biomedical questions, generate \nnew knowledge, and reason. Their datasets were focused on \ndietary supplements and drugs, while evaluation criteria entailed \naccuracy, novelty, and reasoning ability. The results indicate that \nwhile GPT-4 surpassed GPT-3.5 and BKGs in knowledge \nprovision, it proved inconsistent with regard to citations and \nreasoning. Compared to them, BKGs scored higher in accuracy \nand reliability, especially in discovering novel links as well as \nwithin structured reasoning. \nSoman et al.[13] presented a novel framework called KG-\nRAG that integrates a large biomedical knowledge graph \n(SPOKE) with LLaMA 2, GPT-3.5, and GPT-4 (LLMs) to \nproduce accurate biomedical text. They optimized the retrieval of \nrelevant graph context to cut over 50% tokens without losing \naccuracy. It aided LLMs in performing better on biomedical \nquestion answering with very high accuracy boosts, especially in \nthe case of LLaMA 2. They compared KG-RAG to other retrieval \nmethods and indicated its comparatively more robust and \nefficient results. The framework produced reliable evidence-\nbased answers grounded in biomedical knowledge. \nLuo et al.[23] created ChatKBQA, a new framework with a \nquestion-and-answer approach over knowledge bases that first \ngenerates logical forms with the help of fine-tuned LLMs and \nthen retrieves the relevant entities and relations. This generate-\nthen-retrieve approach is supposed to handle a couple of issues \nwith the earlier methods concerning tedious retrieval and error \npropagation. They fine-tuned open-source LLMs like LLaMA 2 \nto change natural-language questions into logical forms with high \naccuracy. The retrieval phase uses unsupervised phrase-level \nsemantic matching in a way that enhances the alignment of \nentities and relations. Experiments on benchmark datasets \nindicate ChatKBQA to be superior to its predecessors, with the \nhighest accuracy to date. \nPusch and Conrad[6] conducted work under a hybrid \napproach conflating LLMs and biomedical Knowledge Graphs \n(KGs) to suppress hallucination errors in question-answering. \nThey proposed query-checking algorithms for validating, \ncorrecting, and executing the KG Cypher queries that LLMs \ngenerated, thereby attaining accurate and understandable \nanswers. The system used retrieval-augmented generation (RAG) \nto ground answers within KG data. The methodology was \nvalidated on a biomedical KG called PrimeKG using 50 \nbenchmark questions, assessing models like GPT-4 Turbo and \nLLaMA 3. Commercially available GPT-4 Turbo obtained'
 'Vol. 06, No. 02, pp. 342 ‚Äì357 (2025) \nISSN: 2708-0757 \n \nJOURNAL OF APPLIED SCIENCE AND TECHNOLOGY TRENDS \n \nwww.jastt.org  \n \n                                                          342 \ndoi: 10.38094/jastt62404  \nA Hybrid LLM‚ÄìKnowledge Graph Framework for Accurate \nBiomedical Question Answering \n \nHavraz Y. Omar1,¬≤*, Abdulhakeem O. Mohammed¬≥  \n \n¬πDepartment of Information Technology, Technical College of Duhok, Duhok Polytechnic University, Duhok, Kurdistan Region, \nIraq. havraz.omar@dpu.edu.krd  \n¬≤ Department of Information Technology, Technical College of Informatics ‚Äì Akre, Akre University for Applied Sciences, Akre, \nKurdistan Region, Iraq. \n¬≥ Department of Computer Science, College of Science, University of Zakho, Zakho, Kurdistan Region, Iraq. \na.mohammed@uoz.edu.krd \n \n*Correspondence: havraz.omar@dpu.edu.krd \n \nAbstract \nBiomedical question answering requires accurate and interpretable systems; however, existing approaches often face challenges such as \nlanguage model hallucinations and limited reasoning when relying solely on standalone knowledge graphs. To address these limitations, \nthis study proposes a hybrid framework that integrates the LLaMA-3B language model with a Neo4j-based drug‚Äìdisease‚Äìsymptom \nknowledge graph. The system translates natural language questions into executable Cypher queries, operates on an iBKH-derived graph \ncomprising over 65,000 entities and 3 million relationships, and returns answers with supporting evidence through a transparent interface. \nExperiments conducted on 60 biomedical questions across three levels of difficulty demonstrate the robustness of the approach: 96% \nexact match for simple queries, 95% for medium queries, and 86.7% for complex queries. Overall, the system achieves Precision@5 of \n96.1%, Recall@5 of 89.0%, F1@5 of 91.0%, Hits@k of 96.1%, and an MRR of 94.4%, while maintaining an average response time of \nonly 6.07 seconds. These results indicate that the system retrieves nearly all relevant answers, ranks them correctly, and delivers them \nwith latency low enough for interactive use. Moreover, unlike cloud-based APIs such as ChatGPT, which require internet connectivity \nand external data transmission, the proposed framework operates fully offline, ensuring privacy, reproducibility, and compliance with \nbiomedical data governance. Overall, this pipeline provides an accurate, efficient, and privacy-preserving solution for biomedical question \nanswering, making it a practical alternative to cloud-dependent approaches in sensitive healthcare contexts. \n \nKeywords: Knowledge Graph, LLM, Question Answering, Neo4j, Biomedical Informatics, Healthcare AI, LLaMA 3. \n \nReceived: August 14th, 2025 / Revised: October 10th, 2025 / Accepted: October 16th, 2025 / Online: October 20th, 2025 \n \nI. INTRODUCTION \nAnswering questions in the biomedical field is a difficult task \ndue to the complexity of medical knowledge and the need for \nprecision. In recent years, large language models  (LLMs) like \nLLaMA, GPT-4 have made progress in understanding and \ngenerating human-like responses to medical questions  [1,  2]. \nThese models can process large amounts of information and \nrespond in natural language, which makes them helpful in \nhealthcare settings [3]. However, they often struggle to provide \naccurate answers when dealing with specialized biomedical \ncontent [4, 5]. \nOne major issue with LLMs is a problem called hallucination, \nwhere the model generates information that sounds right but is \nactually incorrect or unsupported [6]. In medical applications, \nthis can be dangerous, as healthcare professionals rely on precise \nand trustworthy information  [7]. Therefore, researchers are \nexploring ways to combine LLMs with structured sources of \nknowledge to improve their reliability [8]. \nLLM-only systems in biomedicine still hallucinate and are \nhard to verify, limiting safe use [9, 10]. Biomedical knowledge \ngraphs (BKGs) such as iBKH and SPOKE curate multi-source \nfacts and enable multi-hop reasoning, yet they neither interpret \nfree text nor generate answers [11, 12]. Recent hybrids (KG-\naware RAG) improve grounding but often lack explicit path-level \njustifications and robust end-to-end answer evaluation [13, 14]. \nRecent studies have increasingly integrated Knowledge \nGraphs (KGs) with Large Language Models (LLMs) to improve \nfactual accuracy, reasoning, and reduce hallucinations. Notable \nexamples include DR.KNOWS, which combines UMLS-based \nKGs with LLMs for better diagnostic reasoning [15], KnowNet'
 'Omar & Mohammed / Journal of Applied Science and Technology Trends Vol. 06, No. 02, pp. 342 ‚Äì357 (2025) \n \n347 \nIn biomedical research and clinical settings, LLMs help \ntranslate natural language questions from doctors, researchers, or \npatients into precise, structured queries that can be executed on \nbiomedical knowledge graphs and databases. This makes it easier \nto retrieve detailed biomedical information like drug interactions, \ngene-disease associations, and symptom descriptions [42, 43]. \nDespite their power, LLMs can sometimes generate incorrect \nor fabricated responses, a phenomenon known as hallucination, \nwhich poses risks in sensitive biomedical contexts. These \nhallucinations occur because the models generate plausible \nanswers based on patterns learned from data rather than verified \nknowledge. To mitigate this, researchers integrate LLMs with \nbiomedical knowledge graphs to ground answers in factual data, \nsignificantly improving accuracy and reducing misinformation \n[4]. Further improvements come from fine-tuning LLMs on \nbiomedical corpora and carefully engineering prompts, which \nenhance their reliability and relevance in medical question \nanswering. \nAdditionally, combining LLMs with knowledge graphs and \nreasoning techniques is an active area of research that promises \nto increase the interpretability and trustworthiness of AI systems \nin biomedicine. These advances are critical for developing tools \nthat assist clinical decision-making and accelerate biomedical \ndiscoveries [43, 44]. \nIV. METHODS AND MATERIALS \nThis section describes the methodology used to build a \nbiomedical question-answer system. The proposed method \nconsists of three main stages; First, a biomedical knowledge \ngraph is constructed in the data ingestion phase, using structured \ndata sources (e.g., diseases, drugs, symptoms). Second, a \nlanguage model (LLaMA 3) interprets the user‚Äôs question \nwritten in English in the user interaction phase and converts it \ninto a Cypher query. Third, a graphical user interface allows \nusers to type questions and view the results interactively. \nA. System Architecture \nThe proposed framework is organized as a step-by-step \npipeline that integrates a local large language model (LLM) with \na biomedical knowledge graph stored in Neo4j. The overall \nworkflow is illustrated in Fig. 2. Each module performs a \nspecific function, and together they ensure that the system \ndelivers accurate, reliable, and explainable answers. \n \n \nFig. 2. Workflow of the LLaMA 3 and Neo4j-Based QA System \n \nStep 1. User Input (Flask Web Interface): A user submits \na biomedical question in natural language through a Flask-based \nweb interface. The same interface will later display the answer, \nthe executed query, and a compact preview of the retrieved rows. \nStep 2. LLM Processing and Initial Cypher: The text query \nis forwarded to a local LLaMA 3 module, which interprets the \nintent and drafts an initial Cypher pattern suitable for querying \nthe graph. \nStep 3. Cypher Query Generation: The Cypher Query Gen \nblock receives the initial pattern from LLaMA 3, canonicalizes \nand completes it (projection, DISTINCT, filters), and returns a \nfinalized Cypher query to the model. \nStep 4. Query execution on Neo4j: LLaMA 3 passes the \nfinalized query to the Query execution component (inside the \n‚ÄôAnswer modules‚Äô box), which runs it against the Neo4j \nKnowledge Graph. KG supplies knowledge graph data (e.g.,'
 'Omar & Mohammed / Journal of Applied Science and Technology Trends Vol. 06, No. 02, pp. 342 ‚Äì357 (2025) \n \n351 \nThis query searches for a disease node whose name contains \n‚Äôalzheimer‚Äô and follows HAS_SYMPTOM edges to list related \nsymptom names. The system then executes this cypher to retrieve \nanswers. The prompts (such as few-shot examples and schema \nhints) were carefully designed to help LLaMA 3 generate correct \nCypher queries. The model learns how to use the graph‚Äôs labels \nand relationships properly. For example, if a user asks, ‚ÄôWhich \ndrugs treat diabetes?‚Äô, LLaMA might respond with a suitable \nCypher query: \n \nMATCH (d:Drug)-[:TREATS]->(di:Disease) \nWHERE toLower(di.name) CONTAINS ""diabetes"" \nRETURN d.name  \n \nThis queries for drug nodes that have a TREATS edge to a \ndiabetes disease node. By leveraging LLaMA 3 in this way, our \nsystem can flexibly handle many phrasing variations without \nmanual mapping rules. \nD. Model Configuration & Decoding \nWe run a local LLaMA 3.2-3B model in GGUF format (llama-\n3.2-3b-instruct-q4_k_m.gguf) via llama.cpp, as shown in Table \nIV. \nTABLE IV.  MODEL RUNTIME AND DECODING SETTINGS \nRuntime settings \nDecoding settings \nn_ctx = 1024 \ntemperature = 0.2 \nn_threads = 12 \ntop_p = 0.95 \nn_gpu_layers = 33 \ntop_k = 40 \nn_batch = 512 \nrepeat_penalty = 1.1 \n \nmax_tokens = 80 \n \nseed = 42 \n \nE. Graph Subset and Versioning \nWe use an iBKH derived subgraph (‚âà65.8k nodes; ‚âà3.0M \nedges) spanning DRUG, DISEASE, SYMPTOM, PATHWAY. IDs are \nnormalized to CURIEs and duplicates collapsed across \nUMLS/DrugBank/DisGeNET/SIDER/KEGG. Each edge stores \nprovenance/licensing metadata (source, source_version, license, \nretrieved_at, \nevidence_pmids/urls) \nand, \nwhen \navailable, \ntemporal fields (valid_from, valid_to). We report coverage as \nthe \npercentage \nof \nevaluated \nquestions \nwhose \ngold \nentities/relations are present. \nF. Query Execution and Reliability \nAfter a Cypher query is generated, it is executed on the Neo4j \ndatabase through the official Neo4j Python driver, which \nmanages the secure connection and returns the results. Instead of \nrestricting the output with a fixed LIMIT (e.g., LIMIT 5), the \nsystem retrieves candidate results and evaluates them using \nstandardized retrieval metrics such as Hits@1, Hits@5, and \nHits@10. This approach ensures that the system remains \nresponsive while providing a fair assessment of ranking quality \nacross different cutoff levels, rather than depending solely on a \nfixed number of returned items. Neo4j‚Äôs indexing on key node \nproperties, such as name and primary identifiers, also helps speed \nup lookups as the knowledge graph grows. In cases where the \nlanguage model generates an incomplete or incorrect query, such \nas referencing nodes or relationships that do not exist, the system \ncatches the error and either retries with a simpler prompt or \ninforms the user. Together, these steps make sure that queries run \nquickly, return valid results, and keep the overall experience \nsmooth and reliable for biomedical question-answering. \nG. User Interface for Query Transparency \nThe system includes a lightweight, cross-platform graphical \nuser interface (GUI) implemented as a web application using the \nFlask framework in Python, with HTML and Bootstrap for \ninteractive visualization. The interface is designed to make the \nquestion-answering process transparent and accessible to users \nwithout technical expertise. It consists of three main panels: \n1. Input Panel: Where the user can enter a biomedical \nquestion in natural language. \n2. Query Panel: Which displays the Cypher query generated \nby the language model, allowing users to verify how their \nquestion was interpreted. \n3. Results Panel: Which presents the retrieved answers in a \nclear, readable format, accompanied by a brief natural \nlanguage explanation generated by the system. \nBy showing both the query and the answer, the GUI promotes \nuser trust and enables validation of the system‚Äôs reasoning \nprocess. The interface is lightweight enough to run smoothly on \nstandard desktop machines without additional dependencies, \nmaking it practical for local deployments in clinical or research \nsettings. Fig. 4 illustrates the overall layout. \nV. EXPERIMENTAL RESULTS \nIn this section, the proposed system is evaluated based on its \nability to translate natural-language biomedical questions into \nexecutable Cypher queries over the iBKH knowledge graph. The \nassessment is conducted end-to-end and focuses on three main \naspects: (i) the accuracy of query generation, measured by the \nproportion of correctly produced Cypher queries; (ii) system \nefficiency, quantified through total response time from question \nsubmission to final answer delivery, covering both query \ngeneration and execution; and (iii) the quality of retrieved \nanswers, evaluated using standardized information retrieval \nmetrics including Precision@k, Recall@k, F1@k, Hits@k, and \nMean Reciprocal Rank (MRR). For clarity, all these metrics are \nformally defined in the following subsections, with their \ncorresponding equations, and will be used consistently \nthroughout the results section. Together, these dimensions \nprovide a comprehensive view of both correctness and efficiency \nacross simple, medium, and complex biomedical queries. \nA. Description of the Experimental Data \nTo evaluate the proposed system, this work used a carefully \nselected set of biomedical questions designed to test how well the \nsystem understands natural language and converts it into Cypher \nqueries for a biomedical knowledge graph. \nTo evaluate the system, a benchmark of 60 biomedical \nquestions was constructed by the authors, guided by the schema \nand relations in iBKH. The questions were grouped into simple, \nmedium, and complex levels to assess performance across \ndifferent reasoning requirements. Gold-standard answers were \nmanually prepared for each question to enable quantitative \nevaluation using Exact Match, Precision@k, Recall@k, F1@k, \nHits@k, and MRR. The complete set of 60 questions is available \nat this link https://drive.google.com/drive/my-drive.'
 'Omar & Mohammed / Journal of Applied Science and Technology Trends Vol. 06, No. 02, pp. 342 ‚Äì357 (2025) \n \n346 \nscientific literature, clinical records, genomic databases, and \nexperimental findings [19, 31]. \nSuch integration creates a comprehensive biomedical \nknowledge base that supports advanced analytics and discovery. \nFor example, biomedical knowledge graphs can reveal previously \nunknown relationships (e.g., between drugs and diseases) and \nhelp prioritize potential biomarkers for complex conditions. The \nIntegrative Biomedical Knowledge Hub (iBKH) is one such \nlarge-scale graph that consolidates diverse biomedical resources \ninto a unified hub, enabling discovery at scale [11]. Beyond \niBKH, large biomedical knowledge graphs such as SPOKE \nfurther illustrate how graph integration accelerates research and \nsupports precision-medicine use cases [12]. Overall, these graphs \nserve as foundational resources for data-driven and personalized \nmedicine. These knowledge graphs serve as foundational \nresources for precision medicine, where treatment can be tailored \nto the individual‚Äôs biological profile, improving outcomes and \nminimizing side effects [19, 31]. \n \nTABLE I.  SUMMARY OF RELATED RESEARCH ON BIOMEDICAL KGS AND QUESTION ANSWERING \nRef. \nYear \nData/Graph \nMethod \nBaselines \nKey Metric \nLimitation \n[11] \n2023 \niBKH (18 biomedical \nsources, Neo4j) \nIntegrative KG + Graph \nlearning; drug repurposing \ncase \nKnown drugs, \nAlzheimer‚Äôs \nstudy \nPredictive performance \n(drug repurposing) \nLimited to Alzheimer‚Äôs case \nstudy; scalability and updates not \ndetailed \n[19] \n2023 \nDisease KG (9,400 \nnodes, 45,000 relations \nin Neo4j) \nCypher queries for disease‚Äì\ndrug‚Äìsymptom reasoning \nCross-referential \ndisease DB \nNew relation inference; \ncomplex query \nanswering \nLimited to single domain; lacks \nlarge-scale evaluation \n[3] \n2023 \nBKGs vs. GPT-3.5/4 \nComparative QA study: \nLLMs vs. KGs \nGPT-3.5, GPT-4, \nKG reasoning \nAccuracy, Novelty, \nReasoning \nGPT-4 inconsistent in \nreasoning/citations; KG less fluent \nbut more reliable \n[13] \n2024 \nSPOKE KG + \nLLaMA2, GPT-3.5, \nGPT-4 \nKG-optimized retrieval for \nLLMs (RAG) \nOther retrieval \nmethods \nAccuracy, token \nreduction >50% \nFocus on retrieval optimization, \nnot KG construction \n[23] \n2024 \nBenchmark KB datasets \nGenerate-then-retrieve \n(LLM ‚Üí logical form ‚Üí \nKB retrieval) \nPrior KBQA \nmethods \nAccuracy (highest to \ndate) \nRisk of error in logical form \ngeneration \n[6] \n2024 \nPrimeKG \nLLM + KG hybrid, Cypher \nquery validation, RAG \nGPT-4 Turbo, \nLLaMA 3 \nAccuracy, \nExplainability \nDependent on KG coverage; \ncomputationally intensive \n[22] \n2025 \nPan-cancer KG (PcQA \ndataset) \nKG-enhanced reasoning \n(subgraph selection) \nSOTA KGQA \nmethods \nOutperformed SOTA on \nPcQA \nLimited to pan-cancer focus; no \nfine-tuning explored \n[26] \n2025 \nDynamic Medical KG + \nNeo4j \nLLM agents + multi-hop \nreasoning \nMEDQA, \nMedMCQA \nbaselines \nAccuracy, \nInterpretability \nHigh system complexity; requires \ncontinuous updating \n[24] \n2025 \nSynthCypher dataset \n(Neo4j + synthetic \nqueries) \nLLM-supervised Cypher \ngeneration and verification \nSPIDER \nbenchmark \nCypher accuracy 40% \nSynthetic dataset may not capture \nall real-world cases \n[29] \n2025 \nUMLS KG + Neo4j \nHybrid GraphRAG \nLLM-only QA \nAccuracy, Citation \nfidelity \nMore complex pipeline; relies on \nexternal vector store \n[30] \n2025 \nClinical data (10k \npatients, 1,894 nodes, \nNeo4j) \nKG construction using \nGPT-4 for entity/relation \nextraction \nOther KG \nconstruction \nmethods \nHigh F1-scores \nFocus only on sepsis; limited \ngeneralization \n[20] \n2025 \nLocal KG from \nbiomedical documents \nMulti-level summarization \nover KG for QA \nRAG baselines \nQA accuracy, reasoning \nTested mainly on document QA; \nscalability not proven \n \nB. Neo4j Graph Database \nTo manage the complexity and large size of biomedical \nknowledge graphs, specialized graph databases are needed. \nNeo4j is one of the most popular graph databases designed to \nstore and query data structured as nodes (entities) and \nrelationships (edges), both of which can have descriptive \nproperties [32, 33]. It uses the property graph model, which \nmakes it easy to represent complex, connected biomedical data \nsuch as drug-gene interactions or disease pathways. Neo4j‚Äôs \nCypher query language is especially advantageous because it \nallows users to write expressive and efficient queries to explore \nmulti-step connections in the data [34]. \nNeo4j works well for biomedical data because it can quickly \nrun complicated queries over highly interconnected datasets. This \nis important in biology and medicine, where relationships \nbetween entities like proteins, diseases, and drugs are often \ncomplex and layered. Studies have shown that Neo4j handles \nlarge biomedical graphs efficiently, making it a favorite among \nresearchers and industry users alike [33, 35, 36]. Its indexing and \ncaching mechanisms also help speed up query processing and \ndata retrieval [37]. \nMoreover, \nNeo4j \nintegrates \nsmoothly \nwith \nmany \nprogramming languages and analytics tools, which makes it \neasier to build interactive biomedical applications and clinical \ndecision support systems that can turn complex graph data into \nuseful insights [38, 39]. \nC. Large Language Models (LLMs) in Biomedical Question \nAnswering \nLarge Language Models (LLMs) are powerful AI systems \ntrained on vast amounts of text data. They learn the structure and \npatterns of language, enabling them to understand questions, \ngenerate responses, summarize information, and perform other \ncomplex language tasks. Well-known models such as LLaMA \nand GPT-3 have greatly advanced the field of natural language \nprocessing by showing strong performance across many tasks \n[40, 41].'
 'Omar & Mohammed / Journal of Applied Science and Technology Trends Vol. 06, No. 02, pp. 342 ‚Äì357 (2025) \n \n348 \ndrugs, diseases, symptoms) and execution returns structured data \n(tuples) relevant to the question. \nStep 5. Answer Synthesis: The structured tuples flow to \nAnswer Synthesis, which aggregates and formats them into a \nconcise raw answer. This raw answer is sent back to LLaMA 3 \nto optionally refine the phrasing while preserving the retrieved \nfacts. \nStep 6. Result Presentation: LLaMA 3 produces the final \nanswer, which the interface displays together with the executed \nCypher query and an optional preview of the returned rows, \nimproving transparency and trust. \nThe pipeline couples LLM-based language understanding \n(LLaMA 3) with a schema-grounded Neo4j knowledge graph. \nThe Cypher Query Gen refines the query formulation, Query \nExecution retrieves evidence and Answer Synthesis converts \nstructured results into readable outputs that produce answers that \nare accurate, interpretable, and easy to audit directly from the \ndisplayed query and evidence. \nB. Dataset and Knowledge Graph Construction \n1) \nDataset \nThe integrated Biomedical Knowledge Hub (iBKH), a large \nbiomedical knowledge base, forms the first level of the system \nand integrates information from various curated high-quality \nbiomedical databases. This implies that the data set includes \nvarious types of entities, such as diseases, symptoms, drugs, \nbiological pathways, etc. This study used the representative \nsubset of the iBKH dataset, which contained 65828 biomedical \nentities. These entities are semantically interconnected through a \ntotal of 3004166 relationships, thus creating a rich knowledge \ngraph. The iBKH dataset was originally introduced in [11], and it \nis freely available at (https://github.com/wcm-wanglab/iBKH). \nThis dataset is the core semantic foundation upon which this \nstudy is built. The knowledge graph is populated from multiple \ntabular sources (CSV files), each listing entities or relationships. \nThe main input files and their contents are as follows: \n‚Ä¢ Disease \nvocabulary(disease_vocab.csv): \nContains \ncolumns such as primary (a unique disease ID), name, \ndo_id (Disease Ontology ID), kegg_id, and umls_cui \n(UMLS Concept ID). Each row represents a disease node \nwith external identifiers. \n‚Ä¢ Drug vocabulary (drug_vocab.csv): Includes primary \n(unique drug ID), name, drugbank_id, kegg_id, \npharmgkb_id, umls_cui, mesh_id, iDISK_id and CID \n(PubChem ID). Each row defines a drug node with \nstandard database identifiers. \n‚Ä¢ Symptom vocabulary (symptom_vocab.csv): Contains \nprimary (unique symptom ID), name, mesh_id, umls_cui \nand iDISK_id. Each row defines a symptom node. \n‚Ä¢ Side \neffect \nvocabulary \n(side_effect_vocab.csv): \nIncludes primary (unique side-effect ID) and name. Each \nrow represents a side-effect node (with UMLS ID when \navailable). \n‚Ä¢ Pathway vocabulary (pathway_vocab.csv): Contains \nprimary (unique pathway ID), name, reactome_id, go_id, \nand kegg_id. Each row defines a biological pathway \nnode. \nRelationship files (each row typically contains two entity IDs \nand one or more boolean flags or codes) include: \n‚Ä¢ Disease‚ÄìSymptom links (Di_Sy_res.csv): Rows include \nDisease and Symptom IDs, a presence flag (1 or 0) and a \ndata source. If Present = 1, a HAS_SYMPTOM edge is \ncreated from the disease to the symptom, with properties \nfor presence and source. \n‚Ä¢ Disease‚ÄìDisease links (di_di_res.csv): Rows include \nDisease_1 and Disease_2 IDs with binary flags for is_a \nand Resemble. If is_a = 1, an (IS_A) edge is created \n(Disease_1 ‚Üí Disease_2); if Resemble = 1, a \nRESEMBLES edge is created. The source field is used \nfor provenance. \n‚Ä¢ Drug‚ÄìDisease links (D_Di_res.csv): Includes Drug and \nDisease IDs with several binary flags. If a flag equals 1, \na corresponding edge is created: \no \nTREATS (Treats = 1) \no \nPALLIATES (Palliates = 1) \no \nASSOCIATED_WITH (Associate = 1) \no \nALLEVIATES_REDUCES (alleviates = 1) \no \nTREATMENT_THERAPY (treatment/therapy = \n1) \no \nINHIBITS_CELL_GROWTH \n(inhibits \ncell \ngrowth = 1) \no \nHAS_BIOMARKER (biomarkers = 1) \no \nPREVENTS_SUPPRESSES \n(prevents/suppresses = 1) \no \nROLE_IN_PATHOGENESIS (role in disease \npathogenesis = 1) \n‚Ä¢ Drug‚ÄìSideEffect links (D_SE_res.csv): Contains Drug \nand SideEffect IDs with a Source column. Each row \ncreates a CAUSES edge from the drug to the side effect, \nwith source as an edge property. \n‚Ä¢ Drug‚ÄìDrug interactions (D_D_res.csv): Rows include \nDrug_1 and Drug_2 IDs with flags for Interaction and \nResemble. If Interaction = 1, an INTERACTS_WITH \nedge is created (bidirectional). If Resemble = 1, a \nRESEMBLES edge is added. \n‚Ä¢ Drug‚ÄìPathway links (D_Pwy_res.csv): Includes Drug \nID and Pathway ID. Each row generates an \nASSOCIATED_WITH edge from the drug to the \npathway. \n‚Ä¢ Disease‚ÄìPathway links (Di_Pwy_res.csv): Contains \nDisease ID and Pathway ID. Each row creates an \nASSOCIATED_WITH edge from the disease to the \npathway. \n2) \nData Upload Performance \nThe time required to upload different types of entities and \nrelationships into the Neo4j biomedical knowledge graph, \nmeasured in seconds. These measurements reflect both the size \nand complexity of the data being processed. \nAs shown in Table II, the longest upload time is for Drug-\nDrug Relationships, which takes approximately 190 seconds due \nto the large number of edges (over 3 million). Following this, \nDisease-Disease and Drug-Disease Relationships also require \nconsiderable time for loading. On the other hand, individual'
 'Omar & Mohammed / Journal of Applied Science and Technology Trends Vol. 06, No. 02, pp. 342 ‚Äì357 (2025) \n \n343 \nfor visualizing and validating LLM outputs [16], and MedKA for \nKG-enhanced question answering [17]. \nTo address these challenges, several recent works have \nexplored the integration of large language models with \nbiomedical knowledge graphs (KGs). A biomedical KG is a \nstructured network that connects entities such as diseases, drugs, \nand symptoms using defined relationships [18, 19]. These graphs \nstore verified medical knowledge from trusted databases, \nallowing for more accurate and explainable responses [12]. KGs \nare especially useful in multi-step reasoning tasks, where finding \nan answer requires connecting different pieces of information \n[20]. These entities and relationships can be visually represented \nin a biomedical knowledge graph, as shown in Fig. 1, where \nnodes represent medical concepts such as drugs, diseases, \nsymptoms, and pathways, and edges denote their semantic \nrelationships. \n \nFig. 1. Overview of Biomedical Knowledge Graph Entities and Relationships \n \nOne example of a widely used biomedical KG is SPOKE, \nwhich includes millions of nodes and relationships from over 40 \nbiomedical databases [12]. Integrating an LLM with a KG allows \nthe strengths of both technologies to work together: the LLM \nprovides language understanding, and the KG provides \nstructured, factual knowledge [21, 22]. A common method is \nretrieval-augmented generation (RAG), where the LLM retrieves \ninformation from the KG and uses it to generate more accurate \nresponses [13, 23]. In more advanced setups, the LLM can even \ngenerate queries like Cypher to fetch specific data from the graph \n[24, 25]. Neo4j is a popular graph database that supports fast and \nflexible storage and querying of knowledge graphs using Cypher \n[21]. It is well-suited for biomedical applications because it \nallows easy exploration of complex medical relationships. Recent \nwork has shown that combining Neo4j with LLMs can lead to \nbetter accuracy, fewer hallucinations, and more explainable \nresults [24, 26]. \nDespite improvements, building a reliable hybrid system that \ncombines an LLM with a biomedical KG remains a technical \nchallenge. Some approaches require complex pipelines or large \ntraining datasets, while others rely on fine-tuning specific to a \nnarrow set of questions [27, 28]. There is still a need for systems \nthat are both accurate and easy to scale, especially in domains like \nmedicine where the cost of errors is high [22]. \nRecent advances in KG-augmented LLMs have improved \nperformance, yet biomedical QA continues to face three practical \ngaps: \n1. Traceability: LLM-only or text-retrieval-only pipelines \nrarely provide graph-grounded justifications; users lack the \nability to inspect the exact nodes and edges that support an \nanswer. \n2. Evaluation: Prior work often judges quality via surface-\nform checks (e.g., matching a Cypher template), which fails to \ncapture end-to-end answer correctness or ranking quality \nacross different difficulty levels. \n3. Deployment: Many solutions assume cloud resources or \ndomain-specific fine-tuning, yet biomedical contexts typically \ndemand a local, privacy-preserving system with low latency \nand reproducible behavior. \nTimestamp-aware execution and periodic KG refresh help \navoid deprecated or contraindicated links, making the tool better \nsuited for safety-critical clinical contexts (e.g., drug‚Äìdrug \ninteractions). \nTo address these limitations, Our work introduces a locally \ndeployable pipeline that translates biomedical questions into \nexecutable Cypher queries over a Neo4j knowledge graph. The \nsystem returns answers with supporting nodes and edges, and is'
 'Omar & Mohammed / Journal of Applied Science and Technology Trends Vol. 06, No. 02, pp. 342 ‚Äì357 (2025) \n \n345 \nrecord-high accuracy, while open-source models achieved \nimpressive gains through prompt optimization. \nFeng et al.[22] developed the Knowledge Graph-based \nThought (KGT) framework that integrated LLMs with a pan-\ncancer knowledge graph for biomedical question answering. \nKGT was designed to reason on the knowledge graph schema and \nidentify optimal subgraphs to use for directing accurate answer \ngeneration, all without fine-tuning the LLMs. The framework is \nbenchmarked against a new dataset (PcQA) designed specifically \nfor pan-cancer KGQA tasks and has outperformed all existing \nstate-of-the-art approaches by a rather large margin. KGT‚Äôs \npracticality in biomedical issues was highlighted through case \nstudies for drug repositioning, drug resistance, and biomarker \ndiscovery. Their approach exhibited robust adaptability among \nvarious LLMs. \nRezaei et al.[26] developed AMG-RAG, a dynamic \nframework that utilizes autonomous LLM agents with medical \nsearch tools in the continuous construction and real-time updating \nof Medical Knowledge Graphs (MKGs). Their system \nincorporated confidence scoring and multi-hop reasoning to \nimprove accuracy and interpretability in medical question \nanswering. AMG-RAG outperformed size models on both very \nhard MEDQA benchmarks and more accessible MedMCQA \nones, proving that it could conduct efficient reasoning based on \ncurrent structured medical knowledge. They also used Neo4j to \nmanage the knowledge graphs while adding external searches to \nensure the latest data. \nTiwari et al.[24] presented Auto-Cypher, a recent automated \npipeline for producing high-quality synthetic data for training \nLLMs by mapping natural language to Cypher queries for graph \ndatabases like Neo4j. The pipeline deployed the novelty of LLM-\nas-database-filler to synthesize Neo4j databases for the execution \nof generated queries to ensure their correctness. A sizable dataset \ncalled SynthCypher was created, spanning multiple domains and \ncomplex queries, leading to a 40% improvement in LLM \nperformance on Cypher generation. The datasets were used to \nfine-tune open-source models such as LLaMA, Mistral, and \nQwen, and the SPIDER benchmark was adapted for evaluation \npurposes. \nMohammed et al.[29] proposed a hybridized GraphRAG \nframework combining Neo4j-based UMLS knowledge graphs \nwith a vector store for medical textbooks to create an improved \nU.S.M.L.E.-style clinical question-answering approach. The \nproject integrated symbolic reasoning from knowledge graphs \nwith semantic retrieval performed on text embeddings to enhance \nrelevance and accuracy via adaptive re-ranking and query \nexpansion. The system had the answers produced by GPT-4o-\nMini, with different prompting strategies encouraging evidence-\nbased and traceable responses grounded in verified medical \nknowledge. Experiments showed that the hybrid approach \nimproved factual accuracy and citation fidelity as compared to \nthe L.L.M.-only approach, enhancing transparency and \nreliability. It is shown that binding both structured and \nunstructured medical knowledge sources could aid in \nameliorating hallucinations and hence improve clinical \ntrustworthiness in AI-driven medical QA. \nYang et al.[30] articulated sepsis knowledge graph was \ncrafted by combining multicenter clinical data from over 10,000 \npatients with the help of GPT-4 for entity recognition and \nrelationship extraction. Real-world data were collected from \nthree hospitals and integrated with clinical guidelines and \ndatabases from the public domain. The knowledge graph \ncontained 1,894 nodes and 2,021 relationships pertaining to \ndiseases, symptoms, biomarkers, treatments, and complications. \nGPT outperformed other models in every resolution on sepsis-\nspecific datasets to obtain high F1-score results. The constructed \ngraph highlighted complex interactions in sepsis for assisting \nclinical decision-making and was implemented on Neo4j. \nGuan et al.[20] proposed a novel method for constructing a \nlocal knowledge graph from retrieved biomedical documents by \nextracting propositional claims. They carried out layer wise \nsummarization on this graph to capture multi-document \nrelationships and provide comprehensive contextual information \nto a language model for question-answering purposes. The \nmethod resolved issues in multi-document biomedical QA, such \nas noise cancellation and efficient context usage. They then tested \ntheir method on several benchmarks for biomedical question \nanswering, achieving performance at least comparable to, if not \nbetter than, existing retrieval-augmented generation (RAG) \nbaselines. The study established enhanced reasoning and answer \naccuracy of the model achieved through structured graph \nsummarization.   \nPrevious studies have improved biomedical QA using KGs \nand LLMs, but important gaps remain. Most systems lack \ntransparent, graph-based justifications, rely on limited evaluation \nmethods, or depend on cloud resources that reduce privacy and \nreproducibility. Our framework addresses these gaps by \nproviding visible Cypher queries with evidence subgraphs, \napplying comprehensive performance metrics across difficulty \nlevels, and ensuring fully local, privacy-preserving deployment. \nTable I summarizes key previous studies on biomedical \nknowledge graphs and question answering, outlining their \nmethods, datasets, and main limitations. \nIII. PRELIMINARIES \nThis section outlines the fundamental concepts required to \nunderstand the proposed system. It introduces biomedical \nknowledge graphs, explains how Neo4j stores data in graph \nform, and describes the use of Cypher for querying. It also \nprovides a brief overview of large language models (LLMs) and \ntheir role in interpreting natural language. \n \n \nA. Biomedical Knowledge Graphs \nBiomedical Knowledge Graphs (BKGs) provide a structured \nrepresentation of complex biomedical information by modeling \ndiverse medical entities, such as diseases, drugs, symptoms, and \nbiological pathways, as interconnected nodes within a graph \nstructure. The edges in these graphs represent the semantic \nrelationships between these entities, including ‚Äôtreats‚Äô, ‚Äôcauses‚Äô, \n‚Äôinteracts with‚Äô and many others, as illustrated in Fig 1. This \nform of representation enables the integration of heterogeneous \nbiomedical data from a wide range of sources, including']"
"How does the PharmGKB identifier integration in the drug vocabulary contribute to the construction and semantic richness of the iBKH biomedical knowledge graph, particularly in relation to drug-disease and drug-pathway relationships?","['<1-hop>\n\nOmar & Mohammed / Journal of Applied Science and Technology Trends Vol. 06, No. 02, pp. 342 ‚Äì357 (2025) 350 documentation, indexes ‚Äúenable quicker and more efficient pattern matching‚Äù by allowing the query planner to rapidly locate nodes by label and property. With the schema in place, data was imported using Cypher‚Äôs LOAD CSV commands. For each vocabulary file, rows with nonnull primary IDs were processed: the code used MERGE to create (or match existing) nodes with the given label and primary property, and then SET the remaining attributes from the CSV columns. For example, in importing disease_vocab.csv, each row produced a node (:Disease primary: <id>) with properties name, do_id, kegg_id, and umls_cui set from the row (empty strings were turned into null). Similarly, drug_vocab.csv rows produced drug nodes with properties for DrugBank, KEGG, PharmGKB IDs, etc. This approach follows best practice: using MERGE on the unique ID ensures that nodes are not duplicated during multiple passes. After all nodes were created, the relationship CSVs were loaded. Each row in those files was matched to the corresponding source and target nodes by their primary IDs, and conditional logic was used to create edges. For example, the disease- symptom file (Di_Sy_res.csv) was processed by matching a disease node and a symptom node for each row, then executing MERGE (d)-[r:HAS_SYMPTOM]-(s) if the present column is nonzero; the edge was given a present property and a source property from the CSV. The disease-disease file (di_di_res.csv) was processed by matching disease nodes d1 and d2:If is_a = 1, a (:Disease)-[:IS_A]->(:Disease) edge was merged; if Resemble = 1, a (:Disease)-[:RESEMBLES]->(:Disease) edge was merged. Drug-disease relationships were handled similarly: the script matched a Drug node and a Disease node for each row of D_Di_res.csv, then for each flag that was 1, it merged the corresponding edge label (such as TREATS, PALLIATES, ASSOCIATED_WITH, etc.) from the drug to the disease. Drug‚Äì side-effect rows produced (:Drug)-[:CAUSES]->(:SideEffect) edges with the source noted, and drug‚Äìdrug rows produced either INTERACTS_WITH or RESEMBLES edges between matched drug pairs. Finally, the drug-pathway and disease-pathway files each produced:ASSOCIATED_WITH edges linking drugs or diseases to pathway nodes. In this graph model, most relationships are directional (for example, a drug TREATS a disease; a disease does not TREAT a drug). This follows common practice in biomedical KGs. The same relations (such as RESEMBLES or INTERACTS_WITH) are inherently symmetric, but were stored as two directed edges or one undirected edge depending on implementation. All relationship creation steps used Cypher‚Äôs MERGE so that repeated loads or out-of-order imports did not create duplicate edges. This study used a static KG snapshot for reproducibility, but the system also supports incremental updates through the Neo4j MERGE and batch import functions. A summary of the different types of nodes and relationships is provided, together with their counts, descriptions, and examples in Table III. TABLE III. DETAILED SUMMARY OF KNOWLEDGE GRAPH COMPONENTS IN IBKH SUBSET Component Type Entity/Relationship Count Description Examples Disease 19,236 Medical conditions and disorders Diabetes, Hypertension, Cancer Drug 37,997 Pharmaceutical compounds Aspirin, Insulin, Amoxicillin'
 '<2-hop>\n\nB. Dataset and Knowledge Graph Construction 1) Dataset The integrated Biomedical Knowledge Hub (iBKH), a large biomedical knowledge base, forms the first level of the system and integrates information from various curated high-quality biomedical databases. This implies that the data set includes various types of entities, such as diseases, symptoms, drugs, biological pathways, etc. This study used the representative subset of the iBKH dataset, which contained 65828 biomedical entities. These entities are semantically interconnected through a total of 3004166 relationships, thus creating a rich knowledge graph. The iBKH dataset was originally introduced in [11], and it is freely available at (https://github.com/wcm-wanglab/iBKH). This dataset is the core semantic foundation upon which this study is built. The knowledge graph is populated from multiple tabular sources (CSV files), each listing entities or relationships. The main input files and their contents are as follows: ‚Ä¢ Disease vocabulary(disease_vocab.csv): Contains columns such as primary (a unique disease ID), name, do_id (Disease Ontology ID), kegg_id, and umls_cui (UMLS Concept ID). Each row represents a disease node with external identifiers. ‚Ä¢ Drug vocabulary (drug_vocab.csv): Includes primary (unique drug ID), name, drugbank_id, kegg_id, pharmgkb_id, umls_cui, mesh_id, iDISK_id and CID (PubChem ID). Each row defines a drug node with standard database identifiers. ‚Ä¢ Symptom vocabulary (symptom_vocab.csv): Contains primary (unique symptom ID), name, mesh_id, umls_cui and iDISK_id. Each row defines a symptom node. ‚Ä¢ Side effect vocabulary (side_effect_vocab.csv): Includes primary (unique side-effect ID) and name. Each row represents a side-effect node (with UMLS ID when available). ‚Ä¢ Pathway vocabulary (pathway_vocab.csv): Contains primary (unique pathway ID), name, reactome_id, go_id, and kegg_id. Each row defines a biological pathway node. Relationship files (each row typically contains two entity IDs and one or more boolean flags or codes) include: ‚Ä¢ Disease‚ÄìSymptom links (Di_Sy_res.csv): Rows include Disease and Symptom IDs, a presence flag (1 or 0) and a data source. If Present = 1, a HAS_SYMPTOM edge is created from the disease to the symptom, with properties for presence and source. ‚Ä¢ Disease‚ÄìDisease links (di_di_res.csv): Rows include Disease_1 and Disease_2 IDs with binary flags for is_a and Resemble. If is_a = 1, an (IS_A) edge is created (Disease_1 ‚Üí Disease_2); if Resemble = 1, a RESEMBLES edge is created. The source field is used for provenance. ‚Ä¢ Drug‚ÄìDisease links (D_Di_res.csv): Includes Drug and Disease IDs with several binary flags. If a flag equals 1, a corresponding edge is created: o TREATS (Treats = 1) o PALLIATES (Palliates = 1) o ASSOCIATED_WITH (Associate = 1) o ALLEVIATES_REDUCES (alleviates = 1) o TREATMENT_THERAPY (treatment/therapy = 1) o INHIBITS_CELL_GROWTH (inhibits cell growth = 1) o HAS_BIOMARKER (biomarkers = 1) o PREVENTS_SUPPRESSES (prevents/suppresses = 1) o ROLE_IN_PATHOGENESIS (role in disease pathogenesis = 1) ‚Ä¢ Drug‚ÄìSideEffect links (D_SE_res.csv): Contains Drug and SideEffect IDs with a Source column. Each row creates a CAUSES edge from the drug to the side effect, with source as an edge property. ‚Ä¢ Drug‚ÄìDrug interactions (D_D_res.csv): Rows include Drug_1 and Drug_2 IDs with flags for Interaction and Resemble. If Interaction = 1, an INTERACTS_WITH edge is created (bidirectional). If Resemble = 1, a RESEMBLES edge is added. ‚Ä¢ Drug‚ÄìPathway links (D_Pwy_res.csv): Includes Drug ID and Pathway ID. Each row generates an ASSOCIATED_WITH edge from the drug to the pathway. ‚Ä¢ Disease‚ÄìPathway links (Di_Pwy_res.csv): Contains Disease ID and Pathway ID. Each row creates an ASSOCIATED_WITH edge from the disease to the pathway. 2) Data Upload Performance The time required to upload different types of entities and relationships into the Neo4j biomedical knowledge graph, measured in seconds. These measurements reflect both the size and complexity of the data being processed. As shown in Table II, the longest upload time is for Drug- Drug Relationships, which takes approximately 190 seconds due to the large number of edges (over 3 million). Following this, Disease-Disease and Drug-Disease Relationships also require considerable time for loading. On the other hand, individual']","The PharmGKB identifier is included as a property in the drug nodes created from the drug_vocab.csv file, which contains unique drug IDs along with standard database identifiers such as drugbank_id, kegg_id, pharmgkb_id, umls_cui, mesh_id, iDISK_id, and CID. This integration ensures that each drug node in the iBKH knowledge graph is linked to PharmGKB, a curated biomedical database, enhancing the semantic interoperability and accuracy of the graph. The iBKH knowledge graph uses these drug nodes to establish multiple types of relationships, including drug-disease links (such as TREATS, PALLIATES, ASSOCIATED_WITH) and drug-pathway associations, by matching drug nodes with disease and pathway nodes through their unique IDs. This structured incorporation of PharmGKB identifiers facilitates precise merging of nodes and edges using Cypher's MERGE command, preventing duplication and enabling rich, directional relationships that reflect real biomedical interactions. Consequently, the PharmGKB integration significantly contributes to the comprehensive and semantically rich representation of drug-related biomedical knowledge within the iBKH knowledge graph.",multi_hop_specific_query_synthesizer,"The integration of PharmGKB identifiers within the drug vocabulary significantly enhances the construction and semantic richness of the iBKH biomedical knowledge graph, particularly in the context of drug-disease and drug-pathway relationships, as follows:

1. **Standardized and Unique Drug Identification**:  
   Each drug node in the iBKH is created using a unique primary drug ID and is enriched with multiple standardized external identifiers, including PharmGKB IDs, alongside DrugBank, KEGG, UMLS, MeSH, and PubChem IDs. The inclusion of PharmGKB identifiers ensures that drugs are consistently and uniquely represented across the graph, preventing duplication and enabling precise node matching during data import and relationship creation.

2. **Semantic Interoperability and Cross-Database Linking**:  
   PharmGKB is a well-established pharmacogenomics knowledge resource that links drugs to genetic and clinical information. By incorporating PharmGKB IDs into drug nodes, the iBKH knowledge graph can semantically interoperate with other biomedical databases and resources that also use PharmGKB identifiers. This facilitates richer integration of pharmacogenomic data, which is crucial for understanding drug effects, mechanisms, and personalized medicine applications.

3. **Enhanced Relationship Mapping for Drug-Disease and Drug-Pathway Edges**:  
   The drug vocabulary enriched with PharmGKB IDs supports accurate and reliable matching of drug nodes when creating relationships such as TREATS, PALLIATES, ASSOCIATED_WITH, and others linking drugs to diseases (from D_Di_res.csv) and pathways (from D_Pwy_res.csv). This precise identification ensures that edges represent true biological and clinical associations, improving the graph‚Äôs semantic accuracy.

4. **Improved Querying and Reasoning Capabilities**:  
   With PharmGKB IDs integrated, queries that involve drug-related information can leverage these standardized identifiers to retrieve comprehensive and semantically consistent data. This is particularly important for multi-hop queries involving drug-disease and drug-pathway relationships, enabling more effective biomedical question answering and discovery.

5. **Support for Pharmacogenomics and Precision Medicine Use Cases**:  
   Since PharmGKB focuses on pharmacogenomics, its integration enriches the iBKH with genetic and clinical context related to drugs. This allows the knowledge graph to better support precision medicine applications, such as identifying drug responses influenced by genetic variants, understanding drug mechanisms in pathways, and exploring drug repositioning opportunities.

In summary, the PharmGKB identifier integration in the drug vocabulary contributes to the iBKH knowledge graph by providing a standardized, interoperable, and semantically rich foundation for drug entities. This enhances the accuracy and depth of drug-disease and drug-pathway relationships, thereby improving the graph‚Äôs utility for biomedical research, clinical decision support, and personalized medicine.","['Omar & Mohammed / Journal of Applied Science and Technology Trends Vol. 06, No. 02, pp. 342 ‚Äì357 (2025) \n \n350 \ndocumentation, indexes ‚Äúenable quicker and more efficient \npattern matching‚Äù by allowing the query planner to rapidly locate \nnodes by label and property. \nWith the schema in place, data was imported using Cypher‚Äôs \nLOAD CSV commands. For each vocabulary file, rows with \nnonnull primary IDs were processed: the code used MERGE to \ncreate (or match existing) nodes with the given label and primary \nproperty, and then SET the remaining attributes from the CSV \ncolumns. For example, in importing disease_vocab.csv, each row \nproduced a node (:Disease primary: <id>) with properties name, \ndo_id, kegg_id, and umls_cui set from the row (empty strings \nwere turned into null). Similarly, drug_vocab.csv rows produced \ndrug nodes with properties for DrugBank, KEGG, PharmGKB \nIDs, etc. This approach follows best practice: using MERGE on \nthe unique ID ensures that nodes are not duplicated during \nmultiple passes. \nAfter all nodes were created, the relationship CSVs were \nloaded. Each row in those files was matched to the corresponding \nsource and target nodes by their primary IDs, and conditional \nlogic was used to create edges. For example, the disease-\nsymptom file (Di_Sy_res.csv) was processed by matching a \ndisease node and a symptom node for each row, then executing \nMERGE (d)-[r:HAS_SYMPTOM]-(s) if the present column is \nnonzero; the edge was given a present property and a source \nproperty from the CSV. The disease-disease file (di_di_res.csv) \nwas processed by matching disease nodes d1 and d2:If is_a = 1, \na (:Disease)-[:IS_A]->(:Disease) edge was merged; if Resemble \n= 1, a (:Disease)-[:RESEMBLES]->(:Disease) edge was merged. \nDrug-disease relationships were handled similarly: the script \nmatched a Drug node and a Disease node for each row of \nD_Di_res.csv, then for each flag that was 1, it merged the \ncorresponding edge label (such as TREATS, PALLIATES, \nASSOCIATED_WITH, etc.) from the drug to the disease. Drug‚Äì\nside-effect rows produced (:Drug)-[:CAUSES]->(:SideEffect) \nedges with the source noted, and drug‚Äìdrug rows produced either \nINTERACTS_WITH or RESEMBLES edges between matched \ndrug pairs. Finally, the drug-pathway and disease-pathway files \neach produced:ASSOCIATED_WITH edges linking drugs or \ndiseases to pathway nodes. \nIn this graph model, most relationships are directional (for \nexample, a drug TREATS a disease; a disease does not TREAT \na drug). This follows common practice in biomedical KGs. The \nsame relations (such as RESEMBLES or INTERACTS_WITH) \nare inherently symmetric, but were stored as two directed edges \nor one undirected edge depending on implementation. All \nrelationship creation steps used Cypher‚Äôs MERGE so that \nrepeated loads or out-of-order imports did not create duplicate \nedges. \nThis study used a static KG snapshot for reproducibility, but \nthe system also supports incremental updates through the Neo4j \nMERGE and batch import functions. A summary of the different \ntypes of nodes and relationships is provided, together with their \ncounts, descriptions, and examples in Table III.\n \nTABLE III.  DETAILED SUMMARY OF KNOWLEDGE GRAPH COMPONENTS IN IBKH SUBSET \nComponent Type \nEntity/Relationship \nCount \nDescription \nExamples \n \nDisease \n19,236 \nMedical conditions and disorders \nDiabetes, Hypertension, Cancer \n \nDrug \n37,997 \nPharmaceutical compounds \nAspirin, Insulin, Amoxicillin \nNodes \nSymptom \n1,361 \nClinical signs and symptoms \nHeadache, Fever, Nausea \n \nSideEffect \n4,251 \nNegative outcomes of drugs \nNausea, Drowsiness \n \nPathway \n2,983 \nBiological processes and pathways \nApoptosis, Glycolysis \n \nASSOCIATED_WITH \n101,534 \nGeneral associations (disease‚Äì\npathway, drug‚Äìpathway, etc.) \nDiabetes ASSOCIATED_WITH Pathway \n \nCAUSES \n145,321 \nDrug-SideEffect relationships \nAspirin CAUSES Gastric_Bleeding \n \nHAS_SYMPTOM \n3,357 \nDisease-symptom links \nCOVID-19 HAS_SYMPTOM Fever \n \nHAS_BIOMARKER \n412 \nBiomarkers linked to disease \nPSA HAS_BIOMARKER Prostate_Cancer \nRelationships \nINHIBITS_CELL_GROWTH \n1,508 \nDrugs inhibiting cell growth \nChemo INHIBITS_CELL_GROWTH Tumor \n \nINTERACTS_WITH \n2,682,142 \nDrug-drug interactions \nAspirin INTERACTS_WITH Warfarin \n \nIS_A \n10,529 \nSubtype hierarchy \nFlu IS_A Viral_Infection \n \nPALLIATES \n388 \nDrug palliates disease \nMorphine PALLIATES Cancer \n \nPREVENTS_SUPPRESSES \n859 \nPreventive links \nVaccine PREVENTS_SUPPRESSES Measles \n \nRESEMBLES \n7,000 \nSimilarity relationships \nDrugA RESEMBLES DrugB \n \nTREATMENT_THERAPY \n44,852 \nTherapy relationships \nRadiotherapy TREATMENT_THERAPY Tumor \n \nTREATS \n5,491 \nDrug-disease links \nInsulin TREATS Diabetes \n \nALLEVIATES_REDUCES \n‚àº180,000 \nSymptom relief \nParacetamol ALLEVIATES_REDUCES Fever \nTotal \nNodes \n65,828 \nTotal biomedical entities \n‚Äî \n \nRelationships \n3,004,166 \nTotal knowledge graph links \n‚Äî \nC. Natural Language to Cypher Query Translation \nA key feature of the system is its ability to accept questions \nwritten in plain English and automatically generate the \ncorresponding Cypher queries. This is accomplished using \nMeta‚Äôs LLaMA 3 large language model, which runs entirely on \na local machine through the open-source llama.cpp framework. \nRunning the model locally ensures low-latency execution and \nkeeps sensitive queries within the user‚Äôs environment. \nTo generate a Cypher query, LLaMA 3 is prompted with \nexamples of natural language questions along with their correct \nCypher translations. The prompt also includes instructions on \nhow to navigate the structure of the graph schema. When a user \nenters a question (e.g., ‚ÄôWhat are the symptoms of Alzheimer‚Äôs \ndisease?‚Äô), the system inserts it into the prompt and asks LLaMA \n3 to produce a corresponding query. For example, the model may \nreturn: \nMATCH (d:Disease)-[:HAS_SYMPTOM]->(s:Symptom) \nWHERE toLower(d.name) CONTAINS ""alzheimer"" \nRETURN s.name'
 'Omar & Mohammed / Journal of Applied Science and Technology Trends Vol. 06, No. 02, pp. 342 ‚Äì357 (2025) \n \n348 \ndrugs, diseases, symptoms) and execution returns structured data \n(tuples) relevant to the question. \nStep 5. Answer Synthesis: The structured tuples flow to \nAnswer Synthesis, which aggregates and formats them into a \nconcise raw answer. This raw answer is sent back to LLaMA 3 \nto optionally refine the phrasing while preserving the retrieved \nfacts. \nStep 6. Result Presentation: LLaMA 3 produces the final \nanswer, which the interface displays together with the executed \nCypher query and an optional preview of the returned rows, \nimproving transparency and trust. \nThe pipeline couples LLM-based language understanding \n(LLaMA 3) with a schema-grounded Neo4j knowledge graph. \nThe Cypher Query Gen refines the query formulation, Query \nExecution retrieves evidence and Answer Synthesis converts \nstructured results into readable outputs that produce answers that \nare accurate, interpretable, and easy to audit directly from the \ndisplayed query and evidence. \nB. Dataset and Knowledge Graph Construction \n1) \nDataset \nThe integrated Biomedical Knowledge Hub (iBKH), a large \nbiomedical knowledge base, forms the first level of the system \nand integrates information from various curated high-quality \nbiomedical databases. This implies that the data set includes \nvarious types of entities, such as diseases, symptoms, drugs, \nbiological pathways, etc. This study used the representative \nsubset of the iBKH dataset, which contained 65828 biomedical \nentities. These entities are semantically interconnected through a \ntotal of 3004166 relationships, thus creating a rich knowledge \ngraph. The iBKH dataset was originally introduced in [11], and it \nis freely available at (https://github.com/wcm-wanglab/iBKH). \nThis dataset is the core semantic foundation upon which this \nstudy is built. The knowledge graph is populated from multiple \ntabular sources (CSV files), each listing entities or relationships. \nThe main input files and their contents are as follows: \n‚Ä¢ Disease \nvocabulary(disease_vocab.csv): \nContains \ncolumns such as primary (a unique disease ID), name, \ndo_id (Disease Ontology ID), kegg_id, and umls_cui \n(UMLS Concept ID). Each row represents a disease node \nwith external identifiers. \n‚Ä¢ Drug vocabulary (drug_vocab.csv): Includes primary \n(unique drug ID), name, drugbank_id, kegg_id, \npharmgkb_id, umls_cui, mesh_id, iDISK_id and CID \n(PubChem ID). Each row defines a drug node with \nstandard database identifiers. \n‚Ä¢ Symptom vocabulary (symptom_vocab.csv): Contains \nprimary (unique symptom ID), name, mesh_id, umls_cui \nand iDISK_id. Each row defines a symptom node. \n‚Ä¢ Side \neffect \nvocabulary \n(side_effect_vocab.csv): \nIncludes primary (unique side-effect ID) and name. Each \nrow represents a side-effect node (with UMLS ID when \navailable). \n‚Ä¢ Pathway vocabulary (pathway_vocab.csv): Contains \nprimary (unique pathway ID), name, reactome_id, go_id, \nand kegg_id. Each row defines a biological pathway \nnode. \nRelationship files (each row typically contains two entity IDs \nand one or more boolean flags or codes) include: \n‚Ä¢ Disease‚ÄìSymptom links (Di_Sy_res.csv): Rows include \nDisease and Symptom IDs, a presence flag (1 or 0) and a \ndata source. If Present = 1, a HAS_SYMPTOM edge is \ncreated from the disease to the symptom, with properties \nfor presence and source. \n‚Ä¢ Disease‚ÄìDisease links (di_di_res.csv): Rows include \nDisease_1 and Disease_2 IDs with binary flags for is_a \nand Resemble. If is_a = 1, an (IS_A) edge is created \n(Disease_1 ‚Üí Disease_2); if Resemble = 1, a \nRESEMBLES edge is created. The source field is used \nfor provenance. \n‚Ä¢ Drug‚ÄìDisease links (D_Di_res.csv): Includes Drug and \nDisease IDs with several binary flags. If a flag equals 1, \na corresponding edge is created: \no \nTREATS (Treats = 1) \no \nPALLIATES (Palliates = 1) \no \nASSOCIATED_WITH (Associate = 1) \no \nALLEVIATES_REDUCES (alleviates = 1) \no \nTREATMENT_THERAPY (treatment/therapy = \n1) \no \nINHIBITS_CELL_GROWTH \n(inhibits \ncell \ngrowth = 1) \no \nHAS_BIOMARKER (biomarkers = 1) \no \nPREVENTS_SUPPRESSES \n(prevents/suppresses = 1) \no \nROLE_IN_PATHOGENESIS (role in disease \npathogenesis = 1) \n‚Ä¢ Drug‚ÄìSideEffect links (D_SE_res.csv): Contains Drug \nand SideEffect IDs with a Source column. Each row \ncreates a CAUSES edge from the drug to the side effect, \nwith source as an edge property. \n‚Ä¢ Drug‚ÄìDrug interactions (D_D_res.csv): Rows include \nDrug_1 and Drug_2 IDs with flags for Interaction and \nResemble. If Interaction = 1, an INTERACTS_WITH \nedge is created (bidirectional). If Resemble = 1, a \nRESEMBLES edge is added. \n‚Ä¢ Drug‚ÄìPathway links (D_Pwy_res.csv): Includes Drug \nID and Pathway ID. Each row generates an \nASSOCIATED_WITH edge from the drug to the \npathway. \n‚Ä¢ Disease‚ÄìPathway links (Di_Pwy_res.csv): Contains \nDisease ID and Pathway ID. Each row creates an \nASSOCIATED_WITH edge from the disease to the \npathway. \n2) \nData Upload Performance \nThe time required to upload different types of entities and \nrelationships into the Neo4j biomedical knowledge graph, \nmeasured in seconds. These measurements reflect both the size \nand complexity of the data being processed. \nAs shown in Table II, the longest upload time is for Drug-\nDrug Relationships, which takes approximately 190 seconds due \nto the large number of edges (over 3 million). Following this, \nDisease-Disease and Drug-Disease Relationships also require \nconsiderable time for loading. On the other hand, individual'
 'D384 Nucleic Acids Research, 2016, Vol. 44, Database issue\net al. (2013) The BioGRID interaction database: 2013 update.\nNucleic Acids Res., 41, D816‚ÄìD823.\n14. Kj√¶rulff,S.K., Wich,L., Kringelum,J., Jacobsen,U.P.,\nKouskoumvekaki,I., Audouze,K., Lund,O., Brunak,S., Oprea,T.I.\nand Taboureau,O. (2013) ChemProt-2.0: visual navigation in a\ndisease chemical biology database. Nucleic Acids Res., 41,\nD464‚ÄìD469.\n15. Hopkins,A.L., Groom,C.R. and Alex,A. (2004) Ligand efficiency: a\nuseful metric for lead selection. Drug Discov. Today, 9, 430‚Äì431.\n16. Geeleher,P., Cox,N.J. and Huang,R.S. (2014) Clinical drug response\ncan be predicted using baseline gene expression levels and in vitro\ndrug sensitivity in cell lines. Genome Biol., 15, R47.\n17. Szklarczyk,D., Franceschini,A., Wyder,S., Forslund,K., Heller,D.,\nHuerta-Cepas,J., Simonovic,M., Roth,A., Santos,A., Tsafou,K.P.\net al. (2015) STRING v10: protein‚Äìprotein interaction networks,\nintegrated over the tree of life. Nucleic Acids Res., 43, D447‚ÄìD452.\n18. Kuhn,M., Szklarczyk,D., Pletscher-Frankild,S., Blicher,T.H., von\nMering,C., Jensen,L.J. and Bork,P. (2014) STITCH 4: integration of\nprotein‚Äìchemical interactions with user data. Nucleic Acids Res., 42,\nD401‚ÄìD407.\n19. Law,V., Knox,C., Djoumbou,Y., Jewison,T., Guo,A.C., Liu,Y.,\nMaciejewski,A., Arndt,D., Wilson,M., Neveu,V. et al. (2014)\nDrugBank 4.0: shedding new light on drug metabolism. Nucleic\nAcids Res., 42, D1091‚ÄìD1097.\n20. Okuno,Y., Tamon,A., Yabuuchi,H., Niijima,S., Minowa,Y.,\nTonomura,K., Kunimoto,R. and Feng,C. (2008) GLIDA:\nGPCR-ligand database for chemical genomics drug\ndiscovery‚Äì‚Äìdatabase and tools update. Nucleic Acids Res., 36,\nD907‚ÄìD912.\n21. G¬®unther,S., Kuhn,M., Dunkel,M., Campillos,M., Senger,C.,\nPetsalaki,E., Ahmed,J., Urdiales,E.G., Gewiess,A., Jensen,L.J. et al.\n(2008) SuperTarget and Matador: resources for exploring drug-target\nrelationships. Nucleic Acids Res., 36, D919‚ÄìD922.\n22. Zhu,F., Shi,Z., Qin,C., Tao,L., Liu,X., Xu,F., Zhang,L., Song,Y.,\nLiu,X., Zhang,J. et al. (2012) Therapeutic target database update\n2012: a resource for facilitating target-oriented drug discovery.\nNucleic Acids Res., 40, D1128‚ÄìD1136.\n23. Davis,A.P., Grondin,C.J., Lennon-Hopkins,K.,\nSaraceni-Richards,C., Sciaky,D., King,B.L., Wiegers,T.C. and\nMattingly,C.J. (2015) The Comparative Toxicogenomics Database‚Äôs\n10th year anniversary: update 2015. Nucleic Acids Res., 43,\nD914‚ÄìD920.\n24. Schaefer,C.F., Anthony,K., Krupa,S., Buchoff,J., Day,M.,\nHannay,T. and Buetow,K.H. (2009) PID: the Pathway Interaction\nDatabase. Nucleic Acids Res., 37, D674‚ÄìD679.\n25. Croft,D., Mundo,A.F., Haw,R., Milacic,M., Weiser,J., Wu,G.,\nCaudy,M., Garapati,P., Gillespie,M., Kamdar,M.R. et al. (2014) The\nReactome pathway knowledgebase. Nucleic Acids Res., 42,\nD472‚ÄìD477.\n26. Caspi,R., Altman,T., Billington,R., Dreher,K., Foerster,H.,\nFulcher,C.A., Holland,T.A., Keseler,I.M., Kothari,A., Kubo,A.\net al. (2014) The MetaCyc database of metabolic pathways and\nenzymes and the BioCyc collection of Pathway/Genome Databases.\nNucleic Acids Res., 42, D459‚ÄìD471.\n27. Bento,A.P., Gaulton,A., Hersey,A., Bellis,L.J., Chambers,J.,\nDavies,M., Kr¬®uger,F.A., Light,Y., Mak,L., McGlinchey,S. et al.\n(2014) The ChEMBL bioactivity database: an update. Nucleic Acids\nRes., 42, D1083‚ÄìD1090.\n28. Roth,B.L., Lopez,E., Patel,S. and Kroeze,W.K. (2000) The\nmultiplicity of serotonin receptors: uselessly diverse molecules or an\nembarrassment of riches? Neuroscientist, 6, 252‚Äì262.\n29. Rose,P.W., Prli¬¥c,A., Bi,C., Bluhm,W.F., Christie,C.H., Dutta,S.,\nGreen,R.K., Goodsell,D.S., Westbrook,J.D., Woo,J. et al. (2015) The\nRCSB Protein Data Bank: views of structural biology for basic and\napplied research and education. Nucleic Acids Res., 43, D345‚ÄìD356.\n30. Anastassiadis,T., Deacon,S.W., Devarajan,K., Ma,H. and\nPeterson,J.R. (2011) Comprehensive assay of kinase catalytic activity\nreveals features of kinase inhibitor selectivity. Nat. Biotechnol., 29,\n1039‚Äì1045.\n31. Davis,M.I., Hunt,J.P., Herrgard,S., Ciceri,P., Wodicka,L.M.,\nPallares,G., Hocker,M., Treiber,D.K. and Zarrinkar,P.P. (2011)\nComprehensive analysis of kinase inhibitor selectivity. Nat.\nBiotechnol., 29, 1046‚Äì1051.\n32. Franceschini,A., Szklarczyk,D., Frankild,S., Kuhn,M.,\nSimonovic,M., Roth,A., Lin,J., Minguez,P., Bork,P., von Mering,C.\net al. (2013) STRING v9.1: protein-protein interaction networks,\nwith increased coverage and integration. Nucleic Acids Res., 41,\nD808‚ÄìD815.\n33. Santos,A., Tsafou,K., Stolte,C., Pletscher-Frankild,S.,\nO‚ÄôDonoghue,S.I. and Jensen,L.J. (2015) Comprehensive comparison\nof large-scale tissue expression datasets. PeerJ, 3, e1054.\n34. Petryszak,R., Burdett,T., Fiorelli,B., Fonseca,N.A.,\nGonzalez-Porta,M., Hastings,E., Huber,W., Jupp,S., Keays,M.,\nKryvych,N. et al. (2014) Expression Atlas update‚Äìa database of gene\nand transcript expression from microarray- and sequencing-based\nfunctional genomics experiments. Nucleic Acids Res., 42,\nD926‚ÄìD932.\n35. O‚ÄôReilly,L.P., Long,O.S., Cobanoglu,M.C., Benson,J.A., Luke,C.J.,\nMiedel,M.T., Hale,P., Perlmutter,D.H., Bahar,I., Silverman,G.A.\net al. (2014) A genome-wide RNAi screen identifies potential drug\ntargets in a C. elegans model of \x031-antitrypsin deficiency. Hum. Mol.\nGenet., 23, 5123‚Äì5132.\n36. Kumar,K.K., Lowe,E.W. Jr, Aboud,A.A., Neely,M.D., Redha,R.,\nBauer,J.A., Odak,M., Weaver,C.D., Meiler,J., Aschner,M. et al.\n(2014) Cellular manganese content is developmentally regulated in\nhuman dopaminergic neurons. Sci. Rep., 4, 6801.\n37. Ligeti,B., P¬¥enzv¬¥alt¬¥o,Z., Vera,R., GyÀùorffy,B. and Pongor,S. (2015) A\nnetwork-based target overlap score for characterizing drug\ncombinations: high correlation with cancer clinical trial results.\nPLoS One, 10, e0129267.\n38. Vogt,I., Prinz,J. and Campillos,M. (2014) Molecularly and clinically\nrelated drugs and diseases are enriched in phenotypically similar\ndrug-disease pairs. Genome Med., 6, 52.\n39. Tung,C.W. (2015) ChemDIS: a chemical-disease inference system\nbased on chemical-protein interactions. J. Cheminform., 7, 25.\n40. Zhou,H., Gao,M. and Skolnick,J. (2015) Comprehensive prediction\nof drug-protein interactions and side effects for the human proteome.\nSci. Rep., 5, 11090.\nDownloaded from https://academic.oup.com/nar/article/44/D1/D380/2503089 by guest on 11 January 2026'
 '2 A registry of semantically annotated APIs\nThe first step in creating a network of interoperable APIs is to\nannotate each API in a semantically precise way. We built this\nAPI annotation system on the OpenAPI specification, the de\nfacto standard for documenting API metadata in a human-\nand machine-readable format. OpenAPI describes basic API\nmetadata (e.g. title, description, version, contact info), as well\nas key information on the operation of the API endpoints (e.g.\nserver URL, endpoint input parameters, endpoint response\nschemas).\nHowever, this basic OpenAPI specification does not include\nkey domain-specific information that is necessary to facilitate\ndownstream API interoperability. Therefore, we defined an\nOpenAPI extension to capture semantically precise annota-\ntions of each API endpoint. These annotations include the se-\nmantic types and identifier namespaces of biomedical entities\nthat are both used in querying (inputs) and found in the re-\nsponse (outputs), the JSON path to the output identifier val-\nues in the JSON response, and the predicate describing the\nrelationship\nbetween\nthe\ninput\nand\noutput\nentities\n(Supplementary Fig. S1).\nWe also chose a strategy to map arbitrary JSON data struc-\ntures to an established biological data model. In this effort,\nwe mapped API output to the Biolink Model (Unni et al.\n2022), a community-developed data model that was adopted\nand extended by the NCATS Translator consortium (Fecho\net al. 2022). The Biolink model defines the universe of seman-\ntic types that can be used in BioThings Explorer, the allowed\nidentifier systems for each semantic type and the allowed\npredicates. We provided a guide to writing these API annota-\ntions in our GitHub repository (https://github.com/biothings/\nbiothings_explorer/blob/main/docs/README-writing-x-bte.\nmd).\nTo annotate and catalog APIs with our OpenAPI extension,\nwe leveraged the SmartAPI registry (https://smart-api.info/)\n(Zaveri et al. 2017). We created SmartAPI registrations for 34\nAPIs with semantic annotations. The selection of these APIs\nthat are queried by BioThings Explorer is set in a local,\ninstance-specific configuration file. This collection of API\nannotations can be thought of as a ‚Äúmeta-knowledge graph‚Äù\n(meta-KG), where the nodes represent types of biomedical en-\ntities (genes, diseases, drugs) and the edges represent APIs that\ndescribe relationships between two types of biomedical enti-\nties. The SmartAPI meta-KG currently contains 35 nodes and\n1817\nedges.\n(The\ncomplete\nmeta-KG\nis\nshown\nin\nSupplementary Table S1 and a partial rendering is shown in\nFig. 1.)\n3 API interoperability using BioThings Explorer\nThe second step in creating our federated biomedical knowl-\nedge graph was to create BioThings Explorer, an engine to au-\ntonomously\nquery\nthe\nSmartAPI\nmeta-KG,\nquery\nthe\nannotated APIs to retrieve associations between biomedical\nentities, and integrate those APIs‚Äô responses. The input to\nBioThings Explorer is a query graph and the syntax for\nencoding the query graph was defined by the NCATS\nTranslator consortium (Fecho et al. 2022). The topology of\nthe query graph and the constraints on its nodes and edges de-\nfine the query (Fig. 2).\nBioThings Explorer executes the query in three distinct\nphases: query-path planning, query-path execution, and inte-\ngration and scoring.\n3.1 Query-path planning\nFor every edge in a query graph, BioThings Explorer consults\nthe SmartAPI registry for APIs that serve those types of associ-\nations (Fig. 2). For example, in Fig. 1, associations between\ndiseases and genes can be found using APIs from the\nComparative Toxicogenomics Database (Davis et al. 2023)\nand the Biolink API from the Monarch Initiative (Mungall\net al. 2017, Unni et al. 2022), while associations between\ngenes and chemicals can be found using MyChem.Info\n(https://mychem.info/) (Lelong et al. 2022). The sequence of\nAPI calls that can satisfy the original query is a ‚Äúquery-path\nplan.‚Äù\n3.2 Query-path execution\nIn this phase, BioThings Explorer programmatically and au-\ntonomously executes each query in each query-path plan\nbased on the semantic annotations for each API identified in\nthe previous phase. BioThings Explorer calls each API, using\nthe SmartAPI annotation to construct calls with the correct\nsyntax and appropriate input identifier, and maps the API\nresponses to the Biolink Model (Unni et al. 2022). BioThings\nExplorer also performs ID-to-object translation, which facili-\ntates the chaining of API calls from one step in the query-path\nto the next step. This ID translation step is critical when suc-\ncessive APIs in the query-path plan use different identifiers to\nrepresent the same biomedical entity (e.g. NCBI Gene ID\nversus Ensembl Gene ID). ID translation is currently handled\nby\nthe\nTranslator\nNode\nNormalizer\n(https://github.com/\nNCATSTranslator/Translator-All/wiki/Node-Normalizer). The\noutput of this phase is a set of edges for each step of the query-\npath, which represent the associations between biomedical enti-\nties retrieved from the APIs.\n3.3 Integration and scoring\nIn this final phase, these sets of edges from the API queries are\nassembled into result sub-graphs, each of which matches the\ntopology of the query graph. Each result is then scored based\non a variety of factors including number of paths, length of\npaths, and semantic similarity between concepts based on the\nNormalized Google Distance (Cilibrasi and Vitanyi 2007).\n4 Deployment and usage\nThe BioThings Explorer knowledge graph is entirely com-\nposed from a federated network of APIs. Because there is no\nlocal assembly and storage of a large knowledge graph,\nBioThings Explorer is a very lightweight application that can\nbe easily deployed on almost any standard personal com-\nputer. The ability of every user to create a local instance of\nBioThings Explorer removes centralized bottlenecks associ-\nated with large queries and/or heavy usage. The code reposi-\ntory that describes the installation process is at https://github.\ncom/biothings/biothings_explorer. BioThings Explorer cur-\nrently\nrelies\non\ntwo\nexternal\ndependencies‚Äîthe\nNode\nNormalizer service and the Biolink Model (Unni et al. 2022).\nHowever, nothing in the BioThings Explorer architecture is\nreliant on these specific tools, so these dependencies can be\nsubstituted for alternatives if desired.\nFor users who prefer not to create a local instance of\nBioThings Explorer, we also maintain a community instance\nfor general use through the NCATS Translator Consortium\n(https://explorer.biothings.io/).\n2\nCallaghan et al.\nDownloaded from https://academic.oup.com/bioinformatics/article/39/9/btad570/7273783 by guest on 11 January 2026'
 'Knowledge gr a ph‚Äìbased thought | 9 \nA\nB\nC\nD\nFigure 4: (A), (B), (C), and (D) r espectiv el y r epr esent the r elational dia gr ams of drug r epositioning, drug r esistance r esearc h, individualized tr eatment, \nand selection and understanding of biomarkers. \nwer e specificall y designed to v alidate the effectiv eness of the KGT \nfr ame work within this study. While the dataset is highly relevant \nto biomedical applications, its scope is primarily focused on vali- \ndating the proposed method. T herefore , it ma y not co ver all poten- \ntial use cases. Additionally, our system curr entl y has the dr awbac k \nof not performing fuzzy matching; if a drug name is misspelled by \ne v en 1 letter, it fails to r etrie v e information from the knowledge \ngr a ph. Ther efor e, we plan to impr ov e this aspect in the future to \nenhance the system‚Äôs usability and reliability. Our ultimate goal \nis to create a robust framework applicable to the rapidly evolving \ndomain of medical knowledge, supporting health care profession- \nals in delivering personalized, precise medication tailored to the \nindividual needs of each patient. \nFinally, we affirm that this study serves as a proof of concept, \naiming to showcase the technical feasibility and initial efficacy of \nthe method, which has not been validated in actual clinical prac- \ntice. In any clinical or medical decision-making, reliance should \nalways be placed on the judgment and guidance of professional \nhealth care practitioners. \nAdditional Files \nSupplementary Table S1. Comparison of PcQA with MetaQA and \nFACTKG in multihop tasks . T he types of intermediate entities are \nindicated in bold. \nSupplementary Table S2. Example of drug repositioning. \nSupplementary Table S3. Example of drug resistance research. \nSupplementary Table S4. Example of individualized treatment. \nSupplementary Table S5. Example of selection and understand- \ning of biomarkers. \nSupplementary Table S6. Prompts for Base and CoT&ICL. \nSupplementary Fig. S1. Prompt for k e y information extraction. \nSupplementary Fig. S2. Prompt for query statement generation. \nSupplementary Fig. S3. Prompt for LLM inference and output. \nSupplementary Fig. S4. Prompt for GPT-4‚Äìbased evaluation. \nSupplementary Fig. S5. (A), (B), (C), and (D) r espectiv el y r epr esent \nthe r elational dia gr ams of drug r epositioning, drug r esistance r e- \nsearc h, individualized tr eatment, and selection and understand- \ning of biomarkers. \nDownloaded from https://academic.oup.com/gigascience/article/doi/10.1093/gigascience/giae082/7943459 by guest on 11 January 2026'
 'Omar & Mohammed / Journal of Applied Science and Technology Trends Vol. 06, No. 02, pp. 342 ‚Äì357 (2025) \n \n346 \nscientific literature, clinical records, genomic databases, and \nexperimental findings [19, 31]. \nSuch integration creates a comprehensive biomedical \nknowledge base that supports advanced analytics and discovery. \nFor example, biomedical knowledge graphs can reveal previously \nunknown relationships (e.g., between drugs and diseases) and \nhelp prioritize potential biomarkers for complex conditions. The \nIntegrative Biomedical Knowledge Hub (iBKH) is one such \nlarge-scale graph that consolidates diverse biomedical resources \ninto a unified hub, enabling discovery at scale [11]. Beyond \niBKH, large biomedical knowledge graphs such as SPOKE \nfurther illustrate how graph integration accelerates research and \nsupports precision-medicine use cases [12]. Overall, these graphs \nserve as foundational resources for data-driven and personalized \nmedicine. These knowledge graphs serve as foundational \nresources for precision medicine, where treatment can be tailored \nto the individual‚Äôs biological profile, improving outcomes and \nminimizing side effects [19, 31]. \n \nTABLE I.  SUMMARY OF RELATED RESEARCH ON BIOMEDICAL KGS AND QUESTION ANSWERING \nRef. \nYear \nData/Graph \nMethod \nBaselines \nKey Metric \nLimitation \n[11] \n2023 \niBKH (18 biomedical \nsources, Neo4j) \nIntegrative KG + Graph \nlearning; drug repurposing \ncase \nKnown drugs, \nAlzheimer‚Äôs \nstudy \nPredictive performance \n(drug repurposing) \nLimited to Alzheimer‚Äôs case \nstudy; scalability and updates not \ndetailed \n[19] \n2023 \nDisease KG (9,400 \nnodes, 45,000 relations \nin Neo4j) \nCypher queries for disease‚Äì\ndrug‚Äìsymptom reasoning \nCross-referential \ndisease DB \nNew relation inference; \ncomplex query \nanswering \nLimited to single domain; lacks \nlarge-scale evaluation \n[3] \n2023 \nBKGs vs. GPT-3.5/4 \nComparative QA study: \nLLMs vs. KGs \nGPT-3.5, GPT-4, \nKG reasoning \nAccuracy, Novelty, \nReasoning \nGPT-4 inconsistent in \nreasoning/citations; KG less fluent \nbut more reliable \n[13] \n2024 \nSPOKE KG + \nLLaMA2, GPT-3.5, \nGPT-4 \nKG-optimized retrieval for \nLLMs (RAG) \nOther retrieval \nmethods \nAccuracy, token \nreduction >50% \nFocus on retrieval optimization, \nnot KG construction \n[23] \n2024 \nBenchmark KB datasets \nGenerate-then-retrieve \n(LLM ‚Üí logical form ‚Üí \nKB retrieval) \nPrior KBQA \nmethods \nAccuracy (highest to \ndate) \nRisk of error in logical form \ngeneration \n[6] \n2024 \nPrimeKG \nLLM + KG hybrid, Cypher \nquery validation, RAG \nGPT-4 Turbo, \nLLaMA 3 \nAccuracy, \nExplainability \nDependent on KG coverage; \ncomputationally intensive \n[22] \n2025 \nPan-cancer KG (PcQA \ndataset) \nKG-enhanced reasoning \n(subgraph selection) \nSOTA KGQA \nmethods \nOutperformed SOTA on \nPcQA \nLimited to pan-cancer focus; no \nfine-tuning explored \n[26] \n2025 \nDynamic Medical KG + \nNeo4j \nLLM agents + multi-hop \nreasoning \nMEDQA, \nMedMCQA \nbaselines \nAccuracy, \nInterpretability \nHigh system complexity; requires \ncontinuous updating \n[24] \n2025 \nSynthCypher dataset \n(Neo4j + synthetic \nqueries) \nLLM-supervised Cypher \ngeneration and verification \nSPIDER \nbenchmark \nCypher accuracy 40% \nSynthetic dataset may not capture \nall real-world cases \n[29] \n2025 \nUMLS KG + Neo4j \nHybrid GraphRAG \nLLM-only QA \nAccuracy, Citation \nfidelity \nMore complex pipeline; relies on \nexternal vector store \n[30] \n2025 \nClinical data (10k \npatients, 1,894 nodes, \nNeo4j) \nKG construction using \nGPT-4 for entity/relation \nextraction \nOther KG \nconstruction \nmethods \nHigh F1-scores \nFocus only on sepsis; limited \ngeneralization \n[20] \n2025 \nLocal KG from \nbiomedical documents \nMulti-level summarization \nover KG for QA \nRAG baselines \nQA accuracy, reasoning \nTested mainly on document QA; \nscalability not proven \n \nB. Neo4j Graph Database \nTo manage the complexity and large size of biomedical \nknowledge graphs, specialized graph databases are needed. \nNeo4j is one of the most popular graph databases designed to \nstore and query data structured as nodes (entities) and \nrelationships (edges), both of which can have descriptive \nproperties [32, 33]. It uses the property graph model, which \nmakes it easy to represent complex, connected biomedical data \nsuch as drug-gene interactions or disease pathways. Neo4j‚Äôs \nCypher query language is especially advantageous because it \nallows users to write expressive and efficient queries to explore \nmulti-step connections in the data [34]. \nNeo4j works well for biomedical data because it can quickly \nrun complicated queries over highly interconnected datasets. This \nis important in biology and medicine, where relationships \nbetween entities like proteins, diseases, and drugs are often \ncomplex and layered. Studies have shown that Neo4j handles \nlarge biomedical graphs efficiently, making it a favorite among \nresearchers and industry users alike [33, 35, 36]. Its indexing and \ncaching mechanisms also help speed up query processing and \ndata retrieval [37]. \nMoreover, \nNeo4j \nintegrates \nsmoothly \nwith \nmany \nprogramming languages and analytics tools, which makes it \neasier to build interactive biomedical applications and clinical \ndecision support systems that can turn complex graph data into \nuseful insights [38, 39]. \nC. Large Language Models (LLMs) in Biomedical Question \nAnswering \nLarge Language Models (LLMs) are powerful AI systems \ntrained on vast amounts of text data. They learn the structure and \npatterns of language, enabling them to understand questions, \ngenerate responses, summarize information, and perform other \ncomplex language tasks. Well-known models such as LLaMA \nand GPT-3 have greatly advanced the field of natural language \nprocessing by showing strong performance across many tasks \n[40, 41].'
 'Omar & Mohammed / Journal of Applied Science and Technology Trends Vol. 06, No. 02, pp. 342 ‚Äì357 (2025) \n \n356 \n \nFig. 7. Side effects of drugs that treat epilepsy \n \nExecuting this query returns drugs associated with epilepsy \nand their corresponding side effects. For example, the query \nidentifies Pregabalin as a treatment for epilepsy and retrieves \nmultiple side effects such as anxiety, arrhythmia, gastrointestinal \npain, and others. The visualization of the graph in Fig. 7, \nhighlights this pattern, where the drug node is linked to epilepsy \nvia TREATS and to several side effect nodes via CAUSES, \nproviding an interpretable biomedical knowledge structure. \nVI. DISCUSSION \nUnder \na \nunified \nevaluation \nprotocol \nreporting \nP@k/R@k/F1@k, Hits@k, and MRR at ùëò‚àà{1,5,10} , the \npipeline exhibits consistent end-to-end behavior across the three \ndifficulty tiers. The ùëò-ablation shows strong early precision (high \nP@1 and elevated MRR), while recall increases with larger ùëò, \nindicating that correct answers typically surface near the top yet \npersist deeper in the ranked slate. At the query-generation level, \nexact-match (EM) is high for simple questions (e.g., ‚àº96%) and \nlower for complex, multi-hop questions (e.g., ‚àº86.7%), which \naligns with increased compositionality. End-to-end latency \n(mean total ‚âà6.07\u2006s from question to final answer) remains within \ninteractive bounds on our local setup. Practically, a fully offline \ndeployment avoids internet connectivity, API keys, and external \ndata \ntransfer, \nstrengthening \nprivacy, \ncompliance, \nand \nreproducibility in biomedical contexts. \nSeveral challenges qualify these findings. First, the \ncomparability challenge: to our knowledge, no prior study \nevaluates iBKH using the same metric suite, making cross-paper \nnumeric comparisons not ‚Äúapples to apples.‚Äù We therefore \ninterpret the results as controlled evidence about this pipeline \nunder a single, consistent protocol rather than as a cross-study \nleaderboard. Beyond comparability, performance may vary with \nother biomedical knowledge graphs; the current iBKH snapshot \nis static, limiting real-time updates; and scaling to larger or \ndynamically refreshed graphs can introduce latency and \nconsistency trade-offs. \nError analysis shows that residual failures concentrate in \ncomplex, multi-hop queries where missing relation hops or brittle \nname-based matching (synonyms, abbreviations, homonyms) \nlead to partial answers. These observations motivate concept-\nlevel normalization via biomedical identifiers (e.g., UMLS, \nSNOMED, RxNorm) with synonym expansion, as well as \nschema-constrained query generation and path-guided decoding \nto better satisfy multi-hop constraints. Finally, correctness is \nassessed primarily at the answer level (Hits@k, MRR, \nprecision/recall) and does not yet include full semantic-\nequivalence checks across alternative Cypher queries, which may \noverlook cases where different queries yield the same correct \nresults. Complementing template EM with result-set equivalence \nchecks, expanding the metric suite (e.g., nDCG/MAP) with per-\nquestion 95% bootstrap confidence intervals, and supporting \nincremental graph updates with distributed storage are promising \nsteps to enhance robustness, scalability, and external validity. \nVII. CONCLUSION \nThis study introduced a hybrid biomedical question \nanswering framework that couples the LLaMA-3B language \nmodel with a Neo4j-based iBKH knowledge graph to enable the \nautomatic generation of executable Cypher queries and to deliver \ntransparent, evidence-grounded answers through justification \nsubgraphs. Evaluation in simple, medium, and complex queries \ndemonstrated consistently high performance, with strong \nprecision, recall, F1, Hits@k, and MRR values, while \nmaintaining low latency suitable for interactive biomedical \napplications. Beyond quantitative performance, the proposed \nsystem provides a reproducible and privacy-preserving solution \nby operating fully offline, a property of particular importance in \nsensitive clinical and research contexts. However, certain \nlimitations remain. The reliance on a static iBKH snapshot \nconstrains coverage and adaptability, recall is lower for complex \nmulti-hop reasoning, and the absence of canonical entity \nnormalization (e.g., UMLS, SNOMED, RxNorm) may reduce \nsemantic robustness. Future research will therefore focus on \nintegrating standardized biomedical entity normalization, \nenabling dynamic and incremental knowledge graph updates, and \nleveraging domain-adapted or fine-tuned biomedical LLMs. \nThese directions are expected to further strengthen the \nrobustness, scalability, and applicability of the framework in real-\nworld biomedical and clinical environments. \n \nREFERENCES \n[1] S. Bubeck, V. Chandrasekaran, R. Eldan, J. Gehrke, E. Horvitz, E. Kamar, \nP. Lee, Y. T. Lee, Y. Li, S. Lundberg, et al., ‚ÄúSparks of artificial general \nintelligence: \nEarly experiments with \ngpt-4,‚Äù arXiv preprint \narXiv:2303.12712, 2023. \n[2] J. Achiam, S. Adler, S. Agarwal, L. Ahmad, I. Akkaya, F. L. Aleman, D. \nAlmeida, J. Altenschmidt, S. Altman, S. Anadkat, et al., ‚ÄúGpt-4 \ntechnical report,‚Äù arXiv preprint arXiv:2303.08774, 2023. \n[3] Y. Hou, J. Yeung, H. Xu, C. Su, F. Wang, and R. Zhang, ‚ÄúFrom answers to \ninsights: unveiling the strengths and limitations of chatgpt and \nbiomedical knowledge graphs,‚Äù Research Square, pp. rs‚Äì3, 2023.'
 'format. Second, because the entire federated KG is never in-\nstantiated in a single place, reasoning and scoring methods\nthat rely on having the entire knowledge graph in memory\ncannot be used with BioThings Explorer.\nIn sum, we believe that knowledge graphs enable many ex-\nciting use cases in biomedical research (Nicholson and Greene\n2020), and that BioThings Explorer is complementary to\nexisting approaches for assembling knowledge graphs, offer-\ning powerful and unique capabilities for both scientific data\nanalysts and tool developers.\nSupplementary data\nSupplementary data are available at Bioinformatics online.\nConflict of interest\nNone declared.\nFunding\nSupport for this work was provided by the National Center\nfor Advancing Translational Sciences, National Institutes of\nHealth, through the Biomedical Data Translator program,\nawards OT2TR003427 and OT2TR003445.\nData availability\nBioThings Explorer is implemented as a NodeJS application.\nThe primary repository for the BioThings Explorer project is\nat https://github.com/biothings/biothings_explorer, which in\nturn links to and incorporates other repositories as sub-\nmodules. All code is released under the Apache 2.0 open-\nsource software license.\nReferences\nCilibrasi RL, Vitanyi PMB. The google similarity distance. IEEE Trans\nKnowl Data Eng 2007;19:370‚Äì83.\nDavis AP, Wiegers TC, Johnson RJ et al. Comparative toxicogenomics\ndatabase\n(CTD):\nupdate\n2023.\nNucleic\nAcids\nRes\n2023;51:\nD1257‚Äì62.\nDowell RD, Jokerst RM, Day A et al. The distributed annotation system.\nBMC Bioinformatics 2001;2:7.\nFecho K, Bizon C, Miller F et al. A biomedical knowledge graph system\nto propose mechanistic hypotheses for real-world environmental\nhealth observations: cohort study and informatics application. JMIR\nMed Inform 2021;9:e26714.\nFecho K, Thessen AE, Baranzini SE et al. Progress toward a universal\nbiomedical data translator. Clin Transl Sci 2022;15:1838‚Äì47.\nHimmelstein DS, Lizee A, Hessler C et al. Systematic integration\nof biomedical knowledge prioritizes drugs for repurposing. Elife\n2017;6:e26726.\nLelong S, Zhou X, Afrasiabi C et al. BioThings SDK: a toolkit for build-\ning\nhigh-performance\ndata\nAPIs\nin\nbiomedical\nresearch.\nBioinformatics 2022;38:2077‚Äì9.\nMayers M, Tu R, Steinecke D et al. Design and application of a knowl-\nedge network for automatic prioritization of drug mechanisms.\nBioinformatics 2022;38:2880‚Äì91.\nMorris JH, Soman K, Akbas RE et al. The scalable precision medicine\nopen knowledge engine (SPOKE): a massive knowledge graph of bio-\nmedical information. Bioinformatics 2023;39(2):btad080.\nMungall CJ, McMurry JA, Ko¬®hler S et al. The monarch initiative: an in-\ntegrative data and analytic platform connecting phenotypes to geno-\ntypes across species. Nucleic Acids Res 2017;45:D712‚Äì22.\nNicholson DN, Greene CS. Constructing knowledge graphs and their\nbiomedical applications. Comput Struct Biotechnol J 2020;18:\n1414‚Äì28.\nRambla J, Baudis M, Ariosa R et al. Beacon v2 and Beacon networks: a\n‚Äòlingua franca‚Äô for federated data discovery in biomedical genomics,\nand beyond. Hum Mutat 2022;43:791‚Äì9.\nUnni DR, Moxon SAT, Bada M et al. Biolink Model: a universal schema\nfor knowledge graphs in clinical, biomedical, and translational sci-\nence. Clin Transl Sci 2022;15:1848‚Äì55.\nWood EC, Glen AK, Kvarfordt LG et al. RTX-KG2: a system for build-\ning a semantically standardized knowledge graph for translational\nbiomedicine. BMC Bioinformatics 2022;23:400.\nZaveri A, Dastgheib S, Wu C et al. smartAPI: towards a more intelligent\nnetwork of web APIs. In: Blomqvist, E., Maynard, D., Gangemi, A.,\net al. (eds) The Semantic Web. ESWC 2017. Lecture Notes in\nComputer Science, 2017, 154‚Äì69. https://link.springer.com/chapter/\n10.1007/978-3-319-58451-5_11#chapter-info.\n4\nCallaghan et al.\nDownloaded from https://academic.oup.com/bioinformatics/article/39/9/btad570/7273783 by guest on 11 January 2026'
 '2 | GigaScience , 2025, Vol. 14 \nLLaMA with medical liter atur e. Additionall y, Med-P aLM [ 9 ] shows \npromising performance on the MedQA exam based on clinical \ncor por a and human feedback. Meanwhile, aiming at the Chinese \nmedical domain, LLMs such as BenTsao [ 10 ], DoctorGLM [ 11 ], and \nHuatuoGPT [ 12 ] are developed on the Chinese medical dialogue \ndata. Mor e r ecentl y, Zhongjing [ 13 ] and ChiMed-GPT [ 14 ] adopted \nfull pipeline training from pretraining, SFT, to reinforcement \nlearning with human feedback (RLHF) [ 15 ]. While fine-tuning can \nreduce hallucinations in LLMs, it brings about considerable train- \ning expenses. Additionally, it poses a critical challenge known \nas catastrophic forgetting. This issue manifests when a model \nfor gets its pr e viousl y learned information as a consequence of \nparameter modifications during the acquisition of new tasks. \nThis forgetfulness results in a deterioration of performance on \nprior tasks, consequently constraining the model‚Äôs practical \napplicability [ 16 , 17 ]. \nIn addition to fine-tuning, r esearc hers also enhance the out- \nput of LLMs through the field of prompt engineering. Prompt en- \ngineering focuses on the creation and optimization of prompts \nto impr ov e the effectiv eness of LLMs acr oss v arious a pplications \nand r esearc h domains [ 18 ]. It can enhance the ca pabilities of LLMs \nin a wide range of complex tasks, including question answering, \nsentiment classification, and commonsense reasoning. Chain-of- \nthought (CoT) prompts [ 19 ] enable complex reasoning capabili- \nties by incor por ating intermediate r easoning steps . T he Automatic \nPr ompt Engineer (APE) pr oposes an automatic pr ompt gener ation \nmethod aimed at enhancing the performance of LLMs [ 20 ]. Prompt \nengineering offers a straightforw ar d approach to harnessing the \npotential of LLMs without fine-tuning. \nOn the other hand, knowledge gr a phs (KGs) ar e r epositories of \nvast quantities of high-quality structured data, offering the po- \ntential to effectiv el y mitigate the issue of factual hallucinations \nwhen integrated with LLMs . Hence , employing KGs for question \nanswering can enhance the precision of the responses and fur- \nnish a dependable foundation for the factual verification of in- \nformation produced by LLMs. Knowledge gr a ph question answer- \ning (KGQA) has long been a hot r esearc h topic. Befor e the ad- \nvent of LLMs, certain studies [ 21‚Äì23 ] typically begin by r etrie ving \na subgr a ph r elated to the question to r educe the searc h space, \nthen perform m ultihop r easoning on this basis . T his r etrie v al- \nplus-r easoning par adigm has shown its adv anta ges ov er dir ect \nr easoning acr oss the entir e KG [ 24 , 25 ]. Additionall y, r esearc hers \ntackle KGQA by parsing the question into a structured query lan- \nguage (e.g., SPARQL) and using a query engine to obtain accu- \nrate answers [ 26 , 27 ]. UniKGQA [ 28 ] introduces a unified fine- \ntuning fr ame work for r etrie v al and r easoning, mor e closel y linking \nthese 2 stages. Ho w ever, traditional KGQA methods usually per- \nform poorly in accurate semantic understanding and high-quality \ntext generation due to the lack of LLMs for r etrie v al and r eason- \ning. Hence, r ecent r esearc h is incr easingl y utilizing external KGs \nto enhance LLMs in addressing KGQA challenges. For instance, \nStructGPT [ 29 ] navigates through knowledge graphs by identify- \ning pathways from an initial seed entity to the target answer en- \ntity, while Think-on-Gr a ph (ToG) [ 30 ] intr oduces iter ativ e explo- \nration of the knowledge gr a ph, whic h can become inefficient with \nv ery lar ge KGs. Additionall y, Reasoning on Gr a phs (RoG) [ 31 ] ne- \ncessitates fine-tuning to accur atel y gener ate and plan the r elation \npaths. KG-GPT [ 32 ] opts for r etrie ving an entire subgraph from the \nknowledge gr a ph and then deduces the answer thr ough infer ence. \nAlthough these methods hav e ac hie v ed gr atifying r esults in gen- \ner al ar eas, as shown in Fig. 1 B, when the intermediate entity in \nthe multihop question is unknown, it is impossible to retrieve the \na ppr opriate knowledge from the KG. \nIn this article, we intr oduce an innov ativ e fr ame work called \nknowledge gr a ph‚Äìbased thought (KGT), whic h integr ates LLMs \nwith KGs through employing LLMs for reasoning on the schema \nof KGs to mitigate factual hallucinations of LLMs, as shown in \nFig. 1 C. Unlike traditional methods, KGT does not dir ectl y r etrie v e \nfactual information based on the question. Instead, it uses LLMs \nto infer entity information on the schema of the knowledge graph, \ngenerating an optimal subgraph based on k e y information di- \nr ectl y extr acted fr om the question and inferr ed information fr om \nthe sc hema. Subsequentl y, the optimal subgr a ph is used to in- \nfer the answer to the question through LLMs. KGT r equir es no \nfine-tuning, offers seamless integration with multiple LLMs, and \nis plug-and-play, facilitating easy deployment. It demonstrates \ngeneralizability, making it adaptable for use with diverse knowl- \nedge gr a phs . T his fr ame work is tailor ed for wide-r anging a ppli- \ncations in numerous biomedical challenges, such as (i) enhanc- \ning clinical decision-making for physicians and medical organi- \nzations, (ii) delivering medical advice to patients and health care \npro viders , (iii) unco vering crucial biomarkers for early disease de- \ntection and tailored therapy, and (iv) exploring novel therapeutic \napplications for existing medications through insights into their \nmechanisms , side effects , and the biological processes of associ- \nated diseases . Furthermore , we utilize the SmartQuerier Oncol- \nogy Knowledge Gr a ph (SOKG), a pan-cancer knowledge gr a ph de- \nveloped by SmartQuerier, to create a benchmark for the KGQA \ntask within biomedicine, named pan-cancer question answering \n(PcQA). We release this benchmark and its accompanying knowl- \nedge gr a ph, whic h is a subgr a ph of the SOKG, in [ 33 ]. This bench- \nmark is curr entl y the sole question-ans wering dataset a vailable \nin the domain of biomedical knowledge gr a phs. \nMaterials and Methods \nKno wledge gr aph introduction \nIn this w ork, w e tac kle the pr oblem of logical r easoning ov er \nthe KG K : E √ó R that store entities ( E ) and relations ( R ). With- \nout loss of generality, KG can be organized as a set of triplets \n{ (e 1 , r, e 2 ) } ‚äÜK, wher e eac h r elation r ‚àà R exists between the pair \nof entities (e 1 , e 2 ) ‚àà E √ó E. We define a relational path { (t 1 , r, t 2 ) } as \na sequence of entity types ( T ) and the relation between them, \nwhere (t 1 , t 2 ) ‚àà T √ó T . In contrast, a relational chain { (e 1 , r, e 2 ) } \nrefers to a specific set of relational triplets between entities. To \nfurther enrich the KG, attribute information is included through \npairs (e , at t r ) , where at t r represents an attribute associated with \nan entity e , thereby enhancing the KG‚Äôs semantic richness \nand precision by incorporating detailed characteristics of each \nentity. \nWithin the specialized realm of pan-cancer resear ch, w e use \na subgr a ph of the SOKG that pr ovides detailed oncological in- \nformation. As depicted in Table 1 , SOKG includes a collection of \nover 3 million entities, which is substantially larger than the en- \ntity count in the compared knowledge graphs, SynLethKG [ 34 ] and \nSDKG [ 35 ], with 540,012 and 165,062 entities, r espectiv el y. Further- \nmor e, SOKG‚Äôs nearl y 6 million unique concept relations exceed \nthose of SynLethKG and SDKG, which have 2,231,921 and 727,318 \nr elations, r espectiv el y. Additionall y, SOKG includes 98 distinct at- \ntribute types, enriching data comprehension and improving the \nefficiency and precision of queries, a capability not matched by \nSynLethKG or SDKG, which do not include comparable attributes. \nFor this r esearc h, we utilize only a subgraph of the SOKG, which \nis available as open data [ 33 ], while the full knowledge graph re- \nmains proprietary. \nDownloaded from https://academic.oup.com/gigascience/article/doi/10.1093/gigascience/giae082/7943459 by guest on 11 January 2026']"
"How do LLaMA and Llama2 models differ in their integration with biomedical knowledge graphs to mitigate hallucinations in large language models, and what impact does this integration have on the performance of retrieval-augmented generation (RAG) systems as demonstrated by meta-analyses?","['<1-hop>\n\n2 | GigaScience , 2025, Vol. 14 LLaMA with medical liter atur e. Additionall y, Med-P aLM [ 9 ] shows promising performance on the MedQA exam based on clinical cor por a and human feedback. Meanwhile, aiming at the Chinese medical domain, LLMs such as BenTsao [ 10 ], DoctorGLM [ 11 ], and HuatuoGPT [ 12 ] are developed on the Chinese medical dialogue data. Mor e r ecentl y, Zhongjing [ 13 ] and ChiMed-GPT [ 14 ] adopted full pipeline training from pretraining, SFT, to reinforcement learning with human feedback (RLHF) [ 15 ]. While fine-tuning can reduce hallucinations in LLMs, it brings about considerable train- ing expenses. Additionally, it poses a critical challenge known as catastrophic forgetting. This issue manifests when a model for gets its pr e viousl y learned information as a consequence of parameter modifications during the acquisition of new tasks. This forgetfulness results in a deterioration of performance on prior tasks, consequently constraining the model‚Äôs practical applicability [ 16 , 17 ]. In addition to fine-tuning, r esearc hers also enhance the out- put of LLMs through the field of prompt engineering. Prompt en- gineering focuses on the creation and optimization of prompts to impr ov e the effectiv eness of LLMs acr oss v arious a pplications and r esearc h domains [ 18 ]. It can enhance the ca pabilities of LLMs in a wide range of complex tasks, including question answering, sentiment classification, and commonsense reasoning. Chain-of- thought (CoT) prompts [ 19 ] enable complex reasoning capabili- ties by incor por ating intermediate r easoning steps . T he Automatic Pr ompt Engineer (APE) pr oposes an automatic pr ompt gener ation method aimed at enhancing the performance of LLMs [ 20 ]. Prompt engineering offers a straightforw ar d approach to harnessing the potential of LLMs without fine-tuning. On the other hand, knowledge gr a phs (KGs) ar e r epositories of vast quantities of high-quality structured data, offering the po- tential to effectiv el y mitigate the issue of factual hallucinations when integrated with LLMs . Hence , employing KGs for question answering can enhance the precision of the responses and fur- nish a dependable foundation for the factual verification of in- formation produced by LLMs. Knowledge gr a ph question answer- ing (KGQA) has long been a hot r esearc h topic. Befor e the ad- vent of LLMs, certain studies [ 21‚Äì23 ] typically begin by r etrie ving a subgr a ph r elated to the question to r educe the searc h space, then perform m ultihop r easoning on this basis . T his r etrie v al- plus-r easoning par adigm has shown its adv anta ges ov er dir ect r easoning acr oss the entir e KG [ 24 , 25 ]. Additionall y, r esearc hers tackle KGQA by parsing the question into a structured query lan- guage (e.g., SPARQL) and using a query engine to obtain accu- rate answers [ 26 , 27 ]. UniKGQA [ 28 ] introduces a unified fine- tuning fr ame work for r etrie v al and r easoning, mor e closel y linking these 2 stages. Ho w ever, traditional KGQA methods usually per- form poorly in accurate semantic understanding and high-quality text generation due to the lack of LLMs for r etrie v al and r eason- ing. Hence, r ecent r esearc h is incr easingl y utilizing external KGs to enhance LLMs in addressing KGQA challenges. For instance, StructGPT [ 29 ] navigates through knowledge graphs by identify- ing pathways from an initial seed entity to the target answer en- tity, while Think-on-Gr a ph (ToG) [ 30 ] intr oduces iter ativ e explo- ration of the knowledge gr a ph, whic h can become inefficient with v ery lar ge KGs. Additionall y, Reasoning on Gr a phs (RoG) [ 31 ] ne- cessitates fine-tuning to accur atel y gener ate and plan the r elation paths. KG-GPT [ 32 ] opts for r etrie ving an entire subgraph from the knowledge gr a ph and then deduces the answer thr ough infer ence. Although these methods hav e ac hie v ed gr atifying r esults in gen- er al ar eas, as shown in Fig. 1 B, when the intermediate entity in the multihop question is unknown, it is impossible to retrieve the a ppr opriate knowledge from the KG. In this article, we intr oduce an innov ativ e fr ame work called knowledge gr a ph‚Äìbased thought (KGT), whic h integr ates LLMs with KGs through employing LLMs for reasoning on the schema of KGs to mitigate factual hallucinations of LLMs, as shown in Fig. 1 C. Unlike traditional methods, KGT does not dir ectl y r etrie v e factual information based on the question. Instead, it uses LLMs to infer entity information on the schema of the knowledge graph, generating an optimal subgraph based on k e y information di- r ectl y extr acted fr om the question and inferr ed information fr om the sc hema. Subsequentl y, the optimal subgr a ph is used to in- fer the answer to the question through LLMs. KGT r equir es no fine-tuning, offers seamless integration with multiple LLMs, and is plug-and-play, facilitating easy deployment. It demonstrates generalizability, making it adaptable for use with diverse knowl- edge gr a phs . T his fr ame work is tailor ed for wide-r anging a ppli- cations in numerous biomedical challenges, such as (i) enhanc- ing clinical decision-making for physicians and medical organi- zations, (ii) delivering medical advice to patients and health care pro viders , (iii) unco vering crucial biomarkers for early disease de- tection and tailored therapy, and (iv) exploring novel therapeutic applications for existing medications through insights into their mechanisms , side effects , and the biological processes of associ- ated diseases . Furthermore , we utilize the'
 '<2-hop>\n\nMeta-analysis Effect size was defined as a metric quantifying the relation\xad ship between variables, including both direction and magni\xad tude.18 For each included study, we calculated the effect size between baseline LLM performance and RAG-enhanced LLM performance. The outcomes focused on the perform\xad ance of generation results, such as accuracy and usefulness. Metrics related to the retrieval process, cost, or speed were not included as outcomes in the meta-analysis. For continu\xad ous outcomes, we used Cohen‚Äôs d, standardized between- group mean difference (SMD), calculated as the difference in means divided by the pooled standard deviation. The stand\xad ard error (SE) of SMD was calculated using the following for\xad mula (1), where n1 and n2 represent the sample sizes of each group.19 For dichotomous measurements, we calculated the log-odds ratio, obtained by transforming the odds ratio (OR) with the natural logarithm, and the associated SE was calcu\xad lated using formula (2), where a, b, c, and d represent the number of successful and failed events in the baseline LLM and RAG-enhanced LLM approaches. For studies reporting multiple outcomes, we used the overall outcome to calculate effect size. If no overall outcome was reported, we averaged the effect sizes of all reported outcomes. We excluded out\xad comes with a sample size of less than 30 to avoid small- sample bias. SESMD ¬º ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi n1 √æ n2 n1n2 √æ SMD2 2 n1 √æ n2 √∞ √û s (1) SElogOR ¬º ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi 1 a √æ 1 b √æ 1 c √æ 1 d r (2) We developed a random-effect meta-analysis model, because of the variability in RAG architectures and evalua\xad tion datasets among the included studies. The random-effect model was used when individual study effects likely con\xad tained additional sources of variance beyond sampling error. Between-study heterogeneity was assessed using Higgins & Thompson‚Äôs I2 statistic, where 25% indicated low heteroge\xad neity, 50% moderate, and 75% substantial.20 We conducted subgroup analyses to explore performance variations across different factors. First, we analyzed the influence of the baseline LLM, referring to the foundation model (eg, GPT-4 or Llama2) that provides the core architec\xad ture for the system. Second, we examined data retrieval strat\xad egies, categorizing them as simple or complex. Simple strategies included fixed-length chunking and basic similarity search, and we performed a subgroup analysis to compare these with complex retrieval strategies. Third, we analyzed differences based on evaluation methods, distinguishing between human evaluations, such as Likert scale ratings for helpfulness and accuracy, and automatic evaluation metrics, including ROUGE-1 and BLEU. Finally, we conducted a sub\xad group analysis based on the type of task, classifying studies 606 Journal of the American Medical Informatics Association, 2025, Vol. 32, No. 4 Downloaded from https://academic.oup.com/jamia/article/32/4/605/7954485 by guest on 11 January 2026']","LLaMA-based models in the biomedical domain have been enhanced through various approaches, including fine-tuning and prompt engineering, to reduce hallucinations and improve performance. The knowledge graph‚Äìbased thought (KGT) framework integrates LLMs like LLaMA with biomedical knowledge graphs by using LLMs to reason on the schema of knowledge graphs rather than directly retrieving factual information. This method generates an optimal subgraph based on key information extracted from the question and inferred schema data, which is then used by LLMs to infer answers. KGT requires no fine-tuning, supports seamless integration with multiple LLMs including LLaMA, and is plug-and-play, facilitating deployment and generalizability across diverse biomedical knowledge graphs. On the other hand, Llama2 serves as a foundational model in retrieval-augmented generation (RAG) systems, where meta-analyses have quantified the effect size of RAG-enhanced LLM performance compared to baseline LLMs. These analyses, using metrics such as Cohen‚Äôs d and log-odds ratios, demonstrate that integrating retrieval mechanisms with Llama2 improves generation accuracy and usefulness. Subgroup analyses further reveal that the choice of baseline LLM, including Llama2, and the complexity of data retrieval strategies influence performance outcomes. Thus, while LLaMA models benefit from knowledge graph integration via frameworks like KGT to mitigate hallucinations and enhance reasoning, Llama2‚Äôs role as a base model in RAG systems is supported by meta-analytic evidence showing improved generation performance when retrieval is incorporated.",multi_hop_specific_query_synthesizer,"Based on the provided context, the differences between LLaMA and Llama2 models in their integration with biomedical knowledge graphs (KGs) to mitigate hallucinations in large language models (LLMs), and the impact of this integration on retrieval-augmented generation (RAG) system performance as demonstrated by meta-analyses, can be summarized as follows:

### Differences in Integration of LLaMA and Llama2 with Biomedical Knowledge Graphs

1. **Model Usage and Fine-tuning:**
   - **LLaMA:** Earlier versions of LLaMA have been used in biomedical question-answering systems that integrate biomedical knowledge graphs (BKGs) such as UMLS or SPOKE. These systems often combine symbolic reasoning from KGs with semantic retrieval from text embeddings to improve accuracy and reduce hallucinations. For example, Mohammed et al. proposed a hybrid GraphRAG framework combining Neo4j-based UMLS knowledge graphs with vector stores, using GPT-4o-Mini for answer generation, showing improved factual accuracy and citation fidelity compared to LLM-only approaches.
   - **Llama2:** Llama2 has been more recently fine-tuned and optimized specifically for biomedical KG integration. For instance, Soman et al. presented KG-RAG, which integrates a large biomedical KG (SPOKE) with LLaMA 2, GPT-3.5, and GPT-4, showing very high accuracy boosts, especially for LLaMA 2. Luo et al. fine-tuned LLaMA 2 to convert natural language questions into logical forms (Cypher queries) for graph databases like Neo4j, achieving superior accuracy in biomedical KG question answering. LLaMA 2‚Äôs integration emphasizes prompt-driven query generation and evidence transparency by surfacing Cypher queries and supporting subgraphs alongside answers.

2. **Technical Approaches:**
   - **LLaMA:** Integration often involves hybrid approaches combining KG symbolic reasoning with text-based retrieval, sometimes using vector stores for semantic search alongside KG queries.
   - **Llama2:** Focuses on fine-tuning for direct translation of natural language to graph queries (Cypher), enabling precise and executable queries over biomedical KGs. This approach supports transparent, evidence-based answers with traceable justifications, improving interpretability and reducing hallucinations.

3. **Deployment and Privacy:**
   - LLaMA-based systems have been deployed locally (e.g., LLaMA 3 inference on-premises) to ensure low latency and strong data control, which is critical in biomedical settings.
   - Llama2-based systems similarly support local deployment but with enhanced fine-tuning and prompt engineering to improve KG query accuracy and answer reliability.

### Impact on RAG System Performance and Meta-Analysis Findings

1. **Performance Improvement:**
   - The meta-analysis of 20 studies comparing baseline LLMs with RAG-enhanced LLMs in biomedical tasks showed a statistically significant pooled effect size with an odds ratio of 1.35 (95% CI: 1.19‚Äì1.53, P = .001), indicating that RAG systems integrating external knowledge sources, including biomedical KGs, significantly improve performance over baseline LLMs.
   - Specifically, LLaMA 2 integration with biomedical KGs (e.g., SPOKE) demonstrated very high accuracy boosts in biomedical question answering compared to other retrieval methods, highlighting its robustness and efficiency.

2. **Hallucination Mitigation:**
   - Both LLaMA and Llama2 models, when integrated with biomedical KGs via RAG, show reduced hallucination rates by grounding responses in structured, verifiable biomedical knowledge.
   - The use of explicit graph queries (Cypher) and evidence subgraphs in Llama2-based systems enhances transparency and traceability, further mitigating hallucinations by providing justifications for answers.

3. **Evaluation and Transparency:**
   - Systems using Llama2 with KG integration emphasize comprehensive evaluation metrics (Exact Match, F1, Precision/Recall, Hits@k, MRR) across question difficulty tiers, supporting rigorous assessment of RAG performance.
   - The transparency of query generation and evidence presentation in Llama2-based frameworks supports better clinical trustworthiness and reproducibility.

4. **Guidelines and Future Directions:**
   - The systematic review and meta-analysis recommend future research focus on:
     - System-level enhancements combining RAG with LLM-powered agents.
     - Knowledge-level enhancements for deeper integration of external knowledge into LLM reasoning.
     - Integration-level enhancements embedding RAG systems within electronic health records (EHRs).
   - These directions are relevant for both LLaMA and Llama2 models to further improve biomedical RAG applications.

### Summary

- **LLaMA** models have been effectively used in hybrid RAG systems combining KGs and text retrieval, improving accuracy and reducing hallucinations, with local deployment for privacy.
- **Llama2** models have advanced this integration by fine-tuning for precise natural language to graph query translation, enabling transparent, evidence-based biomedical QA with superior accuracy.
- Meta-analyses confirm that RAG systems integrating biomedical KGs with LLaMA and Llama2 models significantly outperform baseline LLMs, reducing hallucinations and improving factual accuracy in biomedical applications.

This demonstrates that the integration of biomedical knowledge graphs with LLaMA and Llama2 models via retrieval-augmented generation substantially enhances the reliability and performance of large language models in biomedical question answering and clinical tasks.","['Research and Applications\nImproving large language model applications in \nbiomedicine with retrieval-augmented generation: a \nsystematic review, meta-analysis, and clinical \ndevelopment guidelines\nSiru Liu, PhD\x03,1,2, Allison B. McCoy \n, PhD1, Adam Wright \n, PhD1,3 \n1Department of Biomedical Informatics, Vanderbilt University Medical Center, Nashville, TN 37212, United States, 2Department of Computer \nScience, Vanderbilt University, Nashville, TN 37212, United States, 3Department of Medicine, Vanderbilt University Medical Center, \nNashville, TN 37212, United States\n\x03Corresponding author: Siru Liu, PhD, Department of Biomedical Informatics, Vanderbilt University Medical Center, 2525 West End Ave #1475, Nashville, \nTN 37212, United States (siru.liu@vumc.org)\nAbstract\nObjective: The objectives of this study are to synthesize findings from recent research of retrieval-augmented generation (RAG) and large lan\xad\nguage models (LLMs) in biomedicine and provide clinical development guidelines to improve effectiveness.\nMaterials and Methods: We conducted a systematic literature review and a meta-analysis. The report was created in adherence to the Pre\xad\nferred Reporting Items for Systematic Reviews and Meta-Analyses 2020 analysis. Searches were performed in 3 databases (PubMed, Embase, \nPsycINFO) using terms related to ‚Äúretrieval augmented generation‚Äù and ‚Äúlarge language model,‚Äù for articles published in 2023 and 2024. We \nselected studies that compared baseline LLM performance with RAG performance. We developed a random-effect meta-analysis model, using \nodds ratio as the effect size.\nResults: Among 335 studies, 20 were included in this literature review. The pooled effect size was 1.35, with a 95% confidence interval of \n1.19-1.53, indicating a statistically significant effect (P ¬º .001). We reported clinical tasks, baseline LLMs, retrieval sources and strategies, as \nwell as evaluation methods.\nDiscussion: Building on our literature review, we developed Guidelines for Unified Implementation and Development of Enhanced LLM Appli\xad\ncations with RAG in Clinical Settings to inform clinical applications using RAG.\nConclusion: Overall, RAG implementation showed a 1.35 odds ratio increase in performance compared to baseline LLMs. Future research \nshould focus on (1) system-level enhancement: the combination of RAG and agent, (2) knowledge-level enhancement: deep integration of \nknowledge into LLM, and (3) integration-level enhancement: integrating RAG systems within electronic health records.\nKey words: large language model; retrieval augmented generation; systematic review; meta-analysis. \nIntroduction\nLarge language models (LLMs) have reported remarkable \nperformance in question-answering, summarization, and text \ngeneration.1 Given this, researchers have explored its poten\xad\ntial in biomedical areas.2 For example, several studies \nreported the ability of using LLM to answer patient mes\xad\nsages,3 to analyze alert logic in clinical decision support,4 and \nto make discharge summaries more readable to patients.5\nHowever, several challenges remain.\nLLMs are trained on fixed datasets, which restrict their \nknowledge to information available up to the training cut-off \ndate. For example, GPT-4o‚Äôs training data only includes \ninformation up to October 2023, making it unable to \nrespond accurately to findings that emerged afterward. LLM \ntraining datasets are also generally broad and lack the specif\xad\nicity required for biomedical applications. Finally, not all \nsources used to train the LLMs are reliable and trustworthy. \nTo address these limitations, researchers have performed \nfine-tuning and retrieval-augmented generation (RAG) tech\xad\nniques. Fine-tuning can adapt LLMs to specific domains, but \nit is resource-intensive and does not allow for real-time \nupdates. In contrast, RAG maintains the original LLM archi\xad\ntecture while incorporating relevant context directly into \nqueries, offering more flexibility and control. In addition, \nRAG‚Äôs unique advantage in biomedical applications lies in its \nability to adapt to dynamic environments by delivering up-to- \ndate information and efficiently integrating external knowl\xad\nedge sources with high interpretability.6\nAnother limitation of using LLMs directly is the risk of hal\xad\nlucination, where the model generates incorrect or fabricated \ninformation.7 To mitigate such issues, researchers have pro\xad\nposed RAG as a solution that integrates up-to-date, relevant \ninformation, enhancing both the accuracy and reliability of \nLLM generated responses.8,9 For example, when ChatGPT \nReceived: November 19, 2024; Revised: December 17, 2024; Editorial Decision: January 2, 2025; Accepted: January 3, 2025 \n¬© The Author(s) 2025. Published by Oxford University Press on behalf of the American Medical Informatics Association.  \nThis is an Open Access article distributed under the terms of the Creative Commons Attribution-NonCommercial License (https://creativecommons.org/licenses/ \nby-nc/4.0/), which permits non-commercial re-use, distribution, and reproduction in any medium, provided the original work is properly cited. For commercial \nre-use, please contact reprints@oup.com for reprints and translation rights for reprints. All other permissions can be obtained through our RightsLink service via \nthe Permissions link on the article page on our site‚Äîfor further information please contact journals.permissions@oup.com. \nJournal of the American Medical Informatics Association, 2025, 32(4), 605‚Äì615 \nhttps://doi.org/10.1093/jamia/ocaf008 \nAdvance access publication 15 January 2025 \nResearch and Applications \nDownloaded from https://academic.oup.com/jamia/article/32/4/605/7954485 by guest on 11 January 2026'
 'was asked about medications for peripheral artery disease \npatients without increased bleeding risk, it initially omitted \nlow-dose rivaroxaban. After integrating retrieved text from \nthe 2024 American College of Cardiology / American Heart \nAssociation Guideline for the Management of Lower Extrem\xad\nity Peripheral Artery Disease,10 the model correctly recom\xad\nmended rivaroxaban.\nSeveral guidelines exist for evaluating Artificial Intelligence \n(AI) applications and LLMs in healthcare, including \nDECIDE-AI (Developmental and Exploratory Clinical Inves\xad\ntigations of DEcision support systems driven by Artificial \nIntelligence),11 CLAIM (Checklist for Artificial Intelligence in \nMedical Imaging),12\nand CONSORT-AI (Consolidated \nStandards of Reporting Trials-AI).13 In addition, Tam et al. \nintroduced QUEST, a framework specifically for human eval\xad\nuation of LLMs in healthcare.14 However, these guidelines \ndo not cover RAG applications in clinical settings, emphasiz\xad\ning the need for a more specific guideline.\nDespite the promise of RAG in improving LLM performance \nin clinical settings, there is limited understanding of its overall \neffectiveness comparing with the baseline LLM, adoption in \nclinical domains, and optimal strategies for its development in \nbiomedical applications. The aim of this study is to synthesize \nfindings from recent research of RAG and LLM in biomedicine \nand provide clinical development guidelines to improve effec\xad\ntiveness as well as transparency in future research.\nMaterials and methods\nStudy design\nWe conducted a systematic literature review. The report was \ncreated in adherence to the Preferred Reporting Items for Sys\xad\ntematic Reviews and Meta-Analyses 2020 analysis checklist \n(File S1).15 We also performed a meta-analysis following the \nCochrane Handbook guidelines.16\nScreening papers\nWe searched in 3 databases (PubMed, Embase, and Psy\xad\ncINFO), using terms related to ‚Äúretrieval augmented gener\xad\nation‚Äù and ‚Äúlarge language model.‚Äù PubMed and Embase \nwere selected based on recommendations from the Cochrane \nHandbook, as they are widely recognized for their extensive \ncoverage of biomedical literature.17 PsycINFO was included \nto capture articles from the psychological and behavioral sci\xad\nences. To maintain a high standard of quality and reliability, \nwe focused on peer-reviewed articles and excluded preprints. \nThe specific search terms used for each database are provided \nin File S2. Given that ChatGPT was released on November \n30, 2022, we set the publication filter to search papers pub\xad\nlished in 2023 and 2024. The search was performed on \nDecember 12, 2024. The inclusion criteria were: (1) the study \nmust compare baseline LLM performance with RAG per\xad\nformance and (2) the study must address a biomedical ques\xad\ntion. The exclusion criteria were: (1) literature reviews, \neditorial comments, or viewpoint papers, (2) studies focusing \non LLMs in languages other than English, or (3) studies cen\xad\ntered on a multi-agent system without a focus on RAG. SL \nscreened titles and abstracts, then conducted a full-text \nreview of papers meeting the criteria.\nData extraction\nFor each included study, we extracted the following informa\xad\ntion: author, title, publication year, journal, clinical task, and \nspecialty. Regarding RAG techniques, we gathered details \nabout the baseline LLM, retrieval sources, and strategies used \nin the pre-retrieval, retrieval, and post-retrieval stages. For \nevaluation, we extracted the evaluation method (human, \nautomated, or a combination of both), the number of evalua\xad\ntors, the evaluation dataset, and the evaluation metrics.\nMeta-analysis\nEffect size was defined as a metric quantifying the relation\xad\nship between variables, including both direction and magni\xad\ntude.18 For each included study, we calculated the effect size \nbetween baseline LLM performance and RAG-enhanced \nLLM performance. The outcomes focused on the perform\xad\nance of generation results, such as accuracy and usefulness. \nMetrics related to the retrieval process, cost, or speed were \nnot included as outcomes in the meta-analysis. For continu\xad\nous outcomes, we used Cohen‚Äôs d, standardized between- \ngroup mean difference (SMD), calculated as the difference in \nmeans divided by the pooled standard deviation. The stand\xad\nard error (SE) of SMD was calculated using the following for\xad\nmula (1), where n1 and n2 represent the sample sizes of each \ngroup.19 For dichotomous measurements, we calculated the \nlog-odds ratio, obtained by transforming the odds ratio (OR) \nwith the natural logarithm, and the associated SE was calcu\xad\nlated using formula (2), where a, b, c, and d represent the \nnumber of successful and failed events in the baseline LLM \nand RAG-enhanced LLM approaches. For studies reporting \nmultiple outcomes, we used the overall outcome to calculate \neffect size. If no overall outcome was reported, we averaged \nthe effect sizes of all reported outcomes. We excluded out\xad\ncomes with a sample size of less than 30 to avoid small- \nsample bias. \nSESMD ¬º\nffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi\nn1 √æ n2\nn1n2\n√æ\nSMD2\n2 n1 √æ n2\n√∞\n√û\ns\n(1) \nSElogOR ¬º\nffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi\n1\na √æ 1\nb √æ 1\nc √æ 1\nd\nr\n(2) \nWe developed a random-effect meta-analysis model, \nbecause of the variability in RAG architectures and evalua\xad\ntion datasets among the included studies. The random-effect \nmodel was used when individual study effects likely con\xad\ntained additional sources of variance beyond sampling error. \nBetween-study heterogeneity was assessed using Higgins & \nThompson‚Äôs I2 statistic, where 25% indicated low heteroge\xad\nneity, 50% moderate, and 75% substantial.20\nWe conducted subgroup analyses to explore performance \nvariations across different factors. First, we analyzed the \ninfluence of the baseline LLM, referring to the foundation \nmodel (eg, GPT-4 or Llama2) that provides the core architec\xad\nture for the system. Second, we examined data retrieval strat\xad\negies, categorizing them as simple or complex. Simple \nstrategies included fixed-length chunking and basic similarity \nsearch, and we performed a subgroup analysis to compare \nthese with complex retrieval strategies. Third, we analyzed \ndifferences based on evaluation methods, distinguishing \nbetween human evaluations, such as Likert scale ratings for \nhelpfulness and accuracy, and automatic evaluation metrics, \nincluding ROUGE-1 and BLEU. Finally, we conducted a sub\xad\ngroup analysis based on the type of task, classifying studies \n606                                                                                                      \nJournal of the American Medical Informatics Association, 2025, Vol. 32, No. 4 \nDownloaded from https://academic.oup.com/jamia/article/32/4/605/7954485 by guest on 11 January 2026'
 'Omar & Mohammed / Journal of Applied Science and Technology Trends Vol. 06, No. 02, pp. 342 ‚Äì357 (2025) \n \n344 \nevaluated using Exact Match, Precision, Recall, F1, Hits@k, \nMRR, and latency across simple, medium, and complex question \nsets. Unlike prior template-based methods, our approach enables \ntraceable, outcome-level validation. In summary, the main \ncontributions of this work are as follows: \n‚Ä¢ Hybrid LLM to Cypher QA: A system that translates \nnatural language questions into accurate, executable \nCypher over a Neo4j drug, disease, and symptom KG. \n‚Ä¢ Prompt-driven query generation: Schema,  entity \naware prompting that reliably maps diverse biomedical \nquestions to graph queries. \n‚Ä¢ Evidence transparency: Along with each answer, we \nsurface the generated Cypher and the supporting \njustification subgraph (nodes, edges) plus a brief natural \nlanguage rationale. \n‚Ä¢ Answer-level evaluation: End-to-end assessment using \nExact Match, F1, Precision/Recall, Hits@k, MRR and \nlatency across simple, medium and complex tiers. \n‚Ä¢ Local, reproducible deployment: On-prem LLaMA 3 \ninference (no cloud dependency) suitable for biomedical \nsettings requiring low latency and strong data control. \n \nThe remainder of this paper is organized as follows: Section \n2 reviews related work on biomedical knowledge graphs and \nLLM-based QA systems. Section 3 provides background on \nknowledge graphs, large language models, and question \nanswering frameworks. Section 4 details the proposed \nmethodology, \nincluding \nsystem \narchitecture, \ndataset \nconstruction, and query translation. Section 5 presents the \nexperimental results through both quantitative metrics and \nqualitative case studies. Section 6 discusses the findings, analyzes \nlimitations, and compares performance against baseline models. \nFinally, Section 7 concludes this paper and outlines directions for \nfuture work. \nII. RELATED WORK \nRecently, studies have concentrated on the integration of \nclinical and medical knowledge graphs (LLM) to improve the \nanswer to medical questions. Researchers have derived several \nbiomedical KGs using Neo4j and incorporated the application of \nLLMs like LLaMA and GPT to convert natural language \nquestions into graph queries. Improvements in answer \ncorrectness, reduction of hallucination errors, one-to-many \nrelationships, and support for complex reasoning were the \nobjectives of these efforts. Some frameworks also adopted \nretrieval methods to ground responses in secure data. \nSu et al.[11] developed an integrative Biomedical Knowledge \nHub (iBKH), a huge biomedical knowledge graph that comprises \n18 of the very best data sources. The deployment of the iBKH in \nNeo4j allows for a user-friendly web portal to allow fast and \ninteractive knowledge retrieval. The system implemented \nadvanced graph learning techniques to enable the discovery of \nbiomedical knowledge, illustrated by an example of repurposing \nin silico drugs for Alzheimer‚Äôs disease. iBKH achieved \npromising predictive performance for known drugs and proposed \npossible new drug candidates. \nRajabi and Kafaie[19] proposed a disease knowledge graph \nusing a cross-referential disease database comprising diseases, \nsymptoms, and drugs interconnected with relationships. They \ntransferred the data into Neo4j to create a graph of 9,400 nodes \nand 45,000 relationships representing the semantic links between \nmedical concepts. Applying Cypher queries enabled answering \ncomplex medical questions regarding identifying drugs that may \ncause certain diseases; it was demonstrated that the graph inferred \nnew relationships not explicitly existing in the original data. The \nconclusion was that disease knowledge graphs sped up clinical \ndiscovery and contributed to understanding complex medical \nrelationships. \nHou et al.[3] assessed and contrasted ChatGPT (both GPT-\n3.5 and GPT-4) and the biomedical knowledge graphs (BKGs) \nconcerning their ability to answer biomedical questions, generate \nnew knowledge, and reason. Their datasets were focused on \ndietary supplements and drugs, while evaluation criteria entailed \naccuracy, novelty, and reasoning ability. The results indicate that \nwhile GPT-4 surpassed GPT-3.5 and BKGs in knowledge \nprovision, it proved inconsistent with regard to citations and \nreasoning. Compared to them, BKGs scored higher in accuracy \nand reliability, especially in discovering novel links as well as \nwithin structured reasoning. \nSoman et al.[13] presented a novel framework called KG-\nRAG that integrates a large biomedical knowledge graph \n(SPOKE) with LLaMA 2, GPT-3.5, and GPT-4 (LLMs) to \nproduce accurate biomedical text. They optimized the retrieval of \nrelevant graph context to cut over 50% tokens without losing \naccuracy. It aided LLMs in performing better on biomedical \nquestion answering with very high accuracy boosts, especially in \nthe case of LLaMA 2. They compared KG-RAG to other retrieval \nmethods and indicated its comparatively more robust and \nefficient results. The framework produced reliable evidence-\nbased answers grounded in biomedical knowledge. \nLuo et al.[23] created ChatKBQA, a new framework with a \nquestion-and-answer approach over knowledge bases that first \ngenerates logical forms with the help of fine-tuned LLMs and \nthen retrieves the relevant entities and relations. This generate-\nthen-retrieve approach is supposed to handle a couple of issues \nwith the earlier methods concerning tedious retrieval and error \npropagation. They fine-tuned open-source LLMs like LLaMA 2 \nto change natural-language questions into logical forms with high \naccuracy. The retrieval phase uses unsupervised phrase-level \nsemantic matching in a way that enhances the alignment of \nentities and relations. Experiments on benchmark datasets \nindicate ChatKBQA to be superior to its predecessors, with the \nhighest accuracy to date. \nPusch and Conrad[6] conducted work under a hybrid \napproach conflating LLMs and biomedical Knowledge Graphs \n(KGs) to suppress hallucination errors in question-answering. \nThey proposed query-checking algorithms for validating, \ncorrecting, and executing the KG Cypher queries that LLMs \ngenerated, thereby attaining accurate and understandable \nanswers. The system used retrieval-augmented generation (RAG) \nto ground answers within KG data. The methodology was \nvalidated on a biomedical KG called PrimeKG using 50 \nbenchmark questions, assessing models like GPT-4 Turbo and \nLLaMA 3. Commercially available GPT-4 Turbo obtained'
 'provides an innovative dynamic co-augmentation\nframework for the refinement of large language\nmodels and knowledge graphs. Initially, our ap-\nproach extracts structural insights from the unstruc-\ntured scientific literature, crafting a specialized\nknowledge graph for AD. Subsequently, we employ\na coarse-to-fine sampling technique coupled with\na unique self-aware knowledge retrieval strategy\nto pinpoint relevant information from the knowl-\nedge graph. The extensive evaluation conducted\nin our constructed ADQA benchmark showcases\nthe effectiveness of our method and provides fur-\nther hints into the synergy of LLMs and knowledge\ngraph in the context of AD. In the future, we will\ndo more exploration in adopting and benchmarking\nLLMs in the AD areas.\n7\nLimitations\nIn the development of our AD-KG, our primary\nfocus lies in the exploration of two distinct meth-\nods for extracting relationships between associated\nentities. For entity recognition, we employ a strong\nPubTator annotator directly, without delving into\nthe utilization of LLMs in this context. However,\nwe have observed that LLMs also exhibit promis-\ning entity extraction capabilities in Section 3.2.1.\nWe defer the refinement of methods for extract-\ning entities for KG construction with LLMs to fu-\nture works. Furthermore, a significant contribution\nof our work is the establishment of the ADQA\nbenchmark. Nonetheless, the datasets utilized in\nconstructing ADQA primarily consist of medical\nschool exam questions, potentially exhibiting a do-\nmain gap from the scientific literature informing\nAD-KG. One potential remedy is leveraging Pub-\nmedQA (Jin et al., 2019); however, it is hindered\nby limited data amount. In the future, we will keep\ngathering AD-related QA samples and expanding\nthe size of our ADQA benchmark.\n8\nEthics Statement\nWe have familiarized ourselves with and honour\nthe ethical code set out in the ACL Code of Ethics2.\nThe knowledge graphs constructed in the paper\nare based on published scientific literature from\nPubMed. The ADQA dataset used in the study is\nalso derived from publicly available medical QA\ndatasets that are properly cited. We strive to ensure\nour study upholds ethical principles and not cause\nany kind of safety or privacy concerns. Although\n2https://www.aclweb.org/portal/content/acl-code-ethics\nnot observed in our multiple-choice QA analysis,\nwe recognize the possibility of factual errors and\nhallucinations when using pre-trained LLMs for\nmedical QA tasks in general, and we do not recom-\nmend these models be applied in a practical setting\nat present.\nReferences\nFelix Agbavor and Hualou Liang. 2022. Predicting\ndementia from spontaneous speech using large lan-\nguage models. PLOS Digital Health, 1:1‚Äì14.\nRohan Anil, Andrew M Dai, Orhan Firat, Melvin John-\nson, Dmitry Lepikhin, Alexandre Passos, Siamak\nShakeri, Emanuel Taropa, Paige Bailey, Zhifeng\nChen, et al. 2023. Palm 2 technical report. arXiv\npreprint arXiv:2305.10403.\nAparna Balagopalan, Benjamin Eyre, Frank Rudzicz,\nand Jekaterina Novikova. 2020. To BERT or not\nto BERT: Comparing Speech and Language-Based\nApproaches for Alzheimer‚Äôs Disease Detection. In\nProc. Interspeech 2020, pages 2167‚Äì2171.\nMaciej Besta, Nils Blach, Ales Kubicek, Robert Ger-\nstenberger, Lukas Gianinazzi, Joanna Gajda, Tomasz\nLehmann, Michal Podstawski, Hubert Niewiadomski,\nPiotr Nyczyk, et al. 2023. Graph of thoughts: Solv-\ning elaborate problems with large language models.\narXiv preprint arXiv:2308.09687.\nZhen Bi, Jing Chen, Yinuo Jiang, Feiyu Xiong, Wei Guo,\nHuajun Chen, and Ningyu Zhang. 2024. Codekgc:\nCode language model for generative knowledge\ngraph construction.\nACM Transactions on Asian\nand Low-Resource Language Information Process-\ning, 23(3):1‚Äì16.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. Advances in neural information processing\nsystems, 33:1877‚Äì1901.\nSalvatore Carta, Alessandro Giuliani, Leonardo Piano,\nAlessandro Sebastian Podda, Livio Pompianu, and\nSandro Gabriele Tiddia. 2023. Iterative zero-shot llm\nprompting for knowledge graph construction. arXiv\npreprint arXiv:2307.01128.\nJiawei Chen, Hongyu Lin, Xianpei Han, and Le Sun.\n2024.\nBenchmarking large language models in\nretrieval-augmented generation. In Proceedings of\nthe AAAI Conference on Artificial Intelligence, vol-\nume 38, pages 17754‚Äì17762.\nZeming Chen, Alejandro Hern√°ndez Cano, Angelika\nRomanou, Antoine Bonnet, Kyle Matoba, Francesco\nSalvi, Matteo Pagliardini, Simin Fan, Andreas K√∂pf,\nAmirkeivan Mohtashami, et al. 2023. Meditron-70b:\nScaling medical pretraining for large language mod-\nels. arXiv preprint arXiv:2311.16079.'
 'Joy et. al \n2 \n \n1 Introduction \nLarge language models (LLMs) have rapidly advanced the state of natural-language \nprocessing, reaching or surpassing expert performance across a wide range of \nbiomedical tasks, including cell type annotation, protein-structure prediction and \nautomated synthesis of clinical-trial results 1‚Äì6. However, the underlying generative \nmethodology of these models, which sequentially predict tokens based on statistical \npatterns learned from massive text corpora, renders them susceptible to hallucinations, \ndefined as outputs that are syntactically fluent yet factually incorrect 7,8. Such \ninaccuracies pose significant risks in biomedicine, where even minor errors can \nmisdirect research efforts, delay critical therapeutic discoveries, or compromise patient \nsafety 7,9‚Äì11. Indeed, recent assessments underscore that hallucination rates remain too \nhigh for safe and effective deployment in clinical and research-intensive environments \n12,13. \nEfforts to mitigate these hallucinations through domain-specific pre-training and prompt \nengineering have yielded only incremental improvements, as these approaches \ncontinue to embed knowledge implicitly within opaque model parameters and fail to \nreliably surface evidence provenance 14‚Äì16. Retrieval-augmented generation (RAG) has \nemerged as a promising solution, explicitly grounding model-generated responses by \ndynamically incorporating external, verifiable evidence into prompts 17‚Äì19. Within \nbiomedical question-answering contexts, RAG approaches consistently reduce \nhallucinations and elevate factual accuracy compared to parameter-only models. \nNonetheless, the efficacy of RAG hinges critically on the precision, comprehensiveness, \nand currency of the retrieved contextual evidence 20‚Äì22. \nKnowledge graphs (KGs) are particularly compelling resources for RAG because they \nexplicitly represent biological entities and their relationships, support multi-hop \nmechanistic reasoning, and maintain persistent identifiers that simplify provenance \ntracking 23‚Äì26. Yet most biomedical KGs are tuned to a narrow slice of biology (for \nexample, protein‚Äìprotein interactions) or require extensive curation to remain current, \nlimiting their utility for cross-domain mechanistic reasoning. To address these \nchallenges, BioThings Explorer (BTE) integrates and federates 61 authoritative \nbiomedical APIs into a continuously updated meta-knowledge graph that encompasses \ngenes, pathways, drugs, diseases, phenotypes, and more 27. The API-centric framework \nof BTE returns structured JSON triples annotated with semantic types and evidence \ncitations from reputable biomedical databases such as Gene Ontology, DrugBank, and \nPubmed central using Translator Reasoner API (TRAPI) specification 28‚Äì30. \nHere, we introduce BTE‚ÄìRAG (BioThings Explorer‚ÄìRetrieval-Augmented Generation), \na novel framework that integrates the conversational fluency and reasoning capabilities \nof advanced LLMs with the explicit, multi-domain mechanistic knowledge captured by \nBTE. BTE‚ÄìRAG dynamically executes targeted, query-focused graph traversals to \nretrieve concise, mechanistically pertinent evidence, formulates this evidence into \ndeclarative context statements, and augments model prompts accordingly. \nTo rigorously assess the performance of BTE-RAG in biomedical question answering, \nwe systematically created three specialized benchmark datasets from DrugMechDB, a \ncurated knowledge base containing 5,666 expert-annotated mechanistic pathways with \nliterature validation 31. These datasets consist of gene-centric (n = 798), metabolite-\n.\nCC-BY 4.0 International license\navailable under a\n(which was not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made \nThe copyright holder for this preprint\nthis version posted August 2, 2025. \n; \nhttps://doi.org/10.1101/2025.08.01.668022\ndoi: \nbioRxiv preprint'
 '17. Lefebvre C, Glanville J, Briscoe S, et al. Chapter 4: searching for \nand selecting studies. Cochrane Handbook for Systematic Reviews \nof Interventions Version, Vol. 6. Cochrane, 2024. https://training. \ncochrane.org/handbook/current/chapter-04\n18. Chapter 3 Effect Sizes j Doing Meta-Analysis in R. Accessed Octo\xad\nber 13, 2024. https://bookdown.org/MathiasHarrer/Doing_Meta_ \nAnalysis_in_R/effects.html\n19. Borenstein M, Hedges LV, Higgins JPT, et al. Introduction to \nMeta-Analysis. John Wiley & Sons; 2011.\n20. Higgins JPT, Thompson SG. Quantifying heterogeneity in a meta- \nanalysis. Stat Med. 2002;21:1539-1558. https://doi.org/10.1002/ \nsim.1186\n21. Peters JL, Sutton AJ, Jones DR, et al. Contour-enhanced meta- \nanalysis funnel plots help distinguish publication bias from other \ncauses of asymmetry. J Clin Epidemiol. 2008;61:991-996. https:// \ndoi.org/10.1016/j.jclinepi.2007.11.010\n22. Egger M, Smith GD, Schneider M, et al. Bias in meta-analysis detected \nby a simple, graphical test measures of funnel plot asymmetry. BMJ. \n1997;315:629-634. https://doi.org/10.1136/bmj.315.7109.629\n23. Kreimeyer K, Canzoniero JV, Fatteh M, et al. Using retrieval- \naugmented generation to capture molecularly-driven treatment \nrelationships for precision oncology. Stud Health Technol Inform. \n2024;316:983-987. https://doi.org/10.3233/SHTI240575\n24. Murugan M, Yuan B, Venner E, et al. Empowering personalized \npharmacogenomics with generative AI solutions. J Am Med Inform \nAssoc. 2024;31:1356-1366. https://doi.org/10.1093/jamia/ocae039\n25. Yazaki M, Maki S, Furuya T, et al. Emergency patient triage \nimprovement through a retrieval-augmented generation enhanced \nlarge-scale language model. Prehosp Emerg Care. 2024;1-7. \nhttps://doi.org/10.1080/10903127.2024.2374400\n26. Malik S, Kharel H, Dahiya DS, et al. Assessing ChatGPT4 with \nand without retrieval-augmented generation in anticoagulation \nmanagement for gastrointestinal procedures. Ann Gastroenterol. \n2024;37:514-526. https://doi.org/10.20524/aog.2024.0907\n27. Kresevic S, Giuffr\x12e M, Ajcevic M, et al. Optimization of hepatolog\xad\nical clinical guidelines interpretation by large language models: a \nretrieval augmented generation-based framework. NPJ Digit Med. \n2024;7:102-109. https://doi.org/10.1038/s41746-024-01091-y\n28. Zakka C, Shad R, Chaurasia A, et al. Almanac‚Äìretrieval-aug\xad\nmented language models for clinical medicine. NEJM AI. 2024;1 \n(2):10.1056/aioa2300068. https://doi.org/10.1056/aioa2300068\n29. Long C, Subburam D, Lowe K, et al. ChatENT: augmented large \nlanguage model for expert knowledge retrieval in otolaryngology‚Äì \nhead and neck surgery. Otolaryngol Head Neck Surg. \n2024;171:1042-1051. https://doi.org/10.1002/ohn.864\n30. Wang D, Liang J, Ye J, et al. Enhancement of the performance of \nlarge language models in diabetes education through retrieval-aug\xad\nmented generation: comparative study. J Med Internet Res. \n2024;26:e58041. https://doi.org/10.2196/58041\n31. Rau S, Rau A, Nattenm‚Ç¨uller J, et al. A retrieval-augmented chatbot \nbased on GPT-4 provides appropriate differential diagnosis in gas\xad\ntrointestinal radiology: a proof of concept study. Eur Radiol Exp. \n2024;8:60. https://doi.org/10.1186/s41747-024-00457-x\n32. Morris JH, Soman K, Akbas RE, et al. The scalable precision \nmedicine open knowledge engine (SPOKE): a massive knowl\xad\nedge \ngraph \nof \nbiomedical \ninformation. \nBioinformatics. \n2023;39(2):btad080. \nhttps://doi.org/10.1093/BIOINFOR\xad\nMATICS/BTAD080\n33. Soman K, Rose PW, Morris JH, et al. Biomedical knowl\xad\nedge graph-optimized prompt generation for large language \nmodels. Commun ACM. 2023;66:7-7. https://doi.org/10.1145/ \n3606337\n34. Jeong M, Sohn J, Sung M, et al. Improving medical reasoning \nthrough retrieval and self-reflection with retrieval-augmented large \nlanguage models. Bioinformatics. 2024;40:i119-i129. https://doi. \norg/10.1093/bioinformatics/btae238\n35. Alkhalaf M, Yu P, Yin M, et al. Applying generative AI with \nretrieval augmented generation to summarize and extract key \nclinical information from electronic health records. J Biomed \nInform. \n2024;156:104662. \nhttps://doi.org/10.1016/j.jbi.2024. \n104662\n36. Tarabanis C, Zahid S, Mamalis M, et al. Performance of publicly \navailable large language models on internal medicine board-style \nquestions. PLOS Digit Health. 2024;3:e0000604. https://doi.org/ \n10.1371/journal.pdig.0000604\n37. Glicksberg BS, Timsina P, Patel D, et al. Evaluating the accuracy of \na state-of-the-art large language model for prediction of admis\xad\nsions from the emergency room. J Am Med Inform Assoc. \n2024;31:1921-1928. https://doi.org/10.1093/jamia/ocae103\n38. Zelin C, Chung WK, Jeanne M, et al. Rare disease diagnosis using \nknowledge guided retrieval augmentation for ChatGPT. J Biomed \nInform. 2024;157:104702. https://doi.org/10.1016/j.jbi.2024.104702\n39. Chen X, Wang L, You MK, et al. Evaluating and enhancing large \nlanguage models‚Äô performance in domain-specific medicine: devel\xad\nopment and usability study with DocOA. J Med Internet Res. \n2024;26:e58158. https://doi.org/10.2196/58158\n40. Du X, Novoa-Laurentiev J, Plasaek JM, et al. Enhancing early \ndetection of cognitive decline in the elderly: a comparative study \nutilizing large language models in clinical notes. medRxiv. \n2024;109:105401. https://doi.org/10.1101/2024.04.03.24305298\n41. Zhang Y, Li Y, Cui L, et al. Siren‚Äôs song in the AI Ocean: a survey \non hallucination in large language models. 2023, preprint: not \npeer reviewed. https://arxiv.org/abs/2309.01219\n42. Li X, Ouyang JA. Systematic investigation of knowledge retrieval \nand selection for retrieval augmented generation. 2024, preprint: \nnot peer reviewed. https://arxiv.org/abs/2410.13258\n43. Xiong G, Jin Q, Lu Z, et al. Benchmarking retrieval-augmented \ngeneration for medicine. Findings of the Association for Computa\xad\ntional Linguistics: ACL 2024, 6233-6251. Bangkok, Thailand: \nAssociation for Computational Linguistics. \n44. Hewitt KJ, Wiest IC, Carrero ZI, et al. Large language models as a \ndiagnostic support tool in neuropathology. J Pathol Clin Res. \n2024;10:e70009. https://doi.org/10.1002/2056-4538.70009\n45. Allahverdiyev R, Taha M, Akalin A, et al. ChunkRAG: novel \nLLM-chunk filtering method for RAG systems. October 25, 2024, \npreprint: not peer reviewed. https://arxiv.org/abs/2410.19572\n46. Cai B, Zhang FL, Wang C. Research on chunking algorithms of \ndata de-duplication. Advances in Intelligent Systems and Comput\xad\ning, Vol. 181. 2013:1019-1025. Springer, Berlin, Heidelberg. \nhttps://doi.org/10.1007/978-3-642-31698-2_144\n47. Optimizing RAG with Advanced Chunking Techniques. Accessed \nDecember 15, 2024. https://antematter.io/blogs/optimizing-rag- \nadvanced-chunking-techniques-study\n48. Chen J, Xiao S, Zhang P, et al. M3-Embedding: multi-lingual, \nmulti-functionality, multi-granularity text embeddings through \nself-knowledge distillation. Findings of the Association for Com\xad\nputational Linguistics ACL 2024. 2024:2318-2335. Association \nfor Computational Linguistics. https://doi.org/10.18653/V1/2024. \nFINDINGS-ACL.137\n49. Sawarkar K, Mangal A, Solanki SR. Blended RAG: improving \nRAG (Retriever-Augmented Generation) accuracy with semantic \nsearch and hybrid query-based retrievers. 2024 IEEE 7th Interna\xad\ntional Conference on Multimedia Information Processing and \nRetrieval (MIPR), 155-161. San Jose, CA, USA: IEEE. https://doi. \norg/10.1109/MIPR62202.2024.00031\n50. Edge D, Trinh H, Cheng N, et al. From local to global: a graph \nRAG approach to query-focused summarization. 2024, preprint: \nnot peer reviewed. https://arxiv.org/abs/2404.16130\n51. Wu Z, Hasan A, Wu J, et al. KnowLab_AIMed at MEDIQA- \nCORR 2024: Chain-of-Though (CoT) prompting strategies for \nmedical error detection and correction. Proceedings of the 6th \nClinical Natural Language Processing Workshop. 2024:353-359. \nAssociation for Computational Linguistics. 10.18653/v1/2024. \nclinicalnlp-1.33\n52. Kwon T, Tzu-Iunn Ong K, Kang D, et al. Large language models \nare clinical reasoners: reasoning-aware diagnosis framework with \n614                                                                                                      \nJournal of the American Medical Informatics Association, 2025, Vol. 32, No. 4 \nDownloaded from https://academic.oup.com/jamia/article/32/4/605/7954485 by guest on 11 January 2026'
 'Omar & Mohammed / Journal of Applied Science and Technology Trends Vol. 06, No. 02, pp. 342 ‚Äì357 (2025) \n \n345 \nrecord-high accuracy, while open-source models achieved \nimpressive gains through prompt optimization. \nFeng et al.[22] developed the Knowledge Graph-based \nThought (KGT) framework that integrated LLMs with a pan-\ncancer knowledge graph for biomedical question answering. \nKGT was designed to reason on the knowledge graph schema and \nidentify optimal subgraphs to use for directing accurate answer \ngeneration, all without fine-tuning the LLMs. The framework is \nbenchmarked against a new dataset (PcQA) designed specifically \nfor pan-cancer KGQA tasks and has outperformed all existing \nstate-of-the-art approaches by a rather large margin. KGT‚Äôs \npracticality in biomedical issues was highlighted through case \nstudies for drug repositioning, drug resistance, and biomarker \ndiscovery. Their approach exhibited robust adaptability among \nvarious LLMs. \nRezaei et al.[26] developed AMG-RAG, a dynamic \nframework that utilizes autonomous LLM agents with medical \nsearch tools in the continuous construction and real-time updating \nof Medical Knowledge Graphs (MKGs). Their system \nincorporated confidence scoring and multi-hop reasoning to \nimprove accuracy and interpretability in medical question \nanswering. AMG-RAG outperformed size models on both very \nhard MEDQA benchmarks and more accessible MedMCQA \nones, proving that it could conduct efficient reasoning based on \ncurrent structured medical knowledge. They also used Neo4j to \nmanage the knowledge graphs while adding external searches to \nensure the latest data. \nTiwari et al.[24] presented Auto-Cypher, a recent automated \npipeline for producing high-quality synthetic data for training \nLLMs by mapping natural language to Cypher queries for graph \ndatabases like Neo4j. The pipeline deployed the novelty of LLM-\nas-database-filler to synthesize Neo4j databases for the execution \nof generated queries to ensure their correctness. A sizable dataset \ncalled SynthCypher was created, spanning multiple domains and \ncomplex queries, leading to a 40% improvement in LLM \nperformance on Cypher generation. The datasets were used to \nfine-tune open-source models such as LLaMA, Mistral, and \nQwen, and the SPIDER benchmark was adapted for evaluation \npurposes. \nMohammed et al.[29] proposed a hybridized GraphRAG \nframework combining Neo4j-based UMLS knowledge graphs \nwith a vector store for medical textbooks to create an improved \nU.S.M.L.E.-style clinical question-answering approach. The \nproject integrated symbolic reasoning from knowledge graphs \nwith semantic retrieval performed on text embeddings to enhance \nrelevance and accuracy via adaptive re-ranking and query \nexpansion. The system had the answers produced by GPT-4o-\nMini, with different prompting strategies encouraging evidence-\nbased and traceable responses grounded in verified medical \nknowledge. Experiments showed that the hybrid approach \nimproved factual accuracy and citation fidelity as compared to \nthe L.L.M.-only approach, enhancing transparency and \nreliability. It is shown that binding both structured and \nunstructured medical knowledge sources could aid in \nameliorating hallucinations and hence improve clinical \ntrustworthiness in AI-driven medical QA. \nYang et al.[30] articulated sepsis knowledge graph was \ncrafted by combining multicenter clinical data from over 10,000 \npatients with the help of GPT-4 for entity recognition and \nrelationship extraction. Real-world data were collected from \nthree hospitals and integrated with clinical guidelines and \ndatabases from the public domain. The knowledge graph \ncontained 1,894 nodes and 2,021 relationships pertaining to \ndiseases, symptoms, biomarkers, treatments, and complications. \nGPT outperformed other models in every resolution on sepsis-\nspecific datasets to obtain high F1-score results. The constructed \ngraph highlighted complex interactions in sepsis for assisting \nclinical decision-making and was implemented on Neo4j. \nGuan et al.[20] proposed a novel method for constructing a \nlocal knowledge graph from retrieved biomedical documents by \nextracting propositional claims. They carried out layer wise \nsummarization on this graph to capture multi-document \nrelationships and provide comprehensive contextual information \nto a language model for question-answering purposes. The \nmethod resolved issues in multi-document biomedical QA, such \nas noise cancellation and efficient context usage. They then tested \ntheir method on several benchmarks for biomedical question \nanswering, achieving performance at least comparable to, if not \nbetter than, existing retrieval-augmented generation (RAG) \nbaselines. The study established enhanced reasoning and answer \naccuracy of the model achieved through structured graph \nsummarization.   \nPrevious studies have improved biomedical QA using KGs \nand LLMs, but important gaps remain. Most systems lack \ntransparent, graph-based justifications, rely on limited evaluation \nmethods, or depend on cloud resources that reduce privacy and \nreproducibility. Our framework addresses these gaps by \nproviding visible Cypher queries with evidence subgraphs, \napplying comprehensive performance metrics across difficulty \nlevels, and ensuring fully local, privacy-preserving deployment. \nTable I summarizes key previous studies on biomedical \nknowledge graphs and question answering, outlining their \nmethods, datasets, and main limitations. \nIII. PRELIMINARIES \nThis section outlines the fundamental concepts required to \nunderstand the proposed system. It introduces biomedical \nknowledge graphs, explains how Neo4j stores data in graph \nform, and describes the use of Cypher for querying. It also \nprovides a brief overview of large language models (LLMs) and \ntheir role in interpreting natural language. \n \n \nA. Biomedical Knowledge Graphs \nBiomedical Knowledge Graphs (BKGs) provide a structured \nrepresentation of complex biomedical information by modeling \ndiverse medical entities, such as diseases, drugs, symptoms, and \nbiological pathways, as interconnected nodes within a graph \nstructure. The edges in these graphs represent the semantic \nrelationships between these entities, including ‚Äôtreats‚Äô, ‚Äôcauses‚Äô, \n‚Äôinteracts with‚Äô and many others, as illustrated in Fig 1. This \nform of representation enables the integration of heterogeneous \nbiomedical data from a wide range of sources, including'
 'Limitations\nThis study was limited to peer-reviewed publications avail\xad\nable in biomedical databases (eg, PubMed, Embase), exclud\xad\ning \npreprint \narticles \nfrom \nrepositories \nlike \nArXiv. \nAdditionally, only studies in English language were included, \nwhich might have excluded relevant studies in other lan\xad\nguages. We did not include sources such as IEEE Xplore or \nGoogle Scholar, which might have additional relevant stud\xad\nies. However, our focus was on biomedicine, and we priori\xad\ntized databases specifically tailored to biomedical research to \nmaintain the relevance and quality of the included studies. \nFurthermore, we used free-text searches in the databases, \nwhich activated automatic mapping to Medical Subject \nHeadings (MeSH) and Emtree terms, improving retrieval \naccuracy. However, the limitations of automatic term map\xad\nping cannot be ignored, as it may introduce variability if the \nunderlying algorithms change. To address this, we have docu\xad\nmented all identified papers from our search. The title, publi\xad\ncation year, PMID, PUI, and database source for each study \nare provided in File S3.\nConclusion\nWe conducted a systematic literature review of studies \nexploring the use of RAG and LLM in clinical tasks. RAG \nimplementation showed a 1.35 odds ratio increase in per\xad\nformance compared to baseline LLMs. To improve perform\xad\nance and transparency in future studies, we developed \nguidelines for improving clinical RAG applications based on \ncurrent research findings. Future research could focus on \nthese 3 directions: (1) system-level enhancement: the combi\xad\nnation of RAG and agent, (2) knowledge-level enhancements: \ndeep integration of knowledge into LLM, and (3) integration- \nlevel enhancements: integrating RAG systems within EHRs.\nAuthor contributions\nSiru Liu (Conceptualization, Data curation, Formal analysis, \nInvestigation, Methodology, Software, Visualization, Writing \n‚Äì original draft), Allison B. McCoy (Conceptualization, Writ\xad\ning ‚Äì review & editing), Adam Wright (Conceptualization, \nWriting ‚Äì review & editing).\nSupplementary material\nSupplementary material is available at Journal of the Ameri\xad\ncan Medical Informatics Association online.\nFunding\nThis work was supported by National Institutes of Health \ngrants: R00LM014097-02 and R01LM013995-01.\nConflicts of interest\nThe authors do not have conflicts of interest related to this \nstudy.\nData availability\nThe characteristics and outcomes for each included study \nwere reported in the File S2.\nReferences\n01. Raiaan MAK, Mukta MSH, Fatema K, et al. A review on large lan\xad\nguage models: architectures, applications, taxonomies, open issues \nand challenges. IEEE Access. 2024;12:26839-26874. https://doi. \norg/10.1109/ACCESS.2024.3365742\n02. Thirunavukarasu AJ, Ting DSJ, Elangovan K, et al. Large language \nmodels in medicine. Nat Med. 2023;29:1930-1940. https://doi. \norg/10.1038/s41591-023-02448-8\n03. Ayers JW, Poliak A, Dredze M, et al. Comparing physician and arti\xad\nficial intelligence Chatbot responses to patient questions posted to a \npublic social media forum. JAMA Intern Med. 2023;183:589-596. \nhttps://doi.org/10.1001/jamainternmed.2023.1838\n04. Liu S, Wright AP, Patterson BL, et al. Using AI-generated sugges\xad\ntions from ChatGPT to optimize clinical decision support. J Am \nMed Inform Assoc. 2023;30:1237-1245. https://doi.org/10.1093/ \njamia/ocad072\n05. Zaretsky J, Kim JM, Baskharoun S, et al. Generative artificial intel\xad\nligence to transform inpatient discharge summaries to patient- \nfriendly language and format. JAMA Netw Open. 2024;7: \ne240357. https://doi.org/10.1001/jamanetworkopen.2024.0357\n06. Gao Y, Xiong Y, Gao X, et al. Retrieval-augmented generation for \nlarge language models: a survey. 2023, preprint: not peer reviewed. \nhttps://arxiv.org/abs/2312.10997\n07. Xu Z, Jain S, Kankanhalli M. Hallucination is inevitable: an innate \nlimitation of large language models. January 22, 2024, preprint: \nnot peer reviewed. https://arxiv.org/abs/2401.11817\n08. Shuster K, Poff S, Chen M, et al. Retrieval augmentation reduces \nhallucination in conversation. Findings of the Association for Com\xad\nputational Linguistics, Findings of ACL: EMNLP 2021. Associa\xad\ntion for Computational Linguistics (ACL); 2021:3784-3803.\n09. Ayala  O, Bechard P. Reducing hallucination in structured outputs \nvia Retrieval-Augmented Generation. Proceedings of the 2024 \nConference of the North American Chapter of the Association for \nComputational Linguistics: Human Language Technologies. \nAssociation for Computational Linguistics; 2024;228-38. https:// \ndoi.org/10.18653/v1/2024.naacl-industry.19\n10. Gornik HL, Aronow HD, Goodney PP, et al. 2024 ACC/AHA/ \nAACVPR/APMA/ABC/SCAI/SVM/SVN/SVS/SIR/VESS guideline \nfor the management of lower extremity peripheral artery disease: a \nreport of the American College of Cardiology/American Heart \nAssociation Joint Committee on Clinical Practice Guidelines. Cir\xad\nculation. 2024;149:e1313-e1410. https://doi.org/10.1161/CIR. \n0000000000001251\n11. Vasey B, Nagendran M, Campbell B, et al. Reporting guideline for \nthe early stage clinical evaluation of decision support systems \ndriven by artificial intelligence: DECIDE-AI. BMJ. 2022;377: \ne070904. https://doi.org/10.1136/bmj-2022-070904\n12. Mongan J, Moy L, Kahn CE. Checklist for Artificial Intelligence in \nMedical Imaging (CLAIM): a guide for authors and reviewers. \nRadiol Artif Intell. 2020;2:e200029. https://doi.org/10.1148/ryai. \n2020200029\n13. Martindale APL, Ng B, Ngai V, et al. Concordance of randomised \ncontrolled trials for artificial intelligence interventions with the \nCONSORT-AI \nreporting \nguidelines. \nNat \nCommun. \n2024;15:6376-6311. https://doi.org/10.1038/s41467-024-45355-3\n14. Tam TYC, Sivarajkumar S, Kapoor S, et al. A framework for \nhuman evaluation of large language models in healthcare derived \nfrom literature review. NPJ Digit Med. 2024;7:258. https://doi. \norg/10.1038/s41746-024-01258-7\n15. Shamseer L, Moher D, Clarke M, et al. Preferred reporting items \nfor systematic review and meta-analysis protocols (PRISMA-p) \n2015: elaboration and explanation. BMJ. 2015;350:g7647. \nhttps://doi.org/10.1136/bmj.g7647\n16. Higgins JPT, Thomas J, Chandler J, et al. (editors). Cochrane Hand\xad\nbook for Systematic Reviews of Interventions version 6.5 (updated \nAugust 2024). Cochrane, 2024. Available from www.training. \ncochrane.org/handbook. Date accessed December 11, 2024.\nJournal of the American Medical Informatics Association, 2025, Vol. 32, No. 4                                                                                                      \n613 \nDownloaded from https://academic.oup.com/jamia/article/32/4/605/7954485 by guest on 11 January 2026'
 'offer global information based on user query, offering \nbetter performance than na√Øve RAG on the traditional \nvector databases.50\n6) Implement few-shot learning with CoT for complex clin\xad\nical reasoning. \nFew-shot learning has been shown to enhance LLMs‚Äô \nreasoning capabilities by teaching specific reasoning that \nmay not have been included in their original training. \nSimilarly, CoT techniques can improve complex reason\xad\ning in clinical tasks.51,52 Researchers should generate \nhigh-quality examples and incorporate CoT strategies \ninto the final query to refine specialized reasoning. \n7) Evaluate all stages of the RAG system development. \nMost current studies focus only on final performance, \noverlooking the importance of evaluating each stage of \ndevelopment. It is crucial to formally assess and report \nperformance at the pre-retrieval, retrieval, and post- \nretrieval stages. Evaluating the knowledge boundaries of \nthe baseline LLM, potential conflicts between the LLM \nand external knowledge, and the accuracy and coverage \nof retrieved information helps ensure replicability and \ntransparency. This level of evaluation enables other \nresearchers to understand why a RAG system works (or \ndoes not) and facilitates reproducibility. \n8) Consider cost and time constraints. \nAdvanced retrieval strategies can improve performance \nbut often increase processing time and computational \ncosts. For example, graph-based RAG requires substan\xad\ntial resources for developing knowledge graphs, and \nresponses from global summaries may take longer than \nwith na√Øve RAG methods.50 Another example is to fine- \ntune LLMs to evaluate the needs and the quality of \nretrieval.34 In terms of computational cost, this process \nis expensive, especially when scaling the method to \nlarger datasets or deploying it in a real-time system. \nAlso, a set of extra processes will make the whole speed \nslow. The long response time might have a nonignorable \nnegative impact in situations that need a quick answer, \nespecially common in clinical settings. Researchers \nshould balance performance improvements with time \nand cost considerations. Only 1 study in our review, \nwhich focused on gastrointestinal radiology diagnosis \nbased on imaging descriptions, compared the cost and \nresponse time between LLMs and LLMs with RAG.31\nThe mean response time was 29.8 s for LLM with RAG \nvs 15.7s for LLM alone, with costs of $0.15 and $0.02 \nper case, respectively. Another study used EHR to pre\xad\ndict cognitive decline only reported cost, with LLM: \n$4.49; RAG: $12.51. Another study that used EHR \ndata to predict cognitive decline reported costs of $4.49 \nfor LLM alone and $12.51 for LLM with RAG.53\n9) Establish monitoring and knowledge update strategies. \nAn important concept in AI applications in healthcare, \nalgorithmovigilance, which defined as ‚Äúscientific meth\xad\nods and activities relating to the evaluation, monitoring, \nunderstanding, and prevention of adverse effects of \nalgorithms in health care,‚Äù 54 should also be considered \nin the RAG applications. Researchers need to develop \nlong-term monitoring strategies for the RAG system per\xad\nformance, especially in clinical applications. In addition, \ncurrent studies use fixed external datasets. Researchers \nshould update external knowledge sources as latest \ninformation becomes available. Clear strategies for \nupdating knowledge should be defined, specifying when \nand how updates will occur. \nFor future studies, the first direction could be the system- \nlevel enhancement, the combination of RAG and LLM- \npowered agents. LLM-powered agents are AI systems that \nuse LLMs with complex reasoning and planning capabilities, \nmemory management, interactive capabilities with the envi\xad\nronment, and actions to execute tasks.55,56 Recent research \npoints to the emerging trend of combination of RAG and \nLLM-powered agents, where agents can assist in planning \nand decision making for complex tasks, rather than simple \nretrieval.57 For example, clinicians and patients have diverse \ninformation access needs, some needing to analyze text from \na knowledge base, others needing to incorporate structured \ndata from an EHR. RAG will eventually only become one of \nthe methods for agents to access information. Moreover, \nfuture research could focus on the usage of internal and \nexternal functions and tools, long-term and short-term mem\xad\nory module, self-learning module. For example, a study \ndeveloped an agent to answer questions related to rare dis\xad\neases by expanding beyond RAG with additional tool func\xad\ntions, such as querying phenotypes and performing web \nsearches. This approach improved the overall correctness \nfrom 0.48 to 0.75 compared to the GPT-4 baseline LLM.58\nThe second future direction could focus on the knowledge- \nlevel enhancement: deep integration of external knowledge \ninto LLM. LLM exhibits the knowledge boundaries. RAG \napproaches retrieving external knowledge and then integrates \nit into LLMs in the forms of prompts for the final generation \nto enhance the capabilities of LLMs in perceiving knowledge \nboundaries.59 However, the integration of external knowl\xad\nedge into LLM reasoning is typically limited to providing the \nretrieved data as additional context for the LLM‚Äôs query dur\xad\ning generation. This approach keeps retrieval and generation \nloosely connected, and the LLM‚Äôs output can still be influ\xad\nenced by its inherent knowledge boundaries or by noise in the \nretrieved text, leading to incorrect answers. Additionally, \nwhen the external knowledge source is EHR data, this \nenhancement becomes even more important. Current EHR \ndata is organized in a ‚Äúproblem-oriented medical record‚Äù \n(POMR) format, which collects and displays information in a \nstructured manner.60 LLMs excel in free-form contexts, and \ntheir ability to perform clinical tasks depends on access to \nunstructured text that provides a comprehensive view of the \npatient. Achieving this within the structured POMR format \nin modern EHR systems poses a significant challenge.61\nTherefore, investigating how to realize the deep integration \nof external knowledge with LLM reasoning is an important \ndirection for future research in clinical applications.\nThe final direction is the integration-level enhancement, \nfocusing on integrating RAG systems within EHRs. Current \nresearch has primarily focused on development and testing \noutside of EHR systems. To seamlessly provide support for \nhealthcare providers and patients, future efforts should pri\xad\noritize embedding RAG systems into EHR interfaces. This \nrequires collaboration with EHR vendors to ensure the neces\xad\nsary infrastructure is available. Researchers also can facilitate \nthis integration using data exchange frameworks, such as \nSMART on FHIR.62\n612                                                                                                      \nJournal of the American Medical Informatics Association, 2025, Vol. 32, No. 4 \nDownloaded from https://academic.oup.com/jamia/article/32/4/605/7954485 by guest on 11 January 2026']"
"How does the integration of single-cell RNA sequencing data from the cellxgene Atlas enhance the co-expression networks in STRING version 12.5, and what advantages does this expansion provide for understanding gene expression dynamics across different organisms?","['<1-hop>\n\nNucleic Acids Research , 2025, Vol. 53, Database issue D 737 networks in all domains of life, supporting directed links and tissue-specificity. J. Mol. Biol., 433 , 166835. 13. Kim, C.Y. , Baek, S. , Cha, J. , Yang, S. , Kim, E. , Marcotte, E.M. , Hart, T. and Lee,I. (2022) HumanNet v3: an improved database of human gene networks for disease research. Nucleic Acids Res., 50 , D632‚ÄìD639. 14. Huang, J.K. , Carlin, D.E. , Yu, M.K. , Zhang, W. , Kreisberg, J.F. , Tamayo, P. and Ideker, T. (2018) Systematic evaluation of molecular networks for discovery of disease genes. Cell Syst. , 6 , 484‚Äì495.e5. 15. Wright, S.N. , Colton, S. , Schaffer, L.V. , Pillich, R.T. , Churas, C. , Pratt, D. and Ideker, T. (2024) State of the interactomes: an evaluation of molecular networks for generating biological insights. bioRxiv doi: https:// doi.org/ 10.1101/ 2024.04.26.587073 , 29 April 2024, preprint: not peer reviewed. 16. Ontology Consortium, G. , Aleksander, S.A. , Balhoff, J. , Carbon, S. , Cherry, J.M. , Drabkin, H.J. , Ebert, D. , Feuermann, M. , Gaudet, P. , Harris, N.L. , et al. (2023) The Gene Ontology knowledgebase in 2023. Genetics , 224 , iyad031. 17. Liberzon, A. , Birger, C. , Thorvaldsd√≥ttir, H. , Ghandi, M. , Mesirov, J.P. and Tamayo, P. (2015) The Molecular Signatures Database hallmark gene set collection. Cell Syst. , 1 , 417‚Äì425. 18. Amberger, J.S. , Bocchini, C.A. , Schiettecatte, F. , Scott, A.F. and Hamosh,A. (2015) OMIM.org: Online Mendelian Inheritance in Man (OMIM ¬Æ), an online catalog of human genes and genetic disorders. Nucleic Acids Res. , 43 , D789‚ÄìD798. 19. Snel, B. , Bork, P. and Huynen, M.A. (2002) The identification of functional modules from the genomic association of genes. Proc. Natl Acad. Sci. U.S.A., 99 , 5890‚Äì5895. 20. Guala, D. , Ogris, C. , M√ºller, N. and Sonnhammer, E.L.L. (2020) Genome-wide functional association networks: background, data & state-of-the-art resources. Brief. Bioinform., 21 , 1224. 21. Szklarczyk, D. , Gable, A.L. , Nastou, K.C. , Lyon, D. , Kirsch, R. , Pyysalo, S. , Doncheva, N.T. , Legeay, M. , Fang, T. , Bork, P. , et al. (2021) The STRING database in 2021: customizable protein‚Äìprotein networks, and functional characterization of user-uploaded gene / measurement sets. Nucleic Acids Res., 49 , D605‚ÄìD612. 22. Mering,C.V. (2003) STRING: a database of predicted functional associations between proteins. Nucleic Acids Res. , 31 , 258‚Äì261. 23. Enright, A.J. , Iliopoulos, I. , Kyrpides, N.C. and Ouzounis, C.A. (1999) Protein interaction maps for complete genomes based on gene fusion events. Nature , 402 , 86‚Äì90. 24. Franceschini, A. , Lin, J. , von Mering, C. and Jensen, L.J. (2016) SVD-phy: improved prediction of protein functional associations through singular value decomposition of phylogenetic profiles. Bioinformatics , 32 , 1085‚Äì1087. 25. Koutrouli, M. , Nastou, K. , Piera L√≠ndez, P. , Bouwmeester, R. , Rasmussen, S. , Martens, L. and Jensen, L.J. (2024) FAVA: high-quality functional association networks inferred from scRNA-seq and proteomics data. Bioinformatics , 40 , btae010. 26. Porras, P. , Barrera, E. , Bridge, A. , Del-Toro, N. , Cesareni, G. , Duesbury, M. , Hermjakob, H. , Iannuccelli, M. , Jurisica, I. , Kotlyar, M. , et al. (2020) Towards a unified open access dataset of molecular interactions. Nat. Commun., 11 , 6144. 27. Franceschini, A. , Szklarczyk, D. , Frankild, S. , Kuhn, M. , Simonovic, M. , Roth, A. , Lin, J. , Minguez, P. , Bork, P. , von Mering, C. , et al. (2013) STRING v9.1: protein‚Äìprotein interaction networks, with increased coverage and integration. Nucleic Acids Res., 41 , D808‚ÄìD815. 28. Yu, H. , Luscombe, N.M. , Lu, H.X. , Zhu, X. , Xia, Y. , Han, J .-D.J ., Bertin, N. , Chung, S. , V idal, M. and Gerstein, M. (2004) Annotation transfer between genomes: protein‚Äìprotein interologs and protein‚ÄìDNA regulogs. Genome Res. , 14 , 1107‚Äì1118. 29. Szklarczyk, D. , Gable, A.L. , Lyon, D. , Junge, A. , Wyder, S. , Huerta-Cepas, J. , Simonovic, M. , Doncheva, N.T. , Morris, J.H. , Bork, P. , et al. (2019) STRING v11: protein‚Äìprotein association networks with increased coverage, supporting functional discovery in genome-wide experimental datasets. Nucleic Acids Res., 47 , D607‚ÄìD613. 30. Doncheva, N.T. , Morris, J.H. , Gorodkin, J. and Jensen, L.J. (2019) Cytoscape StringApp: network analysis and visualization of proteomics data. J. Proteome Res., 18 , 623‚Äì632. 31. Jensen, L.J. , Kuhn, M. , Stark, M. , Chaffron, S. , Creevey, C. , Muller, J. , Doerks, T. , Julien, P. , Roth, A. , Simonovic, M. , et al. (2009) STRING 8‚Äîa global view on proteins and their functional interactions in 630 organisms. Nucleic Acids Res. , 37 , D412‚ÄìD416. 32. Nastou, K. , Mehryary, F. , Ohta, T. , Luoma, J. , Pyysalo, S. and Jensen,L.J. (2024) RegulaTome: a corpus of typed, directed, and signed relations between biomedical entities in the scientific literature. Database , 2024 , baae095. 33. Megill, C. , Martin, B. , Weaver, C. , Bell, S. , Prins, L. , Badajoz, S. , McCandless, B. , Pisco, A.O. , Kinsella, M. , Griffin, F. , et al. (2021) cellxgene: a performant, scalable exploration platform for high dimensional sparse matrices. bioRxiv doi: https:// doi.org/ 10.1101/ 2021.04.05.438318 , 6 April 2021, preprint: not peer reviewed. 34. Moreno, P. , Fexova, S. , George, N. , Manning, J.R. , Miao, Z. , Mohammed, S. , Mu√±oz-Pomer, A. , Fullgrabe, A. , Bi, Y. , Bush, N. , et al. (2022) Expression Atlas update: gene and protein expression in multiple species. Nucleic Acids Res. , 50 , D129‚ÄìD140. 35. T immons, J.A. , Szkop, K.J. and Gallagher, I.J. (2015) Multiple sources of bias confound functional enrichment analysis of global -omics data. Genome Biol. , 16 , 186. 36. Benjamini, Y. and Hochberg, Y. (1995) Controlling the false discovery rate: a practical and powerful approach to multiple testing. J. R. Stat. Soc. Ser. B Stat. Methodol., 57 ,'
 '<2-hop>\n\nExpanded co-expression networks The co-expression channel in STRING identifies genes with similar expression profiles across various tissues and condi- tions, revealing their involvement in shared biological pro- cesses. It compiles data from large-scale gene expression stud- ies, analyzing transcript and protein levels to detect gene pairs with coordinated expression. This method highlights poten- tial functional relationships between genes. Importantly, the analysis is free from study bias, as it directly uses raw omics data instead of relying on manually curated or experimental datasets, providing a more precise and objective view of gene interactions. For the upcoming version 12.5, we are expanding the co-expression channel by generating additional networks through the application of FAVA (Functional Association of Variational Autoencoders) ( 25 ) on a wider range of single- cell RNA sequencing data. Specifically, we are incorporating single-cell data from the cellxgene Atlas ( 33 ) to enhance the human and mouse networks and from the EBI Single Cell Ex- pression Atlas to extend coverage to more model organisms ( 34 ). This expansion will provide more detailed, organism- specific co-expression maps, enabling more precise identifi- cation of gene co-expression patterns and cross-species com- parisons, particularly for development, immune response and disease-related gene expression dynamics. Impro v ed enric hment analysis Gene set enrichment analysis is a critical component of high- throughput biological studies, providing key insights into the biological characteristics of datasets. This analysis is pivotal in identifying essential aspects such as biological processes, path- ways and phenotypes, which are crucial for understanding the biological implications of experiments. Typically, overrepre- sentation analysis is employed, wherein a subset of data ex- ceeding a specific cutoff (empirical or customary) is compared against an experimental background. By default, STRING Downloaded from https://academic.oup.com/nar/article/53/D1/D730/7903368 by guest on 11 January 2026']","The integration of single-cell RNA sequencing data from the cellxgene Atlas into STRING version 12.5 enhances the co-expression networks by generating additional networks through the application of FAVA (Functional Association of Variational Autoencoders) on a wider range of single-cell data. This incorporation specifically improves the human and mouse networks and, together with data from the EBI Single Cell Expression Atlas, extends coverage to more model organisms. This expansion provides more detailed, organism-specific co-expression maps, enabling more precise identification of gene co-expression patterns and facilitating cross-species comparisons. These improvements are particularly valuable for studying development, immune response, and disease-related gene expression dynamics, offering a more precise and objective view of gene interactions by directly using raw omics data free from study bias.",multi_hop_specific_query_synthesizer,"The integration of single-cell RNA sequencing (scRNA-seq) data from the cellxgene Atlas into STRING version 12.5 significantly enhances the co-expression networks by providing more detailed and organism-specific gene co-expression maps. This integration is achieved through the application of FAVA (Functional Association of Variational Autoencoders) on a broader range of single-cell RNA sequencing datasets, including those from the cellxgene Atlas for human and mouse, and from the EBI Single Cell Expression Atlas for additional model organisms.

**Enhancement of Co-expression Networks:**

- **Increased Data Diversity and Resolution:** Incorporating scRNA-seq data from the cellxgene Atlas allows STRING to analyze gene expression at the single-cell level, capturing the heterogeneity of gene expression profiles across different cell types, tissues, and conditions with high resolution.
  
- **Organism-Specific Networks:** The use of cellxgene Atlas data enhances the human and mouse co-expression networks specifically, while data from the EBI Single Cell Expression Atlas extends coverage to more model organisms. This results in more precise and tailored co-expression networks for each organism.

- **Improved Functional Associations:** By leveraging scRNA-seq data, STRING can identify gene pairs with coordinated expression patterns more accurately, reflecting functional relationships that are relevant in specific cellular contexts.

**Advantages for Understanding Gene Expression Dynamics:**

- **Detailed and Precise Identification of Co-expression Patterns:** The expanded co-expression networks enable more precise detection of gene co-expression patterns, which is crucial for understanding complex biological processes.

- **Cross-Species Comparisons:** The inclusion of data from multiple organisms facilitates comparative analyses of gene expression dynamics across species, aiding in the identification of conserved and divergent regulatory mechanisms.

- **Focus on Development, Immune Response, and Disease:** The enhanced networks are particularly valuable for studying gene expression dynamics related to development, immune responses, and diseases, where cell-type-specific expression patterns are critical.

- **Reduction of Study Bias:** Since the co-expression analysis uses raw omics data directly (rather than relying on curated or experimental datasets), it provides a more objective and unbiased view of gene interactions.

In summary, the integration of single-cell RNA sequencing data from the cellxgene Atlas into STRING 12.5 enriches the co-expression channel by adding high-resolution, organism-specific expression data. This expansion improves the accuracy and biological relevance of co-expression networks, enabling better understanding of gene expression dynamics within and across different organisms, especially in contexts such as development, immunity, and disease.","['Nucleic Acids Research , 2025, Vol. 53, Database issue \nD 733 \nFigure 1. Illustration of the new ‚Äòregulatory network‚Äô mode in STRING, where the network edges visually indicate the direction, confidence and sources \nof each regulatory interaction. Clicking on an edge within the network brings up a pop-up window with a detailed o v ervie w of the a v ailable e vidence and \nthe annotated type of regulation. Users can further explore the data behind the interactions by accessing the specific evidence viewers linked within the \npop-up, including the text-mining evidence viewer. This viewer presents excerpts from literature pertaining to the inspected interactions, as well as an \nautomatically generated large language model summary of the presented e x cerpts. \nrylation , Catalysis of Small Molecule Conjugation and Cataly- \nsis of Small Molecule Removal . To enhance the interoperabil- \nity of the annotations, these chosen relation types align with \nGene Ontology ( 32 ). \nTo identify and classify a wide array of interaction types, we \nfine-tuned the RoBERT a-large-PM-M3-V oc language model, \na transformer-based model well suited for biological tasks. \nThis model was trained on the RegulaTome dataset for multi- \nlabel extraction of the directed, typed and signed interactions \nmentioned above, and achieved an average F 1 score of 73.5% \n(with a precision of 75.2% and a recall of 71.8%) on a held- \nout test set, although the exact performance varied across dif- \nferent types of relationships. This level of accuracy is substan- \ntial, considering the complexity of the underlying task. Using \nthis model, we processed over 1.2 billion sentence-level pairs \nextracted from all available PubMed abstracts and PMC Open \nAccess full-text documents, and assigned 3.5% of them ( ‚àº43 \nmillion, of which ‚àº18 million in human) with at least one \npositive label indicating directed (72.9%) or signed (33.1%) \nrelationships among the proteins. Furthermore, we made use \nof the existing curated knowledge of regulatory interactions in \nthe database channel, by parsing regulatory information from \nthe SIGNOR, KEGG and Reactome databases. \nTo integrate these interactions into STRING, we followed \na similar score aggregation and benchmarking approach as \nfor physical interactions ( 21 ). Here, we perform benchmark- \ning in five categories for which we can derive gold stan- \ndard datasets of known human regulatory interactions from \nSIGNOR: regulation , upregulation , downregulation , tran- \nscriptional regulation and phosphorylation . The resulting \ncalibration functions are then applied to extracted regula- \ntions of all types to produce the final regulatory confidence \nscores. \nThe new regulatory network has been seamlessly integrated \ninto the existing STRING framework, complementing the \n‚Äòfunctional‚Äô and ‚Äòphysical‚Äô network types. Directional edges \nwithin this network are visually represented by arrows, depict- \ning both bidirectional and unidirectional relationships. The \nnetwork visualization can show varying confidence levels be- \ntween directions (in the ‚Äòconfidence‚Äô view) or the specific di- \nrectionality of the sources (in the ‚Äòevidence‚Äô view). The user \ninterface retains its intuitive design, enabling users to access \nthe type of regulatory event and the evidence for the interac- \ntion by clicking on the edge (Figure 1 ). Additionally, all API \nfunctions have been updated to fully support the new net- \nwork type, which can be accessed by specifying the parameter \nnetw or k_type = regulatory in the API call. \nExpanded co-expression networks \nThe co-expression channel in STRING identifies genes with \nsimilar expression profiles across various tissues and condi- \ntions, revealing their involvement in shared biological pro- \ncesses. It compiles data from large-scale gene expression stud- \nies, analyzing transcript and protein levels to detect gene pairs \nwith coordinated expression. This method highlights poten- \ntial functional relationships between genes. Importantly, the \nanalysis is free from study bias, as it directly uses raw omics \ndata instead of relying on manually curated or experimental \ndatasets, providing a more precise and objective view of gene \ninteractions. \nFor the upcoming version 12.5, we are expanding the \nco-expression channel by generating additional networks \nthrough the application of FAVA (Functional Association of \nVariational Autoencoders) ( 25 ) on a wider range of single- \ncell RNA sequencing data. Specifically, we are incorporating \nsingle-cell data from the cellxgene Atlas ( 33 ) to enhance the \nhuman and mouse networks and from the EBI Single Cell Ex- \npression Atlas to extend coverage to more model organisms \n( 34 ). This expansion will provide more detailed, organism- \nspecific co-expression maps, enabling more precise identifi- \ncation of gene co-expression patterns and cross-species com- \nparisons, particularly for development, immune response and \ndisease-related gene expression dynamics. \nImpro v ed enric hment analysis \nGene set enrichment analysis is a critical component of high- \nthroughput biological studies, providing key insights into the \nbiological characteristics of datasets. This analysis is pivotal in \nidentifying essential aspects such as biological processes, path- \nways and phenotypes, which are crucial for understanding the \nbiological implications of experiments. Typically, overrepre- \nsentation analysis is employed, wherein a subset of data ex- \nceeding a specific cutoff (empirical or customary) is compared \nagainst an experimental background. By default, STRING \nDownloaded from https://academic.oup.com/nar/article/53/D1/D730/7903368 by guest on 11 January 2026'
 'D 734 \nNucleic Acids Research , 2025, Vol. 53, Database issue \nuses the whole genome / proteome background; however, it \nis recommended that users provide a custom background \nlist representing a more realistic universe of genes / proteins \ndetected by their assay ( 35 ). Alternatively, the entire sorted \ndataset can be analyzed using gene set enrichment analysis to \ndetermine whether the distribution of genes at the top, bottom \nor both ends deviates significantly from what would be ex- \npected by chance. STRING supports both of these enrichment \nmethods. The most recent update to STRING introduces vari- \nous improvements, including a new false discovery rate (FDR) \ncorrection, enhanced filtering options and a new graphical vi- \nsualization. \nFDR correction \nIn its analyses, STRING performs a test for each path- \nway (‚Äòterm‚Äô) within a given pathway collection (‚Äòcategory‚Äô). \nLike many other enrichment tools, STRING employs the \nBenjamini‚ÄìHochberg correction ( 36 ) to adjust for multiple \nhypothesis testing. This statistical correction is essential for \nmanaging the FDR effectively. However, the larger the term \ncount and the greater the diversity of the terms, the higher \nthe required correction and the lower the chance of exceed- \ning the alpha level (significance threshold) for a given term. \nOne method used to address this issue involves restricting the \nanalysis to a subset of terms, such as ‚ÄòGO slims‚Äô that focus \non broad, high-level terms with a simplified ontology struc- \nture ( 16 ). However, this approach has some disadvantages: it \nmight exclude terms particularly relevant to a tested gene set, \nit may not cover all functions by design and it is mainly ap- \nplicable to hierarchical classification systems such as the Gene \nOntology. \nSTRING utilizes several hierarchical ontologies, such as \nthe Brenda Tissue Ontology, Disease Ontology, Human Phe- \nnotype Ontology and the aforementioned Gene Ontology; \nthey typically contain many more leaf-ward nodes than nodes \ncloser to the root. The parental nodes must contain all the \ngenes included in the child terms, creating an imbalance in \nthe ontology with many more smaller terms and fewer larger \nterms. Removing smaller terms can substantially increase sta- \ntistical power . However , simply eliminating all small terms \ncould negatively impact discoverability, especially for small \nquery gene sets where the smaller terms might be the most rel- \nevant for biological interpretation. The ontologies are usually \nlarge, with > 10 000 terms (and therefore tests) in the Biolog- \nical Process branch of the Gene Ontology tree. Such strong \ncorrections have the effect that for larger query gene sets, or \nfor backgrounds that cover only part of the proteome, it might \nnot be statistically possible to detect enrichments for a subset \nof the terms; these can therefore be removed from considera- \ntion. \nIn its new version, STRING takes a more flexible \napproach‚Äîby only testing terms that have a priori any statis- \ntical potential for enrichment. This decision is based on sev- \neral parameters: the size of the term, the size of the query set, \nthe background size and the number of tests conducted. By \ncalculating the smallest and largest term sizes that could the- \noretically be enriched given the FDR correction, STRING can \ndetermine which terms are viable for inclusion in the anal- \nysis. Terms outside these bounds are excluded from testing. \nThis methodical exclusion significantly enhances the statisti- \ncal power of the enrichment analysis, without omitting any \nterms that could be enriched. For smaller query sets, this strat- \negy will maintain all terms, as even the smallest tested sets \n(term size = 2) might still be theoretically enriched. How- \never , for larger , less specific sets or for tests against a cus- \ntom smaller background, this approach markedly improves \nthe statistical power, enabling STRING to perform more fo- \ncused analyses without the drawbacks of increased false posi- \ntive rates or omitting potentially biologically interesting terms. \nAlthough this method leverages the characteristics of ontolo- \ngies, it does not require a hierarchy (parent‚Äìchild relationships \nbetween the terms) to function and can be applied to any \npathway / term collection with an imbalance of small terms, \nsuch as PMID-derived gene sets and STRING neighborhood \nclusters. \nTerm filtering \nOne key insight from our online user surveys is that the of- \nten overwhelming number of enrichment results can make it \nchallenging to discern the most relevant enriched terms. We \nhave thus enhanced the display of enrichment results, allow- \ning users to filter and sort them based on (i) FDR, (ii) strength, \n(iii) signal, (iv) term size and (v) term similarity. \nThe newest addition to our filtering options are the ‚Äòterm \nsimilarity‚Äô and ‚Äòsignal‚Äô filters. The ‚Äòterm similarity‚Äô filter uses \nthe Jaccard index to measure the similarity between the gene \nsets of terms within the same category. The method proceeds \nby sorting the terms according to their P -values, prioritizing \nthose with the strongest statistical significance. The algorithm \nthen iterates through the list of terms; any term that exhibits \na similarity exceeding a predetermined, user-defined threshold \nrelative to terms already included in the results is subsequently \nexcluded. This approach requires no additional information \nbeyond what is already available in the dataset and can be \nuniformly applied across all categories of terms. The ‚Äòsignal‚Äô \nfilter is defined as a weighted harmonic mean between the ra- \ntio of observed to expected gene occurrences in an enriched \nterm and its ‚àílog(FDR), respectively. This filter balances the \nimpact of both metrics and provides a more intuitive ordering \nof enriched terms and has been implemented as the default \nsorting criterion. \nThese filters are designed to exclude potentially less relevant \nresults, such as terms that are marginally significant, small or \nshow significant overlap in terms of gene content. This en- \nhancement streamlines the search for relevant observations, \nhelping users focus on the most meaningful results. \nGraphical visualization \nIn response to user feedback requesting more intuitive, graph- \nical visualizations of the detected enrichments, STRING now \nhas adopted an interactive dot plot as a primary display \nmethod for enrichment analysis outcomes. We chose the dot \nplot for its clarity in representing enriched functions across \nthree dimensions: (i) enrichment signal along the X -axis, (ii) \nthe FDR indicated through the color coding of the dots and \n(iii) the term‚Äôs protein count in the network represented by the \nsize of each dot. The terms (listed on the Y -axis) are ranked \nby their enrichment signal or, if grouped by similarity, by the \nenrichment signal of their group first (Figure 2 ). This visual- \nization reflects all filter settings chosen by the user to manage \nthe volume of results displayed. These plots can be addition- \nally customized in the interface and the resulting graphs can be \ndownloaded as PNG for immediate use or as SVG for further \nmodifications. \nDownloaded from https://academic.oup.com/nar/article/53/D1/D730/7903368 by guest on 11 January 2026'
 'D 736 \nNucleic Acids Research , 2025, Vol. 53, Database issue \nsure that clusters are easily identifiable. This functionality ex- \ntends beyond cluster analysis and is applicable to any gene set. \nAs such, it is also available through an API, complementing \nour suite of other API methods. The new API function, named \ngeneset_description , requires only a set of genes as input and \nautomatically performs enrichment analysis. It returns up to \nthree descriptions‚Äîprimary, secondary and tertiary‚Äîbased \non relevance and availability, identical to the cluster naming \nin the user interface. These descriptions are filled sequentially, \nwith the ‚Äòprimary‚Äô always representing the most relevant term. \nOne application of this API is in the stringApp Cytoscape plu- \ngin, which utilizes it to automatically overlay the assigned de- \nscriptions onto each network cluster after clustering is exe- \ncuted. \nNetwork and protein embeddings \nTo facilitate the use of STRING in machine learning applica- \ntions, users can now directly download precomputed ProtT5 \n( 39 ) sequence and cross-species protein network embeddings \nderived from STRING, for all eukaryotes, and utilize the em- \nbeddings on their own labeled datasets. The embeddings en- \ncode information from two aspects. Sequence embeddings can \ncapture protein domains and shorter sequence motifs, while \nnetwork embeddings can complement these with information \non cellular organization such as protein complexes and path- \nways that may not be evident from sequence data alone. \nWhile protein sequence embeddings are inherently compa- \nrable across species due to the universal nature of amino acid \nsequences, the primary obstacle to using network embeddings \nis ensuring that network embeddings from different species \nare directly comparable. This challenge arises because pro- \ntein networks are independent for each species, and tradi- \ntional network embedding methods are not designed to ad- \ndress cross-species comparability. The lack of comparability \nbetween network embeddings prevents the effective transfer \nof knowledge and findings across different species. To address \nthis, we developed a method to align eukaryotic network em- \nbeddings across species using orthologous relationships based \non FedCoder ( 40 ). This alignment technique creates a uni- \nfied embedding space where proteins from different eukary- \notic species can be directly compared. As a result, it enhances \ncross-species protein predictions, particularly in tasks such as \nsubcellular localization and function prediction. \nIn the protein machine learning field, having precomputed \nsequence embeddings and cross-species compatible network \nembeddings from STRING enables researchers to use these re- \nsources directly, eliminating the need to calculate embeddings \nthemselves. This strategy not only reduces the energy footprint \nresulting from redundant calculations across research groups, \nbut also enhances reproducibility in computational biology re- \nsearch promoting more sustainable and consistent scientific \npractices. \nData availability \nThe STRING database is freely available online at https:// \nstring-db.org/. \nA c kno wledg ements \nThe authors wish to thank Yan P. Yuan (EMBL Heidelberg) \nand Dandan Xue (University of Copenhagen) for IT support. \nThomas Rattei (University of Vienna) is thanked for extensive \ncomputational work toward resolving orthology relations. We \nthank the CSC‚ÄîIT Center for Science, Finland, for generous \ncomputational resources. \nFunding \nSwiss Institute of Bioinformatics; Novo Nordisk Founda- \ntion [NNF14CC0001, NNF20SA0035590]; European Molec- \nular Biology Laboratory (EMBL Heidelberg); HORIZON \nEUROPE Marie Sk≈Çodowska-Curie Actions [101023676 to \nK.N.]; Academy of Finland [332844 to F.M. and S.P.]. Fund- \ning for open access charge: University of Zurich. \nConflict of interest statement \nNone declared. \nReferences \n1. Marsh, J.A. and Teichmann, S.A. (2015) Structure, dynamics, \nassembly, and evolution of protein complexes. Annu. Rev. \nBiochem., 84 , 551‚Äì575.\n2. Cesnik, A. , Schaffer, L.V. , Gaur, I. , Jain, M. , Ideker, T. and \nLundberg,E. (2024) Mapping the multiscale proteomic \norganization of cellular and disease phenotypes. Annu. Rev. \nBiomed. Data Sci., 7 , 369‚Äì389.\n3. Milacic, M. , Beavers, D. , Conley, P. , Gong, C. , Gillespie, M. , Griss, J. , \nHaw, R. , Jassal, B. , Matthews, L. , May, B. , et al. (2024) The \nReactome Pathway Knowledgebase 2024. Nucleic Acids Res., 52 , \nD672‚ÄìD678.\n4. Kanehisa, M. , Furumichi, M. , Sato, Y. , Kawashima, M. and \nIshiguro-Watanabe,M. (2023) KEGG for taxonomy-based analysis \nof pathways and genomes. Nucleic Acids Res. , 51 , D587‚ÄìD592. \n5. Oughtred, R. , Stark, C. , Breitkreutz, B.-J. , Rust, J. , Boucher, L. , \nChang, C. , Kolas, N. , O‚ÄôDonnell, L. , Leung, G. , McAdam, R. , et al. \n(2019) The BioGRID interaction database: 2019 update. Nucleic \nAcids Res., 47 , D529‚ÄìD541.\n6. Del Toro, N. , Shrivastava, A. , Ragueneau, E. , Meldal, B. , Combe, C. , \nBarrera, E. , Perfetto, L. , How, K. , Ratan, P. , Shirodkar, G. , et al. \n(2022) The IntAct database: efficient access to fine-grained \nmolecular interaction data. Nucleic Acids Res. , 50 , D648‚ÄìD653. \n7. Licata, L. , Briganti, L. , Peluso, D. , Perfetto, L. , Iannuccelli, M. , \nGaleota, E. , Sacco, F. , Palma, A. , Nardozza, A.P. , Santonico, E. , et al. \n(2012) MINT, the molecular interaction database: 2012 update. \nNucleic Acids Res., 40 , D857‚ÄìD861.\n8. Meldal, B.H.M. , Perfetto, L. , Combe, C. , Lubiana, T. , Ferreira \nCavalcante, J.V. , Bye-A-Jee, H. , Waagmeester, A. , Del-Toro, N. , \nShrivastava, A. , Barrera, E. , et al. (2022) Complex Portal 2022: \nnew curation frontiers. Nucleic Acids Res. , 50 , D578‚ÄìD586. \n9. Lo Surdo, P. , Iannuccelli, M. , Contino, S. , Castagnoli, L. , Licata, L. , \nCesareni, G. and Perfetto, L. (2023) SIGNOR 3.0, the SIGnaling \nnetwork open resource 3.0: 2022 update. Nucleic Acids Res., 51 , \nD631‚ÄìD637.\n10. Szklarczyk, D. , Kirsch, R. , Koutrouli, M. , Nastou, K. , Mehryary, F. , \nHachilif, R. , Gable, A.L. , Fang, T. , Doncheva, N.T. , Pyysalo, S. , et al. \n(2023) The STRING database in 2023: protein‚Äìprotein \nassociation networks and functional enrichment analyses for any \nsequenced genome of interest. Nucleic Acids Res. , 51 , D638‚ÄìD646. \n11. Warde-Farley, D. , Donaldson, S.L. , Comes, O. , Zuberi, K. , \nBadrawi, R. , Chao, P. , Franz, M. , Grouios, C. , Kazi, F. , Lopes, C.T. , \net al. (2010) The GeneMANIA prediction server: biological \nnetwork integration for gene prioritization and predicting gene \nfunction. Nucleic Acids Res. , 38 , W214‚ÄìW220. \n12. Persson, E. , Castresana-Aguirre, M. , Buzzao, D. , Guala, D. and \nSonnhammer,E.L.L. (2021) FunCoup 5: functional association \nDownloaded from https://academic.oup.com/nar/article/53/D1/D730/7903368 by guest on 11 January 2026'
 'D 732 \nNucleic Acids Research , 2025, Vol. 53, Database issue \ncific network modes are fully consistent with the functional \nnetwork, such that if an interaction is present in either the \nphysical or regulatory network, it will, by definition, also be \npresent in the full functional association network (with an \nequal or higher confidence score). To compute the various con- \nfidence scores, the available evidence supporting a given inter- \naction is first separated into ‚Äòevidence channels‚Äô, by the type \nof evidence. For the basic functional association confidence \nscore, seven such channels are used. These include three that \nare based on genomic context predictions‚Äîneighborhood, fu- \nsion and gene co-occurrence‚Äîas well as channels dedicated to \nco-expression, experimental data, curated databases and text \nmining. \nThe genomic context channels focus on associations in- \nferred from genome sequences alone, detecting evolution- \nary patterns such as gene proximity, fusion events and co- \noccurrence across different organisms. For instance, the neigh- \nborhood channel assigns an association score to gene pairs \nthat are located close to each other on the chromosome in \nthe same orientation (in prokaryotic genomes) ( 22 ). The fu- \nsion channel identifies open reading frames that result from \ngene fusion events ( 23 ), while the co-occurrence channel ex- \namines whether genes have a nontrivial but shared distribu- \ntion across genomes ( 24 ), implying a shared history of hori- \nzontal transfers, losses or duplication events and thus likely \nrelated functions. The co-expression channel compiles data \nfrom gene expression studies, analyzing both transcript and \nprotein abundances across various conditions. By comparing \nexpression profiles, it identifies gene pairs with similar expres- \nsion patterns, suggesting functional linkage ( 10 ,25 ). The ex- \nperiments channel aggregates interaction evidence from labo- \nratory assays, including biochemical, biophysical and genetic \nassays. Data are imported from primary repositories such as \nBioGRID ( 5 ) and the IMEx consortium ( 26 ), and the confi- \ndence scores are estimated by globally benchmarking the accu- \nracy of annotated experimental techniques, as well as within- \ndataset performance and consistency for the case of high- \nthroughput experiments ( 10 ). The database channel is based \non well-described, curated protein‚Äìprotein associations from \nexpert-compiled resources, such as KEGG ( 4 ), Reactome ( 3 ) \nand Gene Ontology Complexes ( 16 ). These resources provide \nwell-established pathways, complexes and functional relation- \nships, offering a high level of reliability. Unlike other chan- \nnels, the database channel assigns a uniformly high confidence \nscore to associations, reflecting their established nature. Fi- \nnally, the text-mining channel utilizes a large corpus of sci- \nentific literature, including PubMed abstracts and full-text ar- \nticles, to identify co-mentions of protein names ( 27 ). By statis- \ntically analyzing the frequency of these co-mentions in various \ntextual contexts, this channel uncovers potential associations \nthat may not be evident from structured data alone. In addi- \ntion, for the more specific association modes, dedicated large \nlanguage models are employed to detect sentences supporting \neither physical ( 21 ) or regulatory associations (see below). \nFor each evidence channel, the available interaction evi- \ndence is translated into a confidence score by first quantifying \nthe evidence using channel-specific metrics and then convert- \ning these metrics into likelihoods using calibration curves de- \nrived from prior knowledge (from pathway-map memberships \nin KEGG). For the more specific association modes ‚Äòphysical‚Äô \nand ‚Äòregulatory‚Äô, only channels and evidence that are appli- \ncable to these modes are considered. After this, all channel \nscores that have been computed for a given protein pair in \na given organism are transferred onto related protein pairs \nin other organisms, based on the ‚Äòinterolog‚Äô concept ( 27 ,28 ). \nLastly, a final, combined confidence score is computed by inte- \ngrating the channel-specific subscores probabilistically, under \nthe assumption that evidence in different channels is largely \nindependent. Users of STRING can directly rely on this com- \nbined score for browsing and filtering networks, or they can \nalternatively customize their analyses by enabling or disabling \nspecific channels separately, after which the combined score is \nrecomputed according to their chosen settings. \nAll primary evidence underlying a given interaction can \nbe inspected interactively, in dedicated evidence viewers on- \nline. Furthermore, accessory information is available for \neach protein, such as its three-dimensional structure, do- \nmain composition, annotation and cross-references. Apart \nfrom its core protein-network functionality, STRING also \nimplements features that allow extensive analysis of up- \nloaded user data. These include functional enrichment de- \ntection ( 29 ), experimental bias detection ( 21 ), homology \nsearches, and clustering and organizing large query pro- \ntein sets. All data contained in STRING are freely avail- \nable for download, under a Creative Commons BY 4.0 li- \ncense. Apart from its website, STRING can also be accessed \nvia a dedicated Cytoscape plugin ( 30 ), through an applica- \ntion programming interface (API) ( 31 ), as well as through \nan R / Bioconductor package ( https://www.bioconductor.org/ \npackages/ release/ bioc/ html/ STRINGdb.html ). \nRegulat ory netw orks \nMolecular networks, like those available through STRING, \nhave become invaluable tools in biomedical research, offering \npowerful insights into how molecules work together within \ncells. The functional association networks, while useful for \nmany applications, do not specify the interaction type (e.g. \ncomplex formation or transcriptional regulation) nor the di- \nrection of interaction (i.e. who regulates whom). Constructing \nnetworks with more detailed interaction types and directions \ncan significantly expand their utility, particularly for building \nmathematical models of biological systems or for making log- \nical inferences. The directionality of interactions is essential \nfor interpretation of omics data, which typically reveals more \nabout downstream effects of conditions such as diseases than \nit does about the upstream events that led to it and thus could \nbe targeted therapeutically. \nTo incorporate interactions with specific directionality, sign \nand type into STRING, we leveraged the novel dataset and \ndeep learning-based relation extraction method described in \nRegulaTome ( 32 ). The RegulaTome corpus provides a rich \nand diverse dataset that includes 16 961 relations between \n54 951 entities annotated across over 2500 documents. This \ncorpus is utilized to train a deep learning-based method used \nto annotate and extract the regulatory events throughout the \nentire available literature. \nOn top of undirected physical interactions‚Äîwhich are al- \nready included in STRING‚Äîthe following types of directed \ninteractions can be extracted using the system developed for \nRegulaTome: Regulation , with the signed subtypes of Posi- \ntive Regulation and Negative Regulation , Regulation of Gene \nExpression , Regulation of Degradation and Catalysis of Post- \nTranslational Modifications , covering six subtypes: Catalysis \nof Small Protein Conjugation , Catalysis of Small Protein Re- \nmoval , Catalysis of Phosphorylation , Catalysis of Dephospho- \nDownloaded from https://academic.oup.com/nar/article/53/D1/D730/7903368 by guest on 11 January 2026'
 'Nucleic Acids Research , 2025, Vol. 53, Database issue \nD 735 \nFigure 2. Enrichment analysis interface from the STRING database website. Lef t: Sc hematic of the enrichment analysis tab highlighting various sections \nof the webpage. Top right: A zoomed-in view of the analysis table with two user-highlighted terms (colored lines). Bottom right: A visualization of the \nenrichment depicting the two terms highlighted in corresponding colors. \nFurthermore, a distinctive feature of STRING‚Äôs visualiza- \ntion is the incorporation of similarity groups, which visually \ngroup related terms on the plot, adding an additional dimen- \nsion to the enrichment results. These groups utilize computed \nsimilarities to cluster terms together, highlighting relationships \nbetween the terms and aiding the analysis. The clustering em- \nploys the average linkage method based on the Jaccard index, \nwith a user-specified cutoff applied to form groups that, on \naverage, share the specified similarity. The groups are sorted \nby the maximum signal of their terms, with each term within a \ngroup subsequently sorted by its individual signal. This group- \ning is visually highlighted on the right side of the plot. \nS TRING clust ering and g ene set descriptions \nSTRING offers network clustering options for user-submitted \ngene lists. This feature visually connects nodes that are more \ninterconnected to each other than to other nodes, reflecting \nthe functional modularity of the user‚Äôs gene list. By grouping \nproteins that closely interact, clustering aids in the discovery \nof functional modules and facilitates hypothesis generation. \nClustering is based on the connectivity between nodes, incor- \nporating edge weights (combined scores) reflecting the confi- \ndence level of the interactions. Only visible edges‚Äîthose cor- \nresponding to active channels with confidence higher than the \nuser-specified cutoff‚Äîare considered, ensuring that clustering \nis consistent with the network being shown. \nUsers have the choice of two different clustering algorithms. \n‚Äò K -means clustering‚Äô is implemented via the k -medoids algo- \nrithm [pyclustering package ( 37 )], allowing users to specify \nthe desired number of clusters ( k ). This method forms clusters \nbased on a distance matrix derived from the most probable \npaths between nodes. In case of multiple disconnected compo- \nnents within the network, STRING aims for a balanced distri- \nbution of cluster sizes by iteratively dividing the largest com- \nponents first. In contrast, ‚ÄòMCL clustering‚Äô, implemented with \nstandalone binaries (release 14-137), automatically generates \nnatural divisions based on the network‚Äôs inherent structure. It \nutilizes the Markov cluster algorithm ( 38 ) to simulate stochas- \ntic flow in graphs, identifying groups of highly interconnected \nnodes as clusters based on their combined STRING scores. \nThe user-controlled inflation parameter influences the granu- \nlarity of the clustering. This method is particularly effective \nin capturing the true modular nature of biological networks, \nallowing for intuitive groupings that correspond to biological \nfunctions and relationships. \nResearchers often analyze the biological context of the ob- \ntained clusters to interpret and draw conclusions from net- \nwork data. To aid this process, we have developed a novel \ngene set naming feature that significantly enhances the inter- \npretability of cluster analyses. This feature automatically as- \nsigns the best description for each cluster based on STRING‚Äôs \nrobust statistical enrichment analysis, simplifying the often \ncumbersome task of manually interpreting cluster data. It \nranks the enriched terms by their enrichment signal (see \nthe ‚ÄòImproved enrichment analysis‚Äô section), choosing the pri- \nmary, secondary and tertiary names among various enrich- \nment categories. This prioritization ensures that the names re- \nflect the most statistically significant and biologically pertinent \nattributes of each cluster, providing a clear, immediate under- \nstanding of its functional characteristics. The enrichment pri- \nmarily draws from categories such as Gene Ontology Biolog- \nical Processes, while incorporating a range of other categories \nfor broader annotations. In case the enrichment analysis yields \nno significant results or if the cluster contains only one gene, \nSTRING assigns canonical gene names as descriptions to en- \nDownloaded from https://academic.oup.com/nar/article/53/D1/D730/7903368 by guest on 11 January 2026'
 'Databases and ontologies\nBioThings Explorer: a query engine for a federated\nknowledge graph of biomedical APIs\nJackson Callaghan\n1,‚Ä†, Colleen H. Xu\n1,‚Ä†, Jiwen Xin1,‚Ä†, Marco Alvarado Cano1,\nAnders Riutta\n2, Eric Zhou1, Rohan Juneja1, Yao Yao1, Madhumita Narayan1,\nKristina Hanspers2, Ayushi Agrawal\n2, Alexander R. Pico2, Chunlei Wu\n1,*, Andrew I. Su\n1,*\n1Department of Integrative Structural and Computational Biology, The Scripps Research Institute, La Jolla, CA 92037, United States\n2Data Science and Biotechnology, Gladstone Institutes, University of California, San Francisco, CA 94158, United States\n*Corresponding authors. Department of Integrative Structural and Computational Biology, The Scripps Research Institute, La Jolla, CA 92037, United States.\nE-mails: cwu@scripps.edu (C.W.) and asu@scripps.edu (A.I.S.)\n‚Ä†These authors contributed equally to this work.\nAssociate Editor: Peter N. Robinson\nAbstract\nSummary: Knowledge graphs are an increasingly common data structure for representing biomedical information. These knowledge graphs can\neasily represent heterogeneous types of information, and many algorithms and tools exist for querying and analyzing graphs. Biomedical knowl-\nedge graphs have been used in a variety of applications, including drug repurposing, identiÔ¨Åcation of drug targets, prediction of drug side effects,\nand clinical decision support. Typically, knowledge graphs are constructed by centralization and integration of data from multiple disparate sour-\nces. Here, we describe BioThings Explorer, an application that can query a virtual, federated knowledge graph derived from the aggregated infor-\nmation in a network of biomedical web services. BioThings Explorer leverages semantically precise annotations of the inputs and outputs for\neach resource, and automates the chaining of web service calls to execute multi-step graph queries. Because there is no large, centralized\nknowledge graph to maintain, BioThings Explorer is distributed as a lightweight application that dynamically retrieves information at query time.\nAvailability and implementation: More information can be found at https://explorer.biothings.io and code is available at https://github.com/bio\nthings/biothings_explorer.\n1 Introduction\nWhile downloadable files are the most common way to share\nbiomedical data, application programming interfaces (APIs)\nare another popular and powerful mechanism for data dis-\nsemination. Accessing data through APIs has many comple-\nmentary advantages relative to downloading local copies of\ndata. APIs typically allow users to query for specific subsets\nof the data that are of interest. API queries are often highly\nindexed, leading to efficient data retrieval. Finally, API access\nallows for easy incorporation of the most up-to-date data into\nother computational applications or workflows.\nWhile APIs offer many advantages in terms of data acces-\nsibility, these advantages do not immediately translate into\nefficient data integration. APIs generally follow some com-\nmon architectures and protocols [e.g. representational state\ntransfer (REST), output in JavaScript Object Notation\n(JSON)], but alignment at this technical level does not guar-\nantee either syntactic or semantic interoperability. For ex-\nample, APIs can use different identifiers for the same gene,\ndifferent data structures to represent gene attributes, and\ndifferent terms to describe the relationships between bio-\nmedical entities.\nThere have been some efforts to define and enforce seman-\ntic and syntactic standards to achieve data interoperability.\nExamples of this approach include the Beacon API from the\nGA4GH consortium (Rambla et al. 2022) and the DAS speci-\nfication for sharing annotations of genomic features (Dowell\net al. 2001). These efforts rely on the active participation of\nAPI developers in adopting a community standard for their\nAPI endpoints.\nHere, we explore the use of semantically precise API anno-\ntations as a complementary approach to achieving API inter-\noperability. We divided this work into two components. First,\nwe created an extension of the widely used OpenAPI annota-\ntion standard (http://spec.openapis.org/oas/v3.1.0) to seman-\ntically describe the APIs‚Äô inputs and outputs, and a registry to\norganize these API annotations. Second, we created an appli-\ncation called BioThings Explorer to consume the API meta-\ndata and to execute multi-hop graph queries that span\nmultiple APIs. Together, this architecture allows users to\nquery a large, federated knowledge graph based on an inter-\nconnected network of biomedical APIs. This federated design\noffers a unique approach for creating knowledge graphs that\nis complementary to the more common strategy of centraliza-\ntion and local data integration.\nReceived: 18 April 2023; Revised: 18 August 2023; Editorial Decision: 11 September 2023; Accepted: 12 September 2023\nV\nC The Author(s) 2023. Published by Oxford University Press.\nThis is an Open Access article distributed under the terms of the Creative Commons Attribution License (https://creativecommons.org/licenses/by/4.0/), which\npermits unrestricted reuse, distribution, and reproduction in any medium, provided the original work is properly cited.\nBioinformatics, 2023, 39(9), btad570\nhttps://doi.org/10.1093/bioinformatics/btad570\nAdvance Access Publication Date: 14 September 2023\nApplications Note\nDownloaded from https://academic.oup.com/bioinformatics/article/39/9/btad570/7273783 by guest on 11 January 2026'
 'Nucleic Acids Research , 2025, Vol. 53, Database issue \nD 737 \nnetworks in all domains of life, supporting directed links and \ntissue-specificity. J. Mol. Biol., 433 , 166835.\n13. Kim, C.Y. , Baek, S. , Cha, J. , Yang, S. , Kim, E. , Marcotte, E.M. , Hart, T. \nand Lee,I. (2022) HumanNet v3: an improved database of human \ngene networks for disease research. Nucleic Acids Res., 50 , \nD632‚ÄìD639.\n14. Huang, J.K. , Carlin, D.E. , Yu, M.K. , Zhang, W. , Kreisberg, J.F. , \nTamayo, P. and Ideker, T. (2018) Systematic evaluation of molecular \nnetworks for discovery of disease genes. Cell Syst. , 6 , 484‚Äì495.e5. \n15. Wright, S.N. , Colton, S. , Schaffer, L.V. , Pillich, R.T. , Churas, C. , \nPratt, D. and Ideker, T. (2024) State of the interactomes: an \nevaluation of molecular networks for generating biological \ninsights. bioRxiv doi: \nhttps:// doi.org/ 10.1101/ 2024.04.26.587073 , 29 April 2024, \npreprint: not peer reviewed.\n16. Ontology Consortium, G. , Aleksander, S.A. , Balhoff, J. , Carbon, S. , \nCherry, J.M. , Drabkin, H.J. , Ebert, D. , Feuermann, M. , Gaudet, P. , \nHarris, N.L. , et al. (2023) The Gene Ontology knowledgebase in \n2023. Genetics , 224 , iyad031.\n17. Liberzon, A. , Birger, C. , Thorvaldsd√≥ttir, H. , Ghandi, M. , \nMesirov, J.P. and Tamayo, P. (2015) The Molecular Signatures \nDatabase hallmark gene set collection. Cell Syst. , 1 , 417‚Äì425. \n18. Amberger, J.S. , Bocchini, C.A. , Schiettecatte, F. , Scott, A.F. and \nHamosh,A. (2015) OMIM.org: Online Mendelian Inheritance in \nMan (OMIM ¬Æ), an online catalog of human genes and genetic \ndisorders. Nucleic Acids Res. , 43 , D789‚ÄìD798. \n19. Snel, B. , Bork, P. and Huynen, M.A. (2002) The identification of \nfunctional modules from the genomic association of genes. Proc. \nNatl Acad. Sci. U.S.A., 99 , 5890‚Äì5895.\n20. Guala, D. , Ogris, C. , M√ºller, N. and Sonnhammer, E.L.L. (2020) \nGenome-wide functional association networks: background, data \n& state-of-the-art resources. Brief. Bioinform., 21 , 1224.\n21. Szklarczyk, D. , Gable, A.L. , Nastou, K.C. , Lyon, D. , Kirsch, R. , \nPyysalo, S. , Doncheva, N.T. , Legeay, M. , Fang, T. , Bork, P. , et al. \n(2021) The STRING database in 2021: customizable \nprotein‚Äìprotein networks, and functional characterization of \nuser-uploaded gene / measurement sets. Nucleic Acids Res., 49 , \nD605‚ÄìD612.\n22. Mering,C.V. (2003) STRING: a database of predicted functional \nassociations between proteins. Nucleic Acids Res. , 31 , 258‚Äì261. \n23. Enright, A.J. , Iliopoulos, I. , Kyrpides, N.C. and Ouzounis, C.A. \n(1999) Protein interaction maps for complete genomes based on \ngene fusion events. Nature , 402 , 86‚Äì90.\n24. Franceschini, A. , Lin, J. , von Mering, C. and Jensen, L.J. (2016) \nSVD-phy: improved prediction of protein functional associations \nthrough singular value decomposition of phylogenetic profiles. \nBioinformatics , 32 , 1085‚Äì1087.\n25. Koutrouli, M. , Nastou, K. , Piera L√≠ndez, P. , Bouwmeester, R. , \nRasmussen, S. , Martens, L. and Jensen, L.J. (2024) FAVA: \nhigh-quality functional association networks inferred from \nscRNA-seq and proteomics data. Bioinformatics , 40 , btae010.\n26. Porras, P. , Barrera, E. , Bridge, A. , Del-Toro, N. , Cesareni, G. , \nDuesbury, M. , Hermjakob, H. , Iannuccelli, M. , Jurisica, I. , \nKotlyar, M. , et al. (2020) Towards a unified open access dataset of \nmolecular interactions. Nat. Commun., 11 , 6144.\n27. Franceschini, A. , Szklarczyk, D. , Frankild, S. , Kuhn, M. , \nSimonovic, M. , Roth, A. , Lin, J. , Minguez, P. , Bork, P. , von Mering, C. , \net al. (2013) STRING v9.1: protein‚Äìprotein interaction networks, \nwith increased coverage and integration. Nucleic Acids Res., 41 , \nD808‚ÄìD815.\n28. Yu, H. , Luscombe, N.M. , Lu, H.X. , Zhu, X. , Xia, Y. , Han, J .-D.J ., \nBertin, N. , Chung, S. , V idal, M. and Gerstein, M. (2004) Annotation \ntransfer between genomes: protein‚Äìprotein interologs and \nprotein‚ÄìDNA regulogs. Genome Res. , 14 , 1107‚Äì1118. \n29. Szklarczyk, D. , Gable, A.L. , Lyon, D. , Junge, A. , Wyder, S. , \nHuerta-Cepas, J. , Simonovic, M. , Doncheva, N.T. , Morris, J.H. , \nBork, P. , et al. (2019) STRING v11: protein‚Äìprotein association \nnetworks with increased coverage, supporting functional discovery \nin genome-wide experimental datasets. Nucleic Acids Res., 47 , \nD607‚ÄìD613.\n30. Doncheva, N.T. , Morris, J.H. , Gorodkin, J. and Jensen, L.J. (2019) \nCytoscape StringApp: network analysis and visualization of \nproteomics data. J. Proteome Res., 18 , 623‚Äì632.\n31. Jensen, L.J. , Kuhn, M. , Stark, M. , Chaffron, S. , Creevey, C. , Muller, J. , \nDoerks, T. , Julien, P. , Roth, A. , Simonovic, M. , et al. (2009) STRING \n8‚Äîa global view on proteins and their functional interactions in \n630 organisms. Nucleic Acids Res. , 37 , D412‚ÄìD416. \n32. Nastou, K. , Mehryary, F. , Ohta, T. , Luoma, J. , Pyysalo, S. and \nJensen,L.J. (2024) RegulaTome: a corpus of typed, directed, and \nsigned relations between biomedical entities in the scientific \nliterature. Database , 2024 , baae095.\n33. Megill, C. , Martin, B. , Weaver, C. , Bell, S. , Prins, L. , Badajoz, S. , \nMcCandless, B. , Pisco, A.O. , Kinsella, M. , Griffin, F. , et al. (2021) \ncellxgene: a performant, scalable exploration platform for high \ndimensional sparse matrices. bioRxiv doi: \nhttps:// doi.org/ 10.1101/ 2021.04.05.438318 , 6 April 2021, \npreprint: not peer reviewed.\n34. Moreno, P. , Fexova, S. , George, N. , Manning, J.R. , Miao, Z. , \nMohammed, S. , Mu√±oz-Pomer, A. , Fullgrabe, A. , Bi, Y. , Bush, N. , \net al. (2022) Expression Atlas update: gene and protein expression \nin multiple species. Nucleic Acids Res. , 50 , D129‚ÄìD140. \n35. T immons, J.A. , Szkop, K.J. and Gallagher, I.J. (2015) Multiple \nsources of bias confound functional enrichment analysis of global \n-omics data. Genome Biol. , 16 , 186. \n36. Benjamini, Y. and Hochberg, Y. (1995) Controlling the false \ndiscovery rate: a practical and powerful approach to multiple \ntesting. J. R. Stat. Soc. Ser. B Stat. Methodol., 57 , 289‚Äì300.\n37. Novikov,A. (2019) PyClustering: data mining library. J. Open \nSource Softw., 4 , 1230.\n38. Van Dongen,S. (2008) Graph clustering via a discrete uncoupling \nprocess. SIAM J. Matrix Anal. Appl., 30 , 121‚Äì141.\n39. Elnaggar, A. , Heinzinger, M. , Dallago, C. , Rehawi, G. , Wang, Y. , \nJones, L. , Gibbs, T. , Feher, T. , Angerer, C. , Steinegger, M. , et al. \n(2022) ProtTrans: toward understanding the language of life \nthrough self-supervised learning. IEEE Trans. Pattern Anal. Mach. \nIntell., 44 , 7112‚Äì7127.\n40. Baumgartner, M. , Dell‚ÄôAglio, D. , Paulheim, H. and Bernstein, A. \n(2023) Towards the Web of Embeddings: integrating multiple \nknowledge graph embedding spaces with FedCoder. J. Web \nSemant., 75 , 100741.\nReceived: September 15, 2024. Revised: October 18, 2024. Editorial Decision: October 21, 2024. Accepted: October 29, 2024 \n¬© The Author(s) 2024. Published by Oxford University Press on behalf of Nucleic Acids Research. \nThis is an Open Access article distributed under the terms of the Creative Commons Attribution License (https: // creativecommons.org / licenses / by / 4.0 / ), which permits unrestricted reuse, \ndistribution, and reproduction in any medium, provided the original work is properly cited. \nDownloaded from https://academic.oup.com/nar/article/53/D1/D730/7903368 by guest on 11 January 2026'
 'Nucleic Acids Research, 2016, Vol. 44, Database issue D381\nlows programmatic access, including the ability to disam-\nbiguate queries, modify all network parameters and gen-\nerate images. In order to enable large-scale analysis, which\nmay not be feasible through web-interface or API, the pre-\ncomputed network and the supplementary information are\nfreely available for download.\nSOURCES OF INTERACTIONS\nAlthough there is a plethora of data available from which\nprotein‚Äìchemical networks could be derived, their dis-\npersed nature, different precision, name-space and focus\nmake it cumbersome to assemble a full picture of all avail-\nable knowledge. The STITCH pipeline aggregates high-\nthroughput experiments data, manually curated datasets\nand the results of several prediction methods into a single\nglobal network of protein‚Äìprotein and protein‚Äìchemical in-\nteractions. This does not expose the user to the heterogene-\nity of the underlying data, yet, at the same time, keeps all\nthe primary evidence of the interaction readily accessible.\nA large part of the known interactions comes from man-\nually curated datasets such as DrugBank (19), GPCR-\nligand database (GLIDA) (20), Matador (21), the Thera-\npeutic Targets Database (TTD) (22) and the Comparative\nToxicogenomics Database (CTD) (23), and several path-\nway databases including the Kyoto Encyclopedia of Genes\nand Genomes (KEGG) (12), NCI/Nature Pathway Inter-\naction Database (24), Reactome (25) and BioCyc (26). As\nthere can be overlap between different manually curated\ndatasets, we do not consider multiple reports of identi-\ncal interactions as being independent from each other. In-\nstead, we count redundant interactions only once and do\nnot increase the confidence level. Other large sources of\nprotein‚Äìchemical links are the datasets of experimentally\nvalidated interactions, which include ChEMBL (27), PDSP\nKi Database (28), Protein Data Bank (PDB) (29) and two\nhigh-throughput kinase‚Äìligand interactions studies (30,31).\nAlso in this case, interactions may be reported in different\ndatabases and with different binding affinities. To compute\nthe final confidence score, we only take the strongest re-\nported affinity into account.\nThe sources of verified protein‚Äìchemical interactions are\ncomplemented by automated text mining and a structure-\nbased prediction method (18). The text-mining pipeline in-\nclude co-occurrence text-mining and natural language pro-\ncessing of all MEDLINE abstracts as well as available\nPubMed Central open-access full-text articles (32). The\nnewest addition to the text-mining sources are NIH Re-\nPORTER grant abstracts (https://projectreporter.nih.gov/).\nConsidering co-occurring terms, adding the RePORTER\ndata increased the number of high-confidence interactions\nbetween human proteins and chemicals from 2740 to 4740.\nExtensive benchmarking of each data source allows us to\nprovide unified confidence score for every interaction while\ntaking into account the sources‚Äô predicted precision.\nDISPLAY OF BINDING AFFINITIES IN THE NETWORK\nVIEW\nSmall molecules that activate or inhibit proteins such as en-\nzymes or receptors are among the most studied classes of\nexogenous small molecules. In order to assess the effect and\nconfidence of protein‚Äìligand binding, as well as variabil-\nity in the affinity of known ligands, it is essential to know\nthe binding affinity between the compound and its target.\nUsually, this binding affinity is quantified as the inhibition\nconstant Ki. In some cases, Ki values are not available, but\nother values such as the IC50 or EC50 (half of the maxi-\nmal inhibitory concentration) can serve as an approxima-\ntion. Ki values of drugs vary greatly, from nanomolar in-\nhibition constants to relatively high values, such as 52 \x02M\nbetween aspirin and cyclooxygenase 2 (27). Therefore, for\nany given drug, it is not so much the absolute value of the\nKi, but rather the relative binding affinities that determine\nthe impact on the interaction network.\nIn previous versions of STITCH, Ki values from primary\nsources (27,28) were accessible to the user through the web-\ninterface. In the new release of STITCH, the user can now\nchoose to switch the network view to show the binding\naffinities of all protein‚Äìchemical interactions for which this\nvalue is known (Figure 1). This new network view is similar\nto the STITCH‚Äôs confidence view: the thickness of the edge\nbetween nodes scales with the Ki value. If a Ki is not avail-\nable, EC50 or IC50 will be used to determine the depicted\nstrength of the interaction. If there are multiple measure-\nments available, the lowest value (i.e. highest reported affin-\nity) will be used to determine the thickness of the edge.\nDATA AND FILTERING FOR TISSUE SPECIFICITY\nThe protein‚Äìchemical network in STITCH is global and\nas such considers interactions anywhere in an organism.\nHowever, in multicellular organisms such as humans, not\nall proteins are present in every tissue. STITCH 5 addresses\nthis through a new feature that allows users to filter a\nhuman interaction network so that only the proteins be-\nlieved to be present in a specified tissue are shown (Fig-\nure 2). To provide this feature, STITCH now integrates\ntissue-specific protein expression patterns from two data\nsources. First, the TISSUES resource (33), which combines\nevidence from UniProt annotations, systematic large-scale\ntranscriptomics and proteomics studies, and co-occurrence\ntext mining. For use in STITCH, the text-mining evidence\nwas recomputed based on the same texts used elsewhere in\nSTITCH. Second, STITCH incorporates baseline expres-\nsion patterns from tissues deposited in the Expression At-\nlas (34). Before augmenting the network with tissues data,\nusers have to choose if they want to use data from TISSUES\nor Expression Atlas. The TISSUES resource contains con-\nfidence levels ranging from one (lowest confidence) to five\n(highest confidence). Accordingly, on the STITCH website\nusers can select a tissue and a minimum confidence level.\nIn contrast, datasets from the Expression Atlas are trans-\nformed into percentiles. The confidence score for a protein‚Äì\nprotein interaction in the given tissue is then multiplied with\nthe geometric mean of the two proteins‚Äô expression per-\ncentiles. For protein‚Äìchemical interactions, the confidence\nscore is multiplied with the protein‚Äôs expression percentile.\nTo access the tissue expression patterns, users can search for\ntissues either by typing parts of the tissue names or by select-\ning a tissue from a list. Then, users can submit the changed\nsettings to STITCH. In return, an updated network will be\nDownloaded from https://academic.oup.com/nar/article/44/D1/D380/2503089 by guest on 11 January 2026']"
"How do the improvements introduced in Vol. 53 of Nucleic Acids Research enhance gene set enrichment analysis, and how does the RAG system described in Vol. 32 of JAMIA apply retrieval strategies in clinical decision-making?","['<1-hop>\n\nD 734 Nucleic Acids Research , 2025, Vol. 53, Database issue uses the whole genome / proteome background; however, it is recommended that users provide a custom background list representing a more realistic universe of genes / proteins detected by their assay ( 35 ). Alternatively, the entire sorted dataset can be analyzed using gene set enrichment analysis to determine whether the distribution of genes at the top, bottom or both ends deviates significantly from what would be ex- pected by chance. STRING supports both of these enrichment methods. The most recent update to STRING introduces vari- ous improvements, including a new false discovery rate (FDR) correction, enhanced filtering options and a new graphical vi- sualization.'
 '<2-hop>\n\nFigure 3. An example of using RAG in clinical applications, with identified strategies in 3 stages: pre-retrieval, retrieval, and post-retrieval. The user query seeks recommended medications for symptomatic peripheral artery disease without increased bleeding risk. The system retrieves evidence from clinical guidelines, processes it through embeddings and a vector database, and outputs a response, including rivaroxaban with low-dose aspirin, as recommended by the retrieved guideline information. In this example, GPT-4 suggested a dose of 75 mg Aspirin, but the common low-dose Aspirin is 81 mg. Journal of the American Medical Informatics Association, 2025, Vol. 32, No. 4 609 Downloaded from https://academic.oup.com/jamia/article/32/4/605/7954485 by guest on 11 January 2026']","Vol. 53 of Nucleic Acids Research introduces improvements to STRING, including a new false discovery rate (FDR) correction, enhanced filtering options, and a new graphical visualization, which collectively enhance gene set enrichment analysis by allowing users to analyze either a whole genome/proteome background or a custom background list more realistically representing detected genes or proteins. Meanwhile, Vol. 32 of the Journal of the American Medical Informatics Association describes the use of a Retrieval-Augmented Generation (RAG) system in clinical applications, employing three stages‚Äîpre-retrieval, retrieval, and post-retrieval‚Äîto answer queries such as recommending medications for symptomatic peripheral artery disease without increased bleeding risk. This system retrieves evidence from clinical guidelines, processes it through embeddings and a vector database, and outputs responses like rivaroxaban with low-dose aspirin, demonstrating how retrieval strategies support accurate clinical decision-making.",multi_hop_specific_query_synthesizer,"The improvements introduced in Vol. 53 of *Nucleic Acids Research* enhance gene set enrichment analysis primarily through the STRING database's updated features, which provide more powerful, flexible, and interpretable enrichment tools:

1. **Enhanced Statistical Power via Flexible FDR Correction**:  
   STRING now applies a novel approach to false discovery rate (FDR) correction by only testing terms that have a theoretical potential for enrichment based on parameters such as term size, query set size, background size, and number of tests. This selective testing excludes terms unlikely to be enriched, thereby increasing statistical power without omitting biologically relevant terms. This method improves the ability to detect significant enrichments, especially for larger or less specific gene sets.

2. **Improved Filtering and Sorting of Enrichment Results**:  
   New filtering options allow users to sort and filter enriched terms by FDR, strength, signal, term size, and term similarity. The term similarity filter uses the Jaccard index to remove redundant terms with overlapping gene sets, while the signal filter balances observed-to-expected gene ratios and statistical significance to prioritize the most meaningful terms. These filters help users focus on the most relevant and non-redundant enrichment results.

3. **Advanced Graphical Visualization**:  
   STRING introduced an interactive dot plot visualization that simultaneously displays enrichment signal, FDR, and term size, making it easier for users to interpret and explore enrichment results. The visualization is customizable and exportable, enhancing usability and presentation.

4. **Incorporation of Network-Derived Gene Sets and Clustering**:  
   STRING integrates unsupervised hierarchical clustering of the proteome-wide network into functional modules, which are then used in enrichment analysis. This allows identification of novel functional modules beyond manually curated gene sets, broadening the biological insights obtainable from gene lists.

5. **Comprehensive and Multi-Modal Interaction Data**:  
   STRING combines multiple evidence channels (genomic context, co-expression, experimental data, curated databases, text mining) with confidence scoring, including new regulatory and physical interaction networks with directionality. This rich data foundation supports more accurate and biologically meaningful enrichment analyses.

---

Regarding the RAG (Retrieval-Augmented Generation) system described in Vol. 32 of *JAMIA*, it applies retrieval strategies in clinical decision-making through a structured three-stage process:

1. **Pre-Retrieval Stage**:  
   Strategies include query rewriting (e.g., simplifying or rephrasing queries for better search compatibility), document preprocessing (removing non-textual elements, structuring content), and assessing whether retrieval is necessary for a given query. These steps optimize the input and prepare the data for effective retrieval.

2. **Retrieval Stage**:  
   This involves indexing documents, aligning queries with relevant documents or document chunks, and ranking retrieved information. Techniques include chunking documents into manageable pieces, embedding queries and documents into vector spaces using models like Text-embedding-ada-002 or PubMedBERT, and measuring similarity (e.g., cosine similarity). Advanced methods like Maximal Marginal Relevance improve retrieval quality by balancing relevance and diversity.

3. **Post-Retrieval Stage**:  
   Retrieved information is refined and integrated into the final output using prompt engineering. This includes decomposing retrieved text into claims, verifying claims with external knowledge, tailoring responses to the clinical context (e.g., distinguishing between provider and patient audiences), and incorporating citations from authoritative sources such as clinical guidelines. This stage ensures that the generated clinical recommendations are accurate, contextually appropriate, and evidence-based.

In clinical decision-making, the RAG system enhances large language models (LLMs) by supplementing their inherent knowledge with up-to-date, domain-specific information retrieved from clinical guidelines, EHR data, and biomedical literature. This integration improves the accuracy and relevance of clinical recommendations, supports personalized treatment decisions, and addresses complex clinical queries more effectively than baseline LLMs alone.

---

**Summary:**

- **Vol. 53 Nucleic Acids Research** improves gene set enrichment analysis by enhancing statistical power with selective FDR correction, advanced filtering, intuitive visualization, and integration of network-based gene sets, enabling more accurate and interpretable biological insights.

- **Vol. 32 JAMIA** describes the RAG system's application in clinical decision-making through a three-stage retrieval strategy (pre-retrieval optimization, sophisticated retrieval with embeddings and ranking, and post-retrieval refinement with prompt engineering), which augments LLMs with relevant clinical knowledge to improve decision support accuracy and contextual appropriateness.","[""integrates over 40 publicly available biomedical knowledge \nsources across separate domains, such as genes, proteins, \ndrugs, compounds, and diseases, along with their known \nrelationships.32 Two studies used textbooks, such as Harri\xad\nson's Principles of Internal Medicine, while 3 others utilized \nelectronic health record (EHR) data. Additionally, Zakka \net al. added over 500 markdown files from MDCalc to \nimprove clinical calculation capabilities in LLM.28 Two stud\xad\nies employed real-time online browsing to search academic \nsites, such as PubMed and UpToDate. The amount of \nretrieval resources varied across studies, ranging from a small \ndataset specific to 6 osteoarthritis guidelines to a large dataset \nof EHR data from 7 hospitals.\nRetrieval strategies\nIdentified retrieval strategies were grouped based on the RAG \nstages: pre-retrieval, retrieval, and post-retrieval. Figure 3 \npresents an example of how RAG is applied and lists identi\xad\nfied strategies within each stage.\nIn the pre-retrieval stage, 50% of studies (n ¬º 10) reported \nstrategies, such as query rewriting, document preprocessing, \nand assessing the necessity of retrieval. Zakka et al. simplified \nqueries by rephrasing text into search terms that are better \nsuited for website browsing,28 while Wang et al. focused on \ntechniques such as correcting errors, expanding abbrevia\xad\ntions, and matching synonyms in user queries.30 Soman et al. \nextracted disease entities in queries and retrieved correspond\xad\ning nodes from a knowledge graph.33 Document preprocess\xad\ning involved removing non-textual elements from PMC \npapers (eg, figures, references, and author disclosures),30\nextracted tables from PDFs using pdfplumber, structured the \ncontent with pydantic for seamless integration.25 In addition \nto query modification and document preprocessing, Jeong \net al. fine-tuned a model to determine whether retrieval was \nnecessary for a given query.34\nDuring the data retrieval stage, 85% of studies (n ¬º 17) \nreported strategies regarding indexing, aligning queries with \ndocuments, and ranking retrieval chunks. Chunking methods \nranged from fixed-size chunks35 to recursive splits.36 Embed\xad\nding models such as Text-embedding-ada-002,24,28‚Äì30,36,37\nMiniLM, and PubMedBERT33 were commonly used to con\xad\nvert sentences into vectors. Cosine similarity was the primary \nmetric for measuring query-document alignment. Two stud\xad\nies adopted Maximal Marginal Relevance for search and \nhighlighted its improved performance over similarity-based \nmethods.24,35 A domain-specific retriever, MedCPT, was \nused in one study.34 Another study used the multi-vector \nretriever that leveraged summarized document sections to \nidentify the original content for final answer generation.25\nThe retrieval cutoff parameters varied widely, with probabil\xad\nity thresholds up to 0.83 and the number of retrieved chunks \nranging from 3 to 90.28,36,38 Vector databases like FAISS and \nChroma were frequently reported, and LangChain was \nwidely used for document processing and retrieval.23,25,35,38\nIn the subgroup analysis, 12 studies used simple data retrieval \nstrategies (OR 1.30, 95% CI [1.16, 1.45]), while 5 studies \nused complex data retrieval strategies (OR 1.30, 95% CI \n[1.07, 1.24]), with no statistically significant difference \nobserved between the 2 approaches.\nIn the post-retrieval stage, 65% of studies (n ¬º 13) imple\xad\nmented specific strategies to refine outputs. Murugan et al. \ntailored prompts by providing clear context, defining roles \n(eg, distinguishing between healthcare providers and patients \nto deliver appropriately detailed information), and incorpo\xad\nrating relevant citations from retrieval sources such as the \nClinical \nPharmacogenetics \nImplementation \nConsortium \nguidelines and Food and Drug Administration (FDA) label\xad\ning.24 Soman et al. utilized prompt engineering to integrate \naccurate knowledge sources and statistical evidence, such as \nP-values and z-scores, from the SPOKE knowledge graph \ninto their outputs.33 Wang et al. outlined a detailed process \nin the post-retrieval stage using prompt engineering, which \ninvolved decomposing retrieved text into individual claims, \nverifying each claim with external knowledge sources, \nFigure 3. An example of using RAG in clinical applications, with identified strategies in 3 stages: pre-retrieval, retrieval, and post-retrieval. The user query \nseeks recommended medications for symptomatic peripheral artery disease without increased bleeding risk. The system retrieves evidence from clinical \nguidelines, processes it through embeddings and a vector database, and outputs a response, including rivaroxaban with low-dose aspirin, as recommended \nby the retrieved guideline information. In this example, GPT-4 suggested a dose of 75 mg Aspirin, but the common low-dose Aspirin is 81 mg.\nJournal of the American Medical Informatics Association, 2025, Vol. 32, No. 4                                                                                                      \n609 \nDownloaded from https://academic.oup.com/jamia/article/32/4/605/7954485 by guest on 11 January 2026""
 'was asked about medications for peripheral artery disease \npatients without increased bleeding risk, it initially omitted \nlow-dose rivaroxaban. After integrating retrieved text from \nthe 2024 American College of Cardiology / American Heart \nAssociation Guideline for the Management of Lower Extrem\xad\nity Peripheral Artery Disease,10 the model correctly recom\xad\nmended rivaroxaban.\nSeveral guidelines exist for evaluating Artificial Intelligence \n(AI) applications and LLMs in healthcare, including \nDECIDE-AI (Developmental and Exploratory Clinical Inves\xad\ntigations of DEcision support systems driven by Artificial \nIntelligence),11 CLAIM (Checklist for Artificial Intelligence in \nMedical Imaging),12\nand CONSORT-AI (Consolidated \nStandards of Reporting Trials-AI).13 In addition, Tam et al. \nintroduced QUEST, a framework specifically for human eval\xad\nuation of LLMs in healthcare.14 However, these guidelines \ndo not cover RAG applications in clinical settings, emphasiz\xad\ning the need for a more specific guideline.\nDespite the promise of RAG in improving LLM performance \nin clinical settings, there is limited understanding of its overall \neffectiveness comparing with the baseline LLM, adoption in \nclinical domains, and optimal strategies for its development in \nbiomedical applications. The aim of this study is to synthesize \nfindings from recent research of RAG and LLM in biomedicine \nand provide clinical development guidelines to improve effec\xad\ntiveness as well as transparency in future research.\nMaterials and methods\nStudy design\nWe conducted a systematic literature review. The report was \ncreated in adherence to the Preferred Reporting Items for Sys\xad\ntematic Reviews and Meta-Analyses 2020 analysis checklist \n(File S1).15 We also performed a meta-analysis following the \nCochrane Handbook guidelines.16\nScreening papers\nWe searched in 3 databases (PubMed, Embase, and Psy\xad\ncINFO), using terms related to ‚Äúretrieval augmented gener\xad\nation‚Äù and ‚Äúlarge language model.‚Äù PubMed and Embase \nwere selected based on recommendations from the Cochrane \nHandbook, as they are widely recognized for their extensive \ncoverage of biomedical literature.17 PsycINFO was included \nto capture articles from the psychological and behavioral sci\xad\nences. To maintain a high standard of quality and reliability, \nwe focused on peer-reviewed articles and excluded preprints. \nThe specific search terms used for each database are provided \nin File S2. Given that ChatGPT was released on November \n30, 2022, we set the publication filter to search papers pub\xad\nlished in 2023 and 2024. The search was performed on \nDecember 12, 2024. The inclusion criteria were: (1) the study \nmust compare baseline LLM performance with RAG per\xad\nformance and (2) the study must address a biomedical ques\xad\ntion. The exclusion criteria were: (1) literature reviews, \neditorial comments, or viewpoint papers, (2) studies focusing \non LLMs in languages other than English, or (3) studies cen\xad\ntered on a multi-agent system without a focus on RAG. SL \nscreened titles and abstracts, then conducted a full-text \nreview of papers meeting the criteria.\nData extraction\nFor each included study, we extracted the following informa\xad\ntion: author, title, publication year, journal, clinical task, and \nspecialty. Regarding RAG techniques, we gathered details \nabout the baseline LLM, retrieval sources, and strategies used \nin the pre-retrieval, retrieval, and post-retrieval stages. For \nevaluation, we extracted the evaluation method (human, \nautomated, or a combination of both), the number of evalua\xad\ntors, the evaluation dataset, and the evaluation metrics.\nMeta-analysis\nEffect size was defined as a metric quantifying the relation\xad\nship between variables, including both direction and magni\xad\ntude.18 For each included study, we calculated the effect size \nbetween baseline LLM performance and RAG-enhanced \nLLM performance. The outcomes focused on the perform\xad\nance of generation results, such as accuracy and usefulness. \nMetrics related to the retrieval process, cost, or speed were \nnot included as outcomes in the meta-analysis. For continu\xad\nous outcomes, we used Cohen‚Äôs d, standardized between- \ngroup mean difference (SMD), calculated as the difference in \nmeans divided by the pooled standard deviation. The stand\xad\nard error (SE) of SMD was calculated using the following for\xad\nmula (1), where n1 and n2 represent the sample sizes of each \ngroup.19 For dichotomous measurements, we calculated the \nlog-odds ratio, obtained by transforming the odds ratio (OR) \nwith the natural logarithm, and the associated SE was calcu\xad\nlated using formula (2), where a, b, c, and d represent the \nnumber of successful and failed events in the baseline LLM \nand RAG-enhanced LLM approaches. For studies reporting \nmultiple outcomes, we used the overall outcome to calculate \neffect size. If no overall outcome was reported, we averaged \nthe effect sizes of all reported outcomes. We excluded out\xad\ncomes with a sample size of less than 30 to avoid small- \nsample bias. \nSESMD ¬º\nffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi\nn1 √æ n2\nn1n2\n√æ\nSMD2\n2 n1 √æ n2\n√∞\n√û\ns\n(1) \nSElogOR ¬º\nffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi\n1\na √æ 1\nb √æ 1\nc √æ 1\nd\nr\n(2) \nWe developed a random-effect meta-analysis model, \nbecause of the variability in RAG architectures and evalua\xad\ntion datasets among the included studies. The random-effect \nmodel was used when individual study effects likely con\xad\ntained additional sources of variance beyond sampling error. \nBetween-study heterogeneity was assessed using Higgins & \nThompson‚Äôs I2 statistic, where 25% indicated low heteroge\xad\nneity, 50% moderate, and 75% substantial.20\nWe conducted subgroup analyses to explore performance \nvariations across different factors. First, we analyzed the \ninfluence of the baseline LLM, referring to the foundation \nmodel (eg, GPT-4 or Llama2) that provides the core architec\xad\nture for the system. Second, we examined data retrieval strat\xad\negies, categorizing them as simple or complex. Simple \nstrategies included fixed-length chunking and basic similarity \nsearch, and we performed a subgroup analysis to compare \nthese with complex retrieval strategies. Third, we analyzed \ndifferences based on evaluation methods, distinguishing \nbetween human evaluations, such as Likert scale ratings for \nhelpfulness and accuracy, and automatic evaluation metrics, \nincluding ROUGE-1 and BLEU. Finally, we conducted a sub\xad\ngroup analysis based on the type of task, classifying studies \n606                                                                                                      \nJournal of the American Medical Informatics Association, 2025, Vol. 32, No. 4 \nDownloaded from https://academic.oup.com/jamia/article/32/4/605/7954485 by guest on 11 January 2026'
 'Nucleic Acids Research , 2025, Vol. 53, Database issue \nD 735 \nFigure 2. Enrichment analysis interface from the STRING database website. Lef t: Sc hematic of the enrichment analysis tab highlighting various sections \nof the webpage. Top right: A zoomed-in view of the analysis table with two user-highlighted terms (colored lines). Bottom right: A visualization of the \nenrichment depicting the two terms highlighted in corresponding colors. \nFurthermore, a distinctive feature of STRING‚Äôs visualiza- \ntion is the incorporation of similarity groups, which visually \ngroup related terms on the plot, adding an additional dimen- \nsion to the enrichment results. These groups utilize computed \nsimilarities to cluster terms together, highlighting relationships \nbetween the terms and aiding the analysis. The clustering em- \nploys the average linkage method based on the Jaccard index, \nwith a user-specified cutoff applied to form groups that, on \naverage, share the specified similarity. The groups are sorted \nby the maximum signal of their terms, with each term within a \ngroup subsequently sorted by its individual signal. This group- \ning is visually highlighted on the right side of the plot. \nS TRING clust ering and g ene set descriptions \nSTRING offers network clustering options for user-submitted \ngene lists. This feature visually connects nodes that are more \ninterconnected to each other than to other nodes, reflecting \nthe functional modularity of the user‚Äôs gene list. By grouping \nproteins that closely interact, clustering aids in the discovery \nof functional modules and facilitates hypothesis generation. \nClustering is based on the connectivity between nodes, incor- \nporating edge weights (combined scores) reflecting the confi- \ndence level of the interactions. Only visible edges‚Äîthose cor- \nresponding to active channels with confidence higher than the \nuser-specified cutoff‚Äîare considered, ensuring that clustering \nis consistent with the network being shown. \nUsers have the choice of two different clustering algorithms. \n‚Äò K -means clustering‚Äô is implemented via the k -medoids algo- \nrithm [pyclustering package ( 37 )], allowing users to specify \nthe desired number of clusters ( k ). This method forms clusters \nbased on a distance matrix derived from the most probable \npaths between nodes. In case of multiple disconnected compo- \nnents within the network, STRING aims for a balanced distri- \nbution of cluster sizes by iteratively dividing the largest com- \nponents first. In contrast, ‚ÄòMCL clustering‚Äô, implemented with \nstandalone binaries (release 14-137), automatically generates \nnatural divisions based on the network‚Äôs inherent structure. It \nutilizes the Markov cluster algorithm ( 38 ) to simulate stochas- \ntic flow in graphs, identifying groups of highly interconnected \nnodes as clusters based on their combined STRING scores. \nThe user-controlled inflation parameter influences the granu- \nlarity of the clustering. This method is particularly effective \nin capturing the true modular nature of biological networks, \nallowing for intuitive groupings that correspond to biological \nfunctions and relationships. \nResearchers often analyze the biological context of the ob- \ntained clusters to interpret and draw conclusions from net- \nwork data. To aid this process, we have developed a novel \ngene set naming feature that significantly enhances the inter- \npretability of cluster analyses. This feature automatically as- \nsigns the best description for each cluster based on STRING‚Äôs \nrobust statistical enrichment analysis, simplifying the often \ncumbersome task of manually interpreting cluster data. It \nranks the enriched terms by their enrichment signal (see \nthe ‚ÄòImproved enrichment analysis‚Äô section), choosing the pri- \nmary, secondary and tertiary names among various enrich- \nment categories. This prioritization ensures that the names re- \nflect the most statistically significant and biologically pertinent \nattributes of each cluster, providing a clear, immediate under- \nstanding of its functional characteristics. The enrichment pri- \nmarily draws from categories such as Gene Ontology Biolog- \nical Processes, while incorporating a range of other categories \nfor broader annotations. In case the enrichment analysis yields \nno significant results or if the cluster contains only one gene, \nSTRING assigns canonical gene names as descriptions to en- \nDownloaded from https://academic.oup.com/nar/article/53/D1/D730/7903368 by guest on 11 January 2026'
 'offer global information based on user query, offering \nbetter performance than na√Øve RAG on the traditional \nvector databases.50\n6) Implement few-shot learning with CoT for complex clin\xad\nical reasoning. \nFew-shot learning has been shown to enhance LLMs‚Äô \nreasoning capabilities by teaching specific reasoning that \nmay not have been included in their original training. \nSimilarly, CoT techniques can improve complex reason\xad\ning in clinical tasks.51,52 Researchers should generate \nhigh-quality examples and incorporate CoT strategies \ninto the final query to refine specialized reasoning. \n7) Evaluate all stages of the RAG system development. \nMost current studies focus only on final performance, \noverlooking the importance of evaluating each stage of \ndevelopment. It is crucial to formally assess and report \nperformance at the pre-retrieval, retrieval, and post- \nretrieval stages. Evaluating the knowledge boundaries of \nthe baseline LLM, potential conflicts between the LLM \nand external knowledge, and the accuracy and coverage \nof retrieved information helps ensure replicability and \ntransparency. This level of evaluation enables other \nresearchers to understand why a RAG system works (or \ndoes not) and facilitates reproducibility. \n8) Consider cost and time constraints. \nAdvanced retrieval strategies can improve performance \nbut often increase processing time and computational \ncosts. For example, graph-based RAG requires substan\xad\ntial resources for developing knowledge graphs, and \nresponses from global summaries may take longer than \nwith na√Øve RAG methods.50 Another example is to fine- \ntune LLMs to evaluate the needs and the quality of \nretrieval.34 In terms of computational cost, this process \nis expensive, especially when scaling the method to \nlarger datasets or deploying it in a real-time system. \nAlso, a set of extra processes will make the whole speed \nslow. The long response time might have a nonignorable \nnegative impact in situations that need a quick answer, \nespecially common in clinical settings. Researchers \nshould balance performance improvements with time \nand cost considerations. Only 1 study in our review, \nwhich focused on gastrointestinal radiology diagnosis \nbased on imaging descriptions, compared the cost and \nresponse time between LLMs and LLMs with RAG.31\nThe mean response time was 29.8 s for LLM with RAG \nvs 15.7s for LLM alone, with costs of $0.15 and $0.02 \nper case, respectively. Another study used EHR to pre\xad\ndict cognitive decline only reported cost, with LLM: \n$4.49; RAG: $12.51. Another study that used EHR \ndata to predict cognitive decline reported costs of $4.49 \nfor LLM alone and $12.51 for LLM with RAG.53\n9) Establish monitoring and knowledge update strategies. \nAn important concept in AI applications in healthcare, \nalgorithmovigilance, which defined as ‚Äúscientific meth\xad\nods and activities relating to the evaluation, monitoring, \nunderstanding, and prevention of adverse effects of \nalgorithms in health care,‚Äù 54 should also be considered \nin the RAG applications. Researchers need to develop \nlong-term monitoring strategies for the RAG system per\xad\nformance, especially in clinical applications. In addition, \ncurrent studies use fixed external datasets. Researchers \nshould update external knowledge sources as latest \ninformation becomes available. Clear strategies for \nupdating knowledge should be defined, specifying when \nand how updates will occur. \nFor future studies, the first direction could be the system- \nlevel enhancement, the combination of RAG and LLM- \npowered agents. LLM-powered agents are AI systems that \nuse LLMs with complex reasoning and planning capabilities, \nmemory management, interactive capabilities with the envi\xad\nronment, and actions to execute tasks.55,56 Recent research \npoints to the emerging trend of combination of RAG and \nLLM-powered agents, where agents can assist in planning \nand decision making for complex tasks, rather than simple \nretrieval.57 For example, clinicians and patients have diverse \ninformation access needs, some needing to analyze text from \na knowledge base, others needing to incorporate structured \ndata from an EHR. RAG will eventually only become one of \nthe methods for agents to access information. Moreover, \nfuture research could focus on the usage of internal and \nexternal functions and tools, long-term and short-term mem\xad\nory module, self-learning module. For example, a study \ndeveloped an agent to answer questions related to rare dis\xad\neases by expanding beyond RAG with additional tool func\xad\ntions, such as querying phenotypes and performing web \nsearches. This approach improved the overall correctness \nfrom 0.48 to 0.75 compared to the GPT-4 baseline LLM.58\nThe second future direction could focus on the knowledge- \nlevel enhancement: deep integration of external knowledge \ninto LLM. LLM exhibits the knowledge boundaries. RAG \napproaches retrieving external knowledge and then integrates \nit into LLMs in the forms of prompts for the final generation \nto enhance the capabilities of LLMs in perceiving knowledge \nboundaries.59 However, the integration of external knowl\xad\nedge into LLM reasoning is typically limited to providing the \nretrieved data as additional context for the LLM‚Äôs query dur\xad\ning generation. This approach keeps retrieval and generation \nloosely connected, and the LLM‚Äôs output can still be influ\xad\nenced by its inherent knowledge boundaries or by noise in the \nretrieved text, leading to incorrect answers. Additionally, \nwhen the external knowledge source is EHR data, this \nenhancement becomes even more important. Current EHR \ndata is organized in a ‚Äúproblem-oriented medical record‚Äù \n(POMR) format, which collects and displays information in a \nstructured manner.60 LLMs excel in free-form contexts, and \ntheir ability to perform clinical tasks depends on access to \nunstructured text that provides a comprehensive view of the \npatient. Achieving this within the structured POMR format \nin modern EHR systems poses a significant challenge.61\nTherefore, investigating how to realize the deep integration \nof external knowledge with LLM reasoning is an important \ndirection for future research in clinical applications.\nThe final direction is the integration-level enhancement, \nfocusing on integrating RAG systems within EHRs. Current \nresearch has primarily focused on development and testing \noutside of EHR systems. To seamlessly provide support for \nhealthcare providers and patients, future efforts should pri\xad\noritize embedding RAG systems into EHR interfaces. This \nrequires collaboration with EHR vendors to ensure the neces\xad\nsary infrastructure is available. Researchers also can facilitate \nthis integration using data exchange frameworks, such as \nSMART on FHIR.62\n612                                                                                                      \nJournal of the American Medical Informatics Association, 2025, Vol. 32, No. 4 \nDownloaded from https://academic.oup.com/jamia/article/32/4/605/7954485 by guest on 11 January 2026'
 'Clinical applications of RAG\nRAG techniques have been applied across a broad range of \nmedical specialties, as shown in Table 1. These applications \ninclude clinical decision-making and medical question- \nanswering. In clinical decision making, RAG has supported \npersonalized treatment,23,24 emergency triage,25 and disease \nmanagement.26,27 For medical question-answering, RAG‚Äôs \ncapability has been explored to address complex treatment \nguidelines questions,28 as well as queries focused on specific \nareas, such as head and neck surgery-related questions,29 and \npatient questions regarding diabetes.30 In the subgroup anal\xad\nysis, 13 studies focused on clinical decision-making (OR \n1.46, 95% CI [1.16, 1.71]) and 7 studies focused on medical \nquestion-answering (OR 1.32, 95% CI [1.08, 1.63]), with no \nstatistically significant difference observed between these 2 \ngroups.\nBaseline LLMs\nThe baseline LLMs varied across studies, with GPT-4 being \nthe most common, used in 14 studies, (OR: 1.58, 95% CI: \n1.21-2.04). GPT-3.5, used in 6 studies, showed an OR of \n1.43 (95% CI: 1.06-1.93). Llama2 was applied in 5 studies \n(OR: 1.25, 95% CI: 1.08-1.44).\nRetrieval sources\nRetrieval sources were categorized as pre-stored documents \nand real-time online browsing. Regarding pre-stored docu\xad\nments, 6 studies used clinical guidelines, such as the Emer\xad\ngency Severity Index (ESI) Ver.3 Field Triage.25 Five studies \nused academic articles from sources like PubMed abstracts or \nfull texts, or document sets such as the Radiographics Top 10 \nReading List on Gastrointestinal Imaging.31 Three studies \nused specialized knowledge bases, including ChatENT, \nOncoKB, and RareDis Corpus, while one study employed a \ngeneral biomedical knowledge graph (Scalable Precision \nMedicine Open Knowledge Engine [SPOKE]). SPOKE \nTable 1. Distribution of RAG applications by medical specialty and task \ntype.\nMedical Specialty\nFrequency\nInternal medicine\n4\nGeneral medicine\n3\nOncology\n3\nEmergency medicine\n2\nGastroenterology\n2\nOtolaryngology\n1\nHepatology\n1\nRare diseases\n1\nOrthopedics\n1\nNeurology\n1\nOphthalmology\n1\nTask\nFrequency (%)\nClinical decision-making\n13 (65%)\nMedical question-answering\n7 (35%)\nFigure 2. A forest plot showing the odds ratio (OR) of total impacts of the RAG-enhanced system compared with the baseline LLM system in clinical \ntasks. The left panel lists individual studies (e.g., Zakka et al., Murugan et al., Long et al.) along with their corresponding ORs and 95% confidence \nintervals (CIs). The right panel visualizes these ORs, with square markers proportional to study weights and horizontal lines representing CIs. The overall \npooled OR is 1.35 (95% CI: 1.19‚Äì1.53), represented by a diamond at the bottom.\n608                                                                                                      \nJournal of the American Medical Informatics Association, 2025, Vol. 32, No. 4 \nDownloaded from https://academic.oup.com/jamia/article/32/4/605/7954485 by guest on 11 January 2026'
 'Nucleic Acids Research , 2025, Vol. 53, Database issue \nD 731 \nIntroduction \nThe function of every living cell is primarily governed by a \ncomplex network of interacting proteins, with each protein‚Äôs \nrole determined not only by its molecular activities but also \nby its position within this network ( 1 ,2 ). Connected proteins \nwork together to contribute to common biological processes \nthrough various interaction types, such as physical binding, \ngenetic interactions and regulatory influences. These interac- \ntions can collectively be categorized as functional associa- \ntions , which serve as fundamental operational units within bi- \nological systems. Unraveling protein networks in their various \nmodalities remains a significant research focus. Consequently, \nnumerous databases have been developed over the years to \nmeet specific research needs. These range from carefully cu- \nrated pathway databases such as Reactome ( 3 ) and KEGG \n( 4 ), to databases of experimental interaction evidence curated \nfrom literature such as BioGRID ( 5 ), IntAct ( 6 ) and MINT \n( 7 ), and to those centered on specific modes of interactions, \nincluding Complex Portal for protein co-complexes ( 8 ) and \nSIGNOR for regulatory interactions ( 9 ). Finally, composite \ndatabases such as STRING ( 10 ), GeneMANIA ( 11 ), FunCoup \n( 12 ) and HumanNet ( 13 ) not only incorporate data from these \nsources but also employ an array of computational methods \nto predict additional associations, striving to provide the most \ncomprehensive views of the interactome. \nAmong these databases, STRING is notable for its many \nsources of evidence, its robust scoring system, user-friendly \ninterface and comprehensive suite of enrichment features. It \nis dedicated to assembling a broad set of associations among \nproteins for the complete proteomes of thousands of organ- \nisms across all domains of life. STRING charts interactions \nranging from highly confident, well-documented associations \nto more speculative ones, which are crucial for exploratory \nand computational analyses. The scoring system ensures that \ndata from diverse sources‚Äîincluding automatic text mining, \nhigh- and low-throughput experimental data, and compu- \ntational predictions‚Äîare directly comparable and weighted \nconsistently, regardless of their origin. Additionally, STRING \nextends and maps networks across species by predicting in- \nterologs, thereby broadening the scope of functional associa- \ntions to encompass a diverse range of organisms. This includes \nuncultured novel bacterial species derived from metagenomic \nsamples, and even unpublished proteomes uploaded by users. \nThe development of interaction databases has so far been a \ntrade-off between detailing the exact mode of an interaction‚Äî\nwhether physical or functional, signaling or structural, stable \nor transient‚Äîand maintaining a comprehensive set of interac- \ntions. Given the limitations of the available data and method- \nologies, STRING has historically focused on broadly defined \nfunctional associations, which provided the most useful set \nof interactions for proteome-wide analysis. While these net- \nworks continue to perform well in downstream tasks ( 14 ,15 ), \nthey often lack fine-grained resolution, leaving more special- \nized databases better equipped to offer detailed insights into \nspecific interactions if needed. However, the growing vol- \nume of data and advancements in text-mining technologies \nhave since enabled the composite, general-purpose databases \nto catch up, offering more detailed maps of the interac- \ntion space. Recently, the STRING database introduced a co- \ncomplex (physical) interaction network mode, which details \nmulti-protein assemblies. With the latest update, the database \nnow also includes the largest set of regulatory (directed) inter- \nactions, highlighting the flow of information within cells. This \nmajor enhancement greatly expands the analytical capabilities \nof the database, enabling new types of studies that were not \npreviously possible with nondirected interaction networks. \nInteraction and pathway databases are frequently used to \ngain insights into the functional context of individual pro- \nteins or to understand the biological organization of an entire \nprotein dataset. Many of these databases enhance the inter- \npretation of protein datasets beyond simple interaction anno- \ntations by incorporating an enrichment analysis toolset. This \ntype of analysis involves comparing observed data against ex- \npected distributions, enabling researchers to identify statisti- \ncally significant features or patterns. Enrichment tools typi- \ncally utilize established ontologies and annotations, such as \nGene Ontology ( 16 ), MSigDB hallmark sets ( 17 ) or OMIM \n( 18 ). A few databases also leverage their annotated datasets \nto help understand the functional context of a user‚Äôs input. \nFor example, KEGG ( 4 ) employs pathway and module ab- \nstractions to organize genes into datasets for overrepresenta- \ntion analysis, while Reactome maps interconnected proteins \ninto hierarchically nested modules. Uniquely, STRING uses \nan unsupervised procedure to hierarchically precluster its en- \ntire proteome-wide networks into functional modules. Incor- \nporating these network-derived gene sets into the enrichment \nanalysis facilitates the identification of novel modules, espe- \ncially in areas of the proteome where manual curation has \nbeen less comprehensive so far. \nDatabase content \nThe basic interaction scope in STRING is that of a ‚Äòfunctional \nassociation‚Äô between pairs of proteins. A functional associa- \ntion is defined as a contribution of two non-identical proteins \nto a common function ( 19 ,20 ). This can take many forms; \nfunctionally associated proteins can be in physical proxim- \nity to each other, regulate each other, exhibit genetic epistasis \nor even work antagonistically (as long as this occurs in the \ncontext of a common function). For the purpose of defining \nfunctional associations, the concept of a common function is \ncrucial, but difficult to define‚Äîit should broadly be thought \nof as corresponding to the notion of a ‚Äòpathway‚Äô or ‚Äòfunction \nmodule‚Äô. In practice, STRING roughly follows the functional \ngranularity of the pathway maps in the KEGG database ( 4 ). \nFrom the set of all functional associations in STRING, sub- \nsets are derived that are more specifically annotated with re- \ngard to their mechanism of association. Currently, two such \nmore specific subsets are implemented: ‚Äòphysical‚Äô and ‚Äòregula- \ntory‚Äô. The physical mode refers to pairs of proteins that either \nbind directly or are at least subunits of the same complex ( 21 ). \nThe regulatory mode refers to associated protein pairs that are \nknown to regulate each other‚Äôs activity in at least one direc- \ntion. This mode is described in more detail further below; for \nthe first time in STRING, such connections are annotated not \nonly with confidence scores but also with a directionality. \nAll protein‚Äìprotein associations in STRING are annotated \nwith ‚Äòconfidence scores‚Äô. These scores are fully precomputed; \nthey scale between 0 and 1 and describe the estimated like- \nlihood of a postulated association being correct, given the \navailable evidence. Separate confidence scores are provided \nfor physical and regulatory modes‚Äîthese scores provide es- \ntimates of the likelihood that the proposed association is tak- \ning place and is indeed of the postulated type. The two spe- \nDownloaded from https://academic.oup.com/nar/article/53/D1/D730/7903368 by guest on 11 January 2026'
 'D 734 \nNucleic Acids Research , 2025, Vol. 53, Database issue \nuses the whole genome / proteome background; however, it \nis recommended that users provide a custom background \nlist representing a more realistic universe of genes / proteins \ndetected by their assay ( 35 ). Alternatively, the entire sorted \ndataset can be analyzed using gene set enrichment analysis to \ndetermine whether the distribution of genes at the top, bottom \nor both ends deviates significantly from what would be ex- \npected by chance. STRING supports both of these enrichment \nmethods. The most recent update to STRING introduces vari- \nous improvements, including a new false discovery rate (FDR) \ncorrection, enhanced filtering options and a new graphical vi- \nsualization. \nFDR correction \nIn its analyses, STRING performs a test for each path- \nway (‚Äòterm‚Äô) within a given pathway collection (‚Äòcategory‚Äô). \nLike many other enrichment tools, STRING employs the \nBenjamini‚ÄìHochberg correction ( 36 ) to adjust for multiple \nhypothesis testing. This statistical correction is essential for \nmanaging the FDR effectively. However, the larger the term \ncount and the greater the diversity of the terms, the higher \nthe required correction and the lower the chance of exceed- \ning the alpha level (significance threshold) for a given term. \nOne method used to address this issue involves restricting the \nanalysis to a subset of terms, such as ‚ÄòGO slims‚Äô that focus \non broad, high-level terms with a simplified ontology struc- \nture ( 16 ). However, this approach has some disadvantages: it \nmight exclude terms particularly relevant to a tested gene set, \nit may not cover all functions by design and it is mainly ap- \nplicable to hierarchical classification systems such as the Gene \nOntology. \nSTRING utilizes several hierarchical ontologies, such as \nthe Brenda Tissue Ontology, Disease Ontology, Human Phe- \nnotype Ontology and the aforementioned Gene Ontology; \nthey typically contain many more leaf-ward nodes than nodes \ncloser to the root. The parental nodes must contain all the \ngenes included in the child terms, creating an imbalance in \nthe ontology with many more smaller terms and fewer larger \nterms. Removing smaller terms can substantially increase sta- \ntistical power . However , simply eliminating all small terms \ncould negatively impact discoverability, especially for small \nquery gene sets where the smaller terms might be the most rel- \nevant for biological interpretation. The ontologies are usually \nlarge, with > 10 000 terms (and therefore tests) in the Biolog- \nical Process branch of the Gene Ontology tree. Such strong \ncorrections have the effect that for larger query gene sets, or \nfor backgrounds that cover only part of the proteome, it might \nnot be statistically possible to detect enrichments for a subset \nof the terms; these can therefore be removed from considera- \ntion. \nIn its new version, STRING takes a more flexible \napproach‚Äîby only testing terms that have a priori any statis- \ntical potential for enrichment. This decision is based on sev- \neral parameters: the size of the term, the size of the query set, \nthe background size and the number of tests conducted. By \ncalculating the smallest and largest term sizes that could the- \noretically be enriched given the FDR correction, STRING can \ndetermine which terms are viable for inclusion in the anal- \nysis. Terms outside these bounds are excluded from testing. \nThis methodical exclusion significantly enhances the statisti- \ncal power of the enrichment analysis, without omitting any \nterms that could be enriched. For smaller query sets, this strat- \negy will maintain all terms, as even the smallest tested sets \n(term size = 2) might still be theoretically enriched. How- \never , for larger , less specific sets or for tests against a cus- \ntom smaller background, this approach markedly improves \nthe statistical power, enabling STRING to perform more fo- \ncused analyses without the drawbacks of increased false posi- \ntive rates or omitting potentially biologically interesting terms. \nAlthough this method leverages the characteristics of ontolo- \ngies, it does not require a hierarchy (parent‚Äìchild relationships \nbetween the terms) to function and can be applied to any \npathway / term collection with an imbalance of small terms, \nsuch as PMID-derived gene sets and STRING neighborhood \nclusters. \nTerm filtering \nOne key insight from our online user surveys is that the of- \nten overwhelming number of enrichment results can make it \nchallenging to discern the most relevant enriched terms. We \nhave thus enhanced the display of enrichment results, allow- \ning users to filter and sort them based on (i) FDR, (ii) strength, \n(iii) signal, (iv) term size and (v) term similarity. \nThe newest addition to our filtering options are the ‚Äòterm \nsimilarity‚Äô and ‚Äòsignal‚Äô filters. The ‚Äòterm similarity‚Äô filter uses \nthe Jaccard index to measure the similarity between the gene \nsets of terms within the same category. The method proceeds \nby sorting the terms according to their P -values, prioritizing \nthose with the strongest statistical significance. The algorithm \nthen iterates through the list of terms; any term that exhibits \na similarity exceeding a predetermined, user-defined threshold \nrelative to terms already included in the results is subsequently \nexcluded. This approach requires no additional information \nbeyond what is already available in the dataset and can be \nuniformly applied across all categories of terms. The ‚Äòsignal‚Äô \nfilter is defined as a weighted harmonic mean between the ra- \ntio of observed to expected gene occurrences in an enriched \nterm and its ‚àílog(FDR), respectively. This filter balances the \nimpact of both metrics and provides a more intuitive ordering \nof enriched terms and has been implemented as the default \nsorting criterion. \nThese filters are designed to exclude potentially less relevant \nresults, such as terms that are marginally significant, small or \nshow significant overlap in terms of gene content. This en- \nhancement streamlines the search for relevant observations, \nhelping users focus on the most meaningful results. \nGraphical visualization \nIn response to user feedback requesting more intuitive, graph- \nical visualizations of the detected enrichments, STRING now \nhas adopted an interactive dot plot as a primary display \nmethod for enrichment analysis outcomes. We chose the dot \nplot for its clarity in representing enriched functions across \nthree dimensions: (i) enrichment signal along the X -axis, (ii) \nthe FDR indicated through the color coding of the dots and \n(iii) the term‚Äôs protein count in the network represented by the \nsize of each dot. The terms (listed on the Y -axis) are ranked \nby their enrichment signal or, if grouped by similarity, by the \nenrichment signal of their group first (Figure 2 ). This visual- \nization reflects all filter settings chosen by the user to manage \nthe volume of results displayed. These plots can be addition- \nally customized in the interface and the resulting graphs can be \ndownloaded as PNG for immediate use or as SVG for further \nmodifications. \nDownloaded from https://academic.oup.com/nar/article/53/D1/D730/7903368 by guest on 11 January 2026'
 'D 732 \nNucleic Acids Research , 2025, Vol. 53, Database issue \ncific network modes are fully consistent with the functional \nnetwork, such that if an interaction is present in either the \nphysical or regulatory network, it will, by definition, also be \npresent in the full functional association network (with an \nequal or higher confidence score). To compute the various con- \nfidence scores, the available evidence supporting a given inter- \naction is first separated into ‚Äòevidence channels‚Äô, by the type \nof evidence. For the basic functional association confidence \nscore, seven such channels are used. These include three that \nare based on genomic context predictions‚Äîneighborhood, fu- \nsion and gene co-occurrence‚Äîas well as channels dedicated to \nco-expression, experimental data, curated databases and text \nmining. \nThe genomic context channels focus on associations in- \nferred from genome sequences alone, detecting evolution- \nary patterns such as gene proximity, fusion events and co- \noccurrence across different organisms. For instance, the neigh- \nborhood channel assigns an association score to gene pairs \nthat are located close to each other on the chromosome in \nthe same orientation (in prokaryotic genomes) ( 22 ). The fu- \nsion channel identifies open reading frames that result from \ngene fusion events ( 23 ), while the co-occurrence channel ex- \namines whether genes have a nontrivial but shared distribu- \ntion across genomes ( 24 ), implying a shared history of hori- \nzontal transfers, losses or duplication events and thus likely \nrelated functions. The co-expression channel compiles data \nfrom gene expression studies, analyzing both transcript and \nprotein abundances across various conditions. By comparing \nexpression profiles, it identifies gene pairs with similar expres- \nsion patterns, suggesting functional linkage ( 10 ,25 ). The ex- \nperiments channel aggregates interaction evidence from labo- \nratory assays, including biochemical, biophysical and genetic \nassays. Data are imported from primary repositories such as \nBioGRID ( 5 ) and the IMEx consortium ( 26 ), and the confi- \ndence scores are estimated by globally benchmarking the accu- \nracy of annotated experimental techniques, as well as within- \ndataset performance and consistency for the case of high- \nthroughput experiments ( 10 ). The database channel is based \non well-described, curated protein‚Äìprotein associations from \nexpert-compiled resources, such as KEGG ( 4 ), Reactome ( 3 ) \nand Gene Ontology Complexes ( 16 ). These resources provide \nwell-established pathways, complexes and functional relation- \nships, offering a high level of reliability. Unlike other chan- \nnels, the database channel assigns a uniformly high confidence \nscore to associations, reflecting their established nature. Fi- \nnally, the text-mining channel utilizes a large corpus of sci- \nentific literature, including PubMed abstracts and full-text ar- \nticles, to identify co-mentions of protein names ( 27 ). By statis- \ntically analyzing the frequency of these co-mentions in various \ntextual contexts, this channel uncovers potential associations \nthat may not be evident from structured data alone. In addi- \ntion, for the more specific association modes, dedicated large \nlanguage models are employed to detect sentences supporting \neither physical ( 21 ) or regulatory associations (see below). \nFor each evidence channel, the available interaction evi- \ndence is translated into a confidence score by first quantifying \nthe evidence using channel-specific metrics and then convert- \ning these metrics into likelihoods using calibration curves de- \nrived from prior knowledge (from pathway-map memberships \nin KEGG). For the more specific association modes ‚Äòphysical‚Äô \nand ‚Äòregulatory‚Äô, only channels and evidence that are appli- \ncable to these modes are considered. After this, all channel \nscores that have been computed for a given protein pair in \na given organism are transferred onto related protein pairs \nin other organisms, based on the ‚Äòinterolog‚Äô concept ( 27 ,28 ). \nLastly, a final, combined confidence score is computed by inte- \ngrating the channel-specific subscores probabilistically, under \nthe assumption that evidence in different channels is largely \nindependent. Users of STRING can directly rely on this com- \nbined score for browsing and filtering networks, or they can \nalternatively customize their analyses by enabling or disabling \nspecific channels separately, after which the combined score is \nrecomputed according to their chosen settings. \nAll primary evidence underlying a given interaction can \nbe inspected interactively, in dedicated evidence viewers on- \nline. Furthermore, accessory information is available for \neach protein, such as its three-dimensional structure, do- \nmain composition, annotation and cross-references. Apart \nfrom its core protein-network functionality, STRING also \nimplements features that allow extensive analysis of up- \nloaded user data. These include functional enrichment de- \ntection ( 29 ), experimental bias detection ( 21 ), homology \nsearches, and clustering and organizing large query pro- \ntein sets. All data contained in STRING are freely avail- \nable for download, under a Creative Commons BY 4.0 li- \ncense. Apart from its website, STRING can also be accessed \nvia a dedicated Cytoscape plugin ( 30 ), through an applica- \ntion programming interface (API) ( 31 ), as well as through \nan R / Bioconductor package ( https://www.bioconductor.org/ \npackages/ release/ bioc/ html/ STRINGdb.html ). \nRegulat ory netw orks \nMolecular networks, like those available through STRING, \nhave become invaluable tools in biomedical research, offering \npowerful insights into how molecules work together within \ncells. The functional association networks, while useful for \nmany applications, do not specify the interaction type (e.g. \ncomplex formation or transcriptional regulation) nor the di- \nrection of interaction (i.e. who regulates whom). Constructing \nnetworks with more detailed interaction types and directions \ncan significantly expand their utility, particularly for building \nmathematical models of biological systems or for making log- \nical inferences. The directionality of interactions is essential \nfor interpretation of omics data, which typically reveals more \nabout downstream effects of conditions such as diseases than \nit does about the upstream events that led to it and thus could \nbe targeted therapeutically. \nTo incorporate interactions with specific directionality, sign \nand type into STRING, we leveraged the novel dataset and \ndeep learning-based relation extraction method described in \nRegulaTome ( 32 ). The RegulaTome corpus provides a rich \nand diverse dataset that includes 16 961 relations between \n54 951 entities annotated across over 2500 documents. This \ncorpus is utilized to train a deep learning-based method used \nto annotate and extract the regulatory events throughout the \nentire available literature. \nOn top of undirected physical interactions‚Äîwhich are al- \nready included in STRING‚Äîthe following types of directed \ninteractions can be extracted using the system developed for \nRegulaTome: Regulation , with the signed subtypes of Posi- \ntive Regulation and Negative Regulation , Regulation of Gene \nExpression , Regulation of Degradation and Catalysis of Post- \nTranslational Modifications , covering six subtypes: Catalysis \nof Small Protein Conjugation , Catalysis of Small Protein Re- \nmoval , Catalysis of Phosphorylation , Catalysis of Dephospho- \nDownloaded from https://academic.oup.com/nar/article/53/D1/D730/7903368 by guest on 11 January 2026']"
