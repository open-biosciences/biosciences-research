user_input,retrieved_contexts,reference_contexts,response,reference,faithfulness,answer_relevancy,context_precision,context_recall
Who is Omar and what contributions are associated with him in biomedical knowledge graph research?,"[""Omar & Mohammed / Journal of Applied Science and Technology Trends Vol. 06, No. 02, pp. 342 â€“357 (2025) \n \n352 \nThe dataset contains 60 questions divided into three difficulty \nlevels based on how complex the language is and how deep the \nbiomedical reasoning needs to be: \nâ€¢ Level 1: 25 simple questions focusing mostly on easy-to-\nunderstand facts, such as symptoms of certain diseases or \ndrugs used to treat specific conditions. \nâ€¢ Level 2: 20 medium-level questions that involve more \ndetailed relationships, such as drug interactions and \nSideEffect. \nâ€¢ Level 3: 15 hard questions requiring multi-step reasoning \nacross multiple biomedical concepts or biological \npathways, similar to the complexity found in real clinical \ncases. \nThe set of evaluation questions was designed to span a wide \nrange of common biomedical topics and to reflect clinically \nrelevant query types reported in prior literature. Each question is \npaired with its corresponding gold standard cypher query and \ncategorized by difficulty level, as summarized in Table V, where \nthree illustrative examples are shown. The questions were derived \nfrom publicly available biomedical QA benchmarks and adapted \nfrom established knowledge bases (e.g., iBKH schema relations), \nensuring both coverage and diversity across diseases, drugs, \nsymptoms, and pathways.\nTABLE V.  QUESTION DIFFICULTY LEVELS AND SAMPLE CYPHER QUERIES \nLevel \nDefinition \nExample \nCypher Query \n1 \nSingle-hop question using \none relationship \nWhat are the symptoms of \nAlzheimer? \nMATCH (d:Disease)-[:HAS_SYMPTOM]->(s:Symptom) \nWHERE toLower(d.name) CONTAINS 'alzheimer' \nRETURN s.name AS symptom \n \n2 \nQuestions requiring one \nrelationship \nWhat are the side effects of \ndrugs used to treat asthma? \nWITH toLower('asthma') AS disease_name \nMATCH (d:Disease)<-[:TREATS]-(dr:Drug) \nWHERE toLower(d.name) CONTAINS disease_name \nMATCH (dr:Drug)-[:CAUSES]->(se:SideEffect) \nRETURN DISTINCT se.name AS side_effect, dr.name AS drug \n \n3 \nQuestions requiring two or \nmore relationships \nWhat cholesterol medications \ncause side effects, and what are \nsome of those effects? \nWITH toLower('cholesterol') AS disease_name \nMATCH (d:Disease)<-[:TREATS]-(dr:Drug) \nWHERE toLower(d.name) CONTAINS disease_name \nMATCH (dr:Drug)-[:CAUSES]->(se:SideEffect) \nRETURN DISTINCT dr.name AS drug, se.name AS side_effect \n \n \n \nFig. 4. Graphical User Interface of the Biomedical Knowledge Explorer System \nB. Quantitative Evaluation \nThe performance of the proposed system is evaluated on the \niBKH knowledge graph using Exact Match, Precision@k, \nRecall@k, F1@k, Hits@k, MRR, and total response time across \nsimple, medium, and complex biomedical queries. These metrics \nwere measured for each difficulty level (simple, medium, and \ncomplex) to see how performance changes as questions become \nmore challenging.\n \nTABLE VI.  QUERY GENERATION EXACT MATCH BY DIFFICULTY LEVEL""
 'Omar & Mohammed / Journal of Applied Science and Technology Trends Vol. 06, No. 02, pp. 342 â€“357 (2025) \n \n355 \nFor the question â€™What are the symptoms of brain cancer?â€™ \nThe system generated a Cypher query that correctly followed the \nHAS_SYMPTOM relationship from disease nodes to symptoms \nnodes, filtering by the specified disease name. The results \nretrieved included terms such as aphasia (Broca, Wernicke), \nanomia, agraphia, agnosia, amnesia (including retrograde \namnesia), olfaction disorders, and apnea symptoms consistent \nwith established neurological manifestations of brain tumors. In \nNeo4j, the data formed a clear center-and-spoke pattern, with \nbrain cancer at the center and its associated nodes of symptoms \nradiating outward, as shown in Fig. 5. \nCypher: \nWITH toLower(""brain cancer"") AS disease_name \nMATCH (d:Disease)-[:HAS_SYMPTOM]->(s:Symptom) \nWHERE toLower(d.name) CONTAINS disease_name \nRETURN DISTINCT s,d \nLIMIT 10; \n \n \nFig. 5. Graphical user interface displaying Cypher query and results for breast \ncancer treatment \n \n \nA second query, What are the side effects of insulin?, \nproduced a Cypher statement starting from the drug node for \ninsulin and traversing the CAUSES relationship to the connected \nSideEffect nodes. The resulting list included dizziness, diarrhea, \ncough, back pain, weakness, rash/dermatitis, and hypersensitivity \nside effects well documented in insulin safety profiles. In the \nvisual representation in Fig. 6, insulin appeared centrally \nconnected to these side effect nodes, further strengthening the \ncorrectness of the relationship mapping of the system. \n \nCypher: \nWITH toLower(""insulin"") AS drug_name  \nMATCH (d:Drug)-[:CAUSES]->(se:SideEffect)  \nWHERE toLower(d.name) CONTAINS drug_name  \nRETURN se,d \nLIMIT 20; \n \nFig. 6. Knowledge Graph Visualization of Insulin and Its Related Side Effects \n \n \nThese examples demonstrate the systemâ€™s ability to interpret \nplain English biomedical questions, generate correct Cypher \nqueries, and return results that are clinically plausible, easy to \ninterpret, and directly traceable to specific graph relationships. \nThis supports both the accuracy and the transparency of the \nproposed approach. \nTo illustrate how biomedical questions can be translated into \ngraph queries, consider the natural language question: â€™What are \nthe side effects of drugs that treat epilepsy?â€™. The following \nCypher query demonstrates how this question is mapped in the \nknowledge graph, where diseases are connected to drugs through \nthe TREATS relation and drugs are connected to side effects \nthrough the CAUSES relation. \n \nCypher: \nMATCH (d:Disease)<-[:TREATS]-(dr:Drug)  \nWHERE toLower(d.name) CONTAINS ""epilepsy"" \nMATCH (dr:Drug)-[:CAUSES]->(se:SideEffect) \nRETURN DISTINCT se,d,dr \nlimit 10'
 'Omar & Mohammed / Journal of Applied Science and Technology Trends Vol. 06, No. 02, pp. 342 â€“357 (2025) \n \n356 \n \nFig. 7. Side effects of drugs that treat epilepsy \n \nExecuting this query returns drugs associated with epilepsy \nand their corresponding side effects. For example, the query \nidentifies Pregabalin as a treatment for epilepsy and retrieves \nmultiple side effects such as anxiety, arrhythmia, gastrointestinal \npain, and others. The visualization of the graph in Fig. 7, \nhighlights this pattern, where the drug node is linked to epilepsy \nvia TREATS and to several side effect nodes via CAUSES, \nproviding an interpretable biomedical knowledge structure. \nVI. DISCUSSION \nUnder \na \nunified \nevaluation \nprotocol \nreporting \nP@k/R@k/F1@k, Hits@k, and MRR at ð‘˜âˆˆ{1,5,10} , the \npipeline exhibits consistent end-to-end behavior across the three \ndifficulty tiers. The ð‘˜-ablation shows strong early precision (high \nP@1 and elevated MRR), while recall increases with larger ð‘˜, \nindicating that correct answers typically surface near the top yet \npersist deeper in the ranked slate. At the query-generation level, \nexact-match (EM) is high for simple questions (e.g., âˆ¼96%) and \nlower for complex, multi-hop questions (e.g., âˆ¼86.7%), which \naligns with increased compositionality. End-to-end latency \n(mean total â‰ˆ6.07\u2006s from question to final answer) remains within \ninteractive bounds on our local setup. Practically, a fully offline \ndeployment avoids internet connectivity, API keys, and external \ndata \ntransfer, \nstrengthening \nprivacy, \ncompliance, \nand \nreproducibility in biomedical contexts. \nSeveral challenges qualify these findings. First, the \ncomparability challenge: to our knowledge, no prior study \nevaluates iBKH using the same metric suite, making cross-paper \nnumeric comparisons not â€œapples to apples.â€ We therefore \ninterpret the results as controlled evidence about this pipeline \nunder a single, consistent protocol rather than as a cross-study \nleaderboard. Beyond comparability, performance may vary with \nother biomedical knowledge graphs; the current iBKH snapshot \nis static, limiting real-time updates; and scaling to larger or \ndynamically refreshed graphs can introduce latency and \nconsistency trade-offs. \nError analysis shows that residual failures concentrate in \ncomplex, multi-hop queries where missing relation hops or brittle \nname-based matching (synonyms, abbreviations, homonyms) \nlead to partial answers. These observations motivate concept-\nlevel normalization via biomedical identifiers (e.g., UMLS, \nSNOMED, RxNorm) with synonym expansion, as well as \nschema-constrained query generation and path-guided decoding \nto better satisfy multi-hop constraints. Finally, correctness is \nassessed primarily at the answer level (Hits@k, MRR, \nprecision/recall) and does not yet include full semantic-\nequivalence checks across alternative Cypher queries, which may \noverlook cases where different queries yield the same correct \nresults. Complementing template EM with result-set equivalence \nchecks, expanding the metric suite (e.g., nDCG/MAP) with per-\nquestion 95% bootstrap confidence intervals, and supporting \nincremental graph updates with distributed storage are promising \nsteps to enhance robustness, scalability, and external validity. \nVII. CONCLUSION \nThis study introduced a hybrid biomedical question \nanswering framework that couples the LLaMA-3B language \nmodel with a Neo4j-based iBKH knowledge graph to enable the \nautomatic generation of executable Cypher queries and to deliver \ntransparent, evidence-grounded answers through justification \nsubgraphs. Evaluation in simple, medium, and complex queries \ndemonstrated consistently high performance, with strong \nprecision, recall, F1, Hits@k, and MRR values, while \nmaintaining low latency suitable for interactive biomedical \napplications. Beyond quantitative performance, the proposed \nsystem provides a reproducible and privacy-preserving solution \nby operating fully offline, a property of particular importance in \nsensitive clinical and research contexts. However, certain \nlimitations remain. The reliance on a static iBKH snapshot \nconstrains coverage and adaptability, recall is lower for complex \nmulti-hop reasoning, and the absence of canonical entity \nnormalization (e.g., UMLS, SNOMED, RxNorm) may reduce \nsemantic robustness. Future research will therefore focus on \nintegrating standardized biomedical entity normalization, \nenabling dynamic and incremental knowledge graph updates, and \nleveraging domain-adapted or fine-tuned biomedical LLMs. \nThese directions are expected to further strengthen the \nrobustness, scalability, and applicability of the framework in real-\nworld biomedical and clinical environments. \n \nREFERENCES \n[1] S. Bubeck, V. Chandrasekaran, R. Eldan, J. Gehrke, E. Horvitz, E. Kamar, \nP. Lee, Y. T. Lee, Y. Li, S. Lundberg, et al., â€œSparks of artificial general \nintelligence: \nEarly experiments with \ngpt-4,â€ arXiv preprint \narXiv:2303.12712, 2023. \n[2] J. Achiam, S. Adler, S. Agarwal, L. Ahmad, I. Akkaya, F. L. Aleman, D. \nAlmeida, J. Altenschmidt, S. Altman, S. Anadkat, et al., â€œGpt-4 \ntechnical report,â€ arXiv preprint arXiv:2303.08774, 2023. \n[3] Y. Hou, J. Yeung, H. Xu, C. Su, F. Wang, and R. Zhang, â€œFrom answers to \ninsights: unveiling the strengths and limitations of chatgpt and \nbiomedical knowledge graphs,â€ Research Square, pp. rsâ€“3, 2023.'
 '8 | GigaScience , 2025, Vol. 14 \nTable 5. Ablation study of the KGT fr ame work under Code-Llama-13B \nROUGE (%) \nMethod \nGPT-4 Eval (%) \nBERTScore (%) \nRecall \nPrecision \nF1 score \nKGT (ours) \n92.4 \n97.7 \n87.4 \n87.7 \n86.8 \nWithout GSBI \n71.8 \n95.5 \n68.1 \n69.8 \n66.8 \nWithout QA&GSBI \n69.7 \n94.7 \n55.0 \n66.3 \n58.2 \nWithout QA&GSBI&SC \n24.7 \n77.4 \n14.8 \n12.3 \n12.2 \ncompared to the complete KGT, indicating that each of the 3 mod- \nules is vital for the final performance . Furthermore , such obser- \nvations confirm that our KGT can indeed le v er a ge knowledge to \nenhance the final performance of LLMs. \nImplementation settings \nOur knowledge gr a ph is quite large, with a complex schema, and \ntypicall y involv es input tokens within 1,300. Our experiment does \nnot r equir e fine-tuning, and the infer ence time is r elated to the \nmodel size and computational resources. For example, when us- \ning our method, KGT, with the Code-Llama-13B model on an 80 GB \nA100 GPU, it occupies 33 GB of VRAM. Without any acceleration \nfr ame works, the infer ence r equir es 4 passes, eac h taking ar ound \n20 seconds. \nCase studies \nDrug repositioning \nDrug r epositioning emer ges as a pr omising str ategy to acceler ate \nthe process of drug development. This approach involves identi- \nfying new therapeutic uses for existing drugs, thereby saving time \nand r esources typicall y r equir ed for bringing a ne w drug to market \n[ 45 ]. Our system is capable of investigating the potential reposi- \ntioning of carteolol for the treatment of hemangiomas . T he exam- \nple is shown in Supplementary Table S2 and r elational dia gr am is \nshown in Fig. 4 A. Utilizing the systemâ€™s knowledge gr a ph, a r ela- \ntional chain is delineated, illustrating that propranolol, another \ninhibitor of ADRB1, is effectiv el y emplo y ed in the treatment of \nhemangiomas . T he system harnesses this insight to formulate a \nhypothesis that carteolol, by virtue of its similar mechanism of \ninhibition, could be potentially repositioning for treating heman- \ngiomas [ 46 ]. This hypothesis would serve as a precursor to clin- \nical trials and r esearc h, potentiall y expediting the availability of \nan additional ther a peutic option for patients with hemangiomas. \nDrug resistance resear c h \nDrug resistance in cancer treatment poses a significant challenge \nin clinical oncology. Understanding the genetic basis of resistance \ncan lead to mor e effectiv e tr eatment str ategies and personalized \nmedicine a ppr oac hes. Researc h in drug r esistance involv es deter- \nmining why certain cancer-carrying mutated genes are not re- \nsponsive to specific drugs and finding ways to overcome this re- \nsistance [ 47 ]. Our system is capable of exploring drug resistance \nin cancer. The example is shown in Supplementary Table S3 , and \na relational diagram is shown in Fig. 4 B. The KG data indicate \nthat the ALK-p.L1196M m utation, whic h is associated with gas- \ntric cancer, has a known resistance to nalatinib [ 48 , 49 ]. The LLM \nprocesses this information and infers that due to this resistance, \nnalatinib might not be an effective medication for treating can- \ncers caused by the ALK-p.L1196M mutation. The case highlights \nthe critical importance of understanding specific geneâ€“drug in- \nteractions in drug resistance research. It demonstrates how cer- \ntain gene mutations could render a drug ineffective, which in turn \ncould guide oncologists in choosing alternative treatments or de- \nv eloping ne w drugs that can bypass or tar get the r esistance mec h- \nanisms. By accelerating the process of understanding drug resis- \ntance, these artificial intelligenceâ€“driven systems can contribute \nto impr ov ed patient outcomes and the optimization of cancer \ntr eatment pr otocols. \nIndividualized treatment \nDetails \non \nindividualized \ntr eatment \nar e \npr ovided \nin \nSupplementary Case Studies A . It is important to note that this \nexample is included solely to illustrate the technical capabilities \nof the proposed method. The output generated in this example \nhas not been validated for clinical use, and further validation in \nclinical settings would be r equir ed befor e an y suc h a pplication. \nSelection and understanding of biomarkers \nDetails on selection and understanding of biomarkers are pro- \nvided in Supplementary Case Studies B . \nDiscussion \nIn this article, we introduce a novel framework KGT, which em- \nploys LLMs for reasoning on the schema of KGs, to enhance the \nreasoning abilities of LLMs in areas with missing domain data \nby utilizing domain-specific knowledge gr a phs, suc h as oncology \nknowledge gr a phs, ther eby addr essing the issue of factual hal- \nlucinations in LLMs. Our method excels in extr acting, v alidat- \ning, and refining factual knowledge throughout the LLMsâ€™ rea- \nsoning process. It seamlessly integrates with various LLMs, in- \ncluding open-source models like Code-Llama, and enhances the \ncapabilities of LLMs solely through prompt engineering and in- \ncontext learning without any fine-tuning. This grants it significant \ngeneralizability. \nWe possess an extensive oncology knowledge graph and have \nestablished a benchmark based on it to e v aluate the ca pabilities of \nvarious methods. When tested on PcQA using various open-source \nLLMs, the KGT fr ame work performs exceptionall y well, sur passing \nthe current best methods by 33%. This significant improvement \npositions our a ppr oac h as a pioneering benchmark in biomedical \nKGQA, setting a new standard that advances beyond previously \nestablished best pr actices. Additionall y, thr ough case studies, our \na ppr oac h has been shown to effectiv el y pr ovide ther a peutic plans, \ngener ate v aluable hypotheses for drug r epositioning, identify po- \ntential drug targets, and study drug resistance . T his underscores \nthe pr actical v alue of the KGT fr ame work in deliv ering insight- \nful contributions that aid in the de v elopment and optimization \nof treatment strategies. Each case studyâ€™s conclusions are further \nv alidated by e vidence fr om pr e viousl y published r esearc h pa pers, \nenhancing the credibility and impact of our findings. \nHo w e v er, it is important to note that the constructed QA \ndataset and the corresponding published subset of the SOKG \nDownloaded from https://academic.oup.com/gigascience/article/doi/10.1093/gigascience/giae082/7943459 by guest on 11 January 2026'
 '5 Discussion\nIntegration of existing data from multiple disparate sources is\na key step in assessing the state of current knowledge. There\nare many existing efforts to create biomedical knowledge\ngraphs by integrating locally downloaded data and standard-\nizing it using a common data model (Himmelstein et al. 2017;\nFecho et al. 2021; Mayers et al. 2022; Wood et al. 2022;\nMorris et al. 2023). These efforts result in centralized knowl-\nedge graphs of substantial size, often with millions of nodes\nand tens of millions of edges.\nBioThings Explorer offers a unique strategy for data inte-\ngration, focusing on creating a federated knowledge graph by\nsemantically annotating APIs. Rather than bringing all data\ninto a massive, centralized graph database, this federated de-\nsign instead allows knowledge to remain behind each resour-\nceâ€™s API. Data are retrieved at query time by dynamically\nexecuting API calls and semantically parsing the results. This\narchitecture\nfunctionally\nseparates\ndata\ndissemination\n(through API creation) from data modeling and data integra-\ntion (through semantic annotations).\nThis approach has several advantages. First, by moving the\nrequirements for interoperability from implementation in\ncode to semantic API annotation, we significantly lower the\nbarrier to participation in our API ecosystem. Second, by sep-\narating these roles into distinct layers, we promote the overall\nmodularity of our system. These components can develop and\nevolve in parallel, and these two roles can even be undertaken\nby separate teams (e.g. one team semantically annotates an\nAPI that was created by another team). Third, this design\nfacilitates\nan\niterative\napproach\nto\nAPI\nannotation.\nDevelopers and API annotators can first provide a minimal\nset of API metadata, which can later be extended based on fu-\nture needs and use cases.\nThe federated design of BioThings Explorer also has some\nnotable\nlimitations.\nFirst,\nour\nOpenAPI\nextensions\nin\nSmartAPI to semantically annotate APIs only work on APIs\nthat follow the REST protocol and provide output in JSON\nFigure 1. A visualization of the meta-KG for BioThings Explorer. The nodes in this graph are the semantic types of biomedical entities that BioThings\nExplorer can retrieve associations between (limited to the top eight most common semantic types). The edges between nodes show what associations\nbetween biomedical entities exist in the semantic API network that is accessible through BioThings Explorer. The edge label shows the number of APIs\nthat can retrieve those types of associations, which is also represented by the edge width.\nFigure 2. Deconstruction of a query in BioThings Explorer. (A) A free-text\nrepresentation of a query that can be answered by BioThings Explorer.\n(B) The graph representation of the same query. The exact syntax of this\ngraph query is speciï¬ed in the Translator Reasoner API standard\ndescribed in Fecho et al. (2022) and shown in Supplementary Fig. S2.\n(C) The deconstruction of the graph query into multiple API calls by\nconsulting the meta-KG in the SmartAPI registry.\nBioThings Explorer\n3\nDownloaded from https://academic.oup.com/bioinformatics/article/39/9/btad570/7273783 by guest on 11 January 2026']","['Omar & Mohammed / Journal of Applied Science and Technology Trends Vol. 06, No. 02, pp. 342 â€“357 (2025) 345 record-high accuracy, while open-source models achieved impressive gains through prompt optimization. Feng et al.[22] developed the Knowledge Graph-based Thought (KGT) framework that integrated LLMs with a pan- cancer knowledge graph for biomedical question answering. KGT was designed to reason on the knowledge graph schema and identify optimal subgraphs to use for directing accurate answer generation, all without fine-tuning the LLMs. The framework is benchmarked against a new dataset (PcQA) designed specifically for pan-cancer KGQA tasks and has outperformed all existing state-of-the-art approaches by a rather large margin. KGTâ€™s practicality in biomedical issues was highlighted through case studies for drug repositioning, drug resistance, and biomarker discovery. Their approach exhibited robust adaptability among various LLMs. Rezaei et al.[26] developed AMG-RAG, a dynamic framework that utilizes autonomous LLM agents with medical search tools in the continuous construction and real-time updating of Medical Knowledge Graphs (MKGs). Their system incorporated confidence scoring and multi-hop reasoning to improve accuracy and interpretability in medical question answering. AMG-RAG outperformed size models on both very hard MEDQA benchmarks and more accessible MedMCQA ones, proving that it could conduct efficient reasoning based on current structured medical knowledge. They also used Neo4j to manage the knowledge graphs while adding external searches to ensure the latest data. Tiwari et al.[24] presented Auto-Cypher, a recent automated pipeline for producing high-quality synthetic data for training LLMs by mapping natural language to Cypher queries for graph databases like Neo4j. The pipeline deployed the novelty of LLM- as-database-filler to synthesize Neo4j databases for the execution of generated queries to ensure their correctness. A sizable dataset called SynthCypher was created, spanning multiple domains and complex queries, leading to a 40% improvement in LLM performance on Cypher generation. The datasets were used to fine-tune open-source models such as LLaMA, Mistral, and Qwen, and the SPIDER benchmark was adapted for evaluation purposes. Mohammed et al.[29] proposed a hybridized GraphRAG framework combining Neo4j-based UMLS knowledge graphs with a vector store for medical textbooks to create an improved U.S.M.L.E.-style clinical question-answering approach. The project integrated symbolic reasoning from knowledge graphs with semantic retrieval performed on text embeddings to enhance relevance and accuracy via adaptive re-ranking and query expansion. The system had the answers produced by GPT-4o- Mini, with different prompting strategies encouraging evidence- based and traceable responses grounded in verified medical knowledge. Experiments showed that the hybrid approach improved factual accuracy and citation fidelity as compared to the L.L.M.-only approach, enhancing transparency and reliability. It is shown that binding both structured and unstructured medical knowledge sources could aid in ameliorating hallucinations and hence improve clinical trustworthiness in AI-driven medical QA. Yang et al.[30] articulated sepsis knowledge graph was crafted by combining multicenter clinical data from over 10,000 patients with the help of GPT-4 for entity recognition and relationship extraction. Real-world data were collected from three hospitals and integrated with clinical guidelines and databases from the public domain. The knowledge graph contained 1,894 nodes and 2,021 relationships pertaining to diseases, symptoms, biomarkers, treatments, and complications. GPT outperformed other models in every resolution on sepsis- specific datasets to obtain high F1-score results. The constructed graph highlighted complex interactions in sepsis for assisting clinical decision-making and was implemented on Neo4j. Guan et al.[20] proposed a novel method for constructing a local knowledge graph from retrieved biomedical documents by extracting propositional claims. They carried out layer wise summarization on this graph to capture multi-document relationships and provide comprehensive contextual information to a language model for question-answering purposes. The method resolved issues in multi-document biomedical QA, such as noise cancellation and efficient context usage. They then tested their method on several benchmarks for biomedical question answering, achieving performance at least comparable to, if not better than, existing retrieval-augmented generation (RAG) baselines. The study established enhanced reasoning and answer accuracy of the model achieved through structured graph summarization. Previous studies have improved biomedical QA using KGs and LLMs, but important gaps remain. Most systems lack transparent, graph-based justifications, rely on limited evaluation methods, or depend on cloud resources that reduce privacy and reproducibility. Our framework addresses these gaps by providing visible Cypher queries with evidence subgraphs, applying comprehensive performance metrics across difficulty levels, and ensuring fully local, privacy-preserving deployment. Table I summarizes key previous studies on biomedical knowledge graphs and question answering, outlining their methods, datasets, and main limitations.']","Omar is a researcher who, along with Mohammed, authored a 2025 study published in the *Journal of Applied Science and Technology Trends* focusing on biomedical knowledge graph research. Their contributions include:

1. **Development of a Biomedical Question Answering System**:  
   Omar and Mohammed introduced a hybrid biomedical question answering (QA) framework that integrates a large language model (LLaMA-3B) with a Neo4j-based integrated Biomedical Knowledge Graph (iBKH). This system automatically generates executable Cypher queries from natural language biomedical questions and returns transparent, evidence-grounded answers supported by justification subgraphs.

2. **Evaluation Across Difficulty Levels**:  
   They designed a dataset of 60 biomedical questions categorized into three difficulty levels (simple, medium, complex) to evaluate the systemâ€™s performance. The system demonstrated high accuracy in generating Cypher queries and retrieving clinically plausible answers, with exact match rates around 96% for simple questions and about 86.7% for complex multi-hop questions.

3. **Demonstration of Practical Use Cases**:  
   Through examples and case studies, Omar and Mohammed showed the systemâ€™s ability to:  
   - Interpret plain English biomedical questions and translate them into correct graph queries.  
   - Retrieve relevant biomedical information such as symptoms of diseases, side effects of drugs, and drug interactions.  
   - Support drug repositioning hypotheses, drug resistance research, and individualized treatment planning by leveraging the knowledge graph.

4. **System Advantages**:  
   Their approach operates fully offline, enhancing privacy, compliance, and reproducibility in sensitive clinical and research contexts. It also maintains low latency suitable for interactive applications.

5. **Identified Limitations and Future Directions**:  
   They acknowledged challenges such as reliance on a static snapshot of the knowledge graph, lower recall for complex multi-hop queries, and the absence of canonical biomedical entity normalization (e.g., UMLS, SNOMED). Future work aims to integrate standardized entity normalization, enable dynamic graph updates, and incorporate domain-adapted or fine-tuned biomedical language models to improve robustness and scalability.

6. **Impact on Biomedical KGQA**:  
   Their framework, referred to as KGT, significantly advances biomedical knowledge graph question answering by enhancing the reasoning capabilities of large language models through schema-guided query generation and domain-specific knowledge integration. It outperforms previous methods by a substantial margin (e.g., surpassing prior best methods by 33% on oncology QA benchmarks).

In summary, Omarâ€™s contributions lie in developing and rigorously evaluating a novel, effective biomedical KGQA system that bridges natural language understanding and graph-based biomedical knowledge, providing a reproducible, privacy-preserving, and clinically relevant tool for biomedical research and applications.","Omar is a co-author mentioned in the context of biomedical knowledge graph research, specifically in the publication by Omar & Mohammed in the Journal of Applied Science and Technology Trends. Their work is referenced alongside other significant studies on integrating large language models with biomedical knowledge graphs for question answering, highlighting advances in frameworks that improve accuracy, reasoning, and clinical trustworthiness in medical QA systems.",1.0,0.9569194768734693,0.9999999999666667,1.0
what Cypher do for biomedical data?,"[""Omar & Mohammed / Journal of Applied Science and Technology Trends Vol. 06, No. 02, pp. 342 â€“357 (2025) \n \n352 \nThe dataset contains 60 questions divided into three difficulty \nlevels based on how complex the language is and how deep the \nbiomedical reasoning needs to be: \nâ€¢ Level 1: 25 simple questions focusing mostly on easy-to-\nunderstand facts, such as symptoms of certain diseases or \ndrugs used to treat specific conditions. \nâ€¢ Level 2: 20 medium-level questions that involve more \ndetailed relationships, such as drug interactions and \nSideEffect. \nâ€¢ Level 3: 15 hard questions requiring multi-step reasoning \nacross multiple biomedical concepts or biological \npathways, similar to the complexity found in real clinical \ncases. \nThe set of evaluation questions was designed to span a wide \nrange of common biomedical topics and to reflect clinically \nrelevant query types reported in prior literature. Each question is \npaired with its corresponding gold standard cypher query and \ncategorized by difficulty level, as summarized in Table V, where \nthree illustrative examples are shown. The questions were derived \nfrom publicly available biomedical QA benchmarks and adapted \nfrom established knowledge bases (e.g., iBKH schema relations), \nensuring both coverage and diversity across diseases, drugs, \nsymptoms, and pathways.\nTABLE V.  QUESTION DIFFICULTY LEVELS AND SAMPLE CYPHER QUERIES \nLevel \nDefinition \nExample \nCypher Query \n1 \nSingle-hop question using \none relationship \nWhat are the symptoms of \nAlzheimer? \nMATCH (d:Disease)-[:HAS_SYMPTOM]->(s:Symptom) \nWHERE toLower(d.name) CONTAINS 'alzheimer' \nRETURN s.name AS symptom \n \n2 \nQuestions requiring one \nrelationship \nWhat are the side effects of \ndrugs used to treat asthma? \nWITH toLower('asthma') AS disease_name \nMATCH (d:Disease)<-[:TREATS]-(dr:Drug) \nWHERE toLower(d.name) CONTAINS disease_name \nMATCH (dr:Drug)-[:CAUSES]->(se:SideEffect) \nRETURN DISTINCT se.name AS side_effect, dr.name AS drug \n \n3 \nQuestions requiring two or \nmore relationships \nWhat cholesterol medications \ncause side effects, and what are \nsome of those effects? \nWITH toLower('cholesterol') AS disease_name \nMATCH (d:Disease)<-[:TREATS]-(dr:Drug) \nWHERE toLower(d.name) CONTAINS disease_name \nMATCH (dr:Drug)-[:CAUSES]->(se:SideEffect) \nRETURN DISTINCT dr.name AS drug, se.name AS side_effect \n \n \n \nFig. 4. Graphical User Interface of the Biomedical Knowledge Explorer System \nB. Quantitative Evaluation \nThe performance of the proposed system is evaluated on the \niBKH knowledge graph using Exact Match, Precision@k, \nRecall@k, F1@k, Hits@k, MRR, and total response time across \nsimple, medium, and complex biomedical queries. These metrics \nwere measured for each difficulty level (simple, medium, and \ncomplex) to see how performance changes as questions become \nmore challenging.\n \nTABLE VI.  QUERY GENERATION EXACT MATCH BY DIFFICULTY LEVEL""
 'Omar & Mohammed / Journal of Applied Science and Technology Trends Vol. 06, No. 02, pp. 342 â€“357 (2025) \n \n351 \nThis query searches for a disease node whose name contains \nâ€™alzheimerâ€™ and follows HAS_SYMPTOM edges to list related \nsymptom names. The system then executes this cypher to retrieve \nanswers. The prompts (such as few-shot examples and schema \nhints) were carefully designed to help LLaMA 3 generate correct \nCypher queries. The model learns how to use the graphâ€™s labels \nand relationships properly. For example, if a user asks, â€™Which \ndrugs treat diabetes?â€™, LLaMA might respond with a suitable \nCypher query: \n \nMATCH (d:Drug)-[:TREATS]->(di:Disease) \nWHERE toLower(di.name) CONTAINS ""diabetes"" \nRETURN d.name  \n \nThis queries for drug nodes that have a TREATS edge to a \ndiabetes disease node. By leveraging LLaMA 3 in this way, our \nsystem can flexibly handle many phrasing variations without \nmanual mapping rules. \nD. Model Configuration & Decoding \nWe run a local LLaMA 3.2-3B model in GGUF format (llama-\n3.2-3b-instruct-q4_k_m.gguf) via llama.cpp, as shown in Table \nIV. \nTABLE IV.  MODEL RUNTIME AND DECODING SETTINGS \nRuntime settings \nDecoding settings \nn_ctx = 1024 \ntemperature = 0.2 \nn_threads = 12 \ntop_p = 0.95 \nn_gpu_layers = 33 \ntop_k = 40 \nn_batch = 512 \nrepeat_penalty = 1.1 \n \nmax_tokens = 80 \n \nseed = 42 \n \nE. Graph Subset and Versioning \nWe use an iBKH derived subgraph (â‰ˆ65.8k nodes; â‰ˆ3.0M \nedges) spanning DRUG, DISEASE, SYMPTOM, PATHWAY. IDs are \nnormalized to CURIEs and duplicates collapsed across \nUMLS/DrugBank/DisGeNET/SIDER/KEGG. Each edge stores \nprovenance/licensing metadata (source, source_version, license, \nretrieved_at, \nevidence_pmids/urls) \nand, \nwhen \navailable, \ntemporal fields (valid_from, valid_to). We report coverage as \nthe \npercentage \nof \nevaluated \nquestions \nwhose \ngold \nentities/relations are present. \nF. Query Execution and Reliability \nAfter a Cypher query is generated, it is executed on the Neo4j \ndatabase through the official Neo4j Python driver, which \nmanages the secure connection and returns the results. Instead of \nrestricting the output with a fixed LIMIT (e.g., LIMIT 5), the \nsystem retrieves candidate results and evaluates them using \nstandardized retrieval metrics such as Hits@1, Hits@5, and \nHits@10. This approach ensures that the system remains \nresponsive while providing a fair assessment of ranking quality \nacross different cutoff levels, rather than depending solely on a \nfixed number of returned items. Neo4jâ€™s indexing on key node \nproperties, such as name and primary identifiers, also helps speed \nup lookups as the knowledge graph grows. In cases where the \nlanguage model generates an incomplete or incorrect query, such \nas referencing nodes or relationships that do not exist, the system \ncatches the error and either retries with a simpler prompt or \ninforms the user. Together, these steps make sure that queries run \nquickly, return valid results, and keep the overall experience \nsmooth and reliable for biomedical question-answering. \nG. User Interface for Query Transparency \nThe system includes a lightweight, cross-platform graphical \nuser interface (GUI) implemented as a web application using the \nFlask framework in Python, with HTML and Bootstrap for \ninteractive visualization. The interface is designed to make the \nquestion-answering process transparent and accessible to users \nwithout technical expertise. It consists of three main panels: \n1. Input Panel: Where the user can enter a biomedical \nquestion in natural language. \n2. Query Panel: Which displays the Cypher query generated \nby the language model, allowing users to verify how their \nquestion was interpreted. \n3. Results Panel: Which presents the retrieved answers in a \nclear, readable format, accompanied by a brief natural \nlanguage explanation generated by the system. \nBy showing both the query and the answer, the GUI promotes \nuser trust and enables validation of the systemâ€™s reasoning \nprocess. The interface is lightweight enough to run smoothly on \nstandard desktop machines without additional dependencies, \nmaking it practical for local deployments in clinical or research \nsettings. Fig. 4 illustrates the overall layout. \nV. EXPERIMENTAL RESULTS \nIn this section, the proposed system is evaluated based on its \nability to translate natural-language biomedical questions into \nexecutable Cypher queries over the iBKH knowledge graph. The \nassessment is conducted end-to-end and focuses on three main \naspects: (i) the accuracy of query generation, measured by the \nproportion of correctly produced Cypher queries; (ii) system \nefficiency, quantified through total response time from question \nsubmission to final answer delivery, covering both query \ngeneration and execution; and (iii) the quality of retrieved \nanswers, evaluated using standardized information retrieval \nmetrics including Precision@k, Recall@k, F1@k, Hits@k, and \nMean Reciprocal Rank (MRR). For clarity, all these metrics are \nformally defined in the following subsections, with their \ncorresponding equations, and will be used consistently \nthroughout the results section. Together, these dimensions \nprovide a comprehensive view of both correctness and efficiency \nacross simple, medium, and complex biomedical queries. \nA. Description of the Experimental Data \nTo evaluate the proposed system, this work used a carefully \nselected set of biomedical questions designed to test how well the \nsystem understands natural language and converts it into Cypher \nqueries for a biomedical knowledge graph. \nTo evaluate the system, a benchmark of 60 biomedical \nquestions was constructed by the authors, guided by the schema \nand relations in iBKH. The questions were grouped into simple, \nmedium, and complex levels to assess performance across \ndifferent reasoning requirements. Gold-standard answers were \nmanually prepared for each question to enable quantitative \nevaluation using Exact Match, Precision@k, Recall@k, F1@k, \nHits@k, and MRR. The complete set of 60 questions is available \nat this link https://drive.google.com/drive/my-drive.'
 'D 734 \nNucleic Acids Research , 2025, Vol. 53, Database issue \nuses the whole genome / proteome background; however, it \nis recommended that users provide a custom background \nlist representing a more realistic universe of genes / proteins \ndetected by their assay ( 35 ). Alternatively, the entire sorted \ndataset can be analyzed using gene set enrichment analysis to \ndetermine whether the distribution of genes at the top, bottom \nor both ends deviates significantly from what would be ex- \npected by chance. STRING supports both of these enrichment \nmethods. The most recent update to STRING introduces vari- \nous improvements, including a new false discovery rate (FDR) \ncorrection, enhanced filtering options and a new graphical vi- \nsualization. \nFDR correction \nIn its analyses, STRING performs a test for each path- \nway (â€˜termâ€™) within a given pathway collection (â€˜categoryâ€™). \nLike many other enrichment tools, STRING employs the \nBenjaminiâ€“Hochberg correction ( 36 ) to adjust for multiple \nhypothesis testing. This statistical correction is essential for \nmanaging the FDR effectively. However, the larger the term \ncount and the greater the diversity of the terms, the higher \nthe required correction and the lower the chance of exceed- \ning the alpha level (significance threshold) for a given term. \nOne method used to address this issue involves restricting the \nanalysis to a subset of terms, such as â€˜GO slimsâ€™ that focus \non broad, high-level terms with a simplified ontology struc- \nture ( 16 ). However, this approach has some disadvantages: it \nmight exclude terms particularly relevant to a tested gene set, \nit may not cover all functions by design and it is mainly ap- \nplicable to hierarchical classification systems such as the Gene \nOntology. \nSTRING utilizes several hierarchical ontologies, such as \nthe Brenda Tissue Ontology, Disease Ontology, Human Phe- \nnotype Ontology and the aforementioned Gene Ontology; \nthey typically contain many more leaf-ward nodes than nodes \ncloser to the root. The parental nodes must contain all the \ngenes included in the child terms, creating an imbalance in \nthe ontology with many more smaller terms and fewer larger \nterms. Removing smaller terms can substantially increase sta- \ntistical power . However , simply eliminating all small terms \ncould negatively impact discoverability, especially for small \nquery gene sets where the smaller terms might be the most rel- \nevant for biological interpretation. The ontologies are usually \nlarge, with > 10 000 terms (and therefore tests) in the Biolog- \nical Process branch of the Gene Ontology tree. Such strong \ncorrections have the effect that for larger query gene sets, or \nfor backgrounds that cover only part of the proteome, it might \nnot be statistically possible to detect enrichments for a subset \nof the terms; these can therefore be removed from considera- \ntion. \nIn its new version, STRING takes a more flexible \napproachâ€”by only testing terms that have a priori any statis- \ntical potential for enrichment. This decision is based on sev- \neral parameters: the size of the term, the size of the query set, \nthe background size and the number of tests conducted. By \ncalculating the smallest and largest term sizes that could the- \noretically be enriched given the FDR correction, STRING can \ndetermine which terms are viable for inclusion in the anal- \nysis. Terms outside these bounds are excluded from testing. \nThis methodical exclusion significantly enhances the statisti- \ncal power of the enrichment analysis, without omitting any \nterms that could be enriched. For smaller query sets, this strat- \negy will maintain all terms, as even the smallest tested sets \n(term size = 2) might still be theoretically enriched. How- \never , for larger , less specific sets or for tests against a cus- \ntom smaller background, this approach markedly improves \nthe statistical power, enabling STRING to perform more fo- \ncused analyses without the drawbacks of increased false posi- \ntive rates or omitting potentially biologically interesting terms. \nAlthough this method leverages the characteristics of ontolo- \ngies, it does not require a hierarchy (parentâ€“child relationships \nbetween the terms) to function and can be applied to any \npathway / term collection with an imbalance of small terms, \nsuch as PMID-derived gene sets and STRING neighborhood \nclusters. \nTerm filtering \nOne key insight from our online user surveys is that the of- \nten overwhelming number of enrichment results can make it \nchallenging to discern the most relevant enriched terms. We \nhave thus enhanced the display of enrichment results, allow- \ning users to filter and sort them based on (i) FDR, (ii) strength, \n(iii) signal, (iv) term size and (v) term similarity. \nThe newest addition to our filtering options are the â€˜term \nsimilarityâ€™ and â€˜signalâ€™ filters. The â€˜term similarityâ€™ filter uses \nthe Jaccard index to measure the similarity between the gene \nsets of terms within the same category. The method proceeds \nby sorting the terms according to their P -values, prioritizing \nthose with the strongest statistical significance. The algorithm \nthen iterates through the list of terms; any term that exhibits \na similarity exceeding a predetermined, user-defined threshold \nrelative to terms already included in the results is subsequently \nexcluded. This approach requires no additional information \nbeyond what is already available in the dataset and can be \nuniformly applied across all categories of terms. The â€˜signalâ€™ \nfilter is defined as a weighted harmonic mean between the ra- \ntio of observed to expected gene occurrences in an enriched \nterm and its âˆ’log(FDR), respectively. This filter balances the \nimpact of both metrics and provides a more intuitive ordering \nof enriched terms and has been implemented as the default \nsorting criterion. \nThese filters are designed to exclude potentially less relevant \nresults, such as terms that are marginally significant, small or \nshow significant overlap in terms of gene content. This en- \nhancement streamlines the search for relevant observations, \nhelping users focus on the most meaningful results. \nGraphical visualization \nIn response to user feedback requesting more intuitive, graph- \nical visualizations of the detected enrichments, STRING now \nhas adopted an interactive dot plot as a primary display \nmethod for enrichment analysis outcomes. We chose the dot \nplot for its clarity in representing enriched functions across \nthree dimensions: (i) enrichment signal along the X -axis, (ii) \nthe FDR indicated through the color coding of the dots and \n(iii) the termâ€™s protein count in the network represented by the \nsize of each dot. The terms (listed on the Y -axis) are ranked \nby their enrichment signal or, if grouped by similarity, by the \nenrichment signal of their group first (Figure 2 ). This visual- \nization reflects all filter settings chosen by the user to manage \nthe volume of results displayed. These plots can be addition- \nally customized in the interface and the resulting graphs can be \ndownloaded as PNG for immediate use or as SVG for further \nmodifications. \nDownloaded from https://academic.oup.com/nar/article/53/D1/D730/7903368 by guest on 11 January 2026'
 'Omar & Mohammed / Journal of Applied Science and Technology Trends Vol. 06, No. 02, pp. 342 â€“357 (2025) \n \n353 \nDifficulty Level \nTotal Questions \nCorrect Queries \nCypher Exact Match (EM) (%) \nSimple \n25 \n24 \n96% \nMedium \n20 \n19 \n95% \nComplex \n15 \n13 \n86.7% \navg \n \n \n92.6% \n \nTo better understand the quality of the responses returned by \nthe system, this work examined real examples. One of the test \nquestions was ""What drugs are used to treat breast cancer?"" As \nshown in Fig. 4, the system was able to understand the question, \ngenerate the correct Cypher query using the TREATS \nrelationship, and retrieve the right information from the \nbiomedical knowledge graph. It returned a list of five drugs, \nincluding Leuprolide, Goserelin, and Trastuzumab, which are \nknown treatments for breast cancer. This result shows that the \nsystem is able to connect the question to the right part of the graph \nand provide accurate and helpful answers, even for medical \nquestions that involve specific treatments. \n1) \nExact Match of Query Generation \nAs shown in Table VI, how often the system generated the \ncorrect query for each difficulty level. Here, accuracy is defined \nas the percentage of questions for which the systemâ€™s generated \nCypher query matched the expected query. It is calculated using \nthe Eq. (1)     \n \nExact Match (%) = Correct Queries\nTotal Questions Ã— 100 \n(1) \n \n \nThese findings highlight the purpose of this experiment to test \nwhether the framework can reliably map natural language to \nCypher across varying levels of complexity. The graceful drop \nfrom 96% on simple to 86.7% on complex queries indicates that \nthe system is robust for straightforward questions but still \nchallenged by multi-hop reasoning. This points to clear \nopportunities for improvement, such as synonym expansion, \nconstrained decoding, or enhanced error handling for multi-step \nqueries. \n2) \nLatency \nTable VII reports the average latency per difficulty level and \ndecomposes it into query generation and Neo4j execution. The \ntotal response time is computed as in Eq. (2). Execution time is \neffectively constant across all levels ( â‰ˆ0.04â€“0.05  s), so \nvariation in total latency is dominated by query generation. As \ndifficulty increases, the mean total time rises from 5.12  s \n(simple) to 5.75 s (medium) and 7.35 s (complex). Dispersion \n(Std. Dev.) grows with task complexity 0.72 s (simple), 0.32 s \n(medium), 2.09 s (complex) reflecting more variable planning \nand reasoning needed to assemble correct Cypher for harder \nquestions. Pooled over all questions, the overall mean is 6.07 s \nwith an overall SD of 1.38 s, keeping latencies within single-digit \nseconds and practical for interactive, real-world use. \n \nð‘‡total = ð‘‡gen + ð‘‡exec   \n(2)     \nStandard Deviation (SD). Unless otherwise stated, SD is the \nunbiased sample standard deviation computed over per-question \ntotal times within each difficulty group ðº with ð‘›ðº questions: \n \nð‘‡â€¾ðº= 1\nð‘›ðº\nâˆ‘ð‘‡total\n(ð‘–)\nð‘›ðº\nð‘–=1\n,\nð‘ ðº= âˆš\n1\n\u2009ð‘›ðºâˆ’1\u2009 âˆ‘\nð‘›ðº\nð‘–=1\n(ð‘‡total\n(ð‘–) âˆ’ð‘‡â€¾ðº)\n2\u2009.\n \n \n(3)   \n   \n    (4)    \n \nOverall values are computed by pooling all questions across \nlevels, with ð‘= âˆ‘ð‘›ðº\nðº\n: \n \nð‘‡â€¾overall = 1\nð‘âˆ‘ð‘‡total\n(ð‘–)\nð‘\nð‘–=1\n,\nð‘ overall = âˆš\n1\n\u2009ð‘âˆ’1\u2009 âˆ‘\nð‘\nð‘–=1\n(ð‘‡total\n(ð‘–) âˆ’ð‘‡â€¾overall)\n2\u2009.\n \n \n(5)   \n   \n       \n(6) \n \n \nTABLE VII.  AVERAGE LATENCY (S) BY DIFFICULTY LEVEL (SD = STANDARD \nDEVIATION) \nDifficulty \nLevel \nQuery \nGeneration (s) \nQuery \nExecution (s) \nTotal \nTime (s) \nStd. \nDev. (s) \nSimple \n5.09 \n0.03 \n5.12 \n0.72 \nMedium \n5.69 \n0.01 \n5.75 \n0.32 \nComplex \n6.94 \n0.4 \n7.35 \n2.09 \nOverall \n5.9 \n0.15 \n6.07 \n1.38 \n \n \n \n \n \n3) \nAnswer-Level Evaluation Metrics \nAs shown in Table VIII, the quality of the returned items is \nevaluated using five standardized metrics: Precision@k, \nRecall@k, F1@k, Hits@k, and MRR. Precision@k measures the \nproportion of correct answers among the top-ð‘˜ retrieved items \nEq. (7), while Recall@k quantifies the fraction of gold-standard \nanswers covered within the top-ð‘˜ results Eq. (8). F1@k combines \nboth aspects through the harmonic mean Eq. (9). Hits@k reports \nwhether at least one correct answer appears in the top-ð‘˜ Eq. \n(10, 11), and MRR captures how early the first correct answer is \nretrieved in the ranking Eq. (12). Together, these metrics provide \na comprehensive view of both the correctness and completeness \nof retrieval, as well as the ranking quality across simple, medium \nand complex queries. \n \nPrecision@ð‘˜= |{\u2009relevant âˆ©retrieved@ð‘˜\u2009}|\nð‘˜\n \n(7)   \n   \nPrecision@k measures the fraction of the top-ð‘˜ retrieved \nitems that are correct (i.e., appear in the gold set). For example,'
 'Omar & Mohammed / Journal of Applied Science and Technology Trends Vol. 06, No. 02, pp. 342 â€“357 (2025) \n \n343 \nfor visualizing and validating LLM outputs [16], and MedKA for \nKG-enhanced question answering [17]. \nTo address these challenges, several recent works have \nexplored the integration of large language models with \nbiomedical knowledge graphs (KGs). A biomedical KG is a \nstructured network that connects entities such as diseases, drugs, \nand symptoms using defined relationships [18, 19]. These graphs \nstore verified medical knowledge from trusted databases, \nallowing for more accurate and explainable responses [12]. KGs \nare especially useful in multi-step reasoning tasks, where finding \nan answer requires connecting different pieces of information \n[20]. These entities and relationships can be visually represented \nin a biomedical knowledge graph, as shown in Fig. 1, where \nnodes represent medical concepts such as drugs, diseases, \nsymptoms, and pathways, and edges denote their semantic \nrelationships. \n \nFig. 1. Overview of Biomedical Knowledge Graph Entities and Relationships \n \nOne example of a widely used biomedical KG is SPOKE, \nwhich includes millions of nodes and relationships from over 40 \nbiomedical databases [12]. Integrating an LLM with a KG allows \nthe strengths of both technologies to work together: the LLM \nprovides language understanding, and the KG provides \nstructured, factual knowledge [21, 22]. A common method is \nretrieval-augmented generation (RAG), where the LLM retrieves \ninformation from the KG and uses it to generate more accurate \nresponses [13, 23]. In more advanced setups, the LLM can even \ngenerate queries like Cypher to fetch specific data from the graph \n[24, 25]. Neo4j is a popular graph database that supports fast and \nflexible storage and querying of knowledge graphs using Cypher \n[21]. It is well-suited for biomedical applications because it \nallows easy exploration of complex medical relationships. Recent \nwork has shown that combining Neo4j with LLMs can lead to \nbetter accuracy, fewer hallucinations, and more explainable \nresults [24, 26]. \nDespite improvements, building a reliable hybrid system that \ncombines an LLM with a biomedical KG remains a technical \nchallenge. Some approaches require complex pipelines or large \ntraining datasets, while others rely on fine-tuning specific to a \nnarrow set of questions [27, 28]. There is still a need for systems \nthat are both accurate and easy to scale, especially in domains like \nmedicine where the cost of errors is high [22]. \nRecent advances in KG-augmented LLMs have improved \nperformance, yet biomedical QA continues to face three practical \ngaps: \n1. Traceability: LLM-only or text-retrieval-only pipelines \nrarely provide graph-grounded justifications; users lack the \nability to inspect the exact nodes and edges that support an \nanswer. \n2. Evaluation: Prior work often judges quality via surface-\nform checks (e.g., matching a Cypher template), which fails to \ncapture end-to-end answer correctness or ranking quality \nacross different difficulty levels. \n3. Deployment: Many solutions assume cloud resources or \ndomain-specific fine-tuning, yet biomedical contexts typically \ndemand a local, privacy-preserving system with low latency \nand reproducible behavior. \nTimestamp-aware execution and periodic KG refresh help \navoid deprecated or contraindicated links, making the tool better \nsuited for safety-critical clinical contexts (e.g., drugâ€“drug \ninteractions). \nTo address these limitations, Our work introduces a locally \ndeployable pipeline that translates biomedical questions into \nexecutable Cypher queries over a Neo4j knowledge graph. The \nsystem returns answers with supporting nodes and edges, and is']","['III. PRELIMINARIES This section outlines the fundamental concepts required to understand the proposed system. It introduces biomedical knowledge graphs, explains how Neo4j stores data in graph form, and describes the use of Cypher for querying. It also provides a brief overview of large language models (LLMs) and their role in interpreting natural language. A. Biomedical Knowledge Graphs Biomedical Knowledge Graphs (BKGs) provide a structured representation of complex biomedical information by modeling diverse medical entities, such as diseases, drugs, symptoms, and biological pathways, as interconnected nodes within a graph structure. The edges in these graphs represent the semantic relationships between these entities, including â€™treatsâ€™, â€™causesâ€™, â€™interacts withâ€™ and many others, as illustrated in Fig 1. This form of representation enables the integration of heterogeneous biomedical data from a wide range of sources, including']","Based on the provided context, Cypher is used in the biomedical domain primarily as a query language to interact with biomedical knowledge graphs (KGs), such as the iBKH knowledge graph. Specifically, Cypher enables the system to:

1. **Translate Natural Language Biomedical Questions into Executable Queries:**  
   The system takes biomedical questions posed in natural language (e.g., ""What are the symptoms of Alzheimer?"" or ""What drugs are used to treat breast cancer?"") and converts them into Cypher queries. These queries are then executed on a Neo4j graph database that stores biomedical entities (diseases, drugs, symptoms, pathways) and their relationships.

2. **Retrieve Structured Biomedical Information:**  
   Cypher queries allow the system to efficiently retrieve relevant biomedical data by traversing relationships in the knowledge graph. For example, it can find symptoms linked to a disease, side effects caused by drugs, or drugs that treat a particular condition.

3. **Support Multi-step and Complex Biomedical Reasoning:**  
   Cypher can express queries involving multiple relationships and hops in the graph, enabling answers to complex biomedical questions that require connecting several concepts (e.g., drugs that treat a disease and their side effects).

4. **Enable Transparent and Explainable Querying:**  
   The system displays the generated Cypher queries to users, allowing them to verify how their questions were interpreted and how answers were derived, thus promoting trust and transparency.

5. **Facilitate Efficient and Reliable Query Execution:**  
   Cypher queries are executed on Neo4j using its Python driver, ensuring fast response times and robust error handling (e.g., retrying or simplifying queries if errors occur).

6. **Support Evaluation and Benchmarking:**  
   The system uses gold-standard Cypher queries paired with biomedical questions to quantitatively evaluate the accuracy of query generation and the quality of retrieved answers across different difficulty levels.

In summary, Cypher serves as the critical interface language that bridges natural language biomedical questions and the structured biomedical knowledge graph, enabling accurate, explainable, and efficient biomedical data retrieval and reasoning.","Cypher is used for querying biomedical knowledge graphs stored in Neo4j, enabling retrieval and manipulation of complex biomedical information represented as interconnected nodes and relationships.",0.9736842105263158,0.9561690798069988,0.8874999999778125,1.0
Wht is the role of the Journal of Applied Science and Technology Trends in biomedical data analysis?,"['Omar & Mohammed / Journal of Applied Science and Technology Trends Vol. 06, No. 02, pp. 342 â€“357 (2025) \n \n355 \nFor the question â€™What are the symptoms of brain cancer?â€™ \nThe system generated a Cypher query that correctly followed the \nHAS_SYMPTOM relationship from disease nodes to symptoms \nnodes, filtering by the specified disease name. The results \nretrieved included terms such as aphasia (Broca, Wernicke), \nanomia, agraphia, agnosia, amnesia (including retrograde \namnesia), olfaction disorders, and apnea symptoms consistent \nwith established neurological manifestations of brain tumors. In \nNeo4j, the data formed a clear center-and-spoke pattern, with \nbrain cancer at the center and its associated nodes of symptoms \nradiating outward, as shown in Fig. 5. \nCypher: \nWITH toLower(""brain cancer"") AS disease_name \nMATCH (d:Disease)-[:HAS_SYMPTOM]->(s:Symptom) \nWHERE toLower(d.name) CONTAINS disease_name \nRETURN DISTINCT s,d \nLIMIT 10; \n \n \nFig. 5. Graphical user interface displaying Cypher query and results for breast \ncancer treatment \n \n \nA second query, What are the side effects of insulin?, \nproduced a Cypher statement starting from the drug node for \ninsulin and traversing the CAUSES relationship to the connected \nSideEffect nodes. The resulting list included dizziness, diarrhea, \ncough, back pain, weakness, rash/dermatitis, and hypersensitivity \nside effects well documented in insulin safety profiles. In the \nvisual representation in Fig. 6, insulin appeared centrally \nconnected to these side effect nodes, further strengthening the \ncorrectness of the relationship mapping of the system. \n \nCypher: \nWITH toLower(""insulin"") AS drug_name  \nMATCH (d:Drug)-[:CAUSES]->(se:SideEffect)  \nWHERE toLower(d.name) CONTAINS drug_name  \nRETURN se,d \nLIMIT 20; \n \nFig. 6. Knowledge Graph Visualization of Insulin and Its Related Side Effects \n \n \nThese examples demonstrate the systemâ€™s ability to interpret \nplain English biomedical questions, generate correct Cypher \nqueries, and return results that are clinically plausible, easy to \ninterpret, and directly traceable to specific graph relationships. \nThis supports both the accuracy and the transparency of the \nproposed approach. \nTo illustrate how biomedical questions can be translated into \ngraph queries, consider the natural language question: â€™What are \nthe side effects of drugs that treat epilepsy?â€™. The following \nCypher query demonstrates how this question is mapped in the \nknowledge graph, where diseases are connected to drugs through \nthe TREATS relation and drugs are connected to side effects \nthrough the CAUSES relation. \n \nCypher: \nMATCH (d:Disease)<-[:TREATS]-(dr:Drug)  \nWHERE toLower(d.name) CONTAINS ""epilepsy"" \nMATCH (dr:Drug)-[:CAUSES]->(se:SideEffect) \nRETURN DISTINCT se,d,dr \nlimit 10'
 'Omar & Mohammed / Journal of Applied Science and Technology Trends Vol. 06, No. 02, pp. 342 â€“357 (2025) \n \n347 \nIn biomedical research and clinical settings, LLMs help \ntranslate natural language questions from doctors, researchers, or \npatients into precise, structured queries that can be executed on \nbiomedical knowledge graphs and databases. This makes it easier \nto retrieve detailed biomedical information like drug interactions, \ngene-disease associations, and symptom descriptions [42, 43]. \nDespite their power, LLMs can sometimes generate incorrect \nor fabricated responses, a phenomenon known as hallucination, \nwhich poses risks in sensitive biomedical contexts. These \nhallucinations occur because the models generate plausible \nanswers based on patterns learned from data rather than verified \nknowledge. To mitigate this, researchers integrate LLMs with \nbiomedical knowledge graphs to ground answers in factual data, \nsignificantly improving accuracy and reducing misinformation \n[4]. Further improvements come from fine-tuning LLMs on \nbiomedical corpora and carefully engineering prompts, which \nenhance their reliability and relevance in medical question \nanswering. \nAdditionally, combining LLMs with knowledge graphs and \nreasoning techniques is an active area of research that promises \nto increase the interpretability and trustworthiness of AI systems \nin biomedicine. These advances are critical for developing tools \nthat assist clinical decision-making and accelerate biomedical \ndiscoveries [43, 44]. \nIV. METHODS AND MATERIALS \nThis section describes the methodology used to build a \nbiomedical question-answer system. The proposed method \nconsists of three main stages; First, a biomedical knowledge \ngraph is constructed in the data ingestion phase, using structured \ndata sources (e.g., diseases, drugs, symptoms). Second, a \nlanguage model (LLaMA 3) interprets the userâ€™s question \nwritten in English in the user interaction phase and converts it \ninto a Cypher query. Third, a graphical user interface allows \nusers to type questions and view the results interactively. \nA. System Architecture \nThe proposed framework is organized as a step-by-step \npipeline that integrates a local large language model (LLM) with \na biomedical knowledge graph stored in Neo4j. The overall \nworkflow is illustrated in Fig. 2. Each module performs a \nspecific function, and together they ensure that the system \ndelivers accurate, reliable, and explainable answers. \n \n \nFig. 2. Workflow of the LLaMA 3 and Neo4j-Based QA System \n \nStep 1. User Input (Flask Web Interface): A user submits \na biomedical question in natural language through a Flask-based \nweb interface. The same interface will later display the answer, \nthe executed query, and a compact preview of the retrieved rows. \nStep 2. LLM Processing and Initial Cypher: The text query \nis forwarded to a local LLaMA 3 module, which interprets the \nintent and drafts an initial Cypher pattern suitable for querying \nthe graph. \nStep 3. Cypher Query Generation: The Cypher Query Gen \nblock receives the initial pattern from LLaMA 3, canonicalizes \nand completes it (projection, DISTINCT, filters), and returns a \nfinalized Cypher query to the model. \nStep 4. Query execution on Neo4j: LLaMA 3 passes the \nfinalized query to the Query execution component (inside the \nâ€™Answer modulesâ€™ box), which runs it against the Neo4j \nKnowledge Graph. KG supplies knowledge graph data (e.g.,'
 ""Omar & Mohammed / Journal of Applied Science and Technology Trends Vol. 06, No. 02, pp. 342 â€“357 (2025) \n \n352 \nThe dataset contains 60 questions divided into three difficulty \nlevels based on how complex the language is and how deep the \nbiomedical reasoning needs to be: \nâ€¢ Level 1: 25 simple questions focusing mostly on easy-to-\nunderstand facts, such as symptoms of certain diseases or \ndrugs used to treat specific conditions. \nâ€¢ Level 2: 20 medium-level questions that involve more \ndetailed relationships, such as drug interactions and \nSideEffect. \nâ€¢ Level 3: 15 hard questions requiring multi-step reasoning \nacross multiple biomedical concepts or biological \npathways, similar to the complexity found in real clinical \ncases. \nThe set of evaluation questions was designed to span a wide \nrange of common biomedical topics and to reflect clinically \nrelevant query types reported in prior literature. Each question is \npaired with its corresponding gold standard cypher query and \ncategorized by difficulty level, as summarized in Table V, where \nthree illustrative examples are shown. The questions were derived \nfrom publicly available biomedical QA benchmarks and adapted \nfrom established knowledge bases (e.g., iBKH schema relations), \nensuring both coverage and diversity across diseases, drugs, \nsymptoms, and pathways.\nTABLE V.  QUESTION DIFFICULTY LEVELS AND SAMPLE CYPHER QUERIES \nLevel \nDefinition \nExample \nCypher Query \n1 \nSingle-hop question using \none relationship \nWhat are the symptoms of \nAlzheimer? \nMATCH (d:Disease)-[:HAS_SYMPTOM]->(s:Symptom) \nWHERE toLower(d.name) CONTAINS 'alzheimer' \nRETURN s.name AS symptom \n \n2 \nQuestions requiring one \nrelationship \nWhat are the side effects of \ndrugs used to treat asthma? \nWITH toLower('asthma') AS disease_name \nMATCH (d:Disease)<-[:TREATS]-(dr:Drug) \nWHERE toLower(d.name) CONTAINS disease_name \nMATCH (dr:Drug)-[:CAUSES]->(se:SideEffect) \nRETURN DISTINCT se.name AS side_effect, dr.name AS drug \n \n3 \nQuestions requiring two or \nmore relationships \nWhat cholesterol medications \ncause side effects, and what are \nsome of those effects? \nWITH toLower('cholesterol') AS disease_name \nMATCH (d:Disease)<-[:TREATS]-(dr:Drug) \nWHERE toLower(d.name) CONTAINS disease_name \nMATCH (dr:Drug)-[:CAUSES]->(se:SideEffect) \nRETURN DISTINCT dr.name AS drug, se.name AS side_effect \n \n \n \nFig. 4. Graphical User Interface of the Biomedical Knowledge Explorer System \nB. Quantitative Evaluation \nThe performance of the proposed system is evaluated on the \niBKH knowledge graph using Exact Match, Precision@k, \nRecall@k, F1@k, Hits@k, MRR, and total response time across \nsimple, medium, and complex biomedical queries. These metrics \nwere measured for each difficulty level (simple, medium, and \ncomplex) to see how performance changes as questions become \nmore challenging.\n \nTABLE VI.  QUERY GENERATION EXACT MATCH BY DIFFICULTY LEVEL""
 'Omar & Mohammed / Journal of Applied Science and Technology Trends Vol. 06, No. 02, pp. 342 â€“357 (2025) \n \n343 \nfor visualizing and validating LLM outputs [16], and MedKA for \nKG-enhanced question answering [17]. \nTo address these challenges, several recent works have \nexplored the integration of large language models with \nbiomedical knowledge graphs (KGs). A biomedical KG is a \nstructured network that connects entities such as diseases, drugs, \nand symptoms using defined relationships [18, 19]. These graphs \nstore verified medical knowledge from trusted databases, \nallowing for more accurate and explainable responses [12]. KGs \nare especially useful in multi-step reasoning tasks, where finding \nan answer requires connecting different pieces of information \n[20]. These entities and relationships can be visually represented \nin a biomedical knowledge graph, as shown in Fig. 1, where \nnodes represent medical concepts such as drugs, diseases, \nsymptoms, and pathways, and edges denote their semantic \nrelationships. \n \nFig. 1. Overview of Biomedical Knowledge Graph Entities and Relationships \n \nOne example of a widely used biomedical KG is SPOKE, \nwhich includes millions of nodes and relationships from over 40 \nbiomedical databases [12]. Integrating an LLM with a KG allows \nthe strengths of both technologies to work together: the LLM \nprovides language understanding, and the KG provides \nstructured, factual knowledge [21, 22]. A common method is \nretrieval-augmented generation (RAG), where the LLM retrieves \ninformation from the KG and uses it to generate more accurate \nresponses [13, 23]. In more advanced setups, the LLM can even \ngenerate queries like Cypher to fetch specific data from the graph \n[24, 25]. Neo4j is a popular graph database that supports fast and \nflexible storage and querying of knowledge graphs using Cypher \n[21]. It is well-suited for biomedical applications because it \nallows easy exploration of complex medical relationships. Recent \nwork has shown that combining Neo4j with LLMs can lead to \nbetter accuracy, fewer hallucinations, and more explainable \nresults [24, 26]. \nDespite improvements, building a reliable hybrid system that \ncombines an LLM with a biomedical KG remains a technical \nchallenge. Some approaches require complex pipelines or large \ntraining datasets, while others rely on fine-tuning specific to a \nnarrow set of questions [27, 28]. There is still a need for systems \nthat are both accurate and easy to scale, especially in domains like \nmedicine where the cost of errors is high [22]. \nRecent advances in KG-augmented LLMs have improved \nperformance, yet biomedical QA continues to face three practical \ngaps: \n1. Traceability: LLM-only or text-retrieval-only pipelines \nrarely provide graph-grounded justifications; users lack the \nability to inspect the exact nodes and edges that support an \nanswer. \n2. Evaluation: Prior work often judges quality via surface-\nform checks (e.g., matching a Cypher template), which fails to \ncapture end-to-end answer correctness or ranking quality \nacross different difficulty levels. \n3. Deployment: Many solutions assume cloud resources or \ndomain-specific fine-tuning, yet biomedical contexts typically \ndemand a local, privacy-preserving system with low latency \nand reproducible behavior. \nTimestamp-aware execution and periodic KG refresh help \navoid deprecated or contraindicated links, making the tool better \nsuited for safety-critical clinical contexts (e.g., drugâ€“drug \ninteractions). \nTo address these limitations, Our work introduces a locally \ndeployable pipeline that translates biomedical questions into \nexecutable Cypher queries over a Neo4j knowledge graph. The \nsystem returns answers with supporting nodes and edges, and is'
 'Omar & Mohammed / Journal of Applied Science and Technology Trends Vol. 06, No. 02, pp. 342 â€“357 (2025) \n \n349 \nentities such as Diseases and Drugs are uploaded much faster, \ngenerally under 2 seconds. \nTABLE II.  DATA UPLOAD TIMES FOR DIFFERENT ENTITY AND RELATIONSHIP \nTYPES IN NEO4J \nEntity / Relationship Type \nUpload Time (seconds) \nDisease \n0.81 \nDrugs \n1.08 \nSymptoms \n0.06 \nSide Effects \n0.14 \nPathways \n0.08 \nDisease-Disease Relationships \n30.97 \nDrug-Disease Relationships \n30.28 \nDrug-SideEffect Relationships \n5.24 \nDrug-Drug Relationships \n190.09 \nDrug-Pathway Relationships \n0.14 \nDisease-Pathway Relationships \n0.06 \nDisease-Symptom Relationships \n0.12 \n \nFig. 3, presents a vertical bar chart that visually compares \nthese upload times across the different entity and relationship \ntypes. The chart clearly shows the significant difference in upload \nduration between nodes and edges, emphasizing the higher cost \nof ingesting complex relationships in the graph. \n3) \nExperimental Environment \nIn this study, the proposed biomedical question answering \nsystem was evaluated using a locally hosted environment. All \nexperiments were conducted on a Windows 11 Pro (64-bit) \nsystem equipped with an Intel Core i5-10500H processor running \nat 2.50 GHz (12 logical CPUs), 24 GB of RAM, and an NVIDIA \nGeForce GTX 1650 GPU with Max-Q Design. The Neo4j graph \ndatabase (v4.4.5) was managed through Neo4j Desktop (v1.6.2), \nand the LLaMA 3B language model was executed locally using \noptimized configurations suitable for this hardware setup. \n \nEach Cypher query generated by the system was executed \nmultiple times to calculate an average response time, ensuring \nconsistency across varying levels of question difficulty. The \nknowledge graph was constructed using the iBKH dataset, and \ndata loading and system performance were carefully monitored \nto maintain stability during testing. This experimental setup \nprovides a reliable and reproducible environment for \nbenchmarking the effectiveness and responsiveness of the hybrid \nQA system. \n \nFig. 3. Upload times for various biomedical entities and relationships in Neo4j. \n \nWe run LLaMA 3 locally (not via cloud APIs) to satisfy \nbiomedical privacy/governance (no data leaves the host) and to \nmaximize reproducibility (fixed GGUF checkpoint, pinned \nllama.cpp commit, controlled seeds settings, constant hardware). \nLocal execution yields predictable cost availability and stable \nlatency (no network jitter) and lets us enforce executable Cypher \ngrounding with per edge provenance and timestamp aware \nexecution. \n4) \nKnowledge Graph(KG) Construction \nThe Neo4j graph database was used as the backend to store \nand query the KG, and it is a graph database designed for highly \nconnected data. Before loading data, uniqueness constraints were \ncreated on the primary property for each node label (Disease, \nDrug, Symptom, SideE ffect, Pathway). This enforces that each \nprimary ID appears only once, preventing duplicate entities. For \nefficient lookups in queries, a search index was created based on \nthe name property of each node label. As noted in the Neo4j']","['Omar & Mohammed / Journal of Applied Science and Technology Trends Vol. 06, No. 02, pp. 342 â€“357 (2025) 348 drugs, diseases, symptoms) and execution returns structured data (tuples) relevant to the question. Step 5. Answer Synthesis: The structured tuples flow to Answer Synthesis, which aggregates and formats them into a concise raw answer. This raw answer is sent back to LLaMA 3 to optionally refine the phrasing while preserving the retrieved facts. Step 6. Result Presentation: LLaMA 3 produces the final answer, which the interface displays together with the executed Cypher query and an optional preview of the returned rows, improving transparency and trust. The pipeline couples LLM-based language understanding (LLaMA 3) with a schema-grounded Neo4j knowledge graph. The Cypher Query Gen refines the query formulation, Query Execution retrieves evidence and Answer Synthesis converts structured results into readable outputs that produce answers that are accurate, interpretable, and easy to audit directly from the displayed query and evidence.']","The role of the *Journal of Applied Science and Technology Trends* in biomedical data analysis, as reflected in the provided context, is to publish research that advances the integration of large language models (LLMs) with biomedical knowledge graphs (KGs) to improve biomedical question answering systems. Specifically, the journal features work that:

- Develops methods to translate natural language biomedical questions into precise, executable graph queries (e.g., Cypher queries) over biomedical knowledge graphs stored in Neo4j.
- Demonstrates how combining LLMs with structured biomedical KGs enhances the accuracy, transparency, and interpretability of biomedical data retrieval and analysis.
- Presents system architectures and evaluation methodologies for hybrid QA systems that support clinical and biomedical research by providing traceable, clinically plausible answers grounded in verified biomedical data.
- Addresses challenges in biomedical data analysis such as reducing hallucinations in LLM outputs, ensuring privacy and reproducibility through local deployment, and enabling multi-step reasoning across complex biomedical entities and relationships.
- Provides experimental results and system performance benchmarks that validate the effectiveness of these hybrid approaches in handling a range of biomedical queries from simple fact retrieval to complex multi-hop reasoning.

In summary, the journal serves as a platform for disseminating innovative approaches and validated systems that leverage the synergy between advanced language models and biomedical knowledge graphs to enhance biomedical data analysis, question answering, and clinical decision support.","The Journal of Applied Science and Technology Trends publishes research such as the work by Omar & Mohammed, which details a pipeline that combines LLM-based language understanding (LLaMA 3) with a schema-grounded Neo4j knowledge graph to produce accurate, interpretable, and auditable answers in biomedical data analysis.",1.0,0.9945654747955981,0.99999999998,1.0
How is the UMLS term utilized in the iBKH dataset for biomedical entities?,"['D 734 \nNucleic Acids Research , 2025, Vol. 53, Database issue \nuses the whole genome / proteome background; however, it \nis recommended that users provide a custom background \nlist representing a more realistic universe of genes / proteins \ndetected by their assay ( 35 ). Alternatively, the entire sorted \ndataset can be analyzed using gene set enrichment analysis to \ndetermine whether the distribution of genes at the top, bottom \nor both ends deviates significantly from what would be ex- \npected by chance. STRING supports both of these enrichment \nmethods. The most recent update to STRING introduces vari- \nous improvements, including a new false discovery rate (FDR) \ncorrection, enhanced filtering options and a new graphical vi- \nsualization. \nFDR correction \nIn its analyses, STRING performs a test for each path- \nway (â€˜termâ€™) within a given pathway collection (â€˜categoryâ€™). \nLike many other enrichment tools, STRING employs the \nBenjaminiâ€“Hochberg correction ( 36 ) to adjust for multiple \nhypothesis testing. This statistical correction is essential for \nmanaging the FDR effectively. However, the larger the term \ncount and the greater the diversity of the terms, the higher \nthe required correction and the lower the chance of exceed- \ning the alpha level (significance threshold) for a given term. \nOne method used to address this issue involves restricting the \nanalysis to a subset of terms, such as â€˜GO slimsâ€™ that focus \non broad, high-level terms with a simplified ontology struc- \nture ( 16 ). However, this approach has some disadvantages: it \nmight exclude terms particularly relevant to a tested gene set, \nit may not cover all functions by design and it is mainly ap- \nplicable to hierarchical classification systems such as the Gene \nOntology. \nSTRING utilizes several hierarchical ontologies, such as \nthe Brenda Tissue Ontology, Disease Ontology, Human Phe- \nnotype Ontology and the aforementioned Gene Ontology; \nthey typically contain many more leaf-ward nodes than nodes \ncloser to the root. The parental nodes must contain all the \ngenes included in the child terms, creating an imbalance in \nthe ontology with many more smaller terms and fewer larger \nterms. Removing smaller terms can substantially increase sta- \ntistical power . However , simply eliminating all small terms \ncould negatively impact discoverability, especially for small \nquery gene sets where the smaller terms might be the most rel- \nevant for biological interpretation. The ontologies are usually \nlarge, with > 10 000 terms (and therefore tests) in the Biolog- \nical Process branch of the Gene Ontology tree. Such strong \ncorrections have the effect that for larger query gene sets, or \nfor backgrounds that cover only part of the proteome, it might \nnot be statistically possible to detect enrichments for a subset \nof the terms; these can therefore be removed from considera- \ntion. \nIn its new version, STRING takes a more flexible \napproachâ€”by only testing terms that have a priori any statis- \ntical potential for enrichment. This decision is based on sev- \neral parameters: the size of the term, the size of the query set, \nthe background size and the number of tests conducted. By \ncalculating the smallest and largest term sizes that could the- \noretically be enriched given the FDR correction, STRING can \ndetermine which terms are viable for inclusion in the anal- \nysis. Terms outside these bounds are excluded from testing. \nThis methodical exclusion significantly enhances the statisti- \ncal power of the enrichment analysis, without omitting any \nterms that could be enriched. For smaller query sets, this strat- \negy will maintain all terms, as even the smallest tested sets \n(term size = 2) might still be theoretically enriched. How- \never , for larger , less specific sets or for tests against a cus- \ntom smaller background, this approach markedly improves \nthe statistical power, enabling STRING to perform more fo- \ncused analyses without the drawbacks of increased false posi- \ntive rates or omitting potentially biologically interesting terms. \nAlthough this method leverages the characteristics of ontolo- \ngies, it does not require a hierarchy (parentâ€“child relationships \nbetween the terms) to function and can be applied to any \npathway / term collection with an imbalance of small terms, \nsuch as PMID-derived gene sets and STRING neighborhood \nclusters. \nTerm filtering \nOne key insight from our online user surveys is that the of- \nten overwhelming number of enrichment results can make it \nchallenging to discern the most relevant enriched terms. We \nhave thus enhanced the display of enrichment results, allow- \ning users to filter and sort them based on (i) FDR, (ii) strength, \n(iii) signal, (iv) term size and (v) term similarity. \nThe newest addition to our filtering options are the â€˜term \nsimilarityâ€™ and â€˜signalâ€™ filters. The â€˜term similarityâ€™ filter uses \nthe Jaccard index to measure the similarity between the gene \nsets of terms within the same category. The method proceeds \nby sorting the terms according to their P -values, prioritizing \nthose with the strongest statistical significance. The algorithm \nthen iterates through the list of terms; any term that exhibits \na similarity exceeding a predetermined, user-defined threshold \nrelative to terms already included in the results is subsequently \nexcluded. This approach requires no additional information \nbeyond what is already available in the dataset and can be \nuniformly applied across all categories of terms. The â€˜signalâ€™ \nfilter is defined as a weighted harmonic mean between the ra- \ntio of observed to expected gene occurrences in an enriched \nterm and its âˆ’log(FDR), respectively. This filter balances the \nimpact of both metrics and provides a more intuitive ordering \nof enriched terms and has been implemented as the default \nsorting criterion. \nThese filters are designed to exclude potentially less relevant \nresults, such as terms that are marginally significant, small or \nshow significant overlap in terms of gene content. This en- \nhancement streamlines the search for relevant observations, \nhelping users focus on the most meaningful results. \nGraphical visualization \nIn response to user feedback requesting more intuitive, graph- \nical visualizations of the detected enrichments, STRING now \nhas adopted an interactive dot plot as a primary display \nmethod for enrichment analysis outcomes. We chose the dot \nplot for its clarity in representing enriched functions across \nthree dimensions: (i) enrichment signal along the X -axis, (ii) \nthe FDR indicated through the color coding of the dots and \n(iii) the termâ€™s protein count in the network represented by the \nsize of each dot. The terms (listed on the Y -axis) are ranked \nby their enrichment signal or, if grouped by similarity, by the \nenrichment signal of their group first (Figure 2 ). This visual- \nization reflects all filter settings chosen by the user to manage \nthe volume of results displayed. These plots can be addition- \nally customized in the interface and the resulting graphs can be \ndownloaded as PNG for immediate use or as SVG for further \nmodifications. \nDownloaded from https://academic.oup.com/nar/article/53/D1/D730/7903368 by guest on 11 January 2026'
 'Omar & Mohammed / Journal of Applied Science and Technology Trends Vol. 06, No. 02, pp. 342 â€“357 (2025) \n \n348 \ndrugs, diseases, symptoms) and execution returns structured data \n(tuples) relevant to the question. \nStep 5. Answer Synthesis: The structured tuples flow to \nAnswer Synthesis, which aggregates and formats them into a \nconcise raw answer. This raw answer is sent back to LLaMA 3 \nto optionally refine the phrasing while preserving the retrieved \nfacts. \nStep 6. Result Presentation: LLaMA 3 produces the final \nanswer, which the interface displays together with the executed \nCypher query and an optional preview of the returned rows, \nimproving transparency and trust. \nThe pipeline couples LLM-based language understanding \n(LLaMA 3) with a schema-grounded Neo4j knowledge graph. \nThe Cypher Query Gen refines the query formulation, Query \nExecution retrieves evidence and Answer Synthesis converts \nstructured results into readable outputs that produce answers that \nare accurate, interpretable, and easy to audit directly from the \ndisplayed query and evidence. \nB. Dataset and Knowledge Graph Construction \n1) \nDataset \nThe integrated Biomedical Knowledge Hub (iBKH), a large \nbiomedical knowledge base, forms the first level of the system \nand integrates information from various curated high-quality \nbiomedical databases. This implies that the data set includes \nvarious types of entities, such as diseases, symptoms, drugs, \nbiological pathways, etc. This study used the representative \nsubset of the iBKH dataset, which contained 65828 biomedical \nentities. These entities are semantically interconnected through a \ntotal of 3004166 relationships, thus creating a rich knowledge \ngraph. The iBKH dataset was originally introduced in [11], and it \nis freely available at (https://github.com/wcm-wanglab/iBKH). \nThis dataset is the core semantic foundation upon which this \nstudy is built. The knowledge graph is populated from multiple \ntabular sources (CSV files), each listing entities or relationships. \nThe main input files and their contents are as follows: \nâ€¢ Disease \nvocabulary(disease_vocab.csv): \nContains \ncolumns such as primary (a unique disease ID), name, \ndo_id (Disease Ontology ID), kegg_id, and umls_cui \n(UMLS Concept ID). Each row represents a disease node \nwith external identifiers. \nâ€¢ Drug vocabulary (drug_vocab.csv): Includes primary \n(unique drug ID), name, drugbank_id, kegg_id, \npharmgkb_id, umls_cui, mesh_id, iDISK_id and CID \n(PubChem ID). Each row defines a drug node with \nstandard database identifiers. \nâ€¢ Symptom vocabulary (symptom_vocab.csv): Contains \nprimary (unique symptom ID), name, mesh_id, umls_cui \nand iDISK_id. Each row defines a symptom node. \nâ€¢ Side \neffect \nvocabulary \n(side_effect_vocab.csv): \nIncludes primary (unique side-effect ID) and name. Each \nrow represents a side-effect node (with UMLS ID when \navailable). \nâ€¢ Pathway vocabulary (pathway_vocab.csv): Contains \nprimary (unique pathway ID), name, reactome_id, go_id, \nand kegg_id. Each row defines a biological pathway \nnode. \nRelationship files (each row typically contains two entity IDs \nand one or more boolean flags or codes) include: \nâ€¢ Diseaseâ€“Symptom links (Di_Sy_res.csv): Rows include \nDisease and Symptom IDs, a presence flag (1 or 0) and a \ndata source. If Present = 1, a HAS_SYMPTOM edge is \ncreated from the disease to the symptom, with properties \nfor presence and source. \nâ€¢ Diseaseâ€“Disease links (di_di_res.csv): Rows include \nDisease_1 and Disease_2 IDs with binary flags for is_a \nand Resemble. If is_a = 1, an (IS_A) edge is created \n(Disease_1 â†’ Disease_2); if Resemble = 1, a \nRESEMBLES edge is created. The source field is used \nfor provenance. \nâ€¢ Drugâ€“Disease links (D_Di_res.csv): Includes Drug and \nDisease IDs with several binary flags. If a flag equals 1, \na corresponding edge is created: \no \nTREATS (Treats = 1) \no \nPALLIATES (Palliates = 1) \no \nASSOCIATED_WITH (Associate = 1) \no \nALLEVIATES_REDUCES (alleviates = 1) \no \nTREATMENT_THERAPY (treatment/therapy = \n1) \no \nINHIBITS_CELL_GROWTH \n(inhibits \ncell \ngrowth = 1) \no \nHAS_BIOMARKER (biomarkers = 1) \no \nPREVENTS_SUPPRESSES \n(prevents/suppresses = 1) \no \nROLE_IN_PATHOGENESIS (role in disease \npathogenesis = 1) \nâ€¢ Drugâ€“SideEffect links (D_SE_res.csv): Contains Drug \nand SideEffect IDs with a Source column. Each row \ncreates a CAUSES edge from the drug to the side effect, \nwith source as an edge property. \nâ€¢ Drugâ€“Drug interactions (D_D_res.csv): Rows include \nDrug_1 and Drug_2 IDs with flags for Interaction and \nResemble. If Interaction = 1, an INTERACTS_WITH \nedge is created (bidirectional). If Resemble = 1, a \nRESEMBLES edge is added. \nâ€¢ Drugâ€“Pathway links (D_Pwy_res.csv): Includes Drug \nID and Pathway ID. Each row generates an \nASSOCIATED_WITH edge from the drug to the \npathway. \nâ€¢ Diseaseâ€“Pathway links (Di_Pwy_res.csv): Contains \nDisease ID and Pathway ID. Each row creates an \nASSOCIATED_WITH edge from the disease to the \npathway. \n2) \nData Upload Performance \nThe time required to upload different types of entities and \nrelationships into the Neo4j biomedical knowledge graph, \nmeasured in seconds. These measurements reflect both the size \nand complexity of the data being processed. \nAs shown in Table II, the longest upload time is for Drug-\nDrug Relationships, which takes approximately 190 seconds due \nto the large number of edges (over 3 million). Following this, \nDisease-Disease and Drug-Disease Relationships also require \nconsiderable time for loading. On the other hand, individual'
 'Omar & Mohammed / Journal of Applied Science and Technology Trends Vol. 06, No. 02, pp. 342 â€“357 (2025) \n \n346 \nscientific literature, clinical records, genomic databases, and \nexperimental findings [19, 31]. \nSuch integration creates a comprehensive biomedical \nknowledge base that supports advanced analytics and discovery. \nFor example, biomedical knowledge graphs can reveal previously \nunknown relationships (e.g., between drugs and diseases) and \nhelp prioritize potential biomarkers for complex conditions. The \nIntegrative Biomedical Knowledge Hub (iBKH) is one such \nlarge-scale graph that consolidates diverse biomedical resources \ninto a unified hub, enabling discovery at scale [11]. Beyond \niBKH, large biomedical knowledge graphs such as SPOKE \nfurther illustrate how graph integration accelerates research and \nsupports precision-medicine use cases [12]. Overall, these graphs \nserve as foundational resources for data-driven and personalized \nmedicine. These knowledge graphs serve as foundational \nresources for precision medicine, where treatment can be tailored \nto the individualâ€™s biological profile, improving outcomes and \nminimizing side effects [19, 31]. \n \nTABLE I.  SUMMARY OF RELATED RESEARCH ON BIOMEDICAL KGS AND QUESTION ANSWERING \nRef. \nYear \nData/Graph \nMethod \nBaselines \nKey Metric \nLimitation \n[11] \n2023 \niBKH (18 biomedical \nsources, Neo4j) \nIntegrative KG + Graph \nlearning; drug repurposing \ncase \nKnown drugs, \nAlzheimerâ€™s \nstudy \nPredictive performance \n(drug repurposing) \nLimited to Alzheimerâ€™s case \nstudy; scalability and updates not \ndetailed \n[19] \n2023 \nDisease KG (9,400 \nnodes, 45,000 relations \nin Neo4j) \nCypher queries for diseaseâ€“\ndrugâ€“symptom reasoning \nCross-referential \ndisease DB \nNew relation inference; \ncomplex query \nanswering \nLimited to single domain; lacks \nlarge-scale evaluation \n[3] \n2023 \nBKGs vs. GPT-3.5/4 \nComparative QA study: \nLLMs vs. KGs \nGPT-3.5, GPT-4, \nKG reasoning \nAccuracy, Novelty, \nReasoning \nGPT-4 inconsistent in \nreasoning/citations; KG less fluent \nbut more reliable \n[13] \n2024 \nSPOKE KG + \nLLaMA2, GPT-3.5, \nGPT-4 \nKG-optimized retrieval for \nLLMs (RAG) \nOther retrieval \nmethods \nAccuracy, token \nreduction >50% \nFocus on retrieval optimization, \nnot KG construction \n[23] \n2024 \nBenchmark KB datasets \nGenerate-then-retrieve \n(LLM â†’ logical form â†’ \nKB retrieval) \nPrior KBQA \nmethods \nAccuracy (highest to \ndate) \nRisk of error in logical form \ngeneration \n[6] \n2024 \nPrimeKG \nLLM + KG hybrid, Cypher \nquery validation, RAG \nGPT-4 Turbo, \nLLaMA 3 \nAccuracy, \nExplainability \nDependent on KG coverage; \ncomputationally intensive \n[22] \n2025 \nPan-cancer KG (PcQA \ndataset) \nKG-enhanced reasoning \n(subgraph selection) \nSOTA KGQA \nmethods \nOutperformed SOTA on \nPcQA \nLimited to pan-cancer focus; no \nfine-tuning explored \n[26] \n2025 \nDynamic Medical KG + \nNeo4j \nLLM agents + multi-hop \nreasoning \nMEDQA, \nMedMCQA \nbaselines \nAccuracy, \nInterpretability \nHigh system complexity; requires \ncontinuous updating \n[24] \n2025 \nSynthCypher dataset \n(Neo4j + synthetic \nqueries) \nLLM-supervised Cypher \ngeneration and verification \nSPIDER \nbenchmark \nCypher accuracy 40% \nSynthetic dataset may not capture \nall real-world cases \n[29] \n2025 \nUMLS KG + Neo4j \nHybrid GraphRAG \nLLM-only QA \nAccuracy, Citation \nfidelity \nMore complex pipeline; relies on \nexternal vector store \n[30] \n2025 \nClinical data (10k \npatients, 1,894 nodes, \nNeo4j) \nKG construction using \nGPT-4 for entity/relation \nextraction \nOther KG \nconstruction \nmethods \nHigh F1-scores \nFocus only on sepsis; limited \ngeneralization \n[20] \n2025 \nLocal KG from \nbiomedical documents \nMulti-level summarization \nover KG for QA \nRAG baselines \nQA accuracy, reasoning \nTested mainly on document QA; \nscalability not proven \n \nB. Neo4j Graph Database \nTo manage the complexity and large size of biomedical \nknowledge graphs, specialized graph databases are needed. \nNeo4j is one of the most popular graph databases designed to \nstore and query data structured as nodes (entities) and \nrelationships (edges), both of which can have descriptive \nproperties [32, 33]. It uses the property graph model, which \nmakes it easy to represent complex, connected biomedical data \nsuch as drug-gene interactions or disease pathways. Neo4jâ€™s \nCypher query language is especially advantageous because it \nallows users to write expressive and efficient queries to explore \nmulti-step connections in the data [34]. \nNeo4j works well for biomedical data because it can quickly \nrun complicated queries over highly interconnected datasets. This \nis important in biology and medicine, where relationships \nbetween entities like proteins, diseases, and drugs are often \ncomplex and layered. Studies have shown that Neo4j handles \nlarge biomedical graphs efficiently, making it a favorite among \nresearchers and industry users alike [33, 35, 36]. Its indexing and \ncaching mechanisms also help speed up query processing and \ndata retrieval [37]. \nMoreover, \nNeo4j \nintegrates \nsmoothly \nwith \nmany \nprogramming languages and analytics tools, which makes it \neasier to build interactive biomedical applications and clinical \ndecision support systems that can turn complex graph data into \nuseful insights [38, 39]. \nC. Large Language Models (LLMs) in Biomedical Question \nAnswering \nLarge Language Models (LLMs) are powerful AI systems \ntrained on vast amounts of text data. They learn the structure and \npatterns of language, enabling them to understand questions, \ngenerate responses, summarize information, and perform other \ncomplex language tasks. Well-known models such as LLaMA \nand GPT-3 have greatly advanced the field of natural language \nprocessing by showing strong performance across many tasks \n[40, 41].'
 ""integrates over 40 publicly available biomedical knowledge \nsources across separate domains, such as genes, proteins, \ndrugs, compounds, and diseases, along with their known \nrelationships.32 Two studies used textbooks, such as Harri\xad\nson's Principles of Internal Medicine, while 3 others utilized \nelectronic health record (EHR) data. Additionally, Zakka \net al. added over 500 markdown files from MDCalc to \nimprove clinical calculation capabilities in LLM.28 Two stud\xad\nies employed real-time online browsing to search academic \nsites, such as PubMed and UpToDate. The amount of \nretrieval resources varied across studies, ranging from a small \ndataset specific to 6 osteoarthritis guidelines to a large dataset \nof EHR data from 7 hospitals.\nRetrieval strategies\nIdentified retrieval strategies were grouped based on the RAG \nstages: pre-retrieval, retrieval, and post-retrieval. Figure 3 \npresents an example of how RAG is applied and lists identi\xad\nfied strategies within each stage.\nIn the pre-retrieval stage, 50% of studies (n Â¼ 10) reported \nstrategies, such as query rewriting, document preprocessing, \nand assessing the necessity of retrieval. Zakka et al. simplified \nqueries by rephrasing text into search terms that are better \nsuited for website browsing,28 while Wang et al. focused on \ntechniques such as correcting errors, expanding abbrevia\xad\ntions, and matching synonyms in user queries.30 Soman et al. \nextracted disease entities in queries and retrieved correspond\xad\ning nodes from a knowledge graph.33 Document preprocess\xad\ning involved removing non-textual elements from PMC \npapers (eg, figures, references, and author disclosures),30\nextracted tables from PDFs using pdfplumber, structured the \ncontent with pydantic for seamless integration.25 In addition \nto query modification and document preprocessing, Jeong \net al. fine-tuned a model to determine whether retrieval was \nnecessary for a given query.34\nDuring the data retrieval stage, 85% of studies (n Â¼ 17) \nreported strategies regarding indexing, aligning queries with \ndocuments, and ranking retrieval chunks. Chunking methods \nranged from fixed-size chunks35 to recursive splits.36 Embed\xad\nding models such as Text-embedding-ada-002,24,28â€“30,36,37\nMiniLM, and PubMedBERT33 were commonly used to con\xad\nvert sentences into vectors. Cosine similarity was the primary \nmetric for measuring query-document alignment. Two stud\xad\nies adopted Maximal Marginal Relevance for search and \nhighlighted its improved performance over similarity-based \nmethods.24,35 A domain-specific retriever, MedCPT, was \nused in one study.34 Another study used the multi-vector \nretriever that leveraged summarized document sections to \nidentify the original content for final answer generation.25\nThe retrieval cutoff parameters varied widely, with probabil\xad\nity thresholds up to 0.83 and the number of retrieved chunks \nranging from 3 to 90.28,36,38 Vector databases like FAISS and \nChroma were frequently reported, and LangChain was \nwidely used for document processing and retrieval.23,25,35,38\nIn the subgroup analysis, 12 studies used simple data retrieval \nstrategies (OR 1.30, 95% CI [1.16, 1.45]), while 5 studies \nused complex data retrieval strategies (OR 1.30, 95% CI \n[1.07, 1.24]), with no statistically significant difference \nobserved between the 2 approaches.\nIn the post-retrieval stage, 65% of studies (n Â¼ 13) imple\xad\nmented specific strategies to refine outputs. Murugan et al. \ntailored prompts by providing clear context, defining roles \n(eg, distinguishing between healthcare providers and patients \nto deliver appropriately detailed information), and incorpo\xad\nrating relevant citations from retrieval sources such as the \nClinical \nPharmacogenetics \nImplementation \nConsortium \nguidelines and Food and Drug Administration (FDA) label\xad\ning.24 Soman et al. utilized prompt engineering to integrate \naccurate knowledge sources and statistical evidence, such as \nP-values and z-scores, from the SPOKE knowledge graph \ninto their outputs.33 Wang et al. outlined a detailed process \nin the post-retrieval stage using prompt engineering, which \ninvolved decomposing retrieved text into individual claims, \nverifying each claim with external knowledge sources, \nFigure 3. An example of using RAG in clinical applications, with identified strategies in 3 stages: pre-retrieval, retrieval, and post-retrieval. The user query \nseeks recommended medications for symptomatic peripheral artery disease without increased bleeding risk. The system retrieves evidence from clinical \nguidelines, processes it through embeddings and a vector database, and outputs a response, including rivaroxaban with low-dose aspirin, as recommended \nby the retrieved guideline information. In this example, GPT-4 suggested a dose of 75 mg Aspirin, but the common low-dose Aspirin is 81 mg.\nJournal of the American Medical Informatics Association, 2025, Vol. 32, No. 4                                                                                                      \n609 \nDownloaded from https://academic.oup.com/jamia/article/32/4/605/7954485 by guest on 11 January 2026""
 ""Omar & Mohammed / Journal of Applied Science and Technology Trends Vol. 06, No. 02, pp. 342 â€“357 (2025) \n \n352 \nThe dataset contains 60 questions divided into three difficulty \nlevels based on how complex the language is and how deep the \nbiomedical reasoning needs to be: \nâ€¢ Level 1: 25 simple questions focusing mostly on easy-to-\nunderstand facts, such as symptoms of certain diseases or \ndrugs used to treat specific conditions. \nâ€¢ Level 2: 20 medium-level questions that involve more \ndetailed relationships, such as drug interactions and \nSideEffect. \nâ€¢ Level 3: 15 hard questions requiring multi-step reasoning \nacross multiple biomedical concepts or biological \npathways, similar to the complexity found in real clinical \ncases. \nThe set of evaluation questions was designed to span a wide \nrange of common biomedical topics and to reflect clinically \nrelevant query types reported in prior literature. Each question is \npaired with its corresponding gold standard cypher query and \ncategorized by difficulty level, as summarized in Table V, where \nthree illustrative examples are shown. The questions were derived \nfrom publicly available biomedical QA benchmarks and adapted \nfrom established knowledge bases (e.g., iBKH schema relations), \nensuring both coverage and diversity across diseases, drugs, \nsymptoms, and pathways.\nTABLE V.  QUESTION DIFFICULTY LEVELS AND SAMPLE CYPHER QUERIES \nLevel \nDefinition \nExample \nCypher Query \n1 \nSingle-hop question using \none relationship \nWhat are the symptoms of \nAlzheimer? \nMATCH (d:Disease)-[:HAS_SYMPTOM]->(s:Symptom) \nWHERE toLower(d.name) CONTAINS 'alzheimer' \nRETURN s.name AS symptom \n \n2 \nQuestions requiring one \nrelationship \nWhat are the side effects of \ndrugs used to treat asthma? \nWITH toLower('asthma') AS disease_name \nMATCH (d:Disease)<-[:TREATS]-(dr:Drug) \nWHERE toLower(d.name) CONTAINS disease_name \nMATCH (dr:Drug)-[:CAUSES]->(se:SideEffect) \nRETURN DISTINCT se.name AS side_effect, dr.name AS drug \n \n3 \nQuestions requiring two or \nmore relationships \nWhat cholesterol medications \ncause side effects, and what are \nsome of those effects? \nWITH toLower('cholesterol') AS disease_name \nMATCH (d:Disease)<-[:TREATS]-(dr:Drug) \nWHERE toLower(d.name) CONTAINS disease_name \nMATCH (dr:Drug)-[:CAUSES]->(se:SideEffect) \nRETURN DISTINCT dr.name AS drug, se.name AS side_effect \n \n \n \nFig. 4. Graphical User Interface of the Biomedical Knowledge Explorer System \nB. Quantitative Evaluation \nThe performance of the proposed system is evaluated on the \niBKH knowledge graph using Exact Match, Precision@k, \nRecall@k, F1@k, Hits@k, MRR, and total response time across \nsimple, medium, and complex biomedical queries. These metrics \nwere measured for each difficulty level (simple, medium, and \ncomplex) to see how performance changes as questions become \nmore challenging.\n \nTABLE VI.  QUERY GENERATION EXACT MATCH BY DIFFICULTY LEVEL""]","['B. Dataset and Knowledge Graph Construction 1) Dataset The integrated Biomedical Knowledge Hub (iBKH), a large biomedical knowledge base, forms the first level of the system and integrates information from various curated high-quality biomedical databases. This implies that the data set includes various types of entities, such as diseases, symptoms, drugs, biological pathways, etc. This study used the representative subset of the iBKH dataset, which contained 65828 biomedical entities. These entities are semantically interconnected through a total of 3004166 relationships, thus creating a rich knowledge graph. The iBKH dataset was originally introduced in [11], and it is freely available at (https://github.com/wcm-wanglab/iBKH). This dataset is the core semantic foundation upon which this study is built. The knowledge graph is populated from multiple tabular sources (CSV files), each listing entities or relationships. The main input files and their contents are as follows: â€¢ Disease vocabulary(disease_vocab.csv): Contains columns such as primary (a unique disease ID), name, do_id (Disease Ontology ID), kegg_id, and umls_cui (UMLS Concept ID). Each row represents a disease node with external identifiers. â€¢ Drug vocabulary (drug_vocab.csv): Includes primary (unique drug ID), name, drugbank_id, kegg_id, pharmgkb_id, umls_cui, mesh_id, iDISK_id and CID (PubChem ID). Each row defines a drug node with standard database identifiers. â€¢ Symptom vocabulary (symptom_vocab.csv): Contains primary (unique symptom ID), name, mesh_id, umls_cui and iDISK_id. Each row defines a symptom node. â€¢ Side effect vocabulary (side_effect_vocab.csv): Includes primary (unique side-effect ID) and name. Each row represents a side-effect node (with UMLS ID when available). â€¢ Pathway vocabulary (pathway_vocab.csv): Contains primary (unique pathway ID), name, reactome_id, go_id, and kegg_id. Each row defines a biological pathway node. Relationship files (each row typically contains two entity IDs and one or more boolean flags or codes) include: â€¢ Diseaseâ€“Symptom links (Di_Sy_res.csv): Rows include Disease and Symptom IDs, a presence flag (1 or 0) and a data source. If Present = 1, a HAS_SYMPTOM edge is created from the disease to the symptom, with properties for presence and source. â€¢ Diseaseâ€“Disease links (di_di_res.csv): Rows include Disease_1 and Disease_2 IDs with binary flags for is_a and Resemble. If is_a = 1, an (IS_A) edge is created (Disease_1 â†’ Disease_2); if Resemble = 1, a RESEMBLES edge is created. The source field is used for provenance. â€¢ Drugâ€“Disease links (D_Di_res.csv): Includes Drug and Disease IDs with several binary flags. If a flag equals 1, a corresponding edge is created: o TREATS (Treats = 1) o PALLIATES (Palliates = 1) o ASSOCIATED_WITH (Associate = 1) o ALLEVIATES_REDUCES (alleviates = 1) o TREATMENT_THERAPY (treatment/therapy = 1) o INHIBITS_CELL_GROWTH (inhibits cell growth = 1) o HAS_BIOMARKER (biomarkers = 1) o PREVENTS_SUPPRESSES (prevents/suppresses = 1) o ROLE_IN_PATHOGENESIS (role in disease pathogenesis = 1) â€¢ Drugâ€“SideEffect links (D_SE_res.csv): Contains Drug and SideEffect IDs with a Source column. Each row creates a CAUSES edge from the drug to the side effect, with source as an edge property. â€¢ Drugâ€“Drug interactions (D_D_res.csv): Rows include Drug_1 and Drug_2 IDs with flags for Interaction and Resemble. If Interaction = 1, an INTERACTS_WITH edge is created (bidirectional). If Resemble = 1, a RESEMBLES edge is added. â€¢ Drugâ€“Pathway links (D_Pwy_res.csv): Includes Drug ID and Pathway ID. Each row generates an ASSOCIATED_WITH edge from the drug to the pathway. â€¢ Diseaseâ€“Pathway links (Di_Pwy_res.csv): Contains Disease ID and Pathway ID. Each row creates an ASSOCIATED_WITH edge from the disease to the pathway. 2) Data Upload Performance The time required to upload different types of entities and relationships into the Neo4j biomedical knowledge graph, measured in seconds. These measurements reflect both the size and complexity of the data being processed. As shown in Table II, the longest upload time is for Drug- Drug Relationships, which takes approximately 190 seconds due to the large number of edges (over 3 million). Following this, Disease-Disease and Drug-Disease Relationships also require considerable time for loading. On the other hand, individual']","The UMLS term (specifically, the UMLS Concept Unique Identifier or UMLS CUI) is utilized in the iBKH dataset as a standardized identifier for biomedical entities across various vocabularies. In the iBKH dataset, entities such as diseases, drugs, and symptoms include a column for the UMLS CUI in their respective vocabulary files (e.g., disease_vocab.csv, drug_vocab.csv, symptom_vocab.csv). This inclusion of UMLS CUIs allows for consistent and unified representation of biomedical concepts, facilitating integration and interoperability across multiple biomedical databases and resources within the knowledge graph.

In summary, the UMLS term in the iBKH dataset serves as a key external identifier that links biomedical entities to a widely recognized biomedical ontology, enhancing semantic integration and enabling more effective querying and reasoning over the knowledge graph.","In the iBKH dataset, the UMLS Concept Unique Identifier (UMLS CUI) is used as an external identifier for multiple biomedical entities. Specifically, it appears as a column in the disease vocabulary, drug vocabulary, and symptom vocabulary files, linking each entity to its corresponding UMLS concept.",0.4666666666666667,0.9606212083795082,0.49999999995,1.0
How does the KGT framework leverage biomedical knowledge graphs to enhance large language models' reasoning capabilities and improve biomedical question answering performance?,"['Omar & Mohammed / Journal of Applied Science and Technology Trends Vol. 06, No. 02, pp. 342 â€“357 (2025) \n \n345 \nrecord-high accuracy, while open-source models achieved \nimpressive gains through prompt optimization. \nFeng et al.[22] developed the Knowledge Graph-based \nThought (KGT) framework that integrated LLMs with a pan-\ncancer knowledge graph for biomedical question answering. \nKGT was designed to reason on the knowledge graph schema and \nidentify optimal subgraphs to use for directing accurate answer \ngeneration, all without fine-tuning the LLMs. The framework is \nbenchmarked against a new dataset (PcQA) designed specifically \nfor pan-cancer KGQA tasks and has outperformed all existing \nstate-of-the-art approaches by a rather large margin. KGTâ€™s \npracticality in biomedical issues was highlighted through case \nstudies for drug repositioning, drug resistance, and biomarker \ndiscovery. Their approach exhibited robust adaptability among \nvarious LLMs. \nRezaei et al.[26] developed AMG-RAG, a dynamic \nframework that utilizes autonomous LLM agents with medical \nsearch tools in the continuous construction and real-time updating \nof Medical Knowledge Graphs (MKGs). Their system \nincorporated confidence scoring and multi-hop reasoning to \nimprove accuracy and interpretability in medical question \nanswering. AMG-RAG outperformed size models on both very \nhard MEDQA benchmarks and more accessible MedMCQA \nones, proving that it could conduct efficient reasoning based on \ncurrent structured medical knowledge. They also used Neo4j to \nmanage the knowledge graphs while adding external searches to \nensure the latest data. \nTiwari et al.[24] presented Auto-Cypher, a recent automated \npipeline for producing high-quality synthetic data for training \nLLMs by mapping natural language to Cypher queries for graph \ndatabases like Neo4j. The pipeline deployed the novelty of LLM-\nas-database-filler to synthesize Neo4j databases for the execution \nof generated queries to ensure their correctness. A sizable dataset \ncalled SynthCypher was created, spanning multiple domains and \ncomplex queries, leading to a 40% improvement in LLM \nperformance on Cypher generation. The datasets were used to \nfine-tune open-source models such as LLaMA, Mistral, and \nQwen, and the SPIDER benchmark was adapted for evaluation \npurposes. \nMohammed et al.[29] proposed a hybridized GraphRAG \nframework combining Neo4j-based UMLS knowledge graphs \nwith a vector store for medical textbooks to create an improved \nU.S.M.L.E.-style clinical question-answering approach. The \nproject integrated symbolic reasoning from knowledge graphs \nwith semantic retrieval performed on text embeddings to enhance \nrelevance and accuracy via adaptive re-ranking and query \nexpansion. The system had the answers produced by GPT-4o-\nMini, with different prompting strategies encouraging evidence-\nbased and traceable responses grounded in verified medical \nknowledge. Experiments showed that the hybrid approach \nimproved factual accuracy and citation fidelity as compared to \nthe L.L.M.-only approach, enhancing transparency and \nreliability. It is shown that binding both structured and \nunstructured medical knowledge sources could aid in \nameliorating hallucinations and hence improve clinical \ntrustworthiness in AI-driven medical QA. \nYang et al.[30] articulated sepsis knowledge graph was \ncrafted by combining multicenter clinical data from over 10,000 \npatients with the help of GPT-4 for entity recognition and \nrelationship extraction. Real-world data were collected from \nthree hospitals and integrated with clinical guidelines and \ndatabases from the public domain. The knowledge graph \ncontained 1,894 nodes and 2,021 relationships pertaining to \ndiseases, symptoms, biomarkers, treatments, and complications. \nGPT outperformed other models in every resolution on sepsis-\nspecific datasets to obtain high F1-score results. The constructed \ngraph highlighted complex interactions in sepsis for assisting \nclinical decision-making and was implemented on Neo4j. \nGuan et al.[20] proposed a novel method for constructing a \nlocal knowledge graph from retrieved biomedical documents by \nextracting propositional claims. They carried out layer wise \nsummarization on this graph to capture multi-document \nrelationships and provide comprehensive contextual information \nto a language model for question-answering purposes. The \nmethod resolved issues in multi-document biomedical QA, such \nas noise cancellation and efficient context usage. They then tested \ntheir method on several benchmarks for biomedical question \nanswering, achieving performance at least comparable to, if not \nbetter than, existing retrieval-augmented generation (RAG) \nbaselines. The study established enhanced reasoning and answer \naccuracy of the model achieved through structured graph \nsummarization.   \nPrevious studies have improved biomedical QA using KGs \nand LLMs, but important gaps remain. Most systems lack \ntransparent, graph-based justifications, rely on limited evaluation \nmethods, or depend on cloud resources that reduce privacy and \nreproducibility. Our framework addresses these gaps by \nproviding visible Cypher queries with evidence subgraphs, \napplying comprehensive performance metrics across difficulty \nlevels, and ensuring fully local, privacy-preserving deployment. \nTable I summarizes key previous studies on biomedical \nknowledge graphs and question answering, outlining their \nmethods, datasets, and main limitations. \nIII. PRELIMINARIES \nThis section outlines the fundamental concepts required to \nunderstand the proposed system. It introduces biomedical \nknowledge graphs, explains how Neo4j stores data in graph \nform, and describes the use of Cypher for querying. It also \nprovides a brief overview of large language models (LLMs) and \ntheir role in interpreting natural language. \n \n \nA. Biomedical Knowledge Graphs \nBiomedical Knowledge Graphs (BKGs) provide a structured \nrepresentation of complex biomedical information by modeling \ndiverse medical entities, such as diseases, drugs, symptoms, and \nbiological pathways, as interconnected nodes within a graph \nstructure. The edges in these graphs represent the semantic \nrelationships between these entities, including â€™treatsâ€™, â€™causesâ€™, \nâ€™interacts withâ€™ and many others, as illustrated in Fig 1. This \nform of representation enables the integration of heterogeneous \nbiomedical data from a wide range of sources, including'
 'Geesa Daluwatumulle, Rupika Wijesinghe, and Ruvan\nWeerasinghe. 2023. In silico drug repurposing us-\ning knowledge graph embeddings for alzheimerâ€™s\ndisease. In Proceedings of the 9th International Con-\nference on Bioinformatics Research and Applications,\nICBRA â€™22, page 61â€“66, New York, NY, USA. As-\nsociation for Computing Machinery.\nPreetha Datta, Fedor Vitiugin, Anastasiia Chizhikova,\nand Nitin Sawhney. 2024. Construction of hyper-\nrelational knowledge graphs using pre-trained large\nlanguage models. arXiv preprint arXiv:2403.11786.\nStefan Dernbach, Khushbu Agarwal, Alejandro Zu-\nniga, Michael Henry, and Sutanay Choudhury. 2024.\nGlam: Fine-tuning large language models for domain\nknowledge graph alignment via neighborhood par-\ntitioning and generative subgraph encoding. arXiv\npreprint arXiv:2402.06764.\nTianqing Fang,\nHongming Zhang,\nWeiqi Wang,\nYangqiu Song, and Bin He. 2021. Discos: bridg-\ning the gap between discourse knowledge and com-\nmonsense knowledge. In Proceedings of the Web\nConference 2021, pages 2648â€“2659.\nYingjie Feng, Xiaoyin Xu, Yueting Zhuang, and Min\nZhang. 2023.\nLarge language models improve\nalzheimerâ€™s disease diagnosis using multi-modality\ndata. In 2023 IEEE International Conference on\nMedical Artificial Intelligence (MedAI), pages 61â€“66.\nIEEE.\nJiuzhou Han, Nigel Collier, Wray Buntine, and Ehsan\nShareghi. 2023. Pive: Prompting with iterative verifi-\ncation improving graph-based generative capability\nof llms. arXiv preprint arXiv:2305.12392.\nDan Hendrycks, Collin Burns, Steven Basart, Andy Zou,\nMantas Mazeika, Dawn Song, and Jacob Steinhardt.\n2021. Measuring massive multitask language under-\nstanding. In International Conference on Learning\nRepresentations.\nDaniel Scott Himmelstein, Antoine Lizee, Christine\nHessler, Leo Brueggeman, Sabrina L Chen, Dexter\nHadley, Ari Green, Pouya Khankhanian, and Ser-\ngio E Baranzini. 2017. Systematic integration of\nbiomedical knowledge prioritizes drugs for repurpos-\ning. Elife, 6:e26726.\nKang-Lin Hsieh, German Plascencia-Villa, Ko-Hong\nLin, George Perry, Xiaoqian Jiang, and Yejin Kim.\n2023. Synthesize heterogeneous biological knowl-\nedge via representation learning for alzheimerâ€™s dis-\nease drug repurposing. iScience, 26(1):105678.\nEdward J Hu, Phillip Wallis, Zeyuan Allen-Zhu,\nYuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen,\net al. 2021. Lora: Low-rank adaptation of large lan-\nguage models. In International Conference on Learn-\ning Representations.\nJinhao Jiang, Kun Zhou, Zican Dong, Keming Ye,\nWayne Xin Zhao, and Ji-Rong Wen. 2023. Structgpt:\nA general framework for large language model to\nreason over structured data. In Proceedings of the\n2023 Conference on Empirical Methods in Natural\nLanguage Processing, pages 9237â€“9251.\nDi Jin, Eileen Pan, Nassim Oufattole, Wei-Hung Weng,\nHanyi Fang, and Peter Szolovits. 2021. What disease\ndoes this patient have? a large-scale open domain\nquestion answering dataset from medical exams. Ap-\nplied Sciences, 11(14):6421.\nQiao Jin, Bhuwan Dhingra, Zhengping Liu, William W\nCohen, and Xinghua Lu. 2019. Pubmedqa: A dataset\nfor biomedical research question answering. arXiv\npreprint arXiv:1909.06146.\nNikhil Kandpal, Haikang Deng, Adam Roberts, Eric\nWallace, and Colin Raffel. 2023. Large language\nmodels struggle to learn long-tail knowledge. In In-\nternational Conference on Machine Learning, pages\n15696â€“15707. PMLR.\nJiho Kim, Yeonsu Kwon, Yohan Jo, and Edward Choi.\n2023. Kg-gpt: A general framework for reasoning\non knowledge graphs using large language models.\nIn Findings of the Association for Computational\nLinguistics: EMNLP 2023, pages 9410â€“9421.\nYanis Labrak, Adrien Bazoge, Emmanuel Morin, Pierre-\nAntoine Gourraud, Mickael Rouvier, and Richard\nDufour. 2024.\nBiomistral: A collection of open-\nsource pretrained large language models for medical\ndomains. arXiv preprint arXiv:2402.10373.\nDawei Li, Yanran Li, Jiayi Zhang, Ke Li, Chen Wei,\nJianwei Cui, and Bin Wang. 2022. C3kg: A chi-\nnese commonsense conversation knowledge graph.\nIn Findings of the Association for Computational\nLinguistics: ACL 2022, pages 1369â€“1383.\nDawei Li, Yaxuan Li, Dheeraj Mekala, Shuyao\nLi, Xueqi Wang, William Hogan, Jingbo Shang,\net al. 2023a.\nDail:\nData augmentation for in-\ncontext learning via self-paraphrase. arXiv preprint\narXiv:2311.03319.\nDawei Li, Zhen Tan, Tianlong Chen, and Huan Liu.\n2024. Contextualization distillation from large lan-\nguage model for knowledge graph completion. arXiv\npreprint arXiv:2402.01729.\nDawei Li, Hengyuan Zhang, Yanran Li, and Shiping\nYang. 2023b. Multi-level contrastive learning for\nscript-based character understanding. In Proceedings\nof the 2023 Conference on Empirical Methods in\nNatural Language Processing, pages 5995â€“6013.\nRumeng Li, Xun Wang, and Hong Yu. 2023c. Two\ndirections for clinical data generation with large lan-\nguage models: Data-to-label and label-to-data. In\nProceedings of the 2023 Conference on Empirical\nMethods in Natural Language Processing, pages\n7129â€“7143.\nYucheng Li, Bo Dong, Frank Guerin, and Chenghua Lin.\n2023d. Compressing context to enhance inference\nefficiency of large language models. In Proceedings'
 'Omar & Mohammed / Journal of Applied Science and Technology Trends Vol. 06, No. 02, pp. 342 â€“357 (2025) \n \n344 \nevaluated using Exact Match, Precision, Recall, F1, Hits@k, \nMRR, and latency across simple, medium, and complex question \nsets. Unlike prior template-based methods, our approach enables \ntraceable, outcome-level validation. In summary, the main \ncontributions of this work are as follows: \nâ€¢ Hybrid LLM to Cypher QA: A system that translates \nnatural language questions into accurate, executable \nCypher over a Neo4j drug, disease, and symptom KG. \nâ€¢ Prompt-driven query generation: Schema,  entity \naware prompting that reliably maps diverse biomedical \nquestions to graph queries. \nâ€¢ Evidence transparency: Along with each answer, we \nsurface the generated Cypher and the supporting \njustification subgraph (nodes, edges) plus a brief natural \nlanguage rationale. \nâ€¢ Answer-level evaluation: End-to-end assessment using \nExact Match, F1, Precision/Recall, Hits@k, MRR and \nlatency across simple, medium and complex tiers. \nâ€¢ Local, reproducible deployment: On-prem LLaMA 3 \ninference (no cloud dependency) suitable for biomedical \nsettings requiring low latency and strong data control. \n \nThe remainder of this paper is organized as follows: Section \n2 reviews related work on biomedical knowledge graphs and \nLLM-based QA systems. Section 3 provides background on \nknowledge graphs, large language models, and question \nanswering frameworks. Section 4 details the proposed \nmethodology, \nincluding \nsystem \narchitecture, \ndataset \nconstruction, and query translation. Section 5 presents the \nexperimental results through both quantitative metrics and \nqualitative case studies. Section 6 discusses the findings, analyzes \nlimitations, and compares performance against baseline models. \nFinally, Section 7 concludes this paper and outlines directions for \nfuture work. \nII. RELATED WORK \nRecently, studies have concentrated on the integration of \nclinical and medical knowledge graphs (LLM) to improve the \nanswer to medical questions. Researchers have derived several \nbiomedical KGs using Neo4j and incorporated the application of \nLLMs like LLaMA and GPT to convert natural language \nquestions into graph queries. Improvements in answer \ncorrectness, reduction of hallucination errors, one-to-many \nrelationships, and support for complex reasoning were the \nobjectives of these efforts. Some frameworks also adopted \nretrieval methods to ground responses in secure data. \nSu et al.[11] developed an integrative Biomedical Knowledge \nHub (iBKH), a huge biomedical knowledge graph that comprises \n18 of the very best data sources. The deployment of the iBKH in \nNeo4j allows for a user-friendly web portal to allow fast and \ninteractive knowledge retrieval. The system implemented \nadvanced graph learning techniques to enable the discovery of \nbiomedical knowledge, illustrated by an example of repurposing \nin silico drugs for Alzheimerâ€™s disease. iBKH achieved \npromising predictive performance for known drugs and proposed \npossible new drug candidates. \nRajabi and Kafaie[19] proposed a disease knowledge graph \nusing a cross-referential disease database comprising diseases, \nsymptoms, and drugs interconnected with relationships. They \ntransferred the data into Neo4j to create a graph of 9,400 nodes \nand 45,000 relationships representing the semantic links between \nmedical concepts. Applying Cypher queries enabled answering \ncomplex medical questions regarding identifying drugs that may \ncause certain diseases; it was demonstrated that the graph inferred \nnew relationships not explicitly existing in the original data. The \nconclusion was that disease knowledge graphs sped up clinical \ndiscovery and contributed to understanding complex medical \nrelationships. \nHou et al.[3] assessed and contrasted ChatGPT (both GPT-\n3.5 and GPT-4) and the biomedical knowledge graphs (BKGs) \nconcerning their ability to answer biomedical questions, generate \nnew knowledge, and reason. Their datasets were focused on \ndietary supplements and drugs, while evaluation criteria entailed \naccuracy, novelty, and reasoning ability. The results indicate that \nwhile GPT-4 surpassed GPT-3.5 and BKGs in knowledge \nprovision, it proved inconsistent with regard to citations and \nreasoning. Compared to them, BKGs scored higher in accuracy \nand reliability, especially in discovering novel links as well as \nwithin structured reasoning. \nSoman et al.[13] presented a novel framework called KG-\nRAG that integrates a large biomedical knowledge graph \n(SPOKE) with LLaMA 2, GPT-3.5, and GPT-4 (LLMs) to \nproduce accurate biomedical text. They optimized the retrieval of \nrelevant graph context to cut over 50% tokens without losing \naccuracy. It aided LLMs in performing better on biomedical \nquestion answering with very high accuracy boosts, especially in \nthe case of LLaMA 2. They compared KG-RAG to other retrieval \nmethods and indicated its comparatively more robust and \nefficient results. The framework produced reliable evidence-\nbased answers grounded in biomedical knowledge. \nLuo et al.[23] created ChatKBQA, a new framework with a \nquestion-and-answer approach over knowledge bases that first \ngenerates logical forms with the help of fine-tuned LLMs and \nthen retrieves the relevant entities and relations. This generate-\nthen-retrieve approach is supposed to handle a couple of issues \nwith the earlier methods concerning tedious retrieval and error \npropagation. They fine-tuned open-source LLMs like LLaMA 2 \nto change natural-language questions into logical forms with high \naccuracy. The retrieval phase uses unsupervised phrase-level \nsemantic matching in a way that enhances the alignment of \nentities and relations. Experiments on benchmark datasets \nindicate ChatKBQA to be superior to its predecessors, with the \nhighest accuracy to date. \nPusch and Conrad[6] conducted work under a hybrid \napproach conflating LLMs and biomedical Knowledge Graphs \n(KGs) to suppress hallucination errors in question-answering. \nThey proposed query-checking algorithms for validating, \ncorrecting, and executing the KG Cypher queries that LLMs \ngenerated, thereby attaining accurate and understandable \nanswers. The system used retrieval-augmented generation (RAG) \nto ground answers within KG data. The methodology was \nvalidated on a biomedical KG called PrimeKG using 50 \nbenchmark questions, assessing models like GPT-4 Turbo and \nLLaMA 3. Commercially available GPT-4 Turbo obtained'
 'GigaScience , 2025, 14 , 1â€“12 \nDOI: 10.1093/gigascience/giae082 \nRESEARCH \nKno wledge gr aphâ€“based thought: a kno wledge \ngraphâ€“enhanced LLM framework for pan-cancer \nquestion ans w ering \nYichun Feng \n1 ,2 ,â€¡ , Lu Zhou 2 ,â€¡ , Chao Ma 3 ,â€¡ , Yikai Zheng 2 , Ruikun He 4 ,5 , * , and Yixue Li 1 ,2 , * \n1 Hangzhou Institute for Advanced Study, University of Chinese Academy of Sciences, 310024 Hangzhou, China \n2 Guangzhou National Laboratory, Guangzhou International Bio Island, 510005 Guangzhou, China \n3 Smartquerier Gene Technology (Shanghai) Co ., Ltd. , 200100 Shanghai, China \n4 BYHEALTH Institute of Nutrition & Health, 510663 Guangzhou, China \n5 Shanghai Institute of Nutrition and Health, Chinese Academy of Sciences Shanghai, 200030 Shanghai, China \nâˆ—Corr espondence addr ess . Ruikun He , BYHEALTH Institute of Nutrition & Health, 510663 Guangzhou, China. E-mail: herk@by-health.com ; Yixue Li, Guangzhou \nNational Laboratory, No. 9 XingDaoHuanBei Road, Guangzhou International Bio Island, 510005 Guangzhou. E-mail: yxli@sibs.ac.cn .\nâ€¡ Contributed equally. \nAbstract \nBac kgr ound: In recent years, large language models (LLMs) have shown promise in various domains, notably in biomedical sciences. \nHowev er, their r eal-world application is often limited by issues like erroneous outputs and hallucinatory responses. \nResults: We developed the knowledge graphâ€“based thought (KGT) framework, an innov ati v e solution that integrates LLMs with knowl- \nedge graphs (KGs) to impr ov e their initial r esponses by utilizing v erifia b le information from KGs, thus significantl y r educing factual \nerr ors in r easoning. The KGT fr amew ork demonstr ates str ong adapta bility and performs well acr oss v arious open-source LLMs. No- \nta b l y, KGT can facilitate the discov er y of new uses for existing drugs through potential drugâ€“cancer associations and can assist in \npr edicting r esistance by anal yzing r elev ant biomarkers and genetic mechanisms. To ev aluate the kno wledge graph question ans wer- \ning task within biomedicine , w e utilize a pan-cancer knowledge graph to develop a pan-cancer question answering benchmark, named \npan-cancer question answering. \nConclusions: The KGT fr amew ork substantiall y impr ov es the accuracy and utility of LLMs in the biomedical field. This study serves \nas a proof of concept, demonstrating its exceptional performance in biomedical question answering. \nKe yw ords: pan-cancer knowledge graph, large language model, knowledge graph question answering, prompt engineering \nKe y P oints: \nr We introduce a framework combining large language \nmodels (LLMs) with knowledge gr a phs (KGs) to impr ov e \nfactual accuracy in LLM reasoning. \nr Our system is a flexible arc hitectur e that seamlessly in- \ntegr ates v arious LLMs. \nr Utilizing a pan-cancer knowledge gr a ph, we hav e pr o- \nposed the first knowledge gr a ph question answering \nbenchmark in the field of biomedicine. \nr Case studies r e v eal our method enhanced LLMs in ad- \ndressing biomedical challenges such as drug reposition- \ning, r esistance r esearc h, individualized tr eatment, and \nbiomarker analysis. \nr The method performs favor abl y in comparison to exist- \ning methods. \nIntroduction \nWith the increasing prominence of large language models (LLMs) \nin the field of artificial intelligence, the advent of influential mod- \nels such as ChatGPT [ 1 ] and Llama [ 2 ] consequently catalyze the \nde v elopment of a wide array of applications in biomedicine and \nhealth care. Ho w ever, LLMs still face the challenge of factual hal- \nlucination, wher e they gener ate incorr ect statements due to lim- \nited inherent knowledge [ 3 ]. Factual hallucination presents a sig- \nnificant challenge for the practical use of LLMs, especially in real- \nworld scenarios where factual accuracy is crucial. Consequently, \nthere is a growing focus on addressing factual hallucinations in \nLLMs within the field of natural language processing (NLP) [ 4 , 5 ]. \nLLMs often struggle to ca ptur e and access factual knowledge, \nprimarily due to 3 aspects: the inability to comprehend ques- \ntions due to the lack of contextual information, the insufficient \nknowledge to generate accurate ans wers , and the incapacity to \nrecall specific facts [ 6 ]. Consequently, researchers consider the \nfine-tuning technique as a solution to address these issues. For \nexample, MedAlpaca [ 7 ] builds upon medical data to fine-tune \nStanford Alpaca for applications related to medical question \nans wering and dialogue . ChatDoctor [ 8 ] is designed to simulate \na conversation between a doctor and a patient by fine-tuning \nRecei v ed: May 27, 2024. Re vised: A ugust 14, 2024. Accepted: October 2, 2024 \nÂ© The Author(s) 2025. Published by Oxford Uni v ersity Pr ess GigaScience. This is an Open Access article distributed under the terms of the Cr eati v e Commons \nAttribution License ( https://cr eati v ecommons.org/licenses/by/4.0/ ), which permits unrestricted reuse, distribution, and reproduction in any medium, provided \nthe original work is pr operl y cited. \nDownloaded from https://academic.oup.com/gigascience/article/doi/10.1093/gigascience/giae082/7943459 by guest on 11 January 2026'
 'Vol. 06, No. 02, pp. 342 â€“357 (2025) \nISSN: 2708-0757 \n \nJOURNAL OF APPLIED SCIENCE AND TECHNOLOGY TRENDS \n \nwww.jastt.org  \n \n                                                          342 \ndoi: 10.38094/jastt62404  \nA Hybrid LLMâ€“Knowledge Graph Framework for Accurate \nBiomedical Question Answering \n \nHavraz Y. Omar1,Â²*, Abdulhakeem O. MohammedÂ³  \n \nÂ¹Department of Information Technology, Technical College of Duhok, Duhok Polytechnic University, Duhok, Kurdistan Region, \nIraq. havraz.omar@dpu.edu.krd  \nÂ² Department of Information Technology, Technical College of Informatics â€“ Akre, Akre University for Applied Sciences, Akre, \nKurdistan Region, Iraq. \nÂ³ Department of Computer Science, College of Science, University of Zakho, Zakho, Kurdistan Region, Iraq. \na.mohammed@uoz.edu.krd \n \n*Correspondence: havraz.omar@dpu.edu.krd \n \nAbstract \nBiomedical question answering requires accurate and interpretable systems; however, existing approaches often face challenges such as \nlanguage model hallucinations and limited reasoning when relying solely on standalone knowledge graphs. To address these limitations, \nthis study proposes a hybrid framework that integrates the LLaMA-3B language model with a Neo4j-based drugâ€“diseaseâ€“symptom \nknowledge graph. The system translates natural language questions into executable Cypher queries, operates on an iBKH-derived graph \ncomprising over 65,000 entities and 3 million relationships, and returns answers with supporting evidence through a transparent interface. \nExperiments conducted on 60 biomedical questions across three levels of difficulty demonstrate the robustness of the approach: 96% \nexact match for simple queries, 95% for medium queries, and 86.7% for complex queries. Overall, the system achieves Precision@5 of \n96.1%, Recall@5 of 89.0%, F1@5 of 91.0%, Hits@k of 96.1%, and an MRR of 94.4%, while maintaining an average response time of \nonly 6.07 seconds. These results indicate that the system retrieves nearly all relevant answers, ranks them correctly, and delivers them \nwith latency low enough for interactive use. Moreover, unlike cloud-based APIs such as ChatGPT, which require internet connectivity \nand external data transmission, the proposed framework operates fully offline, ensuring privacy, reproducibility, and compliance with \nbiomedical data governance. Overall, this pipeline provides an accurate, efficient, and privacy-preserving solution for biomedical question \nanswering, making it a practical alternative to cloud-dependent approaches in sensitive healthcare contexts. \n \nKeywords: Knowledge Graph, LLM, Question Answering, Neo4j, Biomedical Informatics, Healthcare AI, LLaMA 3. \n \nReceived: August 14th, 2025 / Revised: October 10th, 2025 / Accepted: October 16th, 2025 / Online: October 20th, 2025 \n \nI. INTRODUCTION \nAnswering questions in the biomedical field is a difficult task \ndue to the complexity of medical knowledge and the need for \nprecision. In recent years, large language models  (LLMs) like \nLLaMA, GPT-4 have made progress in understanding and \ngenerating human-like responses to medical questions  [1,  2]. \nThese models can process large amounts of information and \nrespond in natural language, which makes them helpful in \nhealthcare settings [3]. However, they often struggle to provide \naccurate answers when dealing with specialized biomedical \ncontent [4, 5]. \nOne major issue with LLMs is a problem called hallucination, \nwhere the model generates information that sounds right but is \nactually incorrect or unsupported [6]. In medical applications, \nthis can be dangerous, as healthcare professionals rely on precise \nand trustworthy information  [7]. Therefore, researchers are \nexploring ways to combine LLMs with structured sources of \nknowledge to improve their reliability [8]. \nLLM-only systems in biomedicine still hallucinate and are \nhard to verify, limiting safe use [9, 10]. Biomedical knowledge \ngraphs (BKGs) such as iBKH and SPOKE curate multi-source \nfacts and enable multi-hop reasoning, yet they neither interpret \nfree text nor generate answers [11, 12]. Recent hybrids (KG-\naware RAG) improve grounding but often lack explicit path-level \njustifications and robust end-to-end answer evaluation [13, 14]. \nRecent studies have increasingly integrated Knowledge \nGraphs (KGs) with Large Language Models (LLMs) to improve \nfactual accuracy, reasoning, and reduce hallucinations. Notable \nexamples include DR.KNOWS, which combines UMLS-based \nKGs with LLMs for better diagnostic reasoning [15], KnowNet']","['<1-hop>\n\n8 | GigaScience , 2025, Vol. 14 Table 5. Ablation study of the KGT fr ame work under Code-Llama-13B ROUGE (%) Method GPT-4 Eval (%) BERTScore (%) Recall Precision F1 score KGT (ours) 92.4 97.7 87.4 87.7 86.8 Without GSBI 71.8 95.5 68.1 69.8 66.8 Without QA&GSBI 69.7 94.7 55.0 66.3 58.2 Without QA&GSBI&SC 24.7 77.4 14.8 12.3 12.2 compared to the complete KGT, indicating that each of the 3 mod- ules is vital for the final performance . Furthermore , such obser- vations confirm that our KGT can indeed le v er a ge knowledge to enhance the final performance of LLMs. Implementation settings Our knowledge gr a ph is quite large, with a complex schema, and typicall y involv es input tokens within 1,300. Our experiment does not r equir e fine-tuning, and the infer ence time is r elated to the model size and computational resources. For example, when us- ing our method, KGT, with the Code-Llama-13B model on an 80 GB A100 GPU, it occupies 33 GB of VRAM. Without any acceleration fr ame works, the infer ence r equir es 4 passes, eac h taking ar ound 20 seconds. Case studies Drug repositioning Drug r epositioning emer ges as a pr omising str ategy to acceler ate the process of drug development. This approach involves identi- fying new therapeutic uses for existing drugs, thereby saving time and r esources typicall y r equir ed for bringing a ne w drug to market [ 45 ]. Our system is capable of investigating the potential reposi- tioning of carteolol for the treatment of hemangiomas . T he exam- ple is shown in Supplementary Table S2 and r elational dia gr am is shown in Fig. 4 A. Utilizing the systemâ€™s knowledge gr a ph, a r ela- tional chain is delineated, illustrating that propranolol, another inhibitor of ADRB1, is effectiv el y emplo y ed in the treatment of hemangiomas . T he system harnesses this insight to formulate a hypothesis that carteolol, by virtue of its similar mechanism of inhibition, could be potentially repositioning for treating heman- giomas [ 46 ]. This hypothesis would serve as a precursor to clin- ical trials and r esearc h, potentiall y expediting the availability of an additional ther a peutic option for patients with hemangiomas. Drug resistance resear c h Drug resistance in cancer treatment poses a significant challenge in clinical oncology. Understanding the genetic basis of resistance can lead to mor e effectiv e tr eatment str ategies and personalized medicine a ppr oac hes. Researc h in drug r esistance involv es deter- mining why certain cancer-carrying mutated genes are not re- sponsive to specific drugs and finding ways to overcome this re- sistance [ 47 ]. Our system is capable of exploring drug resistance in cancer. The example is shown in Supplementary Table S3 , and a relational diagram is shown in Fig. 4 B. The KG data indicate that the ALK-p.L1196M m utation, whic h is associated with gas- tric cancer, has a known resistance to nalatinib [ 48 , 49 ]. The LLM processes this information and infers that due to this resistance, nalatinib might not be an effective medication for treating can- cers caused by the ALK-p.L1196M mutation. The case highlights the critical importance of understanding specific geneâ€“drug in- teractions in drug resistance research. It demonstrates how cer- tain gene mutations could render a drug ineffective, which in turn could guide oncologists in choosing alternative treatments or de- v eloping ne w drugs that can bypass or tar get the r esistance mec h- anisms. By accelerating the process of understanding drug resis- tance, these artificial intelligenceâ€“driven systems can contribute to impr ov ed patient outcomes and the optimization of cancer tr eatment pr otocols.'
 '<2-hop>\n\nIndividualized treatment Details on individualized tr eatment ar e pr ovided in Supplementary Case Studies A . It is important to note that this example is included solely to illustrate the technical capabilities of the proposed method. The output generated in this example has not been validated for clinical use, and further validation in clinical settings would be r equir ed befor e an y suc h a pplication. Selection and understanding of biomarkers Details on selection and understanding of biomarkers are pro- vided in Supplementary Case Studies B . Discussion In this article, we introduce a novel framework KGT, which em- ploys LLMs for reasoning on the schema of KGs, to enhance the reasoning abilities of LLMs in areas with missing domain data by utilizing domain-specific knowledge gr a phs, suc h as oncology knowledge gr a phs, ther eby addr essing the issue of factual hal- lucinations in LLMs. Our method excels in extr acting, v alidat- ing, and refining factual knowledge throughout the LLMsâ€™ rea- soning process. It seamlessly integrates with various LLMs, in- cluding open-source models like Code-Llama, and enhances the capabilities of LLMs solely through prompt engineering and in- context learning without any fine-tuning. This grants it significant generalizability. We possess an extensive oncology knowledge graph and have established a benchmark based on it to e v aluate the ca pabilities of various methods. When tested on PcQA using various open-source LLMs, the KGT fr ame work performs exceptionall y well, sur passing the current best methods by 33%. This significant improvement positions our a ppr oac h as a pioneering benchmark in biomedical KGQA, setting a new standard that advances beyond previously established best pr actices. Additionall y, thr ough case studies, our a ppr oac h has been shown to effectiv el y pr ovide ther a peutic plans, gener ate v aluable hypotheses for drug r epositioning, identify po- tential drug targets, and study drug resistance . T his underscores the pr actical v alue of the KGT fr ame work in deliv ering insight- ful contributions that aid in the de v elopment and optimization of treatment strategies. Each case studyâ€™s conclusions are further v alidated by e vidence fr om pr e viousl y published r esearc h pa pers, enhancing the credibility and impact of our findings. Ho w e v er, it is important to note that the constructed QA dataset and the corresponding published subset of the SOKG Downloaded from https://academic.oup.com/gigascience/article/doi/10.1093/gigascience/giae082/7943459 by guest on 11 January 2026'
 '<3-hop>\n\nOmar & Mohammed / Journal of Applied Science and Technology Trends Vol. 06, No. 02, pp. 342 â€“357 (2025) 345 record-high accuracy, while open-source models achieved impressive gains through prompt optimization. Feng et al.[22] developed the Knowledge Graph-based Thought (KGT) framework that integrated LLMs with a pan- cancer knowledge graph for biomedical question answering. KGT was designed to reason on the knowledge graph schema and identify optimal subgraphs to use for directing accurate answer generation, all without fine-tuning the LLMs. The framework is benchmarked against a new dataset (PcQA) designed specifically for pan-cancer KGQA tasks and has outperformed all existing state-of-the-art approaches by a rather large margin. KGTâ€™s practicality in biomedical issues was highlighted through case studies for drug repositioning, drug resistance, and biomarker discovery. Their approach exhibited robust adaptability among various LLMs. Rezaei et al.[26] developed AMG-RAG, a dynamic framework that utilizes autonomous LLM agents with medical search tools in the continuous construction and real-time updating of Medical Knowledge Graphs (MKGs). Their system incorporated confidence scoring and multi-hop reasoning to improve accuracy and interpretability in medical question answering. AMG-RAG outperformed size models on both very hard MEDQA benchmarks and more accessible MedMCQA ones, proving that it could conduct efficient reasoning based on current structured medical knowledge. They also used Neo4j to manage the knowledge graphs while adding external searches to ensure the latest data. Tiwari et al.[24] presented Auto-Cypher, a recent automated pipeline for producing high-quality synthetic data for training LLMs by mapping natural language to Cypher queries for graph databases like Neo4j. The pipeline deployed the novelty of LLM- as-database-filler to synthesize Neo4j databases for the execution of generated queries to ensure their correctness. A sizable dataset called SynthCypher was created, spanning multiple domains and complex queries, leading to a 40% improvement in LLM performance on Cypher generation. The datasets were used to fine-tune open-source models such as LLaMA, Mistral, and Qwen, and the SPIDER benchmark was adapted for evaluation purposes. Mohammed et al.[29] proposed a hybridized GraphRAG framework combining Neo4j-based UMLS knowledge graphs with a vector store for medical textbooks to create an improved U.S.M.L.E.-style clinical question-answering approach. The project integrated symbolic reasoning from knowledge graphs with semantic retrieval performed on text embeddings to enhance relevance and accuracy via adaptive re-ranking and query expansion. The system had the answers produced by GPT-4o- Mini, with different prompting strategies encouraging evidence- based and traceable responses grounded in verified medical knowledge. Experiments showed that the hybrid approach improved factual accuracy and citation fidelity as compared to the L.L.M.-only approach, enhancing transparency and reliability. It is shown that binding both structured and unstructured medical knowledge sources could aid in ameliorating hallucinations and hence improve clinical trustworthiness in AI-driven medical QA. Yang et al.[30] articulated sepsis knowledge graph was crafted by combining multicenter clinical data from over 10,000 patients with the help of GPT-4 for entity recognition and relationship extraction. Real-world data were collected from three hospitals and integrated with clinical guidelines and databases from the public domain. The knowledge graph contained 1,894 nodes and 2,021 relationships pertaining to diseases, symptoms, biomarkers, treatments, and complications. GPT outperformed other models in every resolution on sepsis- specific datasets to obtain high F1-score results. The constructed graph highlighted complex interactions in sepsis for assisting clinical decision-making and was implemented on Neo4j. Guan et al.[20] proposed a novel method for constructing a local knowledge graph from retrieved biomedical documents by extracting propositional claims. They carried out layer wise summarization on this graph to capture multi-document relationships and provide comprehensive contextual information to a language model for question-answering purposes. The method resolved issues in multi-document biomedical QA, such as noise cancellation and efficient context usage. They then tested their method on several benchmarks for biomedical question answering, achieving performance at least comparable to, if not better than, existing retrieval-augmented generation (RAG) baselines. The study established enhanced reasoning and answer accuracy of the model achieved through structured graph summarization. Previous studies have improved biomedical QA using KGs and LLMs, but important gaps remain. Most systems lack transparent, graph-based justifications, rely on limited evaluation methods, or depend on cloud resources that reduce privacy and reproducibility. Our framework addresses these gaps by providing visible Cypher queries with evidence subgraphs, applying comprehensive performance metrics across difficulty levels, and ensuring fully local, privacy-preserving deployment. Table I summarizes key previous studies on biomedical knowledge graphs and question answering, outlining their methods, datasets, and main limitations.'
 '<4-hop>\n\nIII. PRELIMINARIES This section outlines the fundamental concepts required to understand the proposed system. It introduces biomedical knowledge graphs, explains how Neo4j stores data in graph form, and describes the use of Cypher for querying. It also provides a brief overview of large language models (LLMs) and their role in interpreting natural language. A. Biomedical Knowledge Graphs Biomedical Knowledge Graphs (BKGs) provide a structured representation of complex biomedical information by modeling diverse medical entities, such as diseases, drugs, symptoms, and biological pathways, as interconnected nodes within a graph structure. The edges in these graphs represent the semantic relationships between these entities, including â€™treatsâ€™, â€™causesâ€™, â€™interacts withâ€™ and many others, as illustrated in Fig 1. This form of representation enables the integration of heterogeneous biomedical data from a wide range of sources, including']","The KGT (Knowledge Graph-based Thought) framework enhances large language models' (LLMs) reasoning capabilities and biomedical question answering performance by tightly integrating LLMs with a specialized biomedical knowledge graph (KG), specifically a pan-cancer knowledge graph. The key ways KGT leverages biomedical knowledge graphs to achieve these improvements are:

1. **Reasoning on Knowledge Graph Schema and Subgraph Identification**:  
   KGT is designed to reason explicitly over the structure of the biomedical knowledge graph. It identifies optimal subgraphs within the KG that are most relevant to the biomedical question at hand. This targeted subgraph selection guides the LLM in generating accurate and contextually grounded answers without requiring fine-tuning of the LLM itself.

2. **Use of Verifiable and Structured Information to Reduce Hallucinations**:  
   By grounding the LLMâ€™s responses in verifiable information extracted from the KG, KGT significantly reduces factual errors and hallucinations that standalone LLMs often produce. The KG provides a structured, multi-hop relational context that supports more reliable and evidence-based reasoning.

3. **Prompt Engineering and Flexible Integration**:  
   The framework employs prompt-driven query generation that is aware of the KG schema and biomedical entities, enabling the LLM to translate natural language questions into precise graph queries. This approach allows seamless integration with various open-source LLMs, enhancing adaptability and robustness.

4. **Benchmarking and Evaluation on Domain-Specific Tasks**:  
   KGT was benchmarked on a newly developed pan-cancer question answering dataset (PcQA), demonstrating superior performance over existing state-of-the-art methods. This shows that leveraging the KG structure directly improves the accuracy and utility of LLMs in complex biomedical QA tasks.

5. **Practical Biomedical Applications**:  
   Case studies in drug repositioning, drug resistance prediction, and biomarker discovery illustrate KGTâ€™s ability to assist in real-world biomedical challenges by uncovering novel drug-cancer associations and analyzing relevant biomarkers and genetic mechanisms.

In summary, the KGT framework enhances LLM reasoning and biomedical question answering by combining the rich, structured, and verifiable knowledge encoded in biomedical knowledge graphs with the natural language understanding capabilities of LLMs. This integration enables accurate, interpretable, and evidence-backed answers, overcoming limitations of LLM-only approaches and improving performance on complex biomedical queries.","The KGT framework enhances large language models' (LLMs) reasoning capabilities by employing biomedical knowledge graphs (KGs) to provide structured domain-specific knowledge. It reasons on the schema of these KGs to identify optimal subgraphs that guide accurate answer generation without requiring fine-tuning of the LLMs. This approach addresses the issue of factual hallucinations by extracting, validating, and refining factual knowledge during the reasoning process. The framework integrates seamlessly with various LLMs, including open-source models like Code-Llama, and improves performance solely through prompt engineering and in-context learning. Evaluations on a pan-cancer question answering benchmark (PcQA) demonstrate that KGT surpasses existing state-of-the-art methods by 33%, establishing a new standard in biomedical KGQA. Additionally, case studies show KGT's practical value in generating therapeutic plans, drug repositioning hypotheses, identifying drug resistance mechanisms, and biomarker discovery, all supported by evidence from prior research. Thus, by combining LLMs with biomedical knowledge graphs, KGT significantly improves the accuracy, interpretability, and efficiency of biomedical question answering.",1.0,0.9820993785189854,0.8041666666465626,0.8571428571428571
"What evaluation methods and performance metrics have been used to compare RAG-enhanced LLMs with baseline LLMs in clinical tasks, and how do these methods reflect the accuracy and usefulness of the models?","['was asked about medications for peripheral artery disease \npatients without increased bleeding risk, it initially omitted \nlow-dose rivaroxaban. After integrating retrieved text from \nthe 2024 American College of Cardiology / American Heart \nAssociation Guideline for the Management of Lower Extrem\xad\nity Peripheral Artery Disease,10 the model correctly recom\xad\nmended rivaroxaban.\nSeveral guidelines exist for evaluating Artificial Intelligence \n(AI) applications and LLMs in healthcare, including \nDECIDE-AI (Developmental and Exploratory Clinical Inves\xad\ntigations of DEcision support systems driven by Artificial \nIntelligence),11 CLAIM (Checklist for Artificial Intelligence in \nMedical Imaging),12\nand CONSORT-AI (Consolidated \nStandards of Reporting Trials-AI).13 In addition, Tam et al. \nintroduced QUEST, a framework specifically for human eval\xad\nuation of LLMs in healthcare.14 However, these guidelines \ndo not cover RAG applications in clinical settings, emphasiz\xad\ning the need for a more specific guideline.\nDespite the promise of RAG in improving LLM performance \nin clinical settings, there is limited understanding of its overall \neffectiveness comparing with the baseline LLM, adoption in \nclinical domains, and optimal strategies for its development in \nbiomedical applications. The aim of this study is to synthesize \nfindings from recent research of RAG and LLM in biomedicine \nand provide clinical development guidelines to improve effec\xad\ntiveness as well as transparency in future research.\nMaterials and methods\nStudy design\nWe conducted a systematic literature review. The report was \ncreated in adherence to the Preferred Reporting Items for Sys\xad\ntematic Reviews and Meta-Analyses 2020 analysis checklist \n(File S1).15 We also performed a meta-analysis following the \nCochrane Handbook guidelines.16\nScreening papers\nWe searched in 3 databases (PubMed, Embase, and Psy\xad\ncINFO), using terms related to â€œretrieval augmented gener\xad\nationâ€ and â€œlarge language model.â€ PubMed and Embase \nwere selected based on recommendations from the Cochrane \nHandbook, as they are widely recognized for their extensive \ncoverage of biomedical literature.17 PsycINFO was included \nto capture articles from the psychological and behavioral sci\xad\nences. To maintain a high standard of quality and reliability, \nwe focused on peer-reviewed articles and excluded preprints. \nThe specific search terms used for each database are provided \nin File S2. Given that ChatGPT was released on November \n30, 2022, we set the publication filter to search papers pub\xad\nlished in 2023 and 2024. The search was performed on \nDecember 12, 2024. The inclusion criteria were: (1) the study \nmust compare baseline LLM performance with RAG per\xad\nformance and (2) the study must address a biomedical ques\xad\ntion. The exclusion criteria were: (1) literature reviews, \neditorial comments, or viewpoint papers, (2) studies focusing \non LLMs in languages other than English, or (3) studies cen\xad\ntered on a multi-agent system without a focus on RAG. SL \nscreened titles and abstracts, then conducted a full-text \nreview of papers meeting the criteria.\nData extraction\nFor each included study, we extracted the following informa\xad\ntion: author, title, publication year, journal, clinical task, and \nspecialty. Regarding RAG techniques, we gathered details \nabout the baseline LLM, retrieval sources, and strategies used \nin the pre-retrieval, retrieval, and post-retrieval stages. For \nevaluation, we extracted the evaluation method (human, \nautomated, or a combination of both), the number of evalua\xad\ntors, the evaluation dataset, and the evaluation metrics.\nMeta-analysis\nEffect size was defined as a metric quantifying the relation\xad\nship between variables, including both direction and magni\xad\ntude.18 For each included study, we calculated the effect size \nbetween baseline LLM performance and RAG-enhanced \nLLM performance. The outcomes focused on the perform\xad\nance of generation results, such as accuracy and usefulness. \nMetrics related to the retrieval process, cost, or speed were \nnot included as outcomes in the meta-analysis. For continu\xad\nous outcomes, we used Cohenâ€™s d, standardized between- \ngroup mean difference (SMD), calculated as the difference in \nmeans divided by the pooled standard deviation. The stand\xad\nard error (SE) of SMD was calculated using the following for\xad\nmula (1), where n1 and n2 represent the sample sizes of each \ngroup.19 For dichotomous measurements, we calculated the \nlog-odds ratio, obtained by transforming the odds ratio (OR) \nwith the natural logarithm, and the associated SE was calcu\xad\nlated using formula (2), where a, b, c, and d represent the \nnumber of successful and failed events in the baseline LLM \nand RAG-enhanced LLM approaches. For studies reporting \nmultiple outcomes, we used the overall outcome to calculate \neffect size. If no overall outcome was reported, we averaged \nthe effect sizes of all reported outcomes. We excluded out\xad\ncomes with a sample size of less than 30 to avoid small- \nsample bias. \nSESMD Â¼\nffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi\nn1 Ã¾ n2\nn1n2\nÃ¾\nSMD2\n2 n1 Ã¾ n2\nÃ°\nÃž\ns\n(1) \nSElogOR Â¼\nffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi\n1\na Ã¾ 1\nb Ã¾ 1\nc Ã¾ 1\nd\nr\n(2) \nWe developed a random-effect meta-analysis model, \nbecause of the variability in RAG architectures and evalua\xad\ntion datasets among the included studies. The random-effect \nmodel was used when individual study effects likely con\xad\ntained additional sources of variance beyond sampling error. \nBetween-study heterogeneity was assessed using Higgins & \nThompsonâ€™s I2 statistic, where 25% indicated low heteroge\xad\nneity, 50% moderate, and 75% substantial.20\nWe conducted subgroup analyses to explore performance \nvariations across different factors. First, we analyzed the \ninfluence of the baseline LLM, referring to the foundation \nmodel (eg, GPT-4 or Llama2) that provides the core architec\xad\nture for the system. Second, we examined data retrieval strat\xad\negies, categorizing them as simple or complex. Simple \nstrategies included fixed-length chunking and basic similarity \nsearch, and we performed a subgroup analysis to compare \nthese with complex retrieval strategies. Third, we analyzed \ndifferences based on evaluation methods, distinguishing \nbetween human evaluations, such as Likert scale ratings for \nhelpfulness and accuracy, and automatic evaluation metrics, \nincluding ROUGE-1 and BLEU. Finally, we conducted a sub\xad\ngroup analysis based on the type of task, classifying studies \n606                                                                                                      \nJournal of the American Medical Informatics Association, 2025, Vol. 32, No. 4 \nDownloaded from https://academic.oup.com/jamia/article/32/4/605/7954485 by guest on 11 January 2026'
 'conducting safety checks by applying 24 predefined rules to \nensure ethical and factual accuracy, and summarizing the \nresults.30 Glicksberg et al. developed an ensemble model that \ncombined structured and unstructured data to predict hospi\xad\ntal admission probabilities. These predicted probabilities, \nalong with similar historical cases, were incorporated into the \nprompt to enhance the performance of LLM.37 Chen et al. \nused Chain-of-Thought (CoT) prompting to improve LLM \nreasoning capabilities.39 Kresevic et al. customized prompts \nto help the model interpret structured guidelines, combined \nwith few-shot learning using 54 question-answer pairs.27\nJeong et al. fine-tuned LLMs to assess the relevance of \nretrieved evidence, ensure all statements were evidence-based, \nand confirm that the response effectively addressed the \nquery.34\nEvaluation\nNine studies used human evaluation, 8 relied on automated \nevaluation (eg, similarity comparisons between generated \nsentences and original answers), and 3 used a mix of both. \nOutcomes from human evaluation showed an overall OR of \n1.65 (95% CI: 1.36-2.03), while automatic evaluation \nresulted in an OR of 1.20 (95% CI: 1.1-1.41). The differen\xad\nces between the 2 were statistically significant (P < .01). \nThere were 4 human evaluators on average, with the range \nspanning from 1 to 10. Most human evaluators were \nphysicians from relevant specialties according to the \nstudy focus. In one case, 3 diabetic patients were involved in \nevaluating the understandability of diabetes-related patient \nqueries.30\nTwelve studies used self-curated datasets focused on \nresearch tasks. Examples included the ClinicalQA bench\xad\nmark, which comprised 314 open-ended questions about \ntreatment guidelines and clinical calculations generated by \nphysicians,28 and 43 diabetes-related questions sourced from \nthe National Institute of Diabetes and Digestive and Kidney \nDiseases website.30 Simulated cases from medical examina\xad\ntions were also utilized.25\nThree studies used EHR \ndata.35,37,40 Six studies used public benchmark datasets, such \nUS board exam practice questions, MedMCQA29,34 and \nlongform question-answering benchmarks (eg, LiveQA, Med\xad\nicationQA).34 The self-curated datasets averaged 76 ques\xad\ntions, ranging from 7 to 314. The length of public benchmark \ndatasets varied significantly, from 102 questions in the \nLiveQA dataset28 to 194 000 questions in the MedMCQA \ndataset.34\nMost studies reported evaluation metrics for the final \nresponse generation, while 4 (25%) also included specific \nmetrics to evaluate the retrieval process. For instance, 1 study \nmeasured recall in context retrieval,24 another evaluated \nretrieval accuracy,33 and a fine-tuned LLM was developed to \nassess the relevance of retrieved information to the userâ€™s \nquery.34 Additionally, 1 study evaluated the accuracy of \nusing LLMs to extract text from figures and tables during \ndocument preprocessing.27 The final evaluation metrics \nfocused on the generated responses, consistent with those \nused in LLM-only systems. These metrics could be catego\xad\nrized as accuracy, completeness, user perception, safety, hal\xad\nlucination, citation, bias, and language. Accuracy was the \nmost frequently reported metric, covering Likert scale rat\xad\nings, match rates, correct treatment percentages,9 AUC, \nAUPRC, and F1 scores, as well as text similarity metrics like \nROUGE \n(ROUGE-1, \nROUGE-2, \nROUGE-L), \nBLEU, \nMETEOR, and BERTScore,21\nwhich compared LLM- \ngenerated responses to expert-provided answers. Complete\xad\nness metrics assessed whether responses included all neces\xad\nsary information, typically using Likert scales. User \nperception captured subjective feedback from both healthcare \nproviders and patients on understandability, helpfulness, and \nwhether responses met user intent, usually using Likert scales. \nSafety metrics focused both on user-related and system- \nrelated aspects. These metrics assessed potential harm, adver\xad\nsarial safety, and risk management,24 ensuring that outputs \nwere free of harmful content or risks. Scientific validity and \nadherence to evidence were also evaluated.29 One study used \nadversarial prompting, defined as intentionally adding harm\xad\nful directives to a prompt, to evaluate the safety of the RAG \nsystem.28 Hallucinations were primarily identified through \nmanual review, with definitions varying across studies. Some \nstudies defined hallucinations as nonfactual information, \nwhile one study added 2 other types of hallucinations: input- \nconflicting (content deviating from user-provided input) and \ncontextual-conflicting (content conflicting with previously \ngenerated information).27,41 Citation metrics measured the \naccuracy of provided references, with valid references consid\xad\nered those that pointed to established publications, guide\xad\nlines, or research. Bias and language were evaluated for \nclarity and neutrality, ensuring responses were unbiased and \nempathetic to patient concerns.24\nDiscussion\nThis study presents a systematic review of current research \non RAG for clinical tasks. Overall, RAG implementation \nincreased outcomes by 1.35 times compared to baseline \nLLM. We analyzed clinical tasks, baseline LLMs, retrieval \nsources and strategies, as well as evaluation methods. Despite \nthe potential benefits of RAG systems, there remains room \nfor improvement. Building on our literature review, we devel\xad\noped GUIDE-RAG (Guidelines for Unified Implementation \nand Development of Enhanced LLM Applications with RAG \nin Clinical Settings) for future clinical applications using \nRAG (Figure 4).\nGUIDE-RAG:\n1) Define clear clinical tasks and evaluation datasets. \nFuture research should clearly define clinical tasks and \nquestions to maximize the effectiveness of RAGs. Ambi\xad\nguity in questions can hinder performance, particularly \nin less powerful LLMs, making it challenging to achieve \nsignificant improvements in responses generation, even \nwith improved knowledge selection.42 For example, one \nstudy in the review constructed the evaluation dataset \nusing 30 case reports on rare diseases from PubMed.38\nThe authors did not report human performance on the \nself-build dataset. The questions themselves might have \nbeen inherently challenging or ambiguous. As expected, \nthe reported performance showed modest improvement, \nwith an OR of 1.31. \n2) Identify appropriate external resources for specific clini\xad\ncal tasks. \nThe first step in developing a RAG-based clinical system \nis to identify external resources that fill the knowledge \ngaps of the baseline LLM in relation to specific clinical \ntasks. The external knowledge should complement the \nLLMâ€™s existing capabilities to effectively address task \n610                                                                                                      \nJournal of the American Medical Informatics Association, 2025, Vol. 32, No. 4 \nDownloaded from https://academic.oup.com/jamia/article/32/4/605/7954485 by guest on 11 January 2026'
 ""Omar & Mohammed / Journal of Applied Science and Technology Trends Vol. 06, No. 02, pp. 342 â€“357 (2025) \n \n352 \nThe dataset contains 60 questions divided into three difficulty \nlevels based on how complex the language is and how deep the \nbiomedical reasoning needs to be: \nâ€¢ Level 1: 25 simple questions focusing mostly on easy-to-\nunderstand facts, such as symptoms of certain diseases or \ndrugs used to treat specific conditions. \nâ€¢ Level 2: 20 medium-level questions that involve more \ndetailed relationships, such as drug interactions and \nSideEffect. \nâ€¢ Level 3: 15 hard questions requiring multi-step reasoning \nacross multiple biomedical concepts or biological \npathways, similar to the complexity found in real clinical \ncases. \nThe set of evaluation questions was designed to span a wide \nrange of common biomedical topics and to reflect clinically \nrelevant query types reported in prior literature. Each question is \npaired with its corresponding gold standard cypher query and \ncategorized by difficulty level, as summarized in Table V, where \nthree illustrative examples are shown. The questions were derived \nfrom publicly available biomedical QA benchmarks and adapted \nfrom established knowledge bases (e.g., iBKH schema relations), \nensuring both coverage and diversity across diseases, drugs, \nsymptoms, and pathways.\nTABLE V.  QUESTION DIFFICULTY LEVELS AND SAMPLE CYPHER QUERIES \nLevel \nDefinition \nExample \nCypher Query \n1 \nSingle-hop question using \none relationship \nWhat are the symptoms of \nAlzheimer? \nMATCH (d:Disease)-[:HAS_SYMPTOM]->(s:Symptom) \nWHERE toLower(d.name) CONTAINS 'alzheimer' \nRETURN s.name AS symptom \n \n2 \nQuestions requiring one \nrelationship \nWhat are the side effects of \ndrugs used to treat asthma? \nWITH toLower('asthma') AS disease_name \nMATCH (d:Disease)<-[:TREATS]-(dr:Drug) \nWHERE toLower(d.name) CONTAINS disease_name \nMATCH (dr:Drug)-[:CAUSES]->(se:SideEffect) \nRETURN DISTINCT se.name AS side_effect, dr.name AS drug \n \n3 \nQuestions requiring two or \nmore relationships \nWhat cholesterol medications \ncause side effects, and what are \nsome of those effects? \nWITH toLower('cholesterol') AS disease_name \nMATCH (d:Disease)<-[:TREATS]-(dr:Drug) \nWHERE toLower(d.name) CONTAINS disease_name \nMATCH (dr:Drug)-[:CAUSES]->(se:SideEffect) \nRETURN DISTINCT dr.name AS drug, se.name AS side_effect \n \n \n \nFig. 4. Graphical User Interface of the Biomedical Knowledge Explorer System \nB. Quantitative Evaluation \nThe performance of the proposed system is evaluated on the \niBKH knowledge graph using Exact Match, Precision@k, \nRecall@k, F1@k, Hits@k, MRR, and total response time across \nsimple, medium, and complex biomedical queries. These metrics \nwere measured for each difficulty level (simple, medium, and \ncomplex) to see how performance changes as questions become \nmore challenging.\n \nTABLE VI.  QUERY GENERATION EXACT MATCH BY DIFFICULTY LEVEL""
 'offer global information based on user query, offering \nbetter performance than naÃ¯ve RAG on the traditional \nvector databases.50\n6) Implement few-shot learning with CoT for complex clin\xad\nical reasoning. \nFew-shot learning has been shown to enhance LLMsâ€™ \nreasoning capabilities by teaching specific reasoning that \nmay not have been included in their original training. \nSimilarly, CoT techniques can improve complex reason\xad\ning in clinical tasks.51,52 Researchers should generate \nhigh-quality examples and incorporate CoT strategies \ninto the final query to refine specialized reasoning. \n7) Evaluate all stages of the RAG system development. \nMost current studies focus only on final performance, \noverlooking the importance of evaluating each stage of \ndevelopment. It is crucial to formally assess and report \nperformance at the pre-retrieval, retrieval, and post- \nretrieval stages. Evaluating the knowledge boundaries of \nthe baseline LLM, potential conflicts between the LLM \nand external knowledge, and the accuracy and coverage \nof retrieved information helps ensure replicability and \ntransparency. This level of evaluation enables other \nresearchers to understand why a RAG system works (or \ndoes not) and facilitates reproducibility. \n8) Consider cost and time constraints. \nAdvanced retrieval strategies can improve performance \nbut often increase processing time and computational \ncosts. For example, graph-based RAG requires substan\xad\ntial resources for developing knowledge graphs, and \nresponses from global summaries may take longer than \nwith naÃ¯ve RAG methods.50 Another example is to fine- \ntune LLMs to evaluate the needs and the quality of \nretrieval.34 In terms of computational cost, this process \nis expensive, especially when scaling the method to \nlarger datasets or deploying it in a real-time system. \nAlso, a set of extra processes will make the whole speed \nslow. The long response time might have a nonignorable \nnegative impact in situations that need a quick answer, \nespecially common in clinical settings. Researchers \nshould balance performance improvements with time \nand cost considerations. Only 1 study in our review, \nwhich focused on gastrointestinal radiology diagnosis \nbased on imaging descriptions, compared the cost and \nresponse time between LLMs and LLMs with RAG.31\nThe mean response time was 29.8 s for LLM with RAG \nvs 15.7s for LLM alone, with costs of $0.15 and $0.02 \nper case, respectively. Another study used EHR to pre\xad\ndict cognitive decline only reported cost, with LLM: \n$4.49; RAG: $12.51. Another study that used EHR \ndata to predict cognitive decline reported costs of $4.49 \nfor LLM alone and $12.51 for LLM with RAG.53\n9) Establish monitoring and knowledge update strategies. \nAn important concept in AI applications in healthcare, \nalgorithmovigilance, which defined as â€œscientific meth\xad\nods and activities relating to the evaluation, monitoring, \nunderstanding, and prevention of adverse effects of \nalgorithms in health care,â€ 54 should also be considered \nin the RAG applications. Researchers need to develop \nlong-term monitoring strategies for the RAG system per\xad\nformance, especially in clinical applications. In addition, \ncurrent studies use fixed external datasets. Researchers \nshould update external knowledge sources as latest \ninformation becomes available. Clear strategies for \nupdating knowledge should be defined, specifying when \nand how updates will occur. \nFor future studies, the first direction could be the system- \nlevel enhancement, the combination of RAG and LLM- \npowered agents. LLM-powered agents are AI systems that \nuse LLMs with complex reasoning and planning capabilities, \nmemory management, interactive capabilities with the envi\xad\nronment, and actions to execute tasks.55,56 Recent research \npoints to the emerging trend of combination of RAG and \nLLM-powered agents, where agents can assist in planning \nand decision making for complex tasks, rather than simple \nretrieval.57 For example, clinicians and patients have diverse \ninformation access needs, some needing to analyze text from \na knowledge base, others needing to incorporate structured \ndata from an EHR. RAG will eventually only become one of \nthe methods for agents to access information. Moreover, \nfuture research could focus on the usage of internal and \nexternal functions and tools, long-term and short-term mem\xad\nory module, self-learning module. For example, a study \ndeveloped an agent to answer questions related to rare dis\xad\neases by expanding beyond RAG with additional tool func\xad\ntions, such as querying phenotypes and performing web \nsearches. This approach improved the overall correctness \nfrom 0.48 to 0.75 compared to the GPT-4 baseline LLM.58\nThe second future direction could focus on the knowledge- \nlevel enhancement: deep integration of external knowledge \ninto LLM. LLM exhibits the knowledge boundaries. RAG \napproaches retrieving external knowledge and then integrates \nit into LLMs in the forms of prompts for the final generation \nto enhance the capabilities of LLMs in perceiving knowledge \nboundaries.59 However, the integration of external knowl\xad\nedge into LLM reasoning is typically limited to providing the \nretrieved data as additional context for the LLMâ€™s query dur\xad\ning generation. This approach keeps retrieval and generation \nloosely connected, and the LLMâ€™s output can still be influ\xad\nenced by its inherent knowledge boundaries or by noise in the \nretrieved text, leading to incorrect answers. Additionally, \nwhen the external knowledge source is EHR data, this \nenhancement becomes even more important. Current EHR \ndata is organized in a â€œproblem-oriented medical recordâ€ \n(POMR) format, which collects and displays information in a \nstructured manner.60 LLMs excel in free-form contexts, and \ntheir ability to perform clinical tasks depends on access to \nunstructured text that provides a comprehensive view of the \npatient. Achieving this within the structured POMR format \nin modern EHR systems poses a significant challenge.61\nTherefore, investigating how to realize the deep integration \nof external knowledge with LLM reasoning is an important \ndirection for future research in clinical applications.\nThe final direction is the integration-level enhancement, \nfocusing on integrating RAG systems within EHRs. Current \nresearch has primarily focused on development and testing \noutside of EHR systems. To seamlessly provide support for \nhealthcare providers and patients, future efforts should pri\xad\noritize embedding RAG systems into EHR interfaces. This \nrequires collaboration with EHR vendors to ensure the neces\xad\nsary infrastructure is available. Researchers also can facilitate \nthis integration using data exchange frameworks, such as \nSMART on FHIR.62\n612                                                                                                      \nJournal of the American Medical Informatics Association, 2025, Vol. 32, No. 4 \nDownloaded from https://academic.oup.com/jamia/article/32/4/605/7954485 by guest on 11 January 2026'
 'Omar & Mohammed / Journal of Applied Science and Technology Trends Vol. 06, No. 02, pp. 342 â€“357 (2025) \n \n344 \nevaluated using Exact Match, Precision, Recall, F1, Hits@k, \nMRR, and latency across simple, medium, and complex question \nsets. Unlike prior template-based methods, our approach enables \ntraceable, outcome-level validation. In summary, the main \ncontributions of this work are as follows: \nâ€¢ Hybrid LLM to Cypher QA: A system that translates \nnatural language questions into accurate, executable \nCypher over a Neo4j drug, disease, and symptom KG. \nâ€¢ Prompt-driven query generation: Schema,  entity \naware prompting that reliably maps diverse biomedical \nquestions to graph queries. \nâ€¢ Evidence transparency: Along with each answer, we \nsurface the generated Cypher and the supporting \njustification subgraph (nodes, edges) plus a brief natural \nlanguage rationale. \nâ€¢ Answer-level evaluation: End-to-end assessment using \nExact Match, F1, Precision/Recall, Hits@k, MRR and \nlatency across simple, medium and complex tiers. \nâ€¢ Local, reproducible deployment: On-prem LLaMA 3 \ninference (no cloud dependency) suitable for biomedical \nsettings requiring low latency and strong data control. \n \nThe remainder of this paper is organized as follows: Section \n2 reviews related work on biomedical knowledge graphs and \nLLM-based QA systems. Section 3 provides background on \nknowledge graphs, large language models, and question \nanswering frameworks. Section 4 details the proposed \nmethodology, \nincluding \nsystem \narchitecture, \ndataset \nconstruction, and query translation. Section 5 presents the \nexperimental results through both quantitative metrics and \nqualitative case studies. Section 6 discusses the findings, analyzes \nlimitations, and compares performance against baseline models. \nFinally, Section 7 concludes this paper and outlines directions for \nfuture work. \nII. RELATED WORK \nRecently, studies have concentrated on the integration of \nclinical and medical knowledge graphs (LLM) to improve the \nanswer to medical questions. Researchers have derived several \nbiomedical KGs using Neo4j and incorporated the application of \nLLMs like LLaMA and GPT to convert natural language \nquestions into graph queries. Improvements in answer \ncorrectness, reduction of hallucination errors, one-to-many \nrelationships, and support for complex reasoning were the \nobjectives of these efforts. Some frameworks also adopted \nretrieval methods to ground responses in secure data. \nSu et al.[11] developed an integrative Biomedical Knowledge \nHub (iBKH), a huge biomedical knowledge graph that comprises \n18 of the very best data sources. The deployment of the iBKH in \nNeo4j allows for a user-friendly web portal to allow fast and \ninteractive knowledge retrieval. The system implemented \nadvanced graph learning techniques to enable the discovery of \nbiomedical knowledge, illustrated by an example of repurposing \nin silico drugs for Alzheimerâ€™s disease. iBKH achieved \npromising predictive performance for known drugs and proposed \npossible new drug candidates. \nRajabi and Kafaie[19] proposed a disease knowledge graph \nusing a cross-referential disease database comprising diseases, \nsymptoms, and drugs interconnected with relationships. They \ntransferred the data into Neo4j to create a graph of 9,400 nodes \nand 45,000 relationships representing the semantic links between \nmedical concepts. Applying Cypher queries enabled answering \ncomplex medical questions regarding identifying drugs that may \ncause certain diseases; it was demonstrated that the graph inferred \nnew relationships not explicitly existing in the original data. The \nconclusion was that disease knowledge graphs sped up clinical \ndiscovery and contributed to understanding complex medical \nrelationships. \nHou et al.[3] assessed and contrasted ChatGPT (both GPT-\n3.5 and GPT-4) and the biomedical knowledge graphs (BKGs) \nconcerning their ability to answer biomedical questions, generate \nnew knowledge, and reason. Their datasets were focused on \ndietary supplements and drugs, while evaluation criteria entailed \naccuracy, novelty, and reasoning ability. The results indicate that \nwhile GPT-4 surpassed GPT-3.5 and BKGs in knowledge \nprovision, it proved inconsistent with regard to citations and \nreasoning. Compared to them, BKGs scored higher in accuracy \nand reliability, especially in discovering novel links as well as \nwithin structured reasoning. \nSoman et al.[13] presented a novel framework called KG-\nRAG that integrates a large biomedical knowledge graph \n(SPOKE) with LLaMA 2, GPT-3.5, and GPT-4 (LLMs) to \nproduce accurate biomedical text. They optimized the retrieval of \nrelevant graph context to cut over 50% tokens without losing \naccuracy. It aided LLMs in performing better on biomedical \nquestion answering with very high accuracy boosts, especially in \nthe case of LLaMA 2. They compared KG-RAG to other retrieval \nmethods and indicated its comparatively more robust and \nefficient results. The framework produced reliable evidence-\nbased answers grounded in biomedical knowledge. \nLuo et al.[23] created ChatKBQA, a new framework with a \nquestion-and-answer approach over knowledge bases that first \ngenerates logical forms with the help of fine-tuned LLMs and \nthen retrieves the relevant entities and relations. This generate-\nthen-retrieve approach is supposed to handle a couple of issues \nwith the earlier methods concerning tedious retrieval and error \npropagation. They fine-tuned open-source LLMs like LLaMA 2 \nto change natural-language questions into logical forms with high \naccuracy. The retrieval phase uses unsupervised phrase-level \nsemantic matching in a way that enhances the alignment of \nentities and relations. Experiments on benchmark datasets \nindicate ChatKBQA to be superior to its predecessors, with the \nhighest accuracy to date. \nPusch and Conrad[6] conducted work under a hybrid \napproach conflating LLMs and biomedical Knowledge Graphs \n(KGs) to suppress hallucination errors in question-answering. \nThey proposed query-checking algorithms for validating, \ncorrecting, and executing the KG Cypher queries that LLMs \ngenerated, thereby attaining accurate and understandable \nanswers. The system used retrieval-augmented generation (RAG) \nto ground answers within KG data. The methodology was \nvalidated on a biomedical KG called PrimeKG using 50 \nbenchmark questions, assessing models like GPT-4 Turbo and \nLLaMA 3. Commercially available GPT-4 Turbo obtained']","['<1-hop>\n\nLimitations This study was limited to peer-reviewed publications avail\xad able in biomedical databases (eg, PubMed, Embase), exclud\xad ing preprint articles from repositories like ArXiv. Additionally, only studies in English language were included, which might have excluded relevant studies in other lan\xad guages. We did not include sources such as IEEE Xplore or Google Scholar, which might have additional relevant stud\xad ies. However, our focus was on biomedicine, and we priori\xad tized databases specifically tailored to biomedical research to maintain the relevance and quality of the included studies. Furthermore, we used free-text searches in the databases, which activated automatic mapping to Medical Subject Headings (MeSH) and Emtree terms, improving retrieval accuracy. However, the limitations of automatic term map\xad ping cannot be ignored, as it may introduce variability if the underlying algorithms change. To address this, we have docu\xad mented all identified papers from our search. The title, publi\xad cation year, PMID, PUI, and database source for each study are provided in File S3. Conclusion We conducted a systematic literature review of studies exploring the use of RAG and LLM in clinical tasks. RAG implementation showed a 1.35 odds ratio increase in per\xad formance compared to baseline LLMs. To improve perform\xad ance and transparency in future studies, we developed guidelines for improving clinical RAG applications based on current research findings. Future research could focus on these 3 directions: (1) system-level enhancement: the combi\xad nation of RAG and agent, (2) knowledge-level enhancements: deep integration of knowledge into LLM, and (3) integration- level enhancements: integrating RAG systems within EHRs. Author contributions Siru Liu (Conceptualization, Data curation, Formal analysis, Investigation, Methodology, Software, Visualization, Writing â€“ original draft), Allison B. McCoy (Conceptualization, Writ\xad ing â€“ review & editing), Adam Wright (Conceptualization, Writing â€“ review & editing). Supplementary material Supplementary material is available at Journal of the Ameri\xad can Medical Informatics Association online.'
 '<2-hop>\n\nwas asked about medications for peripheral artery disease patients without increased bleeding risk, it initially omitted low-dose rivaroxaban. After integrating retrieved text from the 2024 American College of Cardiology / American Heart Association Guideline for the Management of Lower Extrem\xad ity Peripheral Artery Disease,10 the model correctly recom\xad mended rivaroxaban. Several guidelines exist for evaluating Artificial Intelligence (AI) applications and LLMs in healthcare, including DECIDE-AI (Developmental and Exploratory Clinical Inves\xad tigations of DEcision support systems driven by Artificial Intelligence),11 CLAIM (Checklist for Artificial Intelligence in Medical Imaging),12 and CONSORT-AI (Consolidated Standards of Reporting Trials-AI).13 In addition, Tam et al. introduced QUEST, a framework specifically for human eval\xad uation of LLMs in healthcare.14 However, these guidelines do not cover RAG applications in clinical settings, emphasiz\xad ing the need for a more specific guideline. Despite the promise of RAG in improving LLM performance in clinical settings, there is limited understanding of its overall effectiveness comparing with the baseline LLM, adoption in clinical domains, and optimal strategies for its development in biomedical applications. The aim of this study is to synthesize findings from recent research of RAG and LLM in biomedicine and provide clinical development guidelines to improve effec\xad tiveness as well as transparency in future research. Materials and methods Study design We conducted a systematic literature review. The report was created in adherence to the Preferred Reporting Items for Sys\xad tematic Reviews and Meta-Analyses 2020 analysis checklist (File S1).15 We also performed a meta-analysis following the Cochrane Handbook guidelines.16 Screening papers We searched in 3 databases (PubMed, Embase, and Psy\xad cINFO), using terms related to â€œretrieval augmented gener\xad ationâ€ and â€œlarge language model.â€ PubMed and Embase were selected based on recommendations from the Cochrane Handbook, as they are widely recognized for their extensive coverage of biomedical literature.17 PsycINFO was included to capture articles from the psychological and behavioral sci\xad ences. To maintain a high standard of quality and reliability, we focused on peer-reviewed articles and excluded preprints. The specific search terms used for each database are provided in File S2. Given that ChatGPT was released on November 30, 2022, we set the publication filter to search papers pub\xad lished in 2023 and 2024. The search was performed on December 12, 2024. The inclusion criteria were: (1) the study must compare baseline LLM performance with RAG per\xad formance and (2) the study must address a biomedical ques\xad tion. The exclusion criteria were: (1) literature reviews, editorial comments, or viewpoint papers, (2) studies focusing on LLMs in languages other than English, or (3) studies cen\xad tered on a multi-agent system without a focus on RAG. SL screened titles and abstracts, then conducted a full-text review of papers meeting the criteria. Data extraction For each included study, we extracted the following informa\xad tion: author, title, publication year, journal, clinical task, and specialty. Regarding RAG techniques, we gathered details about the baseline LLM, retrieval sources, and strategies used in the pre-retrieval, retrieval, and post-retrieval stages. For evaluation, we extracted the evaluation method (human, automated, or a combination of both), the number of evalua\xad tors, the evaluation dataset, and the evaluation metrics.'
 '<3-hop>\n\nMeta-analysis Effect size was defined as a metric quantifying the relation\xad ship between variables, including both direction and magni\xad tude.18 For each included study, we calculated the effect size between baseline LLM performance and RAG-enhanced LLM performance. The outcomes focused on the perform\xad ance of generation results, such as accuracy and usefulness. Metrics related to the retrieval process, cost, or speed were not included as outcomes in the meta-analysis. For continu\xad ous outcomes, we used Cohenâ€™s d, standardized between- group mean difference (SMD), calculated as the difference in means divided by the pooled standard deviation. The stand\xad ard error (SE) of SMD was calculated using the following for\xad mula (1), where n1 and n2 represent the sample sizes of each group.19 For dichotomous measurements, we calculated the log-odds ratio, obtained by transforming the odds ratio (OR) with the natural logarithm, and the associated SE was calcu\xad lated using formula (2), where a, b, c, and d represent the number of successful and failed events in the baseline LLM and RAG-enhanced LLM approaches. For studies reporting multiple outcomes, we used the overall outcome to calculate effect size. If no overall outcome was reported, we averaged the effect sizes of all reported outcomes. We excluded out\xad comes with a sample size of less than 30 to avoid small- sample bias. SESMD Â¼ ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi n1 Ã¾ n2 n1n2 Ã¾ SMD2 2 n1 Ã¾ n2 Ã° Ãž s (1) SElogOR Â¼ ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi 1 a Ã¾ 1 b Ã¾ 1 c Ã¾ 1 d r (2) We developed a random-effect meta-analysis model, because of the variability in RAG architectures and evalua\xad tion datasets among the included studies. The random-effect model was used when individual study effects likely con\xad tained additional sources of variance beyond sampling error. Between-study heterogeneity was assessed using Higgins & Thompsonâ€™s I2 statistic, where 25% indicated low heteroge\xad neity, 50% moderate, and 75% substantial.20 We conducted subgroup analyses to explore performance variations across different factors. First, we analyzed the influence of the baseline LLM, referring to the foundation model (eg, GPT-4 or Llama2) that provides the core architec\xad ture for the system. Second, we examined data retrieval strat\xad egies, categorizing them as simple or complex. Simple strategies included fixed-length chunking and basic similarity search, and we performed a subgroup analysis to compare these with complex retrieval strategies. Third, we analyzed differences based on evaluation methods, distinguishing between human evaluations, such as Likert scale ratings for helpfulness and accuracy, and automatic evaluation metrics, including ROUGE-1 and BLEU. Finally, we conducted a sub\xad group analysis based on the type of task, classifying studies 606 Journal of the American Medical Informatics Association, 2025, Vol. 32, No. 4 Downloaded from https://academic.oup.com/jamia/article/32/4/605/7954485 by guest on 11 January 2026']","The evaluation methods and performance metrics used to compare Retrieval-Augmented Generation (RAG)-enhanced Large Language Models (LLMs) with baseline LLMs in clinical tasks, as described in the provided context, include the following:

### Evaluation Methods:
1. **Human Evaluation:**
   - Conducted in nine studies.
   - Typically involved physicians or domain experts as evaluators (average of 4 evaluators, range 1 to 10).
   - Evaluators assessed model outputs on criteria such as accuracy, helpfulness, understandability, and whether responses met user intent.
   - Some evaluations included patients (e.g., diabetic patients assessing understandability).
   - Human evaluation often used Likert scale ratings.
   - Outcomes from human evaluation showed a higher odds ratio (OR) of 1.65 (95% CI: 1.36-2.03) favoring RAG-enhanced LLMs over baseline LLMs.

2. **Automated Evaluation:**
   - Used in eight studies.
   - Metrics included similarity comparisons between generated sentences and expert-provided answers.
   - Common automated metrics were ROUGE (ROUGE-1, ROUGE-2, ROUGE-L), BLEU, METEOR, and BERTScore.
   - Automated evaluation showed a lower but still significant OR of 1.20 (95% CI: 1.1-1.41) favoring RAG-enhanced LLMs.
   - These metrics quantitatively measure text similarity, reflecting accuracy and completeness of generated responses.

3. **Mixed Evaluation:**
   - Three studies used a combination of human and automated evaluation methods.

4. **Meta-Analysis:**
   - Effect sizes were calculated using standardized mean difference (Cohenâ€™s d) for continuous outcomes and log-odds ratio for dichotomous outcomes.
   - A random-effects meta-analysis model was applied to account for variability in RAG architectures and datasets.
   - Subgroup analyses were performed based on baseline LLM type, retrieval strategy complexity, evaluation method (human vs automated), and clinical task type.

5. **Evaluation Datasets:**
   - Included self-curated datasets (e.g., ClinicalQA benchmark with 314 questions), public benchmarks (e.g., US board exam questions, MedMCQA, LiveQA, MedicationQA), and real-world data such as Electronic Health Records (EHR).
   - Datasets varied in size and complexity, with some focusing on simple fact-based questions and others on complex multi-step reasoning.

6. **Stage-wise Evaluation:**
   - Some studies evaluated not only the final response generation but also intermediate stages such as retrieval accuracy and relevance.
   - Metrics for retrieval included recall, retrieval accuracy, and relevance assessments.

### Performance Metrics:
1. **Accuracy:**
   - Most frequently reported metric.
   - Measured by correct treatment percentages, match rates, AUC, AUPRC, F1 scores, and text similarity metrics (ROUGE, BLEU, METEOR, BERTScore).
   - Reflects how correctly the model answers clinical questions or generates factual information.

2. **Completeness:**
   - Assessed whether responses included all necessary information.
   - Often measured using Likert scales in human evaluations.

3. **User Perception:**
   - Subjective feedback on understandability, helpfulness, and whether the response met user intent.
   - Important for assessing the practical usefulness of the model outputs in clinical settings.

4. **Safety:**
   - Metrics focused on avoiding harmful content, adversarial safety, and risk management.
   - Included assessments of factual accuracy and ethical considerations.
   - Some studies used adversarial prompting to test model robustness.

5. **Hallucination:**
   - Identification of nonfactual or conflicting information, primarily through manual review.
   - Important for ensuring reliability and trustworthiness.

6. **Citation Accuracy:**
   - Evaluated whether references provided by the model were valid and pointed to established sources.

7. **Bias and Language Quality:**
   - Evaluated for clarity, neutrality, and empathy, ensuring responses were unbiased and patient-centered.

8. **Retrieval-Specific Metrics:**
   - Recall, precision, and relevance of retrieved documents or knowledge.
   - Hits@k, Mean Reciprocal Rank (MRR), and latency were used in some studies, especially those involving knowledge graph queries.

### Reflection on Accuracy and Usefulness:
- The combination of human and automated evaluations provides a comprehensive assessment of both factual correctness and practical utility.
- Human evaluations capture nuanced aspects such as helpfulness and understandability, which are critical for clinical adoption.
- Automated metrics offer objective, reproducible measures of accuracy and completeness.
- Safety and hallucination assessments ensure that models do not produce harmful or misleading information, which is vital in healthcare.
- Citation and bias evaluations contribute to the trustworthiness and ethical deployment of these models.
- Stage-wise evaluation of retrieval and generation processes helps identify bottlenecks and improve system transparency and reproducibility.
- Meta-analytic results showing statistically significant improvements in accuracy and usefulness metrics for RAG-enhanced LLMs over baseline LLMs support the effectiveness of RAG in clinical applications.

---

**In summary,** the evaluation of RAG-enhanced LLMs in clinical tasks employs a mix of human and automated methods, using metrics that assess accuracy, completeness, safety, user perception, and retrieval quality. These methods collectively reflect both the factual correctness and practical usefulness of the models, ensuring that improvements over baseline LLMs are meaningful and clinically relevant.","Studies comparing RAG-enhanced LLMs with baseline LLMs in clinical tasks have employed various evaluation methods, including human evaluations such as Likert scale ratings for helpfulness and accuracy, and automated evaluation metrics like ROUGE-1 and BLEU. Several guidelines exist for evaluating AI applications and LLMs in healthcare, including DECIDE-AI, CLAIM, CONSORT-AI, and the QUEST framework for human evaluation of LLMs, though these do not specifically cover RAG applications, highlighting the need for more tailored guidelines. Performance metrics focused on generation results such as accuracy and usefulness were analyzed in a meta-analysis using effect size measures like Cohenâ€™s d and log-odds ratio. The meta-analysis excluded metrics related to retrieval process, cost, or speed, concentrating on the quality of generated outputs. Subgroup analyses further explored variations in performance based on baseline LLM models, retrieval strategies, evaluation methods, and task types, demonstrating a comprehensive approach to assessing RAG and LLM performance in biomedical applications.",0.989010989010989,0.9540277294803875,0.99999999995,1.0
How does the system leverage cypher query generation and multi-hop reasoning with confidence scoring to ensure accurate and reliable biomedical question answering over the iBKH knowledge graph?,"['Omar & Mohammed / Journal of Applied Science and Technology Trends Vol. 06, No. 02, pp. 342 â€“357 (2025) \n \n356 \n \nFig. 7. Side effects of drugs that treat epilepsy \n \nExecuting this query returns drugs associated with epilepsy \nand their corresponding side effects. For example, the query \nidentifies Pregabalin as a treatment for epilepsy and retrieves \nmultiple side effects such as anxiety, arrhythmia, gastrointestinal \npain, and others. The visualization of the graph in Fig. 7, \nhighlights this pattern, where the drug node is linked to epilepsy \nvia TREATS and to several side effect nodes via CAUSES, \nproviding an interpretable biomedical knowledge structure. \nVI. DISCUSSION \nUnder \na \nunified \nevaluation \nprotocol \nreporting \nP@k/R@k/F1@k, Hits@k, and MRR at ð‘˜âˆˆ{1,5,10} , the \npipeline exhibits consistent end-to-end behavior across the three \ndifficulty tiers. The ð‘˜-ablation shows strong early precision (high \nP@1 and elevated MRR), while recall increases with larger ð‘˜, \nindicating that correct answers typically surface near the top yet \npersist deeper in the ranked slate. At the query-generation level, \nexact-match (EM) is high for simple questions (e.g., âˆ¼96%) and \nlower for complex, multi-hop questions (e.g., âˆ¼86.7%), which \naligns with increased compositionality. End-to-end latency \n(mean total â‰ˆ6.07\u2006s from question to final answer) remains within \ninteractive bounds on our local setup. Practically, a fully offline \ndeployment avoids internet connectivity, API keys, and external \ndata \ntransfer, \nstrengthening \nprivacy, \ncompliance, \nand \nreproducibility in biomedical contexts. \nSeveral challenges qualify these findings. First, the \ncomparability challenge: to our knowledge, no prior study \nevaluates iBKH using the same metric suite, making cross-paper \nnumeric comparisons not â€œapples to apples.â€ We therefore \ninterpret the results as controlled evidence about this pipeline \nunder a single, consistent protocol rather than as a cross-study \nleaderboard. Beyond comparability, performance may vary with \nother biomedical knowledge graphs; the current iBKH snapshot \nis static, limiting real-time updates; and scaling to larger or \ndynamically refreshed graphs can introduce latency and \nconsistency trade-offs. \nError analysis shows that residual failures concentrate in \ncomplex, multi-hop queries where missing relation hops or brittle \nname-based matching (synonyms, abbreviations, homonyms) \nlead to partial answers. These observations motivate concept-\nlevel normalization via biomedical identifiers (e.g., UMLS, \nSNOMED, RxNorm) with synonym expansion, as well as \nschema-constrained query generation and path-guided decoding \nto better satisfy multi-hop constraints. Finally, correctness is \nassessed primarily at the answer level (Hits@k, MRR, \nprecision/recall) and does not yet include full semantic-\nequivalence checks across alternative Cypher queries, which may \noverlook cases where different queries yield the same correct \nresults. Complementing template EM with result-set equivalence \nchecks, expanding the metric suite (e.g., nDCG/MAP) with per-\nquestion 95% bootstrap confidence intervals, and supporting \nincremental graph updates with distributed storage are promising \nsteps to enhance robustness, scalability, and external validity. \nVII. CONCLUSION \nThis study introduced a hybrid biomedical question \nanswering framework that couples the LLaMA-3B language \nmodel with a Neo4j-based iBKH knowledge graph to enable the \nautomatic generation of executable Cypher queries and to deliver \ntransparent, evidence-grounded answers through justification \nsubgraphs. Evaluation in simple, medium, and complex queries \ndemonstrated consistently high performance, with strong \nprecision, recall, F1, Hits@k, and MRR values, while \nmaintaining low latency suitable for interactive biomedical \napplications. Beyond quantitative performance, the proposed \nsystem provides a reproducible and privacy-preserving solution \nby operating fully offline, a property of particular importance in \nsensitive clinical and research contexts. However, certain \nlimitations remain. The reliance on a static iBKH snapshot \nconstrains coverage and adaptability, recall is lower for complex \nmulti-hop reasoning, and the absence of canonical entity \nnormalization (e.g., UMLS, SNOMED, RxNorm) may reduce \nsemantic robustness. Future research will therefore focus on \nintegrating standardized biomedical entity normalization, \nenabling dynamic and incremental knowledge graph updates, and \nleveraging domain-adapted or fine-tuned biomedical LLMs. \nThese directions are expected to further strengthen the \nrobustness, scalability, and applicability of the framework in real-\nworld biomedical and clinical environments. \n \nREFERENCES \n[1] S. Bubeck, V. Chandrasekaran, R. Eldan, J. Gehrke, E. Horvitz, E. Kamar, \nP. Lee, Y. T. Lee, Y. Li, S. Lundberg, et al., â€œSparks of artificial general \nintelligence: \nEarly experiments with \ngpt-4,â€ arXiv preprint \narXiv:2303.12712, 2023. \n[2] J. Achiam, S. Adler, S. Agarwal, L. Ahmad, I. Akkaya, F. L. Aleman, D. \nAlmeida, J. Altenschmidt, S. Altman, S. Anadkat, et al., â€œGpt-4 \ntechnical report,â€ arXiv preprint arXiv:2303.08774, 2023. \n[3] Y. Hou, J. Yeung, H. Xu, C. Su, F. Wang, and R. Zhang, â€œFrom answers to \ninsights: unveiling the strengths and limitations of chatgpt and \nbiomedical knowledge graphs,â€ Research Square, pp. rsâ€“3, 2023.'
 'Omar & Mohammed / Journal of Applied Science and Technology Trends Vol. 06, No. 02, pp. 342 â€“357 (2025) \n \n344 \nevaluated using Exact Match, Precision, Recall, F1, Hits@k, \nMRR, and latency across simple, medium, and complex question \nsets. Unlike prior template-based methods, our approach enables \ntraceable, outcome-level validation. In summary, the main \ncontributions of this work are as follows: \nâ€¢ Hybrid LLM to Cypher QA: A system that translates \nnatural language questions into accurate, executable \nCypher over a Neo4j drug, disease, and symptom KG. \nâ€¢ Prompt-driven query generation: Schema,  entity \naware prompting that reliably maps diverse biomedical \nquestions to graph queries. \nâ€¢ Evidence transparency: Along with each answer, we \nsurface the generated Cypher and the supporting \njustification subgraph (nodes, edges) plus a brief natural \nlanguage rationale. \nâ€¢ Answer-level evaluation: End-to-end assessment using \nExact Match, F1, Precision/Recall, Hits@k, MRR and \nlatency across simple, medium and complex tiers. \nâ€¢ Local, reproducible deployment: On-prem LLaMA 3 \ninference (no cloud dependency) suitable for biomedical \nsettings requiring low latency and strong data control. \n \nThe remainder of this paper is organized as follows: Section \n2 reviews related work on biomedical knowledge graphs and \nLLM-based QA systems. Section 3 provides background on \nknowledge graphs, large language models, and question \nanswering frameworks. Section 4 details the proposed \nmethodology, \nincluding \nsystem \narchitecture, \ndataset \nconstruction, and query translation. Section 5 presents the \nexperimental results through both quantitative metrics and \nqualitative case studies. Section 6 discusses the findings, analyzes \nlimitations, and compares performance against baseline models. \nFinally, Section 7 concludes this paper and outlines directions for \nfuture work. \nII. RELATED WORK \nRecently, studies have concentrated on the integration of \nclinical and medical knowledge graphs (LLM) to improve the \nanswer to medical questions. Researchers have derived several \nbiomedical KGs using Neo4j and incorporated the application of \nLLMs like LLaMA and GPT to convert natural language \nquestions into graph queries. Improvements in answer \ncorrectness, reduction of hallucination errors, one-to-many \nrelationships, and support for complex reasoning were the \nobjectives of these efforts. Some frameworks also adopted \nretrieval methods to ground responses in secure data. \nSu et al.[11] developed an integrative Biomedical Knowledge \nHub (iBKH), a huge biomedical knowledge graph that comprises \n18 of the very best data sources. The deployment of the iBKH in \nNeo4j allows for a user-friendly web portal to allow fast and \ninteractive knowledge retrieval. The system implemented \nadvanced graph learning techniques to enable the discovery of \nbiomedical knowledge, illustrated by an example of repurposing \nin silico drugs for Alzheimerâ€™s disease. iBKH achieved \npromising predictive performance for known drugs and proposed \npossible new drug candidates. \nRajabi and Kafaie[19] proposed a disease knowledge graph \nusing a cross-referential disease database comprising diseases, \nsymptoms, and drugs interconnected with relationships. They \ntransferred the data into Neo4j to create a graph of 9,400 nodes \nand 45,000 relationships representing the semantic links between \nmedical concepts. Applying Cypher queries enabled answering \ncomplex medical questions regarding identifying drugs that may \ncause certain diseases; it was demonstrated that the graph inferred \nnew relationships not explicitly existing in the original data. The \nconclusion was that disease knowledge graphs sped up clinical \ndiscovery and contributed to understanding complex medical \nrelationships. \nHou et al.[3] assessed and contrasted ChatGPT (both GPT-\n3.5 and GPT-4) and the biomedical knowledge graphs (BKGs) \nconcerning their ability to answer biomedical questions, generate \nnew knowledge, and reason. Their datasets were focused on \ndietary supplements and drugs, while evaluation criteria entailed \naccuracy, novelty, and reasoning ability. The results indicate that \nwhile GPT-4 surpassed GPT-3.5 and BKGs in knowledge \nprovision, it proved inconsistent with regard to citations and \nreasoning. Compared to them, BKGs scored higher in accuracy \nand reliability, especially in discovering novel links as well as \nwithin structured reasoning. \nSoman et al.[13] presented a novel framework called KG-\nRAG that integrates a large biomedical knowledge graph \n(SPOKE) with LLaMA 2, GPT-3.5, and GPT-4 (LLMs) to \nproduce accurate biomedical text. They optimized the retrieval of \nrelevant graph context to cut over 50% tokens without losing \naccuracy. It aided LLMs in performing better on biomedical \nquestion answering with very high accuracy boosts, especially in \nthe case of LLaMA 2. They compared KG-RAG to other retrieval \nmethods and indicated its comparatively more robust and \nefficient results. The framework produced reliable evidence-\nbased answers grounded in biomedical knowledge. \nLuo et al.[23] created ChatKBQA, a new framework with a \nquestion-and-answer approach over knowledge bases that first \ngenerates logical forms with the help of fine-tuned LLMs and \nthen retrieves the relevant entities and relations. This generate-\nthen-retrieve approach is supposed to handle a couple of issues \nwith the earlier methods concerning tedious retrieval and error \npropagation. They fine-tuned open-source LLMs like LLaMA 2 \nto change natural-language questions into logical forms with high \naccuracy. The retrieval phase uses unsupervised phrase-level \nsemantic matching in a way that enhances the alignment of \nentities and relations. Experiments on benchmark datasets \nindicate ChatKBQA to be superior to its predecessors, with the \nhighest accuracy to date. \nPusch and Conrad[6] conducted work under a hybrid \napproach conflating LLMs and biomedical Knowledge Graphs \n(KGs) to suppress hallucination errors in question-answering. \nThey proposed query-checking algorithms for validating, \ncorrecting, and executing the KG Cypher queries that LLMs \ngenerated, thereby attaining accurate and understandable \nanswers. The system used retrieval-augmented generation (RAG) \nto ground answers within KG data. The methodology was \nvalidated on a biomedical KG called PrimeKG using 50 \nbenchmark questions, assessing models like GPT-4 Turbo and \nLLaMA 3. Commercially available GPT-4 Turbo obtained'
 'Omar & Mohammed / Journal of Applied Science and Technology Trends Vol. 06, No. 02, pp. 342 â€“357 (2025) \n \n345 \nrecord-high accuracy, while open-source models achieved \nimpressive gains through prompt optimization. \nFeng et al.[22] developed the Knowledge Graph-based \nThought (KGT) framework that integrated LLMs with a pan-\ncancer knowledge graph for biomedical question answering. \nKGT was designed to reason on the knowledge graph schema and \nidentify optimal subgraphs to use for directing accurate answer \ngeneration, all without fine-tuning the LLMs. The framework is \nbenchmarked against a new dataset (PcQA) designed specifically \nfor pan-cancer KGQA tasks and has outperformed all existing \nstate-of-the-art approaches by a rather large margin. KGTâ€™s \npracticality in biomedical issues was highlighted through case \nstudies for drug repositioning, drug resistance, and biomarker \ndiscovery. Their approach exhibited robust adaptability among \nvarious LLMs. \nRezaei et al.[26] developed AMG-RAG, a dynamic \nframework that utilizes autonomous LLM agents with medical \nsearch tools in the continuous construction and real-time updating \nof Medical Knowledge Graphs (MKGs). Their system \nincorporated confidence scoring and multi-hop reasoning to \nimprove accuracy and interpretability in medical question \nanswering. AMG-RAG outperformed size models on both very \nhard MEDQA benchmarks and more accessible MedMCQA \nones, proving that it could conduct efficient reasoning based on \ncurrent structured medical knowledge. They also used Neo4j to \nmanage the knowledge graphs while adding external searches to \nensure the latest data. \nTiwari et al.[24] presented Auto-Cypher, a recent automated \npipeline for producing high-quality synthetic data for training \nLLMs by mapping natural language to Cypher queries for graph \ndatabases like Neo4j. The pipeline deployed the novelty of LLM-\nas-database-filler to synthesize Neo4j databases for the execution \nof generated queries to ensure their correctness. A sizable dataset \ncalled SynthCypher was created, spanning multiple domains and \ncomplex queries, leading to a 40% improvement in LLM \nperformance on Cypher generation. The datasets were used to \nfine-tune open-source models such as LLaMA, Mistral, and \nQwen, and the SPIDER benchmark was adapted for evaluation \npurposes. \nMohammed et al.[29] proposed a hybridized GraphRAG \nframework combining Neo4j-based UMLS knowledge graphs \nwith a vector store for medical textbooks to create an improved \nU.S.M.L.E.-style clinical question-answering approach. The \nproject integrated symbolic reasoning from knowledge graphs \nwith semantic retrieval performed on text embeddings to enhance \nrelevance and accuracy via adaptive re-ranking and query \nexpansion. The system had the answers produced by GPT-4o-\nMini, with different prompting strategies encouraging evidence-\nbased and traceable responses grounded in verified medical \nknowledge. Experiments showed that the hybrid approach \nimproved factual accuracy and citation fidelity as compared to \nthe L.L.M.-only approach, enhancing transparency and \nreliability. It is shown that binding both structured and \nunstructured medical knowledge sources could aid in \nameliorating hallucinations and hence improve clinical \ntrustworthiness in AI-driven medical QA. \nYang et al.[30] articulated sepsis knowledge graph was \ncrafted by combining multicenter clinical data from over 10,000 \npatients with the help of GPT-4 for entity recognition and \nrelationship extraction. Real-world data were collected from \nthree hospitals and integrated with clinical guidelines and \ndatabases from the public domain. The knowledge graph \ncontained 1,894 nodes and 2,021 relationships pertaining to \ndiseases, symptoms, biomarkers, treatments, and complications. \nGPT outperformed other models in every resolution on sepsis-\nspecific datasets to obtain high F1-score results. The constructed \ngraph highlighted complex interactions in sepsis for assisting \nclinical decision-making and was implemented on Neo4j. \nGuan et al.[20] proposed a novel method for constructing a \nlocal knowledge graph from retrieved biomedical documents by \nextracting propositional claims. They carried out layer wise \nsummarization on this graph to capture multi-document \nrelationships and provide comprehensive contextual information \nto a language model for question-answering purposes. The \nmethod resolved issues in multi-document biomedical QA, such \nas noise cancellation and efficient context usage. They then tested \ntheir method on several benchmarks for biomedical question \nanswering, achieving performance at least comparable to, if not \nbetter than, existing retrieval-augmented generation (RAG) \nbaselines. The study established enhanced reasoning and answer \naccuracy of the model achieved through structured graph \nsummarization.   \nPrevious studies have improved biomedical QA using KGs \nand LLMs, but important gaps remain. Most systems lack \ntransparent, graph-based justifications, rely on limited evaluation \nmethods, or depend on cloud resources that reduce privacy and \nreproducibility. Our framework addresses these gaps by \nproviding visible Cypher queries with evidence subgraphs, \napplying comprehensive performance metrics across difficulty \nlevels, and ensuring fully local, privacy-preserving deployment. \nTable I summarizes key previous studies on biomedical \nknowledge graphs and question answering, outlining their \nmethods, datasets, and main limitations. \nIII. PRELIMINARIES \nThis section outlines the fundamental concepts required to \nunderstand the proposed system. It introduces biomedical \nknowledge graphs, explains how Neo4j stores data in graph \nform, and describes the use of Cypher for querying. It also \nprovides a brief overview of large language models (LLMs) and \ntheir role in interpreting natural language. \n \n \nA. Biomedical Knowledge Graphs \nBiomedical Knowledge Graphs (BKGs) provide a structured \nrepresentation of complex biomedical information by modeling \ndiverse medical entities, such as diseases, drugs, symptoms, and \nbiological pathways, as interconnected nodes within a graph \nstructure. The edges in these graphs represent the semantic \nrelationships between these entities, including â€™treatsâ€™, â€™causesâ€™, \nâ€™interacts withâ€™ and many others, as illustrated in Fig 1. This \nform of representation enables the integration of heterogeneous \nbiomedical data from a wide range of sources, including'
 'Omar & Mohammed / Journal of Applied Science and Technology Trends Vol. 06, No. 02, pp. 342 â€“357 (2025) \n \n351 \nThis query searches for a disease node whose name contains \nâ€™alzheimerâ€™ and follows HAS_SYMPTOM edges to list related \nsymptom names. The system then executes this cypher to retrieve \nanswers. The prompts (such as few-shot examples and schema \nhints) were carefully designed to help LLaMA 3 generate correct \nCypher queries. The model learns how to use the graphâ€™s labels \nand relationships properly. For example, if a user asks, â€™Which \ndrugs treat diabetes?â€™, LLaMA might respond with a suitable \nCypher query: \n \nMATCH (d:Drug)-[:TREATS]->(di:Disease) \nWHERE toLower(di.name) CONTAINS ""diabetes"" \nRETURN d.name  \n \nThis queries for drug nodes that have a TREATS edge to a \ndiabetes disease node. By leveraging LLaMA 3 in this way, our \nsystem can flexibly handle many phrasing variations without \nmanual mapping rules. \nD. Model Configuration & Decoding \nWe run a local LLaMA 3.2-3B model in GGUF format (llama-\n3.2-3b-instruct-q4_k_m.gguf) via llama.cpp, as shown in Table \nIV. \nTABLE IV.  MODEL RUNTIME AND DECODING SETTINGS \nRuntime settings \nDecoding settings \nn_ctx = 1024 \ntemperature = 0.2 \nn_threads = 12 \ntop_p = 0.95 \nn_gpu_layers = 33 \ntop_k = 40 \nn_batch = 512 \nrepeat_penalty = 1.1 \n \nmax_tokens = 80 \n \nseed = 42 \n \nE. Graph Subset and Versioning \nWe use an iBKH derived subgraph (â‰ˆ65.8k nodes; â‰ˆ3.0M \nedges) spanning DRUG, DISEASE, SYMPTOM, PATHWAY. IDs are \nnormalized to CURIEs and duplicates collapsed across \nUMLS/DrugBank/DisGeNET/SIDER/KEGG. Each edge stores \nprovenance/licensing metadata (source, source_version, license, \nretrieved_at, \nevidence_pmids/urls) \nand, \nwhen \navailable, \ntemporal fields (valid_from, valid_to). We report coverage as \nthe \npercentage \nof \nevaluated \nquestions \nwhose \ngold \nentities/relations are present. \nF. Query Execution and Reliability \nAfter a Cypher query is generated, it is executed on the Neo4j \ndatabase through the official Neo4j Python driver, which \nmanages the secure connection and returns the results. Instead of \nrestricting the output with a fixed LIMIT (e.g., LIMIT 5), the \nsystem retrieves candidate results and evaluates them using \nstandardized retrieval metrics such as Hits@1, Hits@5, and \nHits@10. This approach ensures that the system remains \nresponsive while providing a fair assessment of ranking quality \nacross different cutoff levels, rather than depending solely on a \nfixed number of returned items. Neo4jâ€™s indexing on key node \nproperties, such as name and primary identifiers, also helps speed \nup lookups as the knowledge graph grows. In cases where the \nlanguage model generates an incomplete or incorrect query, such \nas referencing nodes or relationships that do not exist, the system \ncatches the error and either retries with a simpler prompt or \ninforms the user. Together, these steps make sure that queries run \nquickly, return valid results, and keep the overall experience \nsmooth and reliable for biomedical question-answering. \nG. User Interface for Query Transparency \nThe system includes a lightweight, cross-platform graphical \nuser interface (GUI) implemented as a web application using the \nFlask framework in Python, with HTML and Bootstrap for \ninteractive visualization. The interface is designed to make the \nquestion-answering process transparent and accessible to users \nwithout technical expertise. It consists of three main panels: \n1. Input Panel: Where the user can enter a biomedical \nquestion in natural language. \n2. Query Panel: Which displays the Cypher query generated \nby the language model, allowing users to verify how their \nquestion was interpreted. \n3. Results Panel: Which presents the retrieved answers in a \nclear, readable format, accompanied by a brief natural \nlanguage explanation generated by the system. \nBy showing both the query and the answer, the GUI promotes \nuser trust and enables validation of the systemâ€™s reasoning \nprocess. The interface is lightweight enough to run smoothly on \nstandard desktop machines without additional dependencies, \nmaking it practical for local deployments in clinical or research \nsettings. Fig. 4 illustrates the overall layout. \nV. EXPERIMENTAL RESULTS \nIn this section, the proposed system is evaluated based on its \nability to translate natural-language biomedical questions into \nexecutable Cypher queries over the iBKH knowledge graph. The \nassessment is conducted end-to-end and focuses on three main \naspects: (i) the accuracy of query generation, measured by the \nproportion of correctly produced Cypher queries; (ii) system \nefficiency, quantified through total response time from question \nsubmission to final answer delivery, covering both query \ngeneration and execution; and (iii) the quality of retrieved \nanswers, evaluated using standardized information retrieval \nmetrics including Precision@k, Recall@k, F1@k, Hits@k, and \nMean Reciprocal Rank (MRR). For clarity, all these metrics are \nformally defined in the following subsections, with their \ncorresponding equations, and will be used consistently \nthroughout the results section. Together, these dimensions \nprovide a comprehensive view of both correctness and efficiency \nacross simple, medium, and complex biomedical queries. \nA. Description of the Experimental Data \nTo evaluate the proposed system, this work used a carefully \nselected set of biomedical questions designed to test how well the \nsystem understands natural language and converts it into Cypher \nqueries for a biomedical knowledge graph. \nTo evaluate the system, a benchmark of 60 biomedical \nquestions was constructed by the authors, guided by the schema \nand relations in iBKH. The questions were grouped into simple, \nmedium, and complex levels to assess performance across \ndifferent reasoning requirements. Gold-standard answers were \nmanually prepared for each question to enable quantitative \nevaluation using Exact Match, Precision@k, Recall@k, F1@k, \nHits@k, and MRR. The complete set of 60 questions is available \nat this link https://drive.google.com/drive/my-drive.'
 'Vol. 06, No. 02, pp. 342 â€“357 (2025) \nISSN: 2708-0757 \n \nJOURNAL OF APPLIED SCIENCE AND TECHNOLOGY TRENDS \n \nwww.jastt.org  \n \n                                                          342 \ndoi: 10.38094/jastt62404  \nA Hybrid LLMâ€“Knowledge Graph Framework for Accurate \nBiomedical Question Answering \n \nHavraz Y. Omar1,Â²*, Abdulhakeem O. MohammedÂ³  \n \nÂ¹Department of Information Technology, Technical College of Duhok, Duhok Polytechnic University, Duhok, Kurdistan Region, \nIraq. havraz.omar@dpu.edu.krd  \nÂ² Department of Information Technology, Technical College of Informatics â€“ Akre, Akre University for Applied Sciences, Akre, \nKurdistan Region, Iraq. \nÂ³ Department of Computer Science, College of Science, University of Zakho, Zakho, Kurdistan Region, Iraq. \na.mohammed@uoz.edu.krd \n \n*Correspondence: havraz.omar@dpu.edu.krd \n \nAbstract \nBiomedical question answering requires accurate and interpretable systems; however, existing approaches often face challenges such as \nlanguage model hallucinations and limited reasoning when relying solely on standalone knowledge graphs. To address these limitations, \nthis study proposes a hybrid framework that integrates the LLaMA-3B language model with a Neo4j-based drugâ€“diseaseâ€“symptom \nknowledge graph. The system translates natural language questions into executable Cypher queries, operates on an iBKH-derived graph \ncomprising over 65,000 entities and 3 million relationships, and returns answers with supporting evidence through a transparent interface. \nExperiments conducted on 60 biomedical questions across three levels of difficulty demonstrate the robustness of the approach: 96% \nexact match for simple queries, 95% for medium queries, and 86.7% for complex queries. Overall, the system achieves Precision@5 of \n96.1%, Recall@5 of 89.0%, F1@5 of 91.0%, Hits@k of 96.1%, and an MRR of 94.4%, while maintaining an average response time of \nonly 6.07 seconds. These results indicate that the system retrieves nearly all relevant answers, ranks them correctly, and delivers them \nwith latency low enough for interactive use. Moreover, unlike cloud-based APIs such as ChatGPT, which require internet connectivity \nand external data transmission, the proposed framework operates fully offline, ensuring privacy, reproducibility, and compliance with \nbiomedical data governance. Overall, this pipeline provides an accurate, efficient, and privacy-preserving solution for biomedical question \nanswering, making it a practical alternative to cloud-dependent approaches in sensitive healthcare contexts. \n \nKeywords: Knowledge Graph, LLM, Question Answering, Neo4j, Biomedical Informatics, Healthcare AI, LLaMA 3. \n \nReceived: August 14th, 2025 / Revised: October 10th, 2025 / Accepted: October 16th, 2025 / Online: October 20th, 2025 \n \nI. INTRODUCTION \nAnswering questions in the biomedical field is a difficult task \ndue to the complexity of medical knowledge and the need for \nprecision. In recent years, large language models  (LLMs) like \nLLaMA, GPT-4 have made progress in understanding and \ngenerating human-like responses to medical questions  [1,  2]. \nThese models can process large amounts of information and \nrespond in natural language, which makes them helpful in \nhealthcare settings [3]. However, they often struggle to provide \naccurate answers when dealing with specialized biomedical \ncontent [4, 5]. \nOne major issue with LLMs is a problem called hallucination, \nwhere the model generates information that sounds right but is \nactually incorrect or unsupported [6]. In medical applications, \nthis can be dangerous, as healthcare professionals rely on precise \nand trustworthy information  [7]. Therefore, researchers are \nexploring ways to combine LLMs with structured sources of \nknowledge to improve their reliability [8]. \nLLM-only systems in biomedicine still hallucinate and are \nhard to verify, limiting safe use [9, 10]. Biomedical knowledge \ngraphs (BKGs) such as iBKH and SPOKE curate multi-source \nfacts and enable multi-hop reasoning, yet they neither interpret \nfree text nor generate answers [11, 12]. Recent hybrids (KG-\naware RAG) improve grounding but often lack explicit path-level \njustifications and robust end-to-end answer evaluation [13, 14]. \nRecent studies have increasingly integrated Knowledge \nGraphs (KGs) with Large Language Models (LLMs) to improve \nfactual accuracy, reasoning, and reduce hallucinations. Notable \nexamples include DR.KNOWS, which combines UMLS-based \nKGs with LLMs for better diagnostic reasoning [15], KnowNet']","['<1-hop>\n\nOmar & Mohammed / Journal of Applied Science and Technology Trends Vol. 06, No. 02, pp. 342 â€“357 (2025) 351 This query searches for a disease node whose name contains â€™alzheimerâ€™ and follows HAS_SYMPTOM edges to list related symptom names. The system then executes this cypher to retrieve answers. The prompts (such as few-shot examples and schema hints) were carefully designed to help LLaMA 3 generate correct Cypher queries. The model learns how to use the graphâ€™s labels and relationships properly. For example, if a user asks, â€™Which drugs treat diabetes?â€™, LLaMA might respond with a suitable Cypher query: MATCH (d:Drug)-[:TREATS]->(di:Disease) WHERE toLower(di.name) CONTAINS ""diabetes"" RETURN d.name This queries for drug nodes that have a TREATS edge to a diabetes disease node. By leveraging LLaMA 3 in this way, our system can flexibly handle many phrasing variations without manual mapping rules. D. Model Configuration & Decoding We run a local LLaMA 3.2-3B model in GGUF format (llama- 3.2-3b-instruct-q4_k_m.gguf) via llama.cpp, as shown in Table IV. TABLE IV. MODEL RUNTIME AND DECODING SETTINGS Runtime settings Decoding settings n_ctx = 1024 temperature = 0.2 n_threads = 12 top_p = 0.95 n_gpu_layers = 33 top_k = 40 n_batch = 512 repeat_penalty = 1.1 max_tokens = 80 seed = 42 E. Graph Subset and Versioning We use an iBKH derived subgraph (â‰ˆ65.8k nodes; â‰ˆ3.0M edges) spanning DRUG, DISEASE, SYMPTOM, PATHWAY. IDs are normalized to CURIEs and duplicates collapsed across UMLS/DrugBank/DisGeNET/SIDER/KEGG. Each edge stores provenance/licensing metadata (source, source_version, license, retrieved_at, evidence_pmids/urls) and, when available, temporal fields (valid_from, valid_to). We report coverage as the percentage of evaluated questions whose gold entities/relations are present. F. Query Execution and Reliability After a Cypher query is generated, it is executed on the Neo4j database through the official Neo4j Python driver, which manages the secure connection and returns the results. Instead of restricting the output with a fixed LIMIT (e.g., LIMIT 5), the system retrieves candidate results and evaluates them using standardized retrieval metrics such as Hits@1, Hits@5, and Hits@10. This approach ensures that the system remains responsive while providing a fair assessment of ranking quality across different cutoff levels, rather than depending solely on a fixed number of returned items. Neo4jâ€™s indexing on key node properties, such as name and primary identifiers, also helps speed up lookups as the knowledge graph grows. In cases where the language model generates an incomplete or incorrect query, such as referencing nodes or relationships that do not exist, the system catches the error and either retries with a simpler prompt or informs the user. Together, these steps make sure that queries run quickly, return valid results, and keep the overall experience smooth and reliable for biomedical question-answering. G. User Interface for Query Transparency The system includes a lightweight, cross-platform graphical user interface (GUI) implemented as a web application using the Flask framework in Python, with HTML and Bootstrap for interactive visualization. The interface is designed to make the question-answering process transparent and accessible to users without technical expertise. It consists of three main panels: 1. Input Panel: Where the user can enter a biomedical question in natural language. 2. Query Panel: Which displays the Cypher query generated by the language model, allowing users to verify how their question was interpreted. 3. Results Panel: Which presents the retrieved answers in a clear, readable format, accompanied by a brief natural language explanation generated by the system. By showing both the query and the answer, the GUI promotes user trust and enables validation of the systemâ€™s reasoning process. The interface is lightweight enough to run smoothly on standard desktop machines without additional dependencies, making it practical for local deployments in clinical or research settings. Fig. 4 illustrates the overall layout.'
 '<2-hop>\n\nV. EXPERIMENTAL RESULTS In this section, the proposed system is evaluated based on its ability to translate natural-language biomedical questions into executable Cypher queries over the iBKH knowledge graph. The assessment is conducted end-to-end and focuses on three main aspects: (i) the accuracy of query generation, measured by the proportion of correctly produced Cypher queries; (ii) system efficiency, quantified through total response time from question submission to final answer delivery, covering both query generation and execution; and (iii) the quality of retrieved answers, evaluated using standardized information retrieval metrics including Precision@k, Recall@k, F1@k, Hits@k, and Mean Reciprocal Rank (MRR). For clarity, all these metrics are formally defined in the following subsections, with their corresponding equations, and will be used consistently throughout the results section. Together, these dimensions provide a comprehensive view of both correctness and efficiency across simple, medium, and complex biomedical queries. A. Description of the Experimental Data To evaluate the proposed system, this work used a carefully selected set of biomedical questions designed to test how well the system understands natural language and converts it into Cypher queries for a biomedical knowledge graph. To evaluate the system, a benchmark of 60 biomedical questions was constructed by the authors, guided by the schema and relations in iBKH. The questions were grouped into simple, medium, and complex levels to assess performance across different reasoning requirements. Gold-standard answers were manually prepared for each question to enable quantitative evaluation using Exact Match, Precision@k, Recall@k, F1@k, Hits@k, and MRR. The complete set of 60 questions is available at this link https://drive.google.com/drive/my-drive.'
 '<3-hop>\n\nOmar & Mohammed / Journal of Applied Science and Technology Trends Vol. 06, No. 02, pp. 342 â€“357 (2025) 345 record-high accuracy, while open-source models achieved impressive gains through prompt optimization. Feng et al.[22] developed the Knowledge Graph-based Thought (KGT) framework that integrated LLMs with a pan- cancer knowledge graph for biomedical question answering. KGT was designed to reason on the knowledge graph schema and identify optimal subgraphs to use for directing accurate answer generation, all without fine-tuning the LLMs. The framework is benchmarked against a new dataset (PcQA) designed specifically for pan-cancer KGQA tasks and has outperformed all existing state-of-the-art approaches by a rather large margin. KGTâ€™s practicality in biomedical issues was highlighted through case studies for drug repositioning, drug resistance, and biomarker discovery. Their approach exhibited robust adaptability among various LLMs. Rezaei et al.[26] developed AMG-RAG, a dynamic framework that utilizes autonomous LLM agents with medical search tools in the continuous construction and real-time updating of Medical Knowledge Graphs (MKGs). Their system incorporated confidence scoring and multi-hop reasoning to improve accuracy and interpretability in medical question answering. AMG-RAG outperformed size models on both very hard MEDQA benchmarks and more accessible MedMCQA ones, proving that it could conduct efficient reasoning based on current structured medical knowledge. They also used Neo4j to manage the knowledge graphs while adding external searches to ensure the latest data. Tiwari et al.[24] presented Auto-Cypher, a recent automated pipeline for producing high-quality synthetic data for training LLMs by mapping natural language to Cypher queries for graph databases like Neo4j. The pipeline deployed the novelty of LLM- as-database-filler to synthesize Neo4j databases for the execution of generated queries to ensure their correctness. A sizable dataset called SynthCypher was created, spanning multiple domains and complex queries, leading to a 40% improvement in LLM performance on Cypher generation. The datasets were used to fine-tune open-source models such as LLaMA, Mistral, and Qwen, and the SPIDER benchmark was adapted for evaluation purposes. Mohammed et al.[29] proposed a hybridized GraphRAG framework combining Neo4j-based UMLS knowledge graphs with a vector store for medical textbooks to create an improved U.S.M.L.E.-style clinical question-answering approach. The project integrated symbolic reasoning from knowledge graphs with semantic retrieval performed on text embeddings to enhance relevance and accuracy via adaptive re-ranking and query expansion. The system had the answers produced by GPT-4o- Mini, with different prompting strategies encouraging evidence- based and traceable responses grounded in verified medical knowledge. Experiments showed that the hybrid approach improved factual accuracy and citation fidelity as compared to the L.L.M.-only approach, enhancing transparency and reliability. It is shown that binding both structured and unstructured medical knowledge sources could aid in ameliorating hallucinations and hence improve clinical trustworthiness in AI-driven medical QA. Yang et al.[30] articulated sepsis knowledge graph was crafted by combining multicenter clinical data from over 10,000 patients with the help of GPT-4 for entity recognition and relationship extraction. Real-world data were collected from three hospitals and integrated with clinical guidelines and databases from the public domain. The knowledge graph contained 1,894 nodes and 2,021 relationships pertaining to diseases, symptoms, biomarkers, treatments, and complications. GPT outperformed other models in every resolution on sepsis- specific datasets to obtain high F1-score results. The constructed graph highlighted complex interactions in sepsis for assisting clinical decision-making and was implemented on Neo4j. Guan et al.[20] proposed a novel method for constructing a local knowledge graph from retrieved biomedical documents by extracting propositional claims. They carried out layer wise summarization on this graph to capture multi-document relationships and provide comprehensive contextual information to a language model for question-answering purposes. The method resolved issues in multi-document biomedical QA, such as noise cancellation and efficient context usage. They then tested their method on several benchmarks for biomedical question answering, achieving performance at least comparable to, if not better than, existing retrieval-augmented generation (RAG) baselines. The study established enhanced reasoning and answer accuracy of the model achieved through structured graph summarization. Previous studies have improved biomedical QA using KGs and LLMs, but important gaps remain. Most systems lack transparent, graph-based justifications, rely on limited evaluation methods, or depend on cloud resources that reduce privacy and reproducibility. Our framework addresses these gaps by providing visible Cypher queries with evidence subgraphs, applying comprehensive performance metrics across difficulty levels, and ensuring fully local, privacy-preserving deployment. Table I summarizes key previous studies on biomedical knowledge graphs and question answering, outlining their methods, datasets, and main limitations.']","The system ensures accurate and reliable biomedical question answering over the iBKH knowledge graph by integrating several key components and strategies centered around Cypher query generation, multi-hop reasoning, and confidence scoring, as detailed below:

1. **Hybrid LLM-to-Cypher Query Generation**  
   - The system uses a local LLaMA-3B large language model (LLM) to translate natural language biomedical questions into executable Cypher queries tailored for the Neo4j-based iBKH knowledge graph.  
   - Prompt-driven query generation incorporates schema and entity-aware prompting, enabling the LLM to reliably map diverse biomedical questions into precise graph queries that respect the graphâ€™s labels and relationships.  
   - This approach allows flexible handling of various question phrasings without manual rule-based mappings, improving query accuracy.  
   - The system achieves high exact-match rates for query generation: approximately 96% for simple questions and 86.7% for complex, multi-hop questions, reflecting strong performance even with increased compositionality.

2. **Multi-Hop Reasoning with Schema-Constrained Query Generation**  
   - Complex biomedical questions often require multi-hop reasoning, i.e., traversing multiple edges and nodes in the knowledge graph to infer answers.  
   - The system addresses this by generating Cypher queries that explicitly encode multi-hop constraints, guided by the iBKH schema and relationships.  
   - Path-guided decoding and concept-level normalization (using biomedical identifiers like UMLS, SNOMED, RxNorm) help ensure that multi-hop queries correctly capture the intended semantic paths, reducing errors from missing relation hops or ambiguous entity names.  
   - This structured reasoning enables the system to retrieve answers that depend on multiple linked biomedical facts, enhancing answer completeness and correctness.

3. **Confidence Scoring and Robust Answer Evaluation**  
   - After executing generated Cypher queries on the Neo4j database, the system retrieves candidate answers and evaluates them using standardized information retrieval metrics such as Precision@k, Recall@k, F1@k, Hits@k, and Mean Reciprocal Rank (MRR).  
   - These metrics provide confidence scores that reflect the quality and ranking of retrieved answers, ensuring that correct answers surface near the top of the results.  
   - The systemâ€™s evaluation shows strong early precision (high P@1 and MRR) and increasing recall with larger k, indicating reliable answer ranking and coverage.  
   - Error handling mechanisms catch incomplete or incorrect queries (e.g., referencing non-existent nodes or relationships) and either retry with simpler prompts or notify users, maintaining robustness.  
   - The system also provides transparent evidence by displaying the generated Cypher query and the supporting justification subgraph (nodes and edges), along with a natural language rationale, allowing users to verify and trust the answers.

4. **Privacy-Preserving, Fully Offline Deployment**  
   - The entire pipeline runs locally without requiring internet connectivity or external API calls, ensuring data privacy, compliance with biomedical data governance, and reproducibility.  
   - This local setup supports low-latency, interactive biomedical question answering suitable for sensitive clinical and research environments.

In summary, by combining accurate, schema-aware Cypher query generation with multi-hop reasoning capabilities and rigorous confidence scoring through standardized metrics, the system delivers precise, interpretable, and reliable biomedical answers over the iBKH knowledge graph. The transparent interface and offline operation further enhance trustworthiness and practical applicability in biomedical contexts.","The system uses LLaMA 3 to generate Cypher queries by learning to utilize the graph's labels and relationships properly, enabling flexible handling of various natural language biomedical questions without manual mapping rules. After generating a Cypher query, it is executed on the Neo4j database, which manages secure connections and returns results efficiently. The system evaluates retrieved answers using standardized metrics such as Hits@k and Mean Reciprocal Rank (MRR) to assess ranking quality rather than relying on fixed limits, ensuring responsiveness and fairness. Additionally, the system catches errors from incomplete or incorrect queries and retries with simpler prompts or informs the user, maintaining reliability. Multi-hop reasoning and confidence scoring are incorporated as described in related frameworks like AMG-RAG, which utilize autonomous LLM agents and medical search tools to improve accuracy and interpretability in medical question answering. This combination of transparent Cypher query generation, multi-hop reasoning, and confidence scoring over the iBKH knowledge graph enables accurate, efficient, and trustworthy biomedical question answering.",1.0,0.9486962571841895,0.99999999998,0.6666666666666666
How does LLaMA 3 utilize the Integrated Biomedical Knowledge Hub (iBKH) dataset to improve the accuracy and transparency of biomedical question answering in the described system?,"['Omar & Mohammed / Journal of Applied Science and Technology Trends Vol. 06, No. 02, pp. 342 â€“357 (2025) \n \n344 \nevaluated using Exact Match, Precision, Recall, F1, Hits@k, \nMRR, and latency across simple, medium, and complex question \nsets. Unlike prior template-based methods, our approach enables \ntraceable, outcome-level validation. In summary, the main \ncontributions of this work are as follows: \nâ€¢ Hybrid LLM to Cypher QA: A system that translates \nnatural language questions into accurate, executable \nCypher over a Neo4j drug, disease, and symptom KG. \nâ€¢ Prompt-driven query generation: Schema,  entity \naware prompting that reliably maps diverse biomedical \nquestions to graph queries. \nâ€¢ Evidence transparency: Along with each answer, we \nsurface the generated Cypher and the supporting \njustification subgraph (nodes, edges) plus a brief natural \nlanguage rationale. \nâ€¢ Answer-level evaluation: End-to-end assessment using \nExact Match, F1, Precision/Recall, Hits@k, MRR and \nlatency across simple, medium and complex tiers. \nâ€¢ Local, reproducible deployment: On-prem LLaMA 3 \ninference (no cloud dependency) suitable for biomedical \nsettings requiring low latency and strong data control. \n \nThe remainder of this paper is organized as follows: Section \n2 reviews related work on biomedical knowledge graphs and \nLLM-based QA systems. Section 3 provides background on \nknowledge graphs, large language models, and question \nanswering frameworks. Section 4 details the proposed \nmethodology, \nincluding \nsystem \narchitecture, \ndataset \nconstruction, and query translation. Section 5 presents the \nexperimental results through both quantitative metrics and \nqualitative case studies. Section 6 discusses the findings, analyzes \nlimitations, and compares performance against baseline models. \nFinally, Section 7 concludes this paper and outlines directions for \nfuture work. \nII. RELATED WORK \nRecently, studies have concentrated on the integration of \nclinical and medical knowledge graphs (LLM) to improve the \nanswer to medical questions. Researchers have derived several \nbiomedical KGs using Neo4j and incorporated the application of \nLLMs like LLaMA and GPT to convert natural language \nquestions into graph queries. Improvements in answer \ncorrectness, reduction of hallucination errors, one-to-many \nrelationships, and support for complex reasoning were the \nobjectives of these efforts. Some frameworks also adopted \nretrieval methods to ground responses in secure data. \nSu et al.[11] developed an integrative Biomedical Knowledge \nHub (iBKH), a huge biomedical knowledge graph that comprises \n18 of the very best data sources. The deployment of the iBKH in \nNeo4j allows for a user-friendly web portal to allow fast and \ninteractive knowledge retrieval. The system implemented \nadvanced graph learning techniques to enable the discovery of \nbiomedical knowledge, illustrated by an example of repurposing \nin silico drugs for Alzheimerâ€™s disease. iBKH achieved \npromising predictive performance for known drugs and proposed \npossible new drug candidates. \nRajabi and Kafaie[19] proposed a disease knowledge graph \nusing a cross-referential disease database comprising diseases, \nsymptoms, and drugs interconnected with relationships. They \ntransferred the data into Neo4j to create a graph of 9,400 nodes \nand 45,000 relationships representing the semantic links between \nmedical concepts. Applying Cypher queries enabled answering \ncomplex medical questions regarding identifying drugs that may \ncause certain diseases; it was demonstrated that the graph inferred \nnew relationships not explicitly existing in the original data. The \nconclusion was that disease knowledge graphs sped up clinical \ndiscovery and contributed to understanding complex medical \nrelationships. \nHou et al.[3] assessed and contrasted ChatGPT (both GPT-\n3.5 and GPT-4) and the biomedical knowledge graphs (BKGs) \nconcerning their ability to answer biomedical questions, generate \nnew knowledge, and reason. Their datasets were focused on \ndietary supplements and drugs, while evaluation criteria entailed \naccuracy, novelty, and reasoning ability. The results indicate that \nwhile GPT-4 surpassed GPT-3.5 and BKGs in knowledge \nprovision, it proved inconsistent with regard to citations and \nreasoning. Compared to them, BKGs scored higher in accuracy \nand reliability, especially in discovering novel links as well as \nwithin structured reasoning. \nSoman et al.[13] presented a novel framework called KG-\nRAG that integrates a large biomedical knowledge graph \n(SPOKE) with LLaMA 2, GPT-3.5, and GPT-4 (LLMs) to \nproduce accurate biomedical text. They optimized the retrieval of \nrelevant graph context to cut over 50% tokens without losing \naccuracy. It aided LLMs in performing better on biomedical \nquestion answering with very high accuracy boosts, especially in \nthe case of LLaMA 2. They compared KG-RAG to other retrieval \nmethods and indicated its comparatively more robust and \nefficient results. The framework produced reliable evidence-\nbased answers grounded in biomedical knowledge. \nLuo et al.[23] created ChatKBQA, a new framework with a \nquestion-and-answer approach over knowledge bases that first \ngenerates logical forms with the help of fine-tuned LLMs and \nthen retrieves the relevant entities and relations. This generate-\nthen-retrieve approach is supposed to handle a couple of issues \nwith the earlier methods concerning tedious retrieval and error \npropagation. They fine-tuned open-source LLMs like LLaMA 2 \nto change natural-language questions into logical forms with high \naccuracy. The retrieval phase uses unsupervised phrase-level \nsemantic matching in a way that enhances the alignment of \nentities and relations. Experiments on benchmark datasets \nindicate ChatKBQA to be superior to its predecessors, with the \nhighest accuracy to date. \nPusch and Conrad[6] conducted work under a hybrid \napproach conflating LLMs and biomedical Knowledge Graphs \n(KGs) to suppress hallucination errors in question-answering. \nThey proposed query-checking algorithms for validating, \ncorrecting, and executing the KG Cypher queries that LLMs \ngenerated, thereby attaining accurate and understandable \nanswers. The system used retrieval-augmented generation (RAG) \nto ground answers within KG data. The methodology was \nvalidated on a biomedical KG called PrimeKG using 50 \nbenchmark questions, assessing models like GPT-4 Turbo and \nLLaMA 3. Commercially available GPT-4 Turbo obtained'
 'Omar & Mohammed / Journal of Applied Science and Technology Trends Vol. 06, No. 02, pp. 342 â€“357 (2025) \n \n346 \nscientific literature, clinical records, genomic databases, and \nexperimental findings [19, 31]. \nSuch integration creates a comprehensive biomedical \nknowledge base that supports advanced analytics and discovery. \nFor example, biomedical knowledge graphs can reveal previously \nunknown relationships (e.g., between drugs and diseases) and \nhelp prioritize potential biomarkers for complex conditions. The \nIntegrative Biomedical Knowledge Hub (iBKH) is one such \nlarge-scale graph that consolidates diverse biomedical resources \ninto a unified hub, enabling discovery at scale [11]. Beyond \niBKH, large biomedical knowledge graphs such as SPOKE \nfurther illustrate how graph integration accelerates research and \nsupports precision-medicine use cases [12]. Overall, these graphs \nserve as foundational resources for data-driven and personalized \nmedicine. These knowledge graphs serve as foundational \nresources for precision medicine, where treatment can be tailored \nto the individualâ€™s biological profile, improving outcomes and \nminimizing side effects [19, 31]. \n \nTABLE I.  SUMMARY OF RELATED RESEARCH ON BIOMEDICAL KGS AND QUESTION ANSWERING \nRef. \nYear \nData/Graph \nMethod \nBaselines \nKey Metric \nLimitation \n[11] \n2023 \niBKH (18 biomedical \nsources, Neo4j) \nIntegrative KG + Graph \nlearning; drug repurposing \ncase \nKnown drugs, \nAlzheimerâ€™s \nstudy \nPredictive performance \n(drug repurposing) \nLimited to Alzheimerâ€™s case \nstudy; scalability and updates not \ndetailed \n[19] \n2023 \nDisease KG (9,400 \nnodes, 45,000 relations \nin Neo4j) \nCypher queries for diseaseâ€“\ndrugâ€“symptom reasoning \nCross-referential \ndisease DB \nNew relation inference; \ncomplex query \nanswering \nLimited to single domain; lacks \nlarge-scale evaluation \n[3] \n2023 \nBKGs vs. GPT-3.5/4 \nComparative QA study: \nLLMs vs. KGs \nGPT-3.5, GPT-4, \nKG reasoning \nAccuracy, Novelty, \nReasoning \nGPT-4 inconsistent in \nreasoning/citations; KG less fluent \nbut more reliable \n[13] \n2024 \nSPOKE KG + \nLLaMA2, GPT-3.5, \nGPT-4 \nKG-optimized retrieval for \nLLMs (RAG) \nOther retrieval \nmethods \nAccuracy, token \nreduction >50% \nFocus on retrieval optimization, \nnot KG construction \n[23] \n2024 \nBenchmark KB datasets \nGenerate-then-retrieve \n(LLM â†’ logical form â†’ \nKB retrieval) \nPrior KBQA \nmethods \nAccuracy (highest to \ndate) \nRisk of error in logical form \ngeneration \n[6] \n2024 \nPrimeKG \nLLM + KG hybrid, Cypher \nquery validation, RAG \nGPT-4 Turbo, \nLLaMA 3 \nAccuracy, \nExplainability \nDependent on KG coverage; \ncomputationally intensive \n[22] \n2025 \nPan-cancer KG (PcQA \ndataset) \nKG-enhanced reasoning \n(subgraph selection) \nSOTA KGQA \nmethods \nOutperformed SOTA on \nPcQA \nLimited to pan-cancer focus; no \nfine-tuning explored \n[26] \n2025 \nDynamic Medical KG + \nNeo4j \nLLM agents + multi-hop \nreasoning \nMEDQA, \nMedMCQA \nbaselines \nAccuracy, \nInterpretability \nHigh system complexity; requires \ncontinuous updating \n[24] \n2025 \nSynthCypher dataset \n(Neo4j + synthetic \nqueries) \nLLM-supervised Cypher \ngeneration and verification \nSPIDER \nbenchmark \nCypher accuracy 40% \nSynthetic dataset may not capture \nall real-world cases \n[29] \n2025 \nUMLS KG + Neo4j \nHybrid GraphRAG \nLLM-only QA \nAccuracy, Citation \nfidelity \nMore complex pipeline; relies on \nexternal vector store \n[30] \n2025 \nClinical data (10k \npatients, 1,894 nodes, \nNeo4j) \nKG construction using \nGPT-4 for entity/relation \nextraction \nOther KG \nconstruction \nmethods \nHigh F1-scores \nFocus only on sepsis; limited \ngeneralization \n[20] \n2025 \nLocal KG from \nbiomedical documents \nMulti-level summarization \nover KG for QA \nRAG baselines \nQA accuracy, reasoning \nTested mainly on document QA; \nscalability not proven \n \nB. Neo4j Graph Database \nTo manage the complexity and large size of biomedical \nknowledge graphs, specialized graph databases are needed. \nNeo4j is one of the most popular graph databases designed to \nstore and query data structured as nodes (entities) and \nrelationships (edges), both of which can have descriptive \nproperties [32, 33]. It uses the property graph model, which \nmakes it easy to represent complex, connected biomedical data \nsuch as drug-gene interactions or disease pathways. Neo4jâ€™s \nCypher query language is especially advantageous because it \nallows users to write expressive and efficient queries to explore \nmulti-step connections in the data [34]. \nNeo4j works well for biomedical data because it can quickly \nrun complicated queries over highly interconnected datasets. This \nis important in biology and medicine, where relationships \nbetween entities like proteins, diseases, and drugs are often \ncomplex and layered. Studies have shown that Neo4j handles \nlarge biomedical graphs efficiently, making it a favorite among \nresearchers and industry users alike [33, 35, 36]. Its indexing and \ncaching mechanisms also help speed up query processing and \ndata retrieval [37]. \nMoreover, \nNeo4j \nintegrates \nsmoothly \nwith \nmany \nprogramming languages and analytics tools, which makes it \neasier to build interactive biomedical applications and clinical \ndecision support systems that can turn complex graph data into \nuseful insights [38, 39]. \nC. Large Language Models (LLMs) in Biomedical Question \nAnswering \nLarge Language Models (LLMs) are powerful AI systems \ntrained on vast amounts of text data. They learn the structure and \npatterns of language, enabling them to understand questions, \ngenerate responses, summarize information, and perform other \ncomplex language tasks. Well-known models such as LLaMA \nand GPT-3 have greatly advanced the field of natural language \nprocessing by showing strong performance across many tasks \n[40, 41].'
 'Omar & Mohammed / Journal of Applied Science and Technology Trends Vol. 06, No. 02, pp. 342 â€“357 (2025) \n \n348 \ndrugs, diseases, symptoms) and execution returns structured data \n(tuples) relevant to the question. \nStep 5. Answer Synthesis: The structured tuples flow to \nAnswer Synthesis, which aggregates and formats them into a \nconcise raw answer. This raw answer is sent back to LLaMA 3 \nto optionally refine the phrasing while preserving the retrieved \nfacts. \nStep 6. Result Presentation: LLaMA 3 produces the final \nanswer, which the interface displays together with the executed \nCypher query and an optional preview of the returned rows, \nimproving transparency and trust. \nThe pipeline couples LLM-based language understanding \n(LLaMA 3) with a schema-grounded Neo4j knowledge graph. \nThe Cypher Query Gen refines the query formulation, Query \nExecution retrieves evidence and Answer Synthesis converts \nstructured results into readable outputs that produce answers that \nare accurate, interpretable, and easy to audit directly from the \ndisplayed query and evidence. \nB. Dataset and Knowledge Graph Construction \n1) \nDataset \nThe integrated Biomedical Knowledge Hub (iBKH), a large \nbiomedical knowledge base, forms the first level of the system \nand integrates information from various curated high-quality \nbiomedical databases. This implies that the data set includes \nvarious types of entities, such as diseases, symptoms, drugs, \nbiological pathways, etc. This study used the representative \nsubset of the iBKH dataset, which contained 65828 biomedical \nentities. These entities are semantically interconnected through a \ntotal of 3004166 relationships, thus creating a rich knowledge \ngraph. The iBKH dataset was originally introduced in [11], and it \nis freely available at (https://github.com/wcm-wanglab/iBKH). \nThis dataset is the core semantic foundation upon which this \nstudy is built. The knowledge graph is populated from multiple \ntabular sources (CSV files), each listing entities or relationships. \nThe main input files and their contents are as follows: \nâ€¢ Disease \nvocabulary(disease_vocab.csv): \nContains \ncolumns such as primary (a unique disease ID), name, \ndo_id (Disease Ontology ID), kegg_id, and umls_cui \n(UMLS Concept ID). Each row represents a disease node \nwith external identifiers. \nâ€¢ Drug vocabulary (drug_vocab.csv): Includes primary \n(unique drug ID), name, drugbank_id, kegg_id, \npharmgkb_id, umls_cui, mesh_id, iDISK_id and CID \n(PubChem ID). Each row defines a drug node with \nstandard database identifiers. \nâ€¢ Symptom vocabulary (symptom_vocab.csv): Contains \nprimary (unique symptom ID), name, mesh_id, umls_cui \nand iDISK_id. Each row defines a symptom node. \nâ€¢ Side \neffect \nvocabulary \n(side_effect_vocab.csv): \nIncludes primary (unique side-effect ID) and name. Each \nrow represents a side-effect node (with UMLS ID when \navailable). \nâ€¢ Pathway vocabulary (pathway_vocab.csv): Contains \nprimary (unique pathway ID), name, reactome_id, go_id, \nand kegg_id. Each row defines a biological pathway \nnode. \nRelationship files (each row typically contains two entity IDs \nand one or more boolean flags or codes) include: \nâ€¢ Diseaseâ€“Symptom links (Di_Sy_res.csv): Rows include \nDisease and Symptom IDs, a presence flag (1 or 0) and a \ndata source. If Present = 1, a HAS_SYMPTOM edge is \ncreated from the disease to the symptom, with properties \nfor presence and source. \nâ€¢ Diseaseâ€“Disease links (di_di_res.csv): Rows include \nDisease_1 and Disease_2 IDs with binary flags for is_a \nand Resemble. If is_a = 1, an (IS_A) edge is created \n(Disease_1 â†’ Disease_2); if Resemble = 1, a \nRESEMBLES edge is created. The source field is used \nfor provenance. \nâ€¢ Drugâ€“Disease links (D_Di_res.csv): Includes Drug and \nDisease IDs with several binary flags. If a flag equals 1, \na corresponding edge is created: \no \nTREATS (Treats = 1) \no \nPALLIATES (Palliates = 1) \no \nASSOCIATED_WITH (Associate = 1) \no \nALLEVIATES_REDUCES (alleviates = 1) \no \nTREATMENT_THERAPY (treatment/therapy = \n1) \no \nINHIBITS_CELL_GROWTH \n(inhibits \ncell \ngrowth = 1) \no \nHAS_BIOMARKER (biomarkers = 1) \no \nPREVENTS_SUPPRESSES \n(prevents/suppresses = 1) \no \nROLE_IN_PATHOGENESIS (role in disease \npathogenesis = 1) \nâ€¢ Drugâ€“SideEffect links (D_SE_res.csv): Contains Drug \nand SideEffect IDs with a Source column. Each row \ncreates a CAUSES edge from the drug to the side effect, \nwith source as an edge property. \nâ€¢ Drugâ€“Drug interactions (D_D_res.csv): Rows include \nDrug_1 and Drug_2 IDs with flags for Interaction and \nResemble. If Interaction = 1, an INTERACTS_WITH \nedge is created (bidirectional). If Resemble = 1, a \nRESEMBLES edge is added. \nâ€¢ Drugâ€“Pathway links (D_Pwy_res.csv): Includes Drug \nID and Pathway ID. Each row generates an \nASSOCIATED_WITH edge from the drug to the \npathway. \nâ€¢ Diseaseâ€“Pathway links (Di_Pwy_res.csv): Contains \nDisease ID and Pathway ID. Each row creates an \nASSOCIATED_WITH edge from the disease to the \npathway. \n2) \nData Upload Performance \nThe time required to upload different types of entities and \nrelationships into the Neo4j biomedical knowledge graph, \nmeasured in seconds. These measurements reflect both the size \nand complexity of the data being processed. \nAs shown in Table II, the longest upload time is for Drug-\nDrug Relationships, which takes approximately 190 seconds due \nto the large number of edges (over 3 million). Following this, \nDisease-Disease and Drug-Disease Relationships also require \nconsiderable time for loading. On the other hand, individual'
 'Vol. 06, No. 02, pp. 342 â€“357 (2025) \nISSN: 2708-0757 \n \nJOURNAL OF APPLIED SCIENCE AND TECHNOLOGY TRENDS \n \nwww.jastt.org  \n \n                                                          342 \ndoi: 10.38094/jastt62404  \nA Hybrid LLMâ€“Knowledge Graph Framework for Accurate \nBiomedical Question Answering \n \nHavraz Y. Omar1,Â²*, Abdulhakeem O. MohammedÂ³  \n \nÂ¹Department of Information Technology, Technical College of Duhok, Duhok Polytechnic University, Duhok, Kurdistan Region, \nIraq. havraz.omar@dpu.edu.krd  \nÂ² Department of Information Technology, Technical College of Informatics â€“ Akre, Akre University for Applied Sciences, Akre, \nKurdistan Region, Iraq. \nÂ³ Department of Computer Science, College of Science, University of Zakho, Zakho, Kurdistan Region, Iraq. \na.mohammed@uoz.edu.krd \n \n*Correspondence: havraz.omar@dpu.edu.krd \n \nAbstract \nBiomedical question answering requires accurate and interpretable systems; however, existing approaches often face challenges such as \nlanguage model hallucinations and limited reasoning when relying solely on standalone knowledge graphs. To address these limitations, \nthis study proposes a hybrid framework that integrates the LLaMA-3B language model with a Neo4j-based drugâ€“diseaseâ€“symptom \nknowledge graph. The system translates natural language questions into executable Cypher queries, operates on an iBKH-derived graph \ncomprising over 65,000 entities and 3 million relationships, and returns answers with supporting evidence through a transparent interface. \nExperiments conducted on 60 biomedical questions across three levels of difficulty demonstrate the robustness of the approach: 96% \nexact match for simple queries, 95% for medium queries, and 86.7% for complex queries. Overall, the system achieves Precision@5 of \n96.1%, Recall@5 of 89.0%, F1@5 of 91.0%, Hits@k of 96.1%, and an MRR of 94.4%, while maintaining an average response time of \nonly 6.07 seconds. These results indicate that the system retrieves nearly all relevant answers, ranks them correctly, and delivers them \nwith latency low enough for interactive use. Moreover, unlike cloud-based APIs such as ChatGPT, which require internet connectivity \nand external data transmission, the proposed framework operates fully offline, ensuring privacy, reproducibility, and compliance with \nbiomedical data governance. Overall, this pipeline provides an accurate, efficient, and privacy-preserving solution for biomedical question \nanswering, making it a practical alternative to cloud-dependent approaches in sensitive healthcare contexts. \n \nKeywords: Knowledge Graph, LLM, Question Answering, Neo4j, Biomedical Informatics, Healthcare AI, LLaMA 3. \n \nReceived: August 14th, 2025 / Revised: October 10th, 2025 / Accepted: October 16th, 2025 / Online: October 20th, 2025 \n \nI. INTRODUCTION \nAnswering questions in the biomedical field is a difficult task \ndue to the complexity of medical knowledge and the need for \nprecision. In recent years, large language models  (LLMs) like \nLLaMA, GPT-4 have made progress in understanding and \ngenerating human-like responses to medical questions  [1,  2]. \nThese models can process large amounts of information and \nrespond in natural language, which makes them helpful in \nhealthcare settings [3]. However, they often struggle to provide \naccurate answers when dealing with specialized biomedical \ncontent [4, 5]. \nOne major issue with LLMs is a problem called hallucination, \nwhere the model generates information that sounds right but is \nactually incorrect or unsupported [6]. In medical applications, \nthis can be dangerous, as healthcare professionals rely on precise \nand trustworthy information  [7]. Therefore, researchers are \nexploring ways to combine LLMs with structured sources of \nknowledge to improve their reliability [8]. \nLLM-only systems in biomedicine still hallucinate and are \nhard to verify, limiting safe use [9, 10]. Biomedical knowledge \ngraphs (BKGs) such as iBKH and SPOKE curate multi-source \nfacts and enable multi-hop reasoning, yet they neither interpret \nfree text nor generate answers [11, 12]. Recent hybrids (KG-\naware RAG) improve grounding but often lack explicit path-level \njustifications and robust end-to-end answer evaluation [13, 14]. \nRecent studies have increasingly integrated Knowledge \nGraphs (KGs) with Large Language Models (LLMs) to improve \nfactual accuracy, reasoning, and reduce hallucinations. Notable \nexamples include DR.KNOWS, which combines UMLS-based \nKGs with LLMs for better diagnostic reasoning [15], KnowNet'
 'Omar & Mohammed / Journal of Applied Science and Technology Trends Vol. 06, No. 02, pp. 342 â€“357 (2025) \n \n345 \nrecord-high accuracy, while open-source models achieved \nimpressive gains through prompt optimization. \nFeng et al.[22] developed the Knowledge Graph-based \nThought (KGT) framework that integrated LLMs with a pan-\ncancer knowledge graph for biomedical question answering. \nKGT was designed to reason on the knowledge graph schema and \nidentify optimal subgraphs to use for directing accurate answer \ngeneration, all without fine-tuning the LLMs. The framework is \nbenchmarked against a new dataset (PcQA) designed specifically \nfor pan-cancer KGQA tasks and has outperformed all existing \nstate-of-the-art approaches by a rather large margin. KGTâ€™s \npracticality in biomedical issues was highlighted through case \nstudies for drug repositioning, drug resistance, and biomarker \ndiscovery. Their approach exhibited robust adaptability among \nvarious LLMs. \nRezaei et al.[26] developed AMG-RAG, a dynamic \nframework that utilizes autonomous LLM agents with medical \nsearch tools in the continuous construction and real-time updating \nof Medical Knowledge Graphs (MKGs). Their system \nincorporated confidence scoring and multi-hop reasoning to \nimprove accuracy and interpretability in medical question \nanswering. AMG-RAG outperformed size models on both very \nhard MEDQA benchmarks and more accessible MedMCQA \nones, proving that it could conduct efficient reasoning based on \ncurrent structured medical knowledge. They also used Neo4j to \nmanage the knowledge graphs while adding external searches to \nensure the latest data. \nTiwari et al.[24] presented Auto-Cypher, a recent automated \npipeline for producing high-quality synthetic data for training \nLLMs by mapping natural language to Cypher queries for graph \ndatabases like Neo4j. The pipeline deployed the novelty of LLM-\nas-database-filler to synthesize Neo4j databases for the execution \nof generated queries to ensure their correctness. A sizable dataset \ncalled SynthCypher was created, spanning multiple domains and \ncomplex queries, leading to a 40% improvement in LLM \nperformance on Cypher generation. The datasets were used to \nfine-tune open-source models such as LLaMA, Mistral, and \nQwen, and the SPIDER benchmark was adapted for evaluation \npurposes. \nMohammed et al.[29] proposed a hybridized GraphRAG \nframework combining Neo4j-based UMLS knowledge graphs \nwith a vector store for medical textbooks to create an improved \nU.S.M.L.E.-style clinical question-answering approach. The \nproject integrated symbolic reasoning from knowledge graphs \nwith semantic retrieval performed on text embeddings to enhance \nrelevance and accuracy via adaptive re-ranking and query \nexpansion. The system had the answers produced by GPT-4o-\nMini, with different prompting strategies encouraging evidence-\nbased and traceable responses grounded in verified medical \nknowledge. Experiments showed that the hybrid approach \nimproved factual accuracy and citation fidelity as compared to \nthe L.L.M.-only approach, enhancing transparency and \nreliability. It is shown that binding both structured and \nunstructured medical knowledge sources could aid in \nameliorating hallucinations and hence improve clinical \ntrustworthiness in AI-driven medical QA. \nYang et al.[30] articulated sepsis knowledge graph was \ncrafted by combining multicenter clinical data from over 10,000 \npatients with the help of GPT-4 for entity recognition and \nrelationship extraction. Real-world data were collected from \nthree hospitals and integrated with clinical guidelines and \ndatabases from the public domain. The knowledge graph \ncontained 1,894 nodes and 2,021 relationships pertaining to \ndiseases, symptoms, biomarkers, treatments, and complications. \nGPT outperformed other models in every resolution on sepsis-\nspecific datasets to obtain high F1-score results. The constructed \ngraph highlighted complex interactions in sepsis for assisting \nclinical decision-making and was implemented on Neo4j. \nGuan et al.[20] proposed a novel method for constructing a \nlocal knowledge graph from retrieved biomedical documents by \nextracting propositional claims. They carried out layer wise \nsummarization on this graph to capture multi-document \nrelationships and provide comprehensive contextual information \nto a language model for question-answering purposes. The \nmethod resolved issues in multi-document biomedical QA, such \nas noise cancellation and efficient context usage. They then tested \ntheir method on several benchmarks for biomedical question \nanswering, achieving performance at least comparable to, if not \nbetter than, existing retrieval-augmented generation (RAG) \nbaselines. The study established enhanced reasoning and answer \naccuracy of the model achieved through structured graph \nsummarization.   \nPrevious studies have improved biomedical QA using KGs \nand LLMs, but important gaps remain. Most systems lack \ntransparent, graph-based justifications, rely on limited evaluation \nmethods, or depend on cloud resources that reduce privacy and \nreproducibility. Our framework addresses these gaps by \nproviding visible Cypher queries with evidence subgraphs, \napplying comprehensive performance metrics across difficulty \nlevels, and ensuring fully local, privacy-preserving deployment. \nTable I summarizes key previous studies on biomedical \nknowledge graphs and question answering, outlining their \nmethods, datasets, and main limitations. \nIII. PRELIMINARIES \nThis section outlines the fundamental concepts required to \nunderstand the proposed system. It introduces biomedical \nknowledge graphs, explains how Neo4j stores data in graph \nform, and describes the use of Cypher for querying. It also \nprovides a brief overview of large language models (LLMs) and \ntheir role in interpreting natural language. \n \n \nA. Biomedical Knowledge Graphs \nBiomedical Knowledge Graphs (BKGs) provide a structured \nrepresentation of complex biomedical information by modeling \ndiverse medical entities, such as diseases, drugs, symptoms, and \nbiological pathways, as interconnected nodes within a graph \nstructure. The edges in these graphs represent the semantic \nrelationships between these entities, including â€™treatsâ€™, â€™causesâ€™, \nâ€™interacts withâ€™ and many others, as illustrated in Fig 1. This \nform of representation enables the integration of heterogeneous \nbiomedical data from a wide range of sources, including']","['<1-hop>\n\nOmar & Mohammed / Journal of Applied Science and Technology Trends Vol. 06, No. 02, pp. 342 â€“357 (2025) 348 drugs, diseases, symptoms) and execution returns structured data (tuples) relevant to the question. Step 5. Answer Synthesis: The structured tuples flow to Answer Synthesis, which aggregates and formats them into a concise raw answer. This raw answer is sent back to LLaMA 3 to optionally refine the phrasing while preserving the retrieved facts. Step 6. Result Presentation: LLaMA 3 produces the final answer, which the interface displays together with the executed Cypher query and an optional preview of the returned rows, improving transparency and trust. The pipeline couples LLM-based language understanding (LLaMA 3) with a schema-grounded Neo4j knowledge graph. The Cypher Query Gen refines the query formulation, Query Execution retrieves evidence and Answer Synthesis converts structured results into readable outputs that produce answers that are accurate, interpretable, and easy to audit directly from the displayed query and evidence.'
 '<2-hop>\n\nB. Dataset and Knowledge Graph Construction 1) Dataset The integrated Biomedical Knowledge Hub (iBKH), a large biomedical knowledge base, forms the first level of the system and integrates information from various curated high-quality biomedical databases. This implies that the data set includes various types of entities, such as diseases, symptoms, drugs, biological pathways, etc. This study used the representative subset of the iBKH dataset, which contained 65828 biomedical entities. These entities are semantically interconnected through a total of 3004166 relationships, thus creating a rich knowledge graph. The iBKH dataset was originally introduced in [11], and it is freely available at (https://github.com/wcm-wanglab/iBKH). This dataset is the core semantic foundation upon which this study is built. The knowledge graph is populated from multiple tabular sources (CSV files), each listing entities or relationships. The main input files and their contents are as follows: â€¢ Disease vocabulary(disease_vocab.csv): Contains columns such as primary (a unique disease ID), name, do_id (Disease Ontology ID), kegg_id, and umls_cui (UMLS Concept ID). Each row represents a disease node with external identifiers. â€¢ Drug vocabulary (drug_vocab.csv): Includes primary (unique drug ID), name, drugbank_id, kegg_id, pharmgkb_id, umls_cui, mesh_id, iDISK_id and CID (PubChem ID). Each row defines a drug node with standard database identifiers. â€¢ Symptom vocabulary (symptom_vocab.csv): Contains primary (unique symptom ID), name, mesh_id, umls_cui and iDISK_id. Each row defines a symptom node. â€¢ Side effect vocabulary (side_effect_vocab.csv): Includes primary (unique side-effect ID) and name. Each row represents a side-effect node (with UMLS ID when available). â€¢ Pathway vocabulary (pathway_vocab.csv): Contains primary (unique pathway ID), name, reactome_id, go_id, and kegg_id. Each row defines a biological pathway node. Relationship files (each row typically contains two entity IDs and one or more boolean flags or codes) include: â€¢ Diseaseâ€“Symptom links (Di_Sy_res.csv): Rows include Disease and Symptom IDs, a presence flag (1 or 0) and a data source. If Present = 1, a HAS_SYMPTOM edge is created from the disease to the symptom, with properties for presence and source. â€¢ Diseaseâ€“Disease links (di_di_res.csv): Rows include Disease_1 and Disease_2 IDs with binary flags for is_a and Resemble. If is_a = 1, an (IS_A) edge is created (Disease_1 â†’ Disease_2); if Resemble = 1, a RESEMBLES edge is created. The source field is used for provenance. â€¢ Drugâ€“Disease links (D_Di_res.csv): Includes Drug and Disease IDs with several binary flags. If a flag equals 1, a corresponding edge is created: o TREATS (Treats = 1) o PALLIATES (Palliates = 1) o ASSOCIATED_WITH (Associate = 1) o ALLEVIATES_REDUCES (alleviates = 1) o TREATMENT_THERAPY (treatment/therapy = 1) o INHIBITS_CELL_GROWTH (inhibits cell growth = 1) o HAS_BIOMARKER (biomarkers = 1) o PREVENTS_SUPPRESSES (prevents/suppresses = 1) o ROLE_IN_PATHOGENESIS (role in disease pathogenesis = 1) â€¢ Drugâ€“SideEffect links (D_SE_res.csv): Contains Drug and SideEffect IDs with a Source column. Each row creates a CAUSES edge from the drug to the side effect, with source as an edge property. â€¢ Drugâ€“Drug interactions (D_D_res.csv): Rows include Drug_1 and Drug_2 IDs with flags for Interaction and Resemble. If Interaction = 1, an INTERACTS_WITH edge is created (bidirectional). If Resemble = 1, a RESEMBLES edge is added. â€¢ Drugâ€“Pathway links (D_Pwy_res.csv): Includes Drug ID and Pathway ID. Each row generates an ASSOCIATED_WITH edge from the drug to the pathway. â€¢ Diseaseâ€“Pathway links (Di_Pwy_res.csv): Contains Disease ID and Pathway ID. Each row creates an ASSOCIATED_WITH edge from the disease to the pathway. 2) Data Upload Performance The time required to upload different types of entities and relationships into the Neo4j biomedical knowledge graph, measured in seconds. These measurements reflect both the size and complexity of the data being processed. As shown in Table II, the longest upload time is for Drug- Drug Relationships, which takes approximately 190 seconds due to the large number of edges (over 3 million). Following this, Disease-Disease and Drug-Disease Relationships also require considerable time for loading. On the other hand, individual']","LLaMA 3 utilizes the Integrated Biomedical Knowledge Hub (iBKH) dataset to improve the accuracy and transparency of biomedical question answering through a hybrid framework that tightly integrates the LLaMA 3 large language model (LLM) with a Neo4j-based biomedical knowledge graph derived from iBKH. The key ways this integration enhances the system are as follows:

1. **Rich, Structured Biomedical Knowledge Base**:  
   - The iBKH dataset serves as the core semantic foundation of the system. It is a large-scale biomedical knowledge graph containing over 65,000 biomedical entities (such as diseases, drugs, symptoms, pathways, and side effects) interconnected by more than 3 million relationships.  
   - This comprehensive and curated graph integrates multiple high-quality biomedical databases, providing a rich, multi-relational structure that supports complex biomedical reasoning.

2. **Translation of Natural Language to Executable Cypher Queries**:  
   - LLaMA 3 is employed to interpret natural language biomedical questions and translate them into precise, executable Cypher queries that operate on the iBKH Neo4j graph.  
   - The system uses schema- and entity-aware prompting to reliably map diverse biomedical questions into graph queries, ensuring that the queries are accurate and relevant to the underlying knowledge graph schema.

3. **Evidence Transparency and Interpretability**:  
   - Alongside each answer, the system surfaces the exact Cypher query executed and the supporting justification subgraph (the relevant nodes and edges from iBKH) that underpin the answer.  
   - A brief natural language rationale is also provided, enhancing the interpretability and trustworthiness of the responses by allowing users to trace answers back to concrete graph evidence.

4. **Answer Synthesis and Refinement**:  
   - The structured data (tuples) retrieved from the iBKH graph via Cypher queries are aggregated and formatted into concise raw answers.  
   - LLaMA 3 optionally refines the phrasing of these answers while preserving factual correctness, combining the strengths of structured knowledge retrieval with natural language generation.

5. **Comprehensive Evaluation and Robust Performance**:  
   - The system is evaluated end-to-end on biomedical questions of varying complexity, demonstrating high accuracy metrics (e.g., 96% exact match on simple queries, 95% on medium, and 86.7% on complex queries) and strong precision, recall, and ranking performance.  
   - This shows that grounding LLaMA 3â€™s language understanding in the iBKH knowledge graph significantly reduces hallucination errors and improves answer correctness.

6. **Local, Privacy-Preserving Deployment**:  
   - The entire pipeline operates fully offline with on-premises LLaMA 3 inference and Neo4j graph querying, avoiding cloud dependencies.  
   - This ensures compliance with biomedical data governance, enhances reproducibility, and maintains low latency suitable for interactive biomedical applications.

In summary, LLaMA 3 leverages the iBKH dataset by using it as a comprehensive, structured biomedical knowledge graph that grounds the language modelâ€™s question understanding and answer generation. This hybrid approach enables accurate, interpretable, and transparent biomedical question answering, combining the reasoning capabilities of knowledge graphs with the natural language understanding of LLaMA 3.","LLaMA 3 is integrated into a biomedical question answering pipeline that leverages the Integrated Biomedical Knowledge Hub (iBKH) dataset as its core semantic foundation. The iBKH dataset contains 65,828 biomedical entities interconnected by over 3 million relationships, forming a rich knowledge graph with entities such as diseases, symptoms, drugs, and biological pathways. In the system, structured data relevant to a biomedical question is retrieved from this knowledge graph using Cypher queries refined by the Query Generation component. The retrieved structured tuples are then aggregated and formatted into a concise raw answer, which is sent to LLaMA 3. LLaMA 3 optionally refines the phrasing of this answer while preserving the retrieved facts, producing the final answer. This final output is presented alongside the executed Cypher query and an optional preview of the returned rows, enhancing transparency and trust. Thus, LLaMA 3 couples large language model-based understanding with the schema-grounded iBKH knowledge graph to produce answers that are accurate, interpretable, and easy to audit.",1.0,0.9901972799173234,0.99999999998,1.0
"How does the PharmGKB identifier integration in the drug vocabulary contribute to the construction and semantic richness of the iBKH biomedical knowledge graph, particularly in relation to drug-disease and drug-pathway relationships?","['Omar & Mohammed / Journal of Applied Science and Technology Trends Vol. 06, No. 02, pp. 342 â€“357 (2025) \n \n348 \ndrugs, diseases, symptoms) and execution returns structured data \n(tuples) relevant to the question. \nStep 5. Answer Synthesis: The structured tuples flow to \nAnswer Synthesis, which aggregates and formats them into a \nconcise raw answer. This raw answer is sent back to LLaMA 3 \nto optionally refine the phrasing while preserving the retrieved \nfacts. \nStep 6. Result Presentation: LLaMA 3 produces the final \nanswer, which the interface displays together with the executed \nCypher query and an optional preview of the returned rows, \nimproving transparency and trust. \nThe pipeline couples LLM-based language understanding \n(LLaMA 3) with a schema-grounded Neo4j knowledge graph. \nThe Cypher Query Gen refines the query formulation, Query \nExecution retrieves evidence and Answer Synthesis converts \nstructured results into readable outputs that produce answers that \nare accurate, interpretable, and easy to audit directly from the \ndisplayed query and evidence. \nB. Dataset and Knowledge Graph Construction \n1) \nDataset \nThe integrated Biomedical Knowledge Hub (iBKH), a large \nbiomedical knowledge base, forms the first level of the system \nand integrates information from various curated high-quality \nbiomedical databases. This implies that the data set includes \nvarious types of entities, such as diseases, symptoms, drugs, \nbiological pathways, etc. This study used the representative \nsubset of the iBKH dataset, which contained 65828 biomedical \nentities. These entities are semantically interconnected through a \ntotal of 3004166 relationships, thus creating a rich knowledge \ngraph. The iBKH dataset was originally introduced in [11], and it \nis freely available at (https://github.com/wcm-wanglab/iBKH). \nThis dataset is the core semantic foundation upon which this \nstudy is built. The knowledge graph is populated from multiple \ntabular sources (CSV files), each listing entities or relationships. \nThe main input files and their contents are as follows: \nâ€¢ Disease \nvocabulary(disease_vocab.csv): \nContains \ncolumns such as primary (a unique disease ID), name, \ndo_id (Disease Ontology ID), kegg_id, and umls_cui \n(UMLS Concept ID). Each row represents a disease node \nwith external identifiers. \nâ€¢ Drug vocabulary (drug_vocab.csv): Includes primary \n(unique drug ID), name, drugbank_id, kegg_id, \npharmgkb_id, umls_cui, mesh_id, iDISK_id and CID \n(PubChem ID). Each row defines a drug node with \nstandard database identifiers. \nâ€¢ Symptom vocabulary (symptom_vocab.csv): Contains \nprimary (unique symptom ID), name, mesh_id, umls_cui \nand iDISK_id. Each row defines a symptom node. \nâ€¢ Side \neffect \nvocabulary \n(side_effect_vocab.csv): \nIncludes primary (unique side-effect ID) and name. Each \nrow represents a side-effect node (with UMLS ID when \navailable). \nâ€¢ Pathway vocabulary (pathway_vocab.csv): Contains \nprimary (unique pathway ID), name, reactome_id, go_id, \nand kegg_id. Each row defines a biological pathway \nnode. \nRelationship files (each row typically contains two entity IDs \nand one or more boolean flags or codes) include: \nâ€¢ Diseaseâ€“Symptom links (Di_Sy_res.csv): Rows include \nDisease and Symptom IDs, a presence flag (1 or 0) and a \ndata source. If Present = 1, a HAS_SYMPTOM edge is \ncreated from the disease to the symptom, with properties \nfor presence and source. \nâ€¢ Diseaseâ€“Disease links (di_di_res.csv): Rows include \nDisease_1 and Disease_2 IDs with binary flags for is_a \nand Resemble. If is_a = 1, an (IS_A) edge is created \n(Disease_1 â†’ Disease_2); if Resemble = 1, a \nRESEMBLES edge is created. The source field is used \nfor provenance. \nâ€¢ Drugâ€“Disease links (D_Di_res.csv): Includes Drug and \nDisease IDs with several binary flags. If a flag equals 1, \na corresponding edge is created: \no \nTREATS (Treats = 1) \no \nPALLIATES (Palliates = 1) \no \nASSOCIATED_WITH (Associate = 1) \no \nALLEVIATES_REDUCES (alleviates = 1) \no \nTREATMENT_THERAPY (treatment/therapy = \n1) \no \nINHIBITS_CELL_GROWTH \n(inhibits \ncell \ngrowth = 1) \no \nHAS_BIOMARKER (biomarkers = 1) \no \nPREVENTS_SUPPRESSES \n(prevents/suppresses = 1) \no \nROLE_IN_PATHOGENESIS (role in disease \npathogenesis = 1) \nâ€¢ Drugâ€“SideEffect links (D_SE_res.csv): Contains Drug \nand SideEffect IDs with a Source column. Each row \ncreates a CAUSES edge from the drug to the side effect, \nwith source as an edge property. \nâ€¢ Drugâ€“Drug interactions (D_D_res.csv): Rows include \nDrug_1 and Drug_2 IDs with flags for Interaction and \nResemble. If Interaction = 1, an INTERACTS_WITH \nedge is created (bidirectional). If Resemble = 1, a \nRESEMBLES edge is added. \nâ€¢ Drugâ€“Pathway links (D_Pwy_res.csv): Includes Drug \nID and Pathway ID. Each row generates an \nASSOCIATED_WITH edge from the drug to the \npathway. \nâ€¢ Diseaseâ€“Pathway links (Di_Pwy_res.csv): Contains \nDisease ID and Pathway ID. Each row creates an \nASSOCIATED_WITH edge from the disease to the \npathway. \n2) \nData Upload Performance \nThe time required to upload different types of entities and \nrelationships into the Neo4j biomedical knowledge graph, \nmeasured in seconds. These measurements reflect both the size \nand complexity of the data being processed. \nAs shown in Table II, the longest upload time is for Drug-\nDrug Relationships, which takes approximately 190 seconds due \nto the large number of edges (over 3 million). Following this, \nDisease-Disease and Drug-Disease Relationships also require \nconsiderable time for loading. On the other hand, individual'
 '2 A registry of semantically annotated APIs\nThe first step in creating a network of interoperable APIs is to\nannotate each API in a semantically precise way. We built this\nAPI annotation system on the OpenAPI specification, the de\nfacto standard for documenting API metadata in a human-\nand machine-readable format. OpenAPI describes basic API\nmetadata (e.g. title, description, version, contact info), as well\nas key information on the operation of the API endpoints (e.g.\nserver URL, endpoint input parameters, endpoint response\nschemas).\nHowever, this basic OpenAPI specification does not include\nkey domain-specific information that is necessary to facilitate\ndownstream API interoperability. Therefore, we defined an\nOpenAPI extension to capture semantically precise annota-\ntions of each API endpoint. These annotations include the se-\nmantic types and identifier namespaces of biomedical entities\nthat are both used in querying (inputs) and found in the re-\nsponse (outputs), the JSON path to the output identifier val-\nues in the JSON response, and the predicate describing the\nrelationship\nbetween\nthe\ninput\nand\noutput\nentities\n(Supplementary Fig. S1).\nWe also chose a strategy to map arbitrary JSON data struc-\ntures to an established biological data model. In this effort,\nwe mapped API output to the Biolink Model (Unni et al.\n2022), a community-developed data model that was adopted\nand extended by the NCATS Translator consortium (Fecho\net al. 2022). The Biolink model defines the universe of seman-\ntic types that can be used in BioThings Explorer, the allowed\nidentifier systems for each semantic type and the allowed\npredicates. We provided a guide to writing these API annota-\ntions in our GitHub repository (https://github.com/biothings/\nbiothings_explorer/blob/main/docs/README-writing-x-bte.\nmd).\nTo annotate and catalog APIs with our OpenAPI extension,\nwe leveraged the SmartAPI registry (https://smart-api.info/)\n(Zaveri et al. 2017). We created SmartAPI registrations for 34\nAPIs with semantic annotations. The selection of these APIs\nthat are queried by BioThings Explorer is set in a local,\ninstance-specific configuration file. This collection of API\nannotations can be thought of as a â€œmeta-knowledge graphâ€\n(meta-KG), where the nodes represent types of biomedical en-\ntities (genes, diseases, drugs) and the edges represent APIs that\ndescribe relationships between two types of biomedical enti-\nties. The SmartAPI meta-KG currently contains 35 nodes and\n1817\nedges.\n(The\ncomplete\nmeta-KG\nis\nshown\nin\nSupplementary Table S1 and a partial rendering is shown in\nFig. 1.)\n3 API interoperability using BioThings Explorer\nThe second step in creating our federated biomedical knowl-\nedge graph was to create BioThings Explorer, an engine to au-\ntonomously\nquery\nthe\nSmartAPI\nmeta-KG,\nquery\nthe\nannotated APIs to retrieve associations between biomedical\nentities, and integrate those APIsâ€™ responses. The input to\nBioThings Explorer is a query graph and the syntax for\nencoding the query graph was defined by the NCATS\nTranslator consortium (Fecho et al. 2022). The topology of\nthe query graph and the constraints on its nodes and edges de-\nfine the query (Fig. 2).\nBioThings Explorer executes the query in three distinct\nphases: query-path planning, query-path execution, and inte-\ngration and scoring.\n3.1 Query-path planning\nFor every edge in a query graph, BioThings Explorer consults\nthe SmartAPI registry for APIs that serve those types of associ-\nations (Fig. 2). For example, in Fig. 1, associations between\ndiseases and genes can be found using APIs from the\nComparative Toxicogenomics Database (Davis et al. 2023)\nand the Biolink API from the Monarch Initiative (Mungall\net al. 2017, Unni et al. 2022), while associations between\ngenes and chemicals can be found using MyChem.Info\n(https://mychem.info/) (Lelong et al. 2022). The sequence of\nAPI calls that can satisfy the original query is a â€œquery-path\nplan.â€\n3.2 Query-path execution\nIn this phase, BioThings Explorer programmatically and au-\ntonomously executes each query in each query-path plan\nbased on the semantic annotations for each API identified in\nthe previous phase. BioThings Explorer calls each API, using\nthe SmartAPI annotation to construct calls with the correct\nsyntax and appropriate input identifier, and maps the API\nresponses to the Biolink Model (Unni et al. 2022). BioThings\nExplorer also performs ID-to-object translation, which facili-\ntates the chaining of API calls from one step in the query-path\nto the next step. This ID translation step is critical when suc-\ncessive APIs in the query-path plan use different identifiers to\nrepresent the same biomedical entity (e.g. NCBI Gene ID\nversus Ensembl Gene ID). ID translation is currently handled\nby\nthe\nTranslator\nNode\nNormalizer\n(https://github.com/\nNCATSTranslator/Translator-All/wiki/Node-Normalizer). The\noutput of this phase is a set of edges for each step of the query-\npath, which represent the associations between biomedical enti-\nties retrieved from the APIs.\n3.3 Integration and scoring\nIn this final phase, these sets of edges from the API queries are\nassembled into result sub-graphs, each of which matches the\ntopology of the query graph. Each result is then scored based\non a variety of factors including number of paths, length of\npaths, and semantic similarity between concepts based on the\nNormalized Google Distance (Cilibrasi and Vitanyi 2007).\n4 Deployment and usage\nThe BioThings Explorer knowledge graph is entirely com-\nposed from a federated network of APIs. Because there is no\nlocal assembly and storage of a large knowledge graph,\nBioThings Explorer is a very lightweight application that can\nbe easily deployed on almost any standard personal com-\nputer. The ability of every user to create a local instance of\nBioThings Explorer removes centralized bottlenecks associ-\nated with large queries and/or heavy usage. The code reposi-\ntory that describes the installation process is at https://github.\ncom/biothings/biothings_explorer. BioThings Explorer cur-\nrently\nrelies\non\ntwo\nexternal\ndependenciesâ€”the\nNode\nNormalizer service and the Biolink Model (Unni et al. 2022).\nHowever, nothing in the BioThings Explorer architecture is\nreliant on these specific tools, so these dependencies can be\nsubstituted for alternatives if desired.\nFor users who prefer not to create a local instance of\nBioThings Explorer, we also maintain a community instance\nfor general use through the NCATS Translator Consortium\n(https://explorer.biothings.io/).\n2\nCallaghan et al.\nDownloaded from https://academic.oup.com/bioinformatics/article/39/9/btad570/7273783 by guest on 11 January 2026'
 'Omar & Mohammed / Journal of Applied Science and Technology Trends Vol. 06, No. 02, pp. 342 â€“357 (2025) \n \n350 \ndocumentation, indexes â€œenable quicker and more efficient \npattern matchingâ€ by allowing the query planner to rapidly locate \nnodes by label and property. \nWith the schema in place, data was imported using Cypherâ€™s \nLOAD CSV commands. For each vocabulary file, rows with \nnonnull primary IDs were processed: the code used MERGE to \ncreate (or match existing) nodes with the given label and primary \nproperty, and then SET the remaining attributes from the CSV \ncolumns. For example, in importing disease_vocab.csv, each row \nproduced a node (:Disease primary: <id>) with properties name, \ndo_id, kegg_id, and umls_cui set from the row (empty strings \nwere turned into null). Similarly, drug_vocab.csv rows produced \ndrug nodes with properties for DrugBank, KEGG, PharmGKB \nIDs, etc. This approach follows best practice: using MERGE on \nthe unique ID ensures that nodes are not duplicated during \nmultiple passes. \nAfter all nodes were created, the relationship CSVs were \nloaded. Each row in those files was matched to the corresponding \nsource and target nodes by their primary IDs, and conditional \nlogic was used to create edges. For example, the disease-\nsymptom file (Di_Sy_res.csv) was processed by matching a \ndisease node and a symptom node for each row, then executing \nMERGE (d)-[r:HAS_SYMPTOM]-(s) if the present column is \nnonzero; the edge was given a present property and a source \nproperty from the CSV. The disease-disease file (di_di_res.csv) \nwas processed by matching disease nodes d1 and d2:If is_a = 1, \na (:Disease)-[:IS_A]->(:Disease) edge was merged; if Resemble \n= 1, a (:Disease)-[:RESEMBLES]->(:Disease) edge was merged. \nDrug-disease relationships were handled similarly: the script \nmatched a Drug node and a Disease node for each row of \nD_Di_res.csv, then for each flag that was 1, it merged the \ncorresponding edge label (such as TREATS, PALLIATES, \nASSOCIATED_WITH, etc.) from the drug to the disease. Drugâ€“\nside-effect rows produced (:Drug)-[:CAUSES]->(:SideEffect) \nedges with the source noted, and drugâ€“drug rows produced either \nINTERACTS_WITH or RESEMBLES edges between matched \ndrug pairs. Finally, the drug-pathway and disease-pathway files \neach produced:ASSOCIATED_WITH edges linking drugs or \ndiseases to pathway nodes. \nIn this graph model, most relationships are directional (for \nexample, a drug TREATS a disease; a disease does not TREAT \na drug). This follows common practice in biomedical KGs. The \nsame relations (such as RESEMBLES or INTERACTS_WITH) \nare inherently symmetric, but were stored as two directed edges \nor one undirected edge depending on implementation. All \nrelationship creation steps used Cypherâ€™s MERGE so that \nrepeated loads or out-of-order imports did not create duplicate \nedges. \nThis study used a static KG snapshot for reproducibility, but \nthe system also supports incremental updates through the Neo4j \nMERGE and batch import functions. A summary of the different \ntypes of nodes and relationships is provided, together with their \ncounts, descriptions, and examples in Table III.\n \nTABLE III.  DETAILED SUMMARY OF KNOWLEDGE GRAPH COMPONENTS IN IBKH SUBSET \nComponent Type \nEntity/Relationship \nCount \nDescription \nExamples \n \nDisease \n19,236 \nMedical conditions and disorders \nDiabetes, Hypertension, Cancer \n \nDrug \n37,997 \nPharmaceutical compounds \nAspirin, Insulin, Amoxicillin \nNodes \nSymptom \n1,361 \nClinical signs and symptoms \nHeadache, Fever, Nausea \n \nSideEffect \n4,251 \nNegative outcomes of drugs \nNausea, Drowsiness \n \nPathway \n2,983 \nBiological processes and pathways \nApoptosis, Glycolysis \n \nASSOCIATED_WITH \n101,534 \nGeneral associations (diseaseâ€“\npathway, drugâ€“pathway, etc.) \nDiabetes ASSOCIATED_WITH Pathway \n \nCAUSES \n145,321 \nDrug-SideEffect relationships \nAspirin CAUSES Gastric_Bleeding \n \nHAS_SYMPTOM \n3,357 \nDisease-symptom links \nCOVID-19 HAS_SYMPTOM Fever \n \nHAS_BIOMARKER \n412 \nBiomarkers linked to disease \nPSA HAS_BIOMARKER Prostate_Cancer \nRelationships \nINHIBITS_CELL_GROWTH \n1,508 \nDrugs inhibiting cell growth \nChemo INHIBITS_CELL_GROWTH Tumor \n \nINTERACTS_WITH \n2,682,142 \nDrug-drug interactions \nAspirin INTERACTS_WITH Warfarin \n \nIS_A \n10,529 \nSubtype hierarchy \nFlu IS_A Viral_Infection \n \nPALLIATES \n388 \nDrug palliates disease \nMorphine PALLIATES Cancer \n \nPREVENTS_SUPPRESSES \n859 \nPreventive links \nVaccine PREVENTS_SUPPRESSES Measles \n \nRESEMBLES \n7,000 \nSimilarity relationships \nDrugA RESEMBLES DrugB \n \nTREATMENT_THERAPY \n44,852 \nTherapy relationships \nRadiotherapy TREATMENT_THERAPY Tumor \n \nTREATS \n5,491 \nDrug-disease links \nInsulin TREATS Diabetes \n \nALLEVIATES_REDUCES \nâˆ¼180,000 \nSymptom relief \nParacetamol ALLEVIATES_REDUCES Fever \nTotal \nNodes \n65,828 \nTotal biomedical entities \nâ€” \n \nRelationships \n3,004,166 \nTotal knowledge graph links \nâ€” \nC. Natural Language to Cypher Query Translation \nA key feature of the system is its ability to accept questions \nwritten in plain English and automatically generate the \ncorresponding Cypher queries. This is accomplished using \nMetaâ€™s LLaMA 3 large language model, which runs entirely on \na local machine through the open-source llama.cpp framework. \nRunning the model locally ensures low-latency execution and \nkeeps sensitive queries within the userâ€™s environment. \nTo generate a Cypher query, LLaMA 3 is prompted with \nexamples of natural language questions along with their correct \nCypher translations. The prompt also includes instructions on \nhow to navigate the structure of the graph schema. When a user \nenters a question (e.g., â€™What are the symptoms of Alzheimerâ€™s \ndisease?â€™), the system inserts it into the prompt and asks LLaMA \n3 to produce a corresponding query. For example, the model may \nreturn: \nMATCH (d:Disease)-[:HAS_SYMPTOM]->(s:Symptom) \nWHERE toLower(d.name) CONTAINS ""alzheimer"" \nRETURN s.name'
 'Omar & Mohammed / Journal of Applied Science and Technology Trends Vol. 06, No. 02, pp. 342 â€“357 (2025) \n \n356 \n \nFig. 7. Side effects of drugs that treat epilepsy \n \nExecuting this query returns drugs associated with epilepsy \nand their corresponding side effects. For example, the query \nidentifies Pregabalin as a treatment for epilepsy and retrieves \nmultiple side effects such as anxiety, arrhythmia, gastrointestinal \npain, and others. The visualization of the graph in Fig. 7, \nhighlights this pattern, where the drug node is linked to epilepsy \nvia TREATS and to several side effect nodes via CAUSES, \nproviding an interpretable biomedical knowledge structure. \nVI. DISCUSSION \nUnder \na \nunified \nevaluation \nprotocol \nreporting \nP@k/R@k/F1@k, Hits@k, and MRR at ð‘˜âˆˆ{1,5,10} , the \npipeline exhibits consistent end-to-end behavior across the three \ndifficulty tiers. The ð‘˜-ablation shows strong early precision (high \nP@1 and elevated MRR), while recall increases with larger ð‘˜, \nindicating that correct answers typically surface near the top yet \npersist deeper in the ranked slate. At the query-generation level, \nexact-match (EM) is high for simple questions (e.g., âˆ¼96%) and \nlower for complex, multi-hop questions (e.g., âˆ¼86.7%), which \naligns with increased compositionality. End-to-end latency \n(mean total â‰ˆ6.07\u2006s from question to final answer) remains within \ninteractive bounds on our local setup. Practically, a fully offline \ndeployment avoids internet connectivity, API keys, and external \ndata \ntransfer, \nstrengthening \nprivacy, \ncompliance, \nand \nreproducibility in biomedical contexts. \nSeveral challenges qualify these findings. First, the \ncomparability challenge: to our knowledge, no prior study \nevaluates iBKH using the same metric suite, making cross-paper \nnumeric comparisons not â€œapples to apples.â€ We therefore \ninterpret the results as controlled evidence about this pipeline \nunder a single, consistent protocol rather than as a cross-study \nleaderboard. Beyond comparability, performance may vary with \nother biomedical knowledge graphs; the current iBKH snapshot \nis static, limiting real-time updates; and scaling to larger or \ndynamically refreshed graphs can introduce latency and \nconsistency trade-offs. \nError analysis shows that residual failures concentrate in \ncomplex, multi-hop queries where missing relation hops or brittle \nname-based matching (synonyms, abbreviations, homonyms) \nlead to partial answers. These observations motivate concept-\nlevel normalization via biomedical identifiers (e.g., UMLS, \nSNOMED, RxNorm) with synonym expansion, as well as \nschema-constrained query generation and path-guided decoding \nto better satisfy multi-hop constraints. Finally, correctness is \nassessed primarily at the answer level (Hits@k, MRR, \nprecision/recall) and does not yet include full semantic-\nequivalence checks across alternative Cypher queries, which may \noverlook cases where different queries yield the same correct \nresults. Complementing template EM with result-set equivalence \nchecks, expanding the metric suite (e.g., nDCG/MAP) with per-\nquestion 95% bootstrap confidence intervals, and supporting \nincremental graph updates with distributed storage are promising \nsteps to enhance robustness, scalability, and external validity. \nVII. CONCLUSION \nThis study introduced a hybrid biomedical question \nanswering framework that couples the LLaMA-3B language \nmodel with a Neo4j-based iBKH knowledge graph to enable the \nautomatic generation of executable Cypher queries and to deliver \ntransparent, evidence-grounded answers through justification \nsubgraphs. Evaluation in simple, medium, and complex queries \ndemonstrated consistently high performance, with strong \nprecision, recall, F1, Hits@k, and MRR values, while \nmaintaining low latency suitable for interactive biomedical \napplications. Beyond quantitative performance, the proposed \nsystem provides a reproducible and privacy-preserving solution \nby operating fully offline, a property of particular importance in \nsensitive clinical and research contexts. However, certain \nlimitations remain. The reliance on a static iBKH snapshot \nconstrains coverage and adaptability, recall is lower for complex \nmulti-hop reasoning, and the absence of canonical entity \nnormalization (e.g., UMLS, SNOMED, RxNorm) may reduce \nsemantic robustness. Future research will therefore focus on \nintegrating standardized biomedical entity normalization, \nenabling dynamic and incremental knowledge graph updates, and \nleveraging domain-adapted or fine-tuned biomedical LLMs. \nThese directions are expected to further strengthen the \nrobustness, scalability, and applicability of the framework in real-\nworld biomedical and clinical environments. \n \nREFERENCES \n[1] S. Bubeck, V. Chandrasekaran, R. Eldan, J. Gehrke, E. Horvitz, E. Kamar, \nP. Lee, Y. T. Lee, Y. Li, S. Lundberg, et al., â€œSparks of artificial general \nintelligence: \nEarly experiments with \ngpt-4,â€ arXiv preprint \narXiv:2303.12712, 2023. \n[2] J. Achiam, S. Adler, S. Agarwal, L. Ahmad, I. Akkaya, F. L. Aleman, D. \nAlmeida, J. Altenschmidt, S. Altman, S. Anadkat, et al., â€œGpt-4 \ntechnical report,â€ arXiv preprint arXiv:2303.08774, 2023. \n[3] Y. Hou, J. Yeung, H. Xu, C. Su, F. Wang, and R. Zhang, â€œFrom answers to \ninsights: unveiling the strengths and limitations of chatgpt and \nbiomedical knowledge graphs,â€ Research Square, pp. rsâ€“3, 2023.'
 '2 | GigaScience , 2025, Vol. 14 \nLLaMA with medical liter atur e. Additionall y, Med-P aLM [ 9 ] shows \npromising performance on the MedQA exam based on clinical \ncor por a and human feedback. Meanwhile, aiming at the Chinese \nmedical domain, LLMs such as BenTsao [ 10 ], DoctorGLM [ 11 ], and \nHuatuoGPT [ 12 ] are developed on the Chinese medical dialogue \ndata. Mor e r ecentl y, Zhongjing [ 13 ] and ChiMed-GPT [ 14 ] adopted \nfull pipeline training from pretraining, SFT, to reinforcement \nlearning with human feedback (RLHF) [ 15 ]. While fine-tuning can \nreduce hallucinations in LLMs, it brings about considerable train- \ning expenses. Additionally, it poses a critical challenge known \nas catastrophic forgetting. This issue manifests when a model \nfor gets its pr e viousl y learned information as a consequence of \nparameter modifications during the acquisition of new tasks. \nThis forgetfulness results in a deterioration of performance on \nprior tasks, consequently constraining the modelâ€™s practical \napplicability [ 16 , 17 ]. \nIn addition to fine-tuning, r esearc hers also enhance the out- \nput of LLMs through the field of prompt engineering. Prompt en- \ngineering focuses on the creation and optimization of prompts \nto impr ov e the effectiv eness of LLMs acr oss v arious a pplications \nand r esearc h domains [ 18 ]. It can enhance the ca pabilities of LLMs \nin a wide range of complex tasks, including question answering, \nsentiment classification, and commonsense reasoning. Chain-of- \nthought (CoT) prompts [ 19 ] enable complex reasoning capabili- \nties by incor por ating intermediate r easoning steps . T he Automatic \nPr ompt Engineer (APE) pr oposes an automatic pr ompt gener ation \nmethod aimed at enhancing the performance of LLMs [ 20 ]. Prompt \nengineering offers a straightforw ar d approach to harnessing the \npotential of LLMs without fine-tuning. \nOn the other hand, knowledge gr a phs (KGs) ar e r epositories of \nvast quantities of high-quality structured data, offering the po- \ntential to effectiv el y mitigate the issue of factual hallucinations \nwhen integrated with LLMs . Hence , employing KGs for question \nanswering can enhance the precision of the responses and fur- \nnish a dependable foundation for the factual verification of in- \nformation produced by LLMs. Knowledge gr a ph question answer- \ning (KGQA) has long been a hot r esearc h topic. Befor e the ad- \nvent of LLMs, certain studies [ 21â€“23 ] typically begin by r etrie ving \na subgr a ph r elated to the question to r educe the searc h space, \nthen perform m ultihop r easoning on this basis . T his r etrie v al- \nplus-r easoning par adigm has shown its adv anta ges ov er dir ect \nr easoning acr oss the entir e KG [ 24 , 25 ]. Additionall y, r esearc hers \ntackle KGQA by parsing the question into a structured query lan- \nguage (e.g., SPARQL) and using a query engine to obtain accu- \nrate answers [ 26 , 27 ]. UniKGQA [ 28 ] introduces a unified fine- \ntuning fr ame work for r etrie v al and r easoning, mor e closel y linking \nthese 2 stages. Ho w ever, traditional KGQA methods usually per- \nform poorly in accurate semantic understanding and high-quality \ntext generation due to the lack of LLMs for r etrie v al and r eason- \ning. Hence, r ecent r esearc h is incr easingl y utilizing external KGs \nto enhance LLMs in addressing KGQA challenges. For instance, \nStructGPT [ 29 ] navigates through knowledge graphs by identify- \ning pathways from an initial seed entity to the target answer en- \ntity, while Think-on-Gr a ph (ToG) [ 30 ] intr oduces iter ativ e explo- \nration of the knowledge gr a ph, whic h can become inefficient with \nv ery lar ge KGs. Additionall y, Reasoning on Gr a phs (RoG) [ 31 ] ne- \ncessitates fine-tuning to accur atel y gener ate and plan the r elation \npaths. KG-GPT [ 32 ] opts for r etrie ving an entire subgraph from the \nknowledge gr a ph and then deduces the answer thr ough infer ence. \nAlthough these methods hav e ac hie v ed gr atifying r esults in gen- \ner al ar eas, as shown in Fig. 1 B, when the intermediate entity in \nthe multihop question is unknown, it is impossible to retrieve the \na ppr opriate knowledge from the KG. \nIn this article, we intr oduce an innov ativ e fr ame work called \nknowledge gr a phâ€“based thought (KGT), whic h integr ates LLMs \nwith KGs through employing LLMs for reasoning on the schema \nof KGs to mitigate factual hallucinations of LLMs, as shown in \nFig. 1 C. Unlike traditional methods, KGT does not dir ectl y r etrie v e \nfactual information based on the question. Instead, it uses LLMs \nto infer entity information on the schema of the knowledge graph, \ngenerating an optimal subgraph based on k e y information di- \nr ectl y extr acted fr om the question and inferr ed information fr om \nthe sc hema. Subsequentl y, the optimal subgr a ph is used to in- \nfer the answer to the question through LLMs. KGT r equir es no \nfine-tuning, offers seamless integration with multiple LLMs, and \nis plug-and-play, facilitating easy deployment. It demonstrates \ngeneralizability, making it adaptable for use with diverse knowl- \nedge gr a phs . T his fr ame work is tailor ed for wide-r anging a ppli- \ncations in numerous biomedical challenges, such as (i) enhanc- \ning clinical decision-making for physicians and medical organi- \nzations, (ii) delivering medical advice to patients and health care \npro viders , (iii) unco vering crucial biomarkers for early disease de- \ntection and tailored therapy, and (iv) exploring novel therapeutic \napplications for existing medications through insights into their \nmechanisms , side effects , and the biological processes of associ- \nated diseases . Furthermore , we utilize the SmartQuerier Oncol- \nogy Knowledge Gr a ph (SOKG), a pan-cancer knowledge gr a ph de- \nveloped by SmartQuerier, to create a benchmark for the KGQA \ntask within biomedicine, named pan-cancer question answering \n(PcQA). We release this benchmark and its accompanying knowl- \nedge gr a ph, whic h is a subgr a ph of the SOKG, in [ 33 ]. This bench- \nmark is curr entl y the sole question-ans wering dataset a vailable \nin the domain of biomedical knowledge gr a phs. \nMaterials and Methods \nKno wledge gr aph introduction \nIn this w ork, w e tac kle the pr oblem of logical r easoning ov er \nthe KG K : E Ã— R that store entities ( E ) and relations ( R ). With- \nout loss of generality, KG can be organized as a set of triplets \n{ (e 1 , r, e 2 ) } âŠ†K, wher e eac h r elation r âˆˆ R exists between the pair \nof entities (e 1 , e 2 ) âˆˆ E Ã— E. We define a relational path { (t 1 , r, t 2 ) } as \na sequence of entity types ( T ) and the relation between them, \nwhere (t 1 , t 2 ) âˆˆ T Ã— T . In contrast, a relational chain { (e 1 , r, e 2 ) } \nrefers to a specific set of relational triplets between entities. To \nfurther enrich the KG, attribute information is included through \npairs (e , at t r ) , where at t r represents an attribute associated with \nan entity e , thereby enhancing the KGâ€™s semantic richness \nand precision by incorporating detailed characteristics of each \nentity. \nWithin the specialized realm of pan-cancer resear ch, w e use \na subgr a ph of the SOKG that pr ovides detailed oncological in- \nformation. As depicted in Table 1 , SOKG includes a collection of \nover 3 million entities, which is substantially larger than the en- \ntity count in the compared knowledge graphs, SynLethKG [ 34 ] and \nSDKG [ 35 ], with 540,012 and 165,062 entities, r espectiv el y. Further- \nmor e, SOKGâ€™s nearl y 6 million unique concept relations exceed \nthose of SynLethKG and SDKG, which have 2,231,921 and 727,318 \nr elations, r espectiv el y. Additionall y, SOKG includes 98 distinct at- \ntribute types, enriching data comprehension and improving the \nefficiency and precision of queries, a capability not matched by \nSynLethKG or SDKG, which do not include comparable attributes. \nFor this r esearc h, we utilize only a subgraph of the SOKG, which \nis available as open data [ 33 ], while the full knowledge graph re- \nmains proprietary. \nDownloaded from https://academic.oup.com/gigascience/article/doi/10.1093/gigascience/giae082/7943459 by guest on 11 January 2026']","['<1-hop>\n\nOmar & Mohammed / Journal of Applied Science and Technology Trends Vol. 06, No. 02, pp. 342 â€“357 (2025) 350 documentation, indexes â€œenable quicker and more efficient pattern matchingâ€ by allowing the query planner to rapidly locate nodes by label and property. With the schema in place, data was imported using Cypherâ€™s LOAD CSV commands. For each vocabulary file, rows with nonnull primary IDs were processed: the code used MERGE to create (or match existing) nodes with the given label and primary property, and then SET the remaining attributes from the CSV columns. For example, in importing disease_vocab.csv, each row produced a node (:Disease primary: <id>) with properties name, do_id, kegg_id, and umls_cui set from the row (empty strings were turned into null). Similarly, drug_vocab.csv rows produced drug nodes with properties for DrugBank, KEGG, PharmGKB IDs, etc. This approach follows best practice: using MERGE on the unique ID ensures that nodes are not duplicated during multiple passes. After all nodes were created, the relationship CSVs were loaded. Each row in those files was matched to the corresponding source and target nodes by their primary IDs, and conditional logic was used to create edges. For example, the disease- symptom file (Di_Sy_res.csv) was processed by matching a disease node and a symptom node for each row, then executing MERGE (d)-[r:HAS_SYMPTOM]-(s) if the present column is nonzero; the edge was given a present property and a source property from the CSV. The disease-disease file (di_di_res.csv) was processed by matching disease nodes d1 and d2:If is_a = 1, a (:Disease)-[:IS_A]->(:Disease) edge was merged; if Resemble = 1, a (:Disease)-[:RESEMBLES]->(:Disease) edge was merged. Drug-disease relationships were handled similarly: the script matched a Drug node and a Disease node for each row of D_Di_res.csv, then for each flag that was 1, it merged the corresponding edge label (such as TREATS, PALLIATES, ASSOCIATED_WITH, etc.) from the drug to the disease. Drugâ€“ side-effect rows produced (:Drug)-[:CAUSES]->(:SideEffect) edges with the source noted, and drugâ€“drug rows produced either INTERACTS_WITH or RESEMBLES edges between matched drug pairs. Finally, the drug-pathway and disease-pathway files each produced:ASSOCIATED_WITH edges linking drugs or diseases to pathway nodes. In this graph model, most relationships are directional (for example, a drug TREATS a disease; a disease does not TREAT a drug). This follows common practice in biomedical KGs. The same relations (such as RESEMBLES or INTERACTS_WITH) are inherently symmetric, but were stored as two directed edges or one undirected edge depending on implementation. All relationship creation steps used Cypherâ€™s MERGE so that repeated loads or out-of-order imports did not create duplicate edges. This study used a static KG snapshot for reproducibility, but the system also supports incremental updates through the Neo4j MERGE and batch import functions. A summary of the different types of nodes and relationships is provided, together with their counts, descriptions, and examples in Table III. TABLE III. DETAILED SUMMARY OF KNOWLEDGE GRAPH COMPONENTS IN IBKH SUBSET Component Type Entity/Relationship Count Description Examples Disease 19,236 Medical conditions and disorders Diabetes, Hypertension, Cancer Drug 37,997 Pharmaceutical compounds Aspirin, Insulin, Amoxicillin'
 '<2-hop>\n\nB. Dataset and Knowledge Graph Construction 1) Dataset The integrated Biomedical Knowledge Hub (iBKH), a large biomedical knowledge base, forms the first level of the system and integrates information from various curated high-quality biomedical databases. This implies that the data set includes various types of entities, such as diseases, symptoms, drugs, biological pathways, etc. This study used the representative subset of the iBKH dataset, which contained 65828 biomedical entities. These entities are semantically interconnected through a total of 3004166 relationships, thus creating a rich knowledge graph. The iBKH dataset was originally introduced in [11], and it is freely available at (https://github.com/wcm-wanglab/iBKH). This dataset is the core semantic foundation upon which this study is built. The knowledge graph is populated from multiple tabular sources (CSV files), each listing entities or relationships. The main input files and their contents are as follows: â€¢ Disease vocabulary(disease_vocab.csv): Contains columns such as primary (a unique disease ID), name, do_id (Disease Ontology ID), kegg_id, and umls_cui (UMLS Concept ID). Each row represents a disease node with external identifiers. â€¢ Drug vocabulary (drug_vocab.csv): Includes primary (unique drug ID), name, drugbank_id, kegg_id, pharmgkb_id, umls_cui, mesh_id, iDISK_id and CID (PubChem ID). Each row defines a drug node with standard database identifiers. â€¢ Symptom vocabulary (symptom_vocab.csv): Contains primary (unique symptom ID), name, mesh_id, umls_cui and iDISK_id. Each row defines a symptom node. â€¢ Side effect vocabulary (side_effect_vocab.csv): Includes primary (unique side-effect ID) and name. Each row represents a side-effect node (with UMLS ID when available). â€¢ Pathway vocabulary (pathway_vocab.csv): Contains primary (unique pathway ID), name, reactome_id, go_id, and kegg_id. Each row defines a biological pathway node. Relationship files (each row typically contains two entity IDs and one or more boolean flags or codes) include: â€¢ Diseaseâ€“Symptom links (Di_Sy_res.csv): Rows include Disease and Symptom IDs, a presence flag (1 or 0) and a data source. If Present = 1, a HAS_SYMPTOM edge is created from the disease to the symptom, with properties for presence and source. â€¢ Diseaseâ€“Disease links (di_di_res.csv): Rows include Disease_1 and Disease_2 IDs with binary flags for is_a and Resemble. If is_a = 1, an (IS_A) edge is created (Disease_1 â†’ Disease_2); if Resemble = 1, a RESEMBLES edge is created. The source field is used for provenance. â€¢ Drugâ€“Disease links (D_Di_res.csv): Includes Drug and Disease IDs with several binary flags. If a flag equals 1, a corresponding edge is created: o TREATS (Treats = 1) o PALLIATES (Palliates = 1) o ASSOCIATED_WITH (Associate = 1) o ALLEVIATES_REDUCES (alleviates = 1) o TREATMENT_THERAPY (treatment/therapy = 1) o INHIBITS_CELL_GROWTH (inhibits cell growth = 1) o HAS_BIOMARKER (biomarkers = 1) o PREVENTS_SUPPRESSES (prevents/suppresses = 1) o ROLE_IN_PATHOGENESIS (role in disease pathogenesis = 1) â€¢ Drugâ€“SideEffect links (D_SE_res.csv): Contains Drug and SideEffect IDs with a Source column. Each row creates a CAUSES edge from the drug to the side effect, with source as an edge property. â€¢ Drugâ€“Drug interactions (D_D_res.csv): Rows include Drug_1 and Drug_2 IDs with flags for Interaction and Resemble. If Interaction = 1, an INTERACTS_WITH edge is created (bidirectional). If Resemble = 1, a RESEMBLES edge is added. â€¢ Drugâ€“Pathway links (D_Pwy_res.csv): Includes Drug ID and Pathway ID. Each row generates an ASSOCIATED_WITH edge from the drug to the pathway. â€¢ Diseaseâ€“Pathway links (Di_Pwy_res.csv): Contains Disease ID and Pathway ID. Each row creates an ASSOCIATED_WITH edge from the disease to the pathway. 2) Data Upload Performance The time required to upload different types of entities and relationships into the Neo4j biomedical knowledge graph, measured in seconds. These measurements reflect both the size and complexity of the data being processed. As shown in Table II, the longest upload time is for Drug- Drug Relationships, which takes approximately 190 seconds due to the large number of edges (over 3 million). Following this, Disease-Disease and Drug-Disease Relationships also require considerable time for loading. On the other hand, individual']","The integration of PharmGKB identifiers within the drug vocabulary of the iBKH biomedical knowledge graph significantly enhances the graph's construction and semantic richness, particularly regarding drug-disease and drug-pathway relationships. This contribution can be understood through the following points derived from the provided context:

1. **Standardized and Rich Entity Representation**:  
   The drug vocabulary (`drug_vocab.csv`) in iBKH includes multiple standardized identifiers for each drug node, such as DrugBank ID, KEGG ID, UMLS CUI, Mesh ID, PubChem CID, and importantly, the PharmGKB ID. Incorporating PharmGKB IDs ensures that each drug entity is linked to a well-curated pharmacogenomics knowledge base, which enriches the semantic context of the drug nodes beyond basic chemical or pharmacological information.

2. **Facilitating Precise Entity Normalization and Linking**:  
   By including PharmGKB identifiers, the iBKH knowledge graph achieves better entity normalization and disambiguation. This is crucial for accurately matching drugs across different data sources and for integrating heterogeneous biomedical data. Such normalization reduces ambiguity and duplication, enabling more reliable and semantically consistent relationships between drugs and other biomedical entities.

3. **Enhancing Drug-Disease Relationship Semantics**:  
   The presence of PharmGKB IDs in drug nodes supports the creation of detailed and semantically rich drug-disease edges. For example, drug-disease relationships such as TREATS, PALLIATES, ASSOCIATED_WITH, and others are established by matching drug nodes (with PharmGKB IDs) to disease nodes. The PharmGKB integration allows these relationships to be grounded in pharmacogenomic evidence, potentially linking drugs to diseases with known genetic or molecular mechanisms, thus enriching the biological and clinical relevance of these edges.

4. **Improving Drug-Pathway Associations**:  
   Similarly, drug-pathway relationships (ASSOCIATED_WITH edges linking drugs to biological pathways) benefit from PharmGKB integration. PharmGKB contains curated information about drug mechanisms, including pathways affected by drugs. By incorporating PharmGKB IDs, the iBKH graph can more accurately associate drugs with relevant biological pathways, enhancing the graphâ€™s ability to represent mechanistic insights and support pathway-based queries or analyses.

5. **Supporting Complex Querying and Reasoning**:  
   The semantic richness added by PharmGKB IDs enables more sophisticated querying and reasoning over the knowledge graph. For instance, queries that explore how a drug affects a disease through specific pathways or genetic markers can leverage the PharmGKB-linked drug nodes to retrieve precise and biologically meaningful answers. This integration thus underpins the systemâ€™s ability to generate accurate Cypher queries and produce interpretable, evidence-grounded answers.

6. **Overall Contribution to Knowledge Graph Quality**:  
   The inclusion of PharmGKB identifiers aligns with best practices in knowledge graph construction, where multiple external identifiers are used to ensure comprehensive coverage and interoperability. This approach enhances the iBKHâ€™s semantic precision, reduces redundancy, and facilitates integration with other biomedical resources, ultimately improving the graphâ€™s utility for biomedical research and clinical applications.

**In summary**, the PharmGKB identifier integration in the drug vocabulary enriches the iBKH biomedical knowledge graph by providing standardized, pharmacogenomics-informed drug entities. This enrichment improves the accuracy and semantic depth of drug-disease and drug-pathway relationships, enabling more precise, interpretable, and biologically relevant knowledge representation and query answering within the iBKH framework.","The PharmGKB identifier is included as a property in the drug nodes created from the drug_vocab.csv file, which contains unique drug IDs along with standard database identifiers such as drugbank_id, kegg_id, pharmgkb_id, umls_cui, mesh_id, iDISK_id, and CID. This integration ensures that each drug node in the iBKH knowledge graph is linked to PharmGKB, a curated biomedical database, enhancing the semantic interoperability and accuracy of the graph. The iBKH knowledge graph uses these drug nodes to establish multiple types of relationships, including drug-disease links (such as TREATS, PALLIATES, ASSOCIATED_WITH) and drug-pathway associations, by matching drug nodes with disease and pathway nodes through their unique IDs. This structured incorporation of PharmGKB identifiers facilitates precise merging of nodes and edges using Cypher's MERGE command, preventing duplication and enabling rich, directional relationships that reflect real biomedical interactions. Consequently, the PharmGKB integration significantly contributes to the comprehensive and semantically rich representation of drug-related biomedical knowledge within the iBKH knowledge graph.",0.08823529411764706,0.968950104109573,0.8333333332916666,1.0
"How do LLaMA and Llama2 models differ in their integration with biomedical knowledge graphs to mitigate hallucinations in large language models, and what impact does this integration have on the performance of retrieval-augmented generation (RAG) systems as demonstrated by meta-analyses?","['Research and Applications\nImproving large language model applications in \nbiomedicine with retrieval-augmented generation: a \nsystematic review, meta-analysis, and clinical \ndevelopment guidelines\nSiru Liu, PhD\x03,1,2, Allison B. McCoy \n, PhD1, Adam Wright \n, PhD1,3 \n1Department of Biomedical Informatics, Vanderbilt University Medical Center, Nashville, TN 37212, United States, 2Department of Computer \nScience, Vanderbilt University, Nashville, TN 37212, United States, 3Department of Medicine, Vanderbilt University Medical Center, \nNashville, TN 37212, United States\n\x03Corresponding author: Siru Liu, PhD, Department of Biomedical Informatics, Vanderbilt University Medical Center, 2525 West End Ave #1475, Nashville, \nTN 37212, United States (siru.liu@vumc.org)\nAbstract\nObjective: The objectives of this study are to synthesize findings from recent research of retrieval-augmented generation (RAG) and large lan\xad\nguage models (LLMs) in biomedicine and provide clinical development guidelines to improve effectiveness.\nMaterials and Methods: We conducted a systematic literature review and a meta-analysis. The report was created in adherence to the Pre\xad\nferred Reporting Items for Systematic Reviews and Meta-Analyses 2020 analysis. Searches were performed in 3 databases (PubMed, Embase, \nPsycINFO) using terms related to â€œretrieval augmented generationâ€ and â€œlarge language model,â€ for articles published in 2023 and 2024. We \nselected studies that compared baseline LLM performance with RAG performance. We developed a random-effect meta-analysis model, using \nodds ratio as the effect size.\nResults: Among 335 studies, 20 were included in this literature review. The pooled effect size was 1.35, with a 95% confidence interval of \n1.19-1.53, indicating a statistically significant effect (P Â¼ .001). We reported clinical tasks, baseline LLMs, retrieval sources and strategies, as \nwell as evaluation methods.\nDiscussion: Building on our literature review, we developed Guidelines for Unified Implementation and Development of Enhanced LLM Appli\xad\ncations with RAG in Clinical Settings to inform clinical applications using RAG.\nConclusion: Overall, RAG implementation showed a 1.35 odds ratio increase in performance compared to baseline LLMs. Future research \nshould focus on (1) system-level enhancement: the combination of RAG and agent, (2) knowledge-level enhancement: deep integration of \nknowledge into LLM, and (3) integration-level enhancement: integrating RAG systems within electronic health records.\nKey words: large language model; retrieval augmented generation; systematic review; meta-analysis. \nIntroduction\nLarge language models (LLMs) have reported remarkable \nperformance in question-answering, summarization, and text \ngeneration.1 Given this, researchers have explored its poten\xad\ntial in biomedical areas.2 For example, several studies \nreported the ability of using LLM to answer patient mes\xad\nsages,3 to analyze alert logic in clinical decision support,4 and \nto make discharge summaries more readable to patients.5\nHowever, several challenges remain.\nLLMs are trained on fixed datasets, which restrict their \nknowledge to information available up to the training cut-off \ndate. For example, GPT-4oâ€™s training data only includes \ninformation up to October 2023, making it unable to \nrespond accurately to findings that emerged afterward. LLM \ntraining datasets are also generally broad and lack the specif\xad\nicity required for biomedical applications. Finally, not all \nsources used to train the LLMs are reliable and trustworthy. \nTo address these limitations, researchers have performed \nfine-tuning and retrieval-augmented generation (RAG) tech\xad\nniques. Fine-tuning can adapt LLMs to specific domains, but \nit is resource-intensive and does not allow for real-time \nupdates. In contrast, RAG maintains the original LLM archi\xad\ntecture while incorporating relevant context directly into \nqueries, offering more flexibility and control. In addition, \nRAGâ€™s unique advantage in biomedical applications lies in its \nability to adapt to dynamic environments by delivering up-to- \ndate information and efficiently integrating external knowl\xad\nedge sources with high interpretability.6\nAnother limitation of using LLMs directly is the risk of hal\xad\nlucination, where the model generates incorrect or fabricated \ninformation.7 To mitigate such issues, researchers have pro\xad\nposed RAG as a solution that integrates up-to-date, relevant \ninformation, enhancing both the accuracy and reliability of \nLLM generated responses.8,9 For example, when ChatGPT \nReceived: November 19, 2024; Revised: December 17, 2024; Editorial Decision: January 2, 2025; Accepted: January 3, 2025 \nÂ© The Author(s) 2025. Published by Oxford University Press on behalf of the American Medical Informatics Association.  \nThis is an Open Access article distributed under the terms of the Creative Commons Attribution-NonCommercial License (https://creativecommons.org/licenses/ \nby-nc/4.0/), which permits non-commercial re-use, distribution, and reproduction in any medium, provided the original work is properly cited. For commercial \nre-use, please contact reprints@oup.com for reprints and translation rights for reprints. All other permissions can be obtained through our RightsLink service via \nthe Permissions link on the article page on our siteâ€”for further information please contact journals.permissions@oup.com. \nJournal of the American Medical Informatics Association, 2025, 32(4), 605â€“615 \nhttps://doi.org/10.1093/jamia/ocaf008 \nAdvance access publication 15 January 2025 \nResearch and Applications \nDownloaded from https://academic.oup.com/jamia/article/32/4/605/7954485 by guest on 11 January 2026'
 'Omar & Mohammed / Journal of Applied Science and Technology Trends Vol. 06, No. 02, pp. 342 â€“357 (2025) \n \n344 \nevaluated using Exact Match, Precision, Recall, F1, Hits@k, \nMRR, and latency across simple, medium, and complex question \nsets. Unlike prior template-based methods, our approach enables \ntraceable, outcome-level validation. In summary, the main \ncontributions of this work are as follows: \nâ€¢ Hybrid LLM to Cypher QA: A system that translates \nnatural language questions into accurate, executable \nCypher over a Neo4j drug, disease, and symptom KG. \nâ€¢ Prompt-driven query generation: Schema,  entity \naware prompting that reliably maps diverse biomedical \nquestions to graph queries. \nâ€¢ Evidence transparency: Along with each answer, we \nsurface the generated Cypher and the supporting \njustification subgraph (nodes, edges) plus a brief natural \nlanguage rationale. \nâ€¢ Answer-level evaluation: End-to-end assessment using \nExact Match, F1, Precision/Recall, Hits@k, MRR and \nlatency across simple, medium and complex tiers. \nâ€¢ Local, reproducible deployment: On-prem LLaMA 3 \ninference (no cloud dependency) suitable for biomedical \nsettings requiring low latency and strong data control. \n \nThe remainder of this paper is organized as follows: Section \n2 reviews related work on biomedical knowledge graphs and \nLLM-based QA systems. Section 3 provides background on \nknowledge graphs, large language models, and question \nanswering frameworks. Section 4 details the proposed \nmethodology, \nincluding \nsystem \narchitecture, \ndataset \nconstruction, and query translation. Section 5 presents the \nexperimental results through both quantitative metrics and \nqualitative case studies. Section 6 discusses the findings, analyzes \nlimitations, and compares performance against baseline models. \nFinally, Section 7 concludes this paper and outlines directions for \nfuture work. \nII. RELATED WORK \nRecently, studies have concentrated on the integration of \nclinical and medical knowledge graphs (LLM) to improve the \nanswer to medical questions. Researchers have derived several \nbiomedical KGs using Neo4j and incorporated the application of \nLLMs like LLaMA and GPT to convert natural language \nquestions into graph queries. Improvements in answer \ncorrectness, reduction of hallucination errors, one-to-many \nrelationships, and support for complex reasoning were the \nobjectives of these efforts. Some frameworks also adopted \nretrieval methods to ground responses in secure data. \nSu et al.[11] developed an integrative Biomedical Knowledge \nHub (iBKH), a huge biomedical knowledge graph that comprises \n18 of the very best data sources. The deployment of the iBKH in \nNeo4j allows for a user-friendly web portal to allow fast and \ninteractive knowledge retrieval. The system implemented \nadvanced graph learning techniques to enable the discovery of \nbiomedical knowledge, illustrated by an example of repurposing \nin silico drugs for Alzheimerâ€™s disease. iBKH achieved \npromising predictive performance for known drugs and proposed \npossible new drug candidates. \nRajabi and Kafaie[19] proposed a disease knowledge graph \nusing a cross-referential disease database comprising diseases, \nsymptoms, and drugs interconnected with relationships. They \ntransferred the data into Neo4j to create a graph of 9,400 nodes \nand 45,000 relationships representing the semantic links between \nmedical concepts. Applying Cypher queries enabled answering \ncomplex medical questions regarding identifying drugs that may \ncause certain diseases; it was demonstrated that the graph inferred \nnew relationships not explicitly existing in the original data. The \nconclusion was that disease knowledge graphs sped up clinical \ndiscovery and contributed to understanding complex medical \nrelationships. \nHou et al.[3] assessed and contrasted ChatGPT (both GPT-\n3.5 and GPT-4) and the biomedical knowledge graphs (BKGs) \nconcerning their ability to answer biomedical questions, generate \nnew knowledge, and reason. Their datasets were focused on \ndietary supplements and drugs, while evaluation criteria entailed \naccuracy, novelty, and reasoning ability. The results indicate that \nwhile GPT-4 surpassed GPT-3.5 and BKGs in knowledge \nprovision, it proved inconsistent with regard to citations and \nreasoning. Compared to them, BKGs scored higher in accuracy \nand reliability, especially in discovering novel links as well as \nwithin structured reasoning. \nSoman et al.[13] presented a novel framework called KG-\nRAG that integrates a large biomedical knowledge graph \n(SPOKE) with LLaMA 2, GPT-3.5, and GPT-4 (LLMs) to \nproduce accurate biomedical text. They optimized the retrieval of \nrelevant graph context to cut over 50% tokens without losing \naccuracy. It aided LLMs in performing better on biomedical \nquestion answering with very high accuracy boosts, especially in \nthe case of LLaMA 2. They compared KG-RAG to other retrieval \nmethods and indicated its comparatively more robust and \nefficient results. The framework produced reliable evidence-\nbased answers grounded in biomedical knowledge. \nLuo et al.[23] created ChatKBQA, a new framework with a \nquestion-and-answer approach over knowledge bases that first \ngenerates logical forms with the help of fine-tuned LLMs and \nthen retrieves the relevant entities and relations. This generate-\nthen-retrieve approach is supposed to handle a couple of issues \nwith the earlier methods concerning tedious retrieval and error \npropagation. They fine-tuned open-source LLMs like LLaMA 2 \nto change natural-language questions into logical forms with high \naccuracy. The retrieval phase uses unsupervised phrase-level \nsemantic matching in a way that enhances the alignment of \nentities and relations. Experiments on benchmark datasets \nindicate ChatKBQA to be superior to its predecessors, with the \nhighest accuracy to date. \nPusch and Conrad[6] conducted work under a hybrid \napproach conflating LLMs and biomedical Knowledge Graphs \n(KGs) to suppress hallucination errors in question-answering. \nThey proposed query-checking algorithms for validating, \ncorrecting, and executing the KG Cypher queries that LLMs \ngenerated, thereby attaining accurate and understandable \nanswers. The system used retrieval-augmented generation (RAG) \nto ground answers within KG data. The methodology was \nvalidated on a biomedical KG called PrimeKG using 50 \nbenchmark questions, assessing models like GPT-4 Turbo and \nLLaMA 3. Commercially available GPT-4 Turbo obtained'
 'Joy et. al \n2 \n \n1 Introduction \nLarge language models (LLMs) have rapidly advanced the state of natural-language \nprocessing, reaching or surpassing expert performance across a wide range of \nbiomedical tasks, including cell type annotation, protein-structure prediction and \nautomated synthesis of clinical-trial results 1â€“6. However, the underlying generative \nmethodology of these models, which sequentially predict tokens based on statistical \npatterns learned from massive text corpora, renders them susceptible to hallucinations, \ndefined as outputs that are syntactically fluent yet factually incorrect 7,8. Such \ninaccuracies pose significant risks in biomedicine, where even minor errors can \nmisdirect research efforts, delay critical therapeutic discoveries, or compromise patient \nsafety 7,9â€“11. Indeed, recent assessments underscore that hallucination rates remain too \nhigh for safe and effective deployment in clinical and research-intensive environments \n12,13. \nEfforts to mitigate these hallucinations through domain-specific pre-training and prompt \nengineering have yielded only incremental improvements, as these approaches \ncontinue to embed knowledge implicitly within opaque model parameters and fail to \nreliably surface evidence provenance 14â€“16. Retrieval-augmented generation (RAG) has \nemerged as a promising solution, explicitly grounding model-generated responses by \ndynamically incorporating external, verifiable evidence into prompts 17â€“19. Within \nbiomedical question-answering contexts, RAG approaches consistently reduce \nhallucinations and elevate factual accuracy compared to parameter-only models. \nNonetheless, the efficacy of RAG hinges critically on the precision, comprehensiveness, \nand currency of the retrieved contextual evidence 20â€“22. \nKnowledge graphs (KGs) are particularly compelling resources for RAG because they \nexplicitly represent biological entities and their relationships, support multi-hop \nmechanistic reasoning, and maintain persistent identifiers that simplify provenance \ntracking 23â€“26. Yet most biomedical KGs are tuned to a narrow slice of biology (for \nexample, proteinâ€“protein interactions) or require extensive curation to remain current, \nlimiting their utility for cross-domain mechanistic reasoning. To address these \nchallenges, BioThings Explorer (BTE) integrates and federates 61 authoritative \nbiomedical APIs into a continuously updated meta-knowledge graph that encompasses \ngenes, pathways, drugs, diseases, phenotypes, and more 27. The API-centric framework \nof BTE returns structured JSON triples annotated with semantic types and evidence \ncitations from reputable biomedical databases such as Gene Ontology, DrugBank, and \nPubmed central using Translator Reasoner API (TRAPI) specification 28â€“30. \nHere, we introduce BTEâ€“RAG (BioThings Explorerâ€“Retrieval-Augmented Generation), \na novel framework that integrates the conversational fluency and reasoning capabilities \nof advanced LLMs with the explicit, multi-domain mechanistic knowledge captured by \nBTE. BTEâ€“RAG dynamically executes targeted, query-focused graph traversals to \nretrieve concise, mechanistically pertinent evidence, formulates this evidence into \ndeclarative context statements, and augments model prompts accordingly. \nTo rigorously assess the performance of BTE-RAG in biomedical question answering, \nwe systematically created three specialized benchmark datasets from DrugMechDB, a \ncurated knowledge base containing 5,666 expert-annotated mechanistic pathways with \nliterature validation 31. These datasets consist of gene-centric (n = 798), metabolite-\n.\nCC-BY 4.0 International license\navailable under a\n(which was not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made \nThe copyright holder for this preprint\nthis version posted August 2, 2025. \n; \nhttps://doi.org/10.1101/2025.08.01.668022\ndoi: \nbioRxiv preprint'
 'Omar & Mohammed / Journal of Applied Science and Technology Trends Vol. 06, No. 02, pp. 342 â€“357 (2025) \n \n345 \nrecord-high accuracy, while open-source models achieved \nimpressive gains through prompt optimization. \nFeng et al.[22] developed the Knowledge Graph-based \nThought (KGT) framework that integrated LLMs with a pan-\ncancer knowledge graph for biomedical question answering. \nKGT was designed to reason on the knowledge graph schema and \nidentify optimal subgraphs to use for directing accurate answer \ngeneration, all without fine-tuning the LLMs. The framework is \nbenchmarked against a new dataset (PcQA) designed specifically \nfor pan-cancer KGQA tasks and has outperformed all existing \nstate-of-the-art approaches by a rather large margin. KGTâ€™s \npracticality in biomedical issues was highlighted through case \nstudies for drug repositioning, drug resistance, and biomarker \ndiscovery. Their approach exhibited robust adaptability among \nvarious LLMs. \nRezaei et al.[26] developed AMG-RAG, a dynamic \nframework that utilizes autonomous LLM agents with medical \nsearch tools in the continuous construction and real-time updating \nof Medical Knowledge Graphs (MKGs). Their system \nincorporated confidence scoring and multi-hop reasoning to \nimprove accuracy and interpretability in medical question \nanswering. AMG-RAG outperformed size models on both very \nhard MEDQA benchmarks and more accessible MedMCQA \nones, proving that it could conduct efficient reasoning based on \ncurrent structured medical knowledge. They also used Neo4j to \nmanage the knowledge graphs while adding external searches to \nensure the latest data. \nTiwari et al.[24] presented Auto-Cypher, a recent automated \npipeline for producing high-quality synthetic data for training \nLLMs by mapping natural language to Cypher queries for graph \ndatabases like Neo4j. The pipeline deployed the novelty of LLM-\nas-database-filler to synthesize Neo4j databases for the execution \nof generated queries to ensure their correctness. A sizable dataset \ncalled SynthCypher was created, spanning multiple domains and \ncomplex queries, leading to a 40% improvement in LLM \nperformance on Cypher generation. The datasets were used to \nfine-tune open-source models such as LLaMA, Mistral, and \nQwen, and the SPIDER benchmark was adapted for evaluation \npurposes. \nMohammed et al.[29] proposed a hybridized GraphRAG \nframework combining Neo4j-based UMLS knowledge graphs \nwith a vector store for medical textbooks to create an improved \nU.S.M.L.E.-style clinical question-answering approach. The \nproject integrated symbolic reasoning from knowledge graphs \nwith semantic retrieval performed on text embeddings to enhance \nrelevance and accuracy via adaptive re-ranking and query \nexpansion. The system had the answers produced by GPT-4o-\nMini, with different prompting strategies encouraging evidence-\nbased and traceable responses grounded in verified medical \nknowledge. Experiments showed that the hybrid approach \nimproved factual accuracy and citation fidelity as compared to \nthe L.L.M.-only approach, enhancing transparency and \nreliability. It is shown that binding both structured and \nunstructured medical knowledge sources could aid in \nameliorating hallucinations and hence improve clinical \ntrustworthiness in AI-driven medical QA. \nYang et al.[30] articulated sepsis knowledge graph was \ncrafted by combining multicenter clinical data from over 10,000 \npatients with the help of GPT-4 for entity recognition and \nrelationship extraction. Real-world data were collected from \nthree hospitals and integrated with clinical guidelines and \ndatabases from the public domain. The knowledge graph \ncontained 1,894 nodes and 2,021 relationships pertaining to \ndiseases, symptoms, biomarkers, treatments, and complications. \nGPT outperformed other models in every resolution on sepsis-\nspecific datasets to obtain high F1-score results. The constructed \ngraph highlighted complex interactions in sepsis for assisting \nclinical decision-making and was implemented on Neo4j. \nGuan et al.[20] proposed a novel method for constructing a \nlocal knowledge graph from retrieved biomedical documents by \nextracting propositional claims. They carried out layer wise \nsummarization on this graph to capture multi-document \nrelationships and provide comprehensive contextual information \nto a language model for question-answering purposes. The \nmethod resolved issues in multi-document biomedical QA, such \nas noise cancellation and efficient context usage. They then tested \ntheir method on several benchmarks for biomedical question \nanswering, achieving performance at least comparable to, if not \nbetter than, existing retrieval-augmented generation (RAG) \nbaselines. The study established enhanced reasoning and answer \naccuracy of the model achieved through structured graph \nsummarization.   \nPrevious studies have improved biomedical QA using KGs \nand LLMs, but important gaps remain. Most systems lack \ntransparent, graph-based justifications, rely on limited evaluation \nmethods, or depend on cloud resources that reduce privacy and \nreproducibility. Our framework addresses these gaps by \nproviding visible Cypher queries with evidence subgraphs, \napplying comprehensive performance metrics across difficulty \nlevels, and ensuring fully local, privacy-preserving deployment. \nTable I summarizes key previous studies on biomedical \nknowledge graphs and question answering, outlining their \nmethods, datasets, and main limitations. \nIII. PRELIMINARIES \nThis section outlines the fundamental concepts required to \nunderstand the proposed system. It introduces biomedical \nknowledge graphs, explains how Neo4j stores data in graph \nform, and describes the use of Cypher for querying. It also \nprovides a brief overview of large language models (LLMs) and \ntheir role in interpreting natural language. \n \n \nA. Biomedical Knowledge Graphs \nBiomedical Knowledge Graphs (BKGs) provide a structured \nrepresentation of complex biomedical information by modeling \ndiverse medical entities, such as diseases, drugs, symptoms, and \nbiological pathways, as interconnected nodes within a graph \nstructure. The edges in these graphs represent the semantic \nrelationships between these entities, including â€™treatsâ€™, â€™causesâ€™, \nâ€™interacts withâ€™ and many others, as illustrated in Fig 1. This \nform of representation enables the integration of heterogeneous \nbiomedical data from a wide range of sources, including'
 'offer global information based on user query, offering \nbetter performance than naÃ¯ve RAG on the traditional \nvector databases.50\n6) Implement few-shot learning with CoT for complex clin\xad\nical reasoning. \nFew-shot learning has been shown to enhance LLMsâ€™ \nreasoning capabilities by teaching specific reasoning that \nmay not have been included in their original training. \nSimilarly, CoT techniques can improve complex reason\xad\ning in clinical tasks.51,52 Researchers should generate \nhigh-quality examples and incorporate CoT strategies \ninto the final query to refine specialized reasoning. \n7) Evaluate all stages of the RAG system development. \nMost current studies focus only on final performance, \noverlooking the importance of evaluating each stage of \ndevelopment. It is crucial to formally assess and report \nperformance at the pre-retrieval, retrieval, and post- \nretrieval stages. Evaluating the knowledge boundaries of \nthe baseline LLM, potential conflicts between the LLM \nand external knowledge, and the accuracy and coverage \nof retrieved information helps ensure replicability and \ntransparency. This level of evaluation enables other \nresearchers to understand why a RAG system works (or \ndoes not) and facilitates reproducibility. \n8) Consider cost and time constraints. \nAdvanced retrieval strategies can improve performance \nbut often increase processing time and computational \ncosts. For example, graph-based RAG requires substan\xad\ntial resources for developing knowledge graphs, and \nresponses from global summaries may take longer than \nwith naÃ¯ve RAG methods.50 Another example is to fine- \ntune LLMs to evaluate the needs and the quality of \nretrieval.34 In terms of computational cost, this process \nis expensive, especially when scaling the method to \nlarger datasets or deploying it in a real-time system. \nAlso, a set of extra processes will make the whole speed \nslow. The long response time might have a nonignorable \nnegative impact in situations that need a quick answer, \nespecially common in clinical settings. Researchers \nshould balance performance improvements with time \nand cost considerations. Only 1 study in our review, \nwhich focused on gastrointestinal radiology diagnosis \nbased on imaging descriptions, compared the cost and \nresponse time between LLMs and LLMs with RAG.31\nThe mean response time was 29.8 s for LLM with RAG \nvs 15.7s for LLM alone, with costs of $0.15 and $0.02 \nper case, respectively. Another study used EHR to pre\xad\ndict cognitive decline only reported cost, with LLM: \n$4.49; RAG: $12.51. Another study that used EHR \ndata to predict cognitive decline reported costs of $4.49 \nfor LLM alone and $12.51 for LLM with RAG.53\n9) Establish monitoring and knowledge update strategies. \nAn important concept in AI applications in healthcare, \nalgorithmovigilance, which defined as â€œscientific meth\xad\nods and activities relating to the evaluation, monitoring, \nunderstanding, and prevention of adverse effects of \nalgorithms in health care,â€ 54 should also be considered \nin the RAG applications. Researchers need to develop \nlong-term monitoring strategies for the RAG system per\xad\nformance, especially in clinical applications. In addition, \ncurrent studies use fixed external datasets. Researchers \nshould update external knowledge sources as latest \ninformation becomes available. Clear strategies for \nupdating knowledge should be defined, specifying when \nand how updates will occur. \nFor future studies, the first direction could be the system- \nlevel enhancement, the combination of RAG and LLM- \npowered agents. LLM-powered agents are AI systems that \nuse LLMs with complex reasoning and planning capabilities, \nmemory management, interactive capabilities with the envi\xad\nronment, and actions to execute tasks.55,56 Recent research \npoints to the emerging trend of combination of RAG and \nLLM-powered agents, where agents can assist in planning \nand decision making for complex tasks, rather than simple \nretrieval.57 For example, clinicians and patients have diverse \ninformation access needs, some needing to analyze text from \na knowledge base, others needing to incorporate structured \ndata from an EHR. RAG will eventually only become one of \nthe methods for agents to access information. Moreover, \nfuture research could focus on the usage of internal and \nexternal functions and tools, long-term and short-term mem\xad\nory module, self-learning module. For example, a study \ndeveloped an agent to answer questions related to rare dis\xad\neases by expanding beyond RAG with additional tool func\xad\ntions, such as querying phenotypes and performing web \nsearches. This approach improved the overall correctness \nfrom 0.48 to 0.75 compared to the GPT-4 baseline LLM.58\nThe second future direction could focus on the knowledge- \nlevel enhancement: deep integration of external knowledge \ninto LLM. LLM exhibits the knowledge boundaries. RAG \napproaches retrieving external knowledge and then integrates \nit into LLMs in the forms of prompts for the final generation \nto enhance the capabilities of LLMs in perceiving knowledge \nboundaries.59 However, the integration of external knowl\xad\nedge into LLM reasoning is typically limited to providing the \nretrieved data as additional context for the LLMâ€™s query dur\xad\ning generation. This approach keeps retrieval and generation \nloosely connected, and the LLMâ€™s output can still be influ\xad\nenced by its inherent knowledge boundaries or by noise in the \nretrieved text, leading to incorrect answers. Additionally, \nwhen the external knowledge source is EHR data, this \nenhancement becomes even more important. Current EHR \ndata is organized in a â€œproblem-oriented medical recordâ€ \n(POMR) format, which collects and displays information in a \nstructured manner.60 LLMs excel in free-form contexts, and \ntheir ability to perform clinical tasks depends on access to \nunstructured text that provides a comprehensive view of the \npatient. Achieving this within the structured POMR format \nin modern EHR systems poses a significant challenge.61\nTherefore, investigating how to realize the deep integration \nof external knowledge with LLM reasoning is an important \ndirection for future research in clinical applications.\nThe final direction is the integration-level enhancement, \nfocusing on integrating RAG systems within EHRs. Current \nresearch has primarily focused on development and testing \noutside of EHR systems. To seamlessly provide support for \nhealthcare providers and patients, future efforts should pri\xad\noritize embedding RAG systems into EHR interfaces. This \nrequires collaboration with EHR vendors to ensure the neces\xad\nsary infrastructure is available. Researchers also can facilitate \nthis integration using data exchange frameworks, such as \nSMART on FHIR.62\n612                                                                                                      \nJournal of the American Medical Informatics Association, 2025, Vol. 32, No. 4 \nDownloaded from https://academic.oup.com/jamia/article/32/4/605/7954485 by guest on 11 January 2026']","['<1-hop>\n\n2 | GigaScience , 2025, Vol. 14 LLaMA with medical liter atur e. Additionall y, Med-P aLM [ 9 ] shows promising performance on the MedQA exam based on clinical cor por a and human feedback. Meanwhile, aiming at the Chinese medical domain, LLMs such as BenTsao [ 10 ], DoctorGLM [ 11 ], and HuatuoGPT [ 12 ] are developed on the Chinese medical dialogue data. Mor e r ecentl y, Zhongjing [ 13 ] and ChiMed-GPT [ 14 ] adopted full pipeline training from pretraining, SFT, to reinforcement learning with human feedback (RLHF) [ 15 ]. While fine-tuning can reduce hallucinations in LLMs, it brings about considerable train- ing expenses. Additionally, it poses a critical challenge known as catastrophic forgetting. This issue manifests when a model for gets its pr e viousl y learned information as a consequence of parameter modifications during the acquisition of new tasks. This forgetfulness results in a deterioration of performance on prior tasks, consequently constraining the modelâ€™s practical applicability [ 16 , 17 ]. In addition to fine-tuning, r esearc hers also enhance the out- put of LLMs through the field of prompt engineering. Prompt en- gineering focuses on the creation and optimization of prompts to impr ov e the effectiv eness of LLMs acr oss v arious a pplications and r esearc h domains [ 18 ]. It can enhance the ca pabilities of LLMs in a wide range of complex tasks, including question answering, sentiment classification, and commonsense reasoning. Chain-of- thought (CoT) prompts [ 19 ] enable complex reasoning capabili- ties by incor por ating intermediate r easoning steps . T he Automatic Pr ompt Engineer (APE) pr oposes an automatic pr ompt gener ation method aimed at enhancing the performance of LLMs [ 20 ]. Prompt engineering offers a straightforw ar d approach to harnessing the potential of LLMs without fine-tuning. On the other hand, knowledge gr a phs (KGs) ar e r epositories of vast quantities of high-quality structured data, offering the po- tential to effectiv el y mitigate the issue of factual hallucinations when integrated with LLMs . Hence , employing KGs for question answering can enhance the precision of the responses and fur- nish a dependable foundation for the factual verification of in- formation produced by LLMs. Knowledge gr a ph question answer- ing (KGQA) has long been a hot r esearc h topic. Befor e the ad- vent of LLMs, certain studies [ 21â€“23 ] typically begin by r etrie ving a subgr a ph r elated to the question to r educe the searc h space, then perform m ultihop r easoning on this basis . T his r etrie v al- plus-r easoning par adigm has shown its adv anta ges ov er dir ect r easoning acr oss the entir e KG [ 24 , 25 ]. Additionall y, r esearc hers tackle KGQA by parsing the question into a structured query lan- guage (e.g., SPARQL) and using a query engine to obtain accu- rate answers [ 26 , 27 ]. UniKGQA [ 28 ] introduces a unified fine- tuning fr ame work for r etrie v al and r easoning, mor e closel y linking these 2 stages. Ho w ever, traditional KGQA methods usually per- form poorly in accurate semantic understanding and high-quality text generation due to the lack of LLMs for r etrie v al and r eason- ing. Hence, r ecent r esearc h is incr easingl y utilizing external KGs to enhance LLMs in addressing KGQA challenges. For instance, StructGPT [ 29 ] navigates through knowledge graphs by identify- ing pathways from an initial seed entity to the target answer en- tity, while Think-on-Gr a ph (ToG) [ 30 ] intr oduces iter ativ e explo- ration of the knowledge gr a ph, whic h can become inefficient with v ery lar ge KGs. Additionall y, Reasoning on Gr a phs (RoG) [ 31 ] ne- cessitates fine-tuning to accur atel y gener ate and plan the r elation paths. KG-GPT [ 32 ] opts for r etrie ving an entire subgraph from the knowledge gr a ph and then deduces the answer thr ough infer ence. Although these methods hav e ac hie v ed gr atifying r esults in gen- er al ar eas, as shown in Fig. 1 B, when the intermediate entity in the multihop question is unknown, it is impossible to retrieve the a ppr opriate knowledge from the KG. In this article, we intr oduce an innov ativ e fr ame work called knowledge gr a phâ€“based thought (KGT), whic h integr ates LLMs with KGs through employing LLMs for reasoning on the schema of KGs to mitigate factual hallucinations of LLMs, as shown in Fig. 1 C. Unlike traditional methods, KGT does not dir ectl y r etrie v e factual information based on the question. Instead, it uses LLMs to infer entity information on the schema of the knowledge graph, generating an optimal subgraph based on k e y information di- r ectl y extr acted fr om the question and inferr ed information fr om the sc hema. Subsequentl y, the optimal subgr a ph is used to in- fer the answer to the question through LLMs. KGT r equir es no fine-tuning, offers seamless integration with multiple LLMs, and is plug-and-play, facilitating easy deployment. It demonstrates generalizability, making it adaptable for use with diverse knowl- edge gr a phs . T his fr ame work is tailor ed for wide-r anging a ppli- cations in numerous biomedical challenges, such as (i) enhanc- ing clinical decision-making for physicians and medical organi- zations, (ii) delivering medical advice to patients and health care pro viders , (iii) unco vering crucial biomarkers for early disease de- tection and tailored therapy, and (iv) exploring novel therapeutic applications for existing medications through insights into their mechanisms , side effects , and the biological processes of associ- ated diseases . Furthermore , we utilize the'
 '<2-hop>\n\nMeta-analysis Effect size was defined as a metric quantifying the relation\xad ship between variables, including both direction and magni\xad tude.18 For each included study, we calculated the effect size between baseline LLM performance and RAG-enhanced LLM performance. The outcomes focused on the perform\xad ance of generation results, such as accuracy and usefulness. Metrics related to the retrieval process, cost, or speed were not included as outcomes in the meta-analysis. For continu\xad ous outcomes, we used Cohenâ€™s d, standardized between- group mean difference (SMD), calculated as the difference in means divided by the pooled standard deviation. The stand\xad ard error (SE) of SMD was calculated using the following for\xad mula (1), where n1 and n2 represent the sample sizes of each group.19 For dichotomous measurements, we calculated the log-odds ratio, obtained by transforming the odds ratio (OR) with the natural logarithm, and the associated SE was calcu\xad lated using formula (2), where a, b, c, and d represent the number of successful and failed events in the baseline LLM and RAG-enhanced LLM approaches. For studies reporting multiple outcomes, we used the overall outcome to calculate effect size. If no overall outcome was reported, we averaged the effect sizes of all reported outcomes. We excluded out\xad comes with a sample size of less than 30 to avoid small- sample bias. SESMD Â¼ ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi n1 Ã¾ n2 n1n2 Ã¾ SMD2 2 n1 Ã¾ n2 Ã° Ãž s (1) SElogOR Â¼ ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi 1 a Ã¾ 1 b Ã¾ 1 c Ã¾ 1 d r (2) We developed a random-effect meta-analysis model, because of the variability in RAG architectures and evalua\xad tion datasets among the included studies. The random-effect model was used when individual study effects likely con\xad tained additional sources of variance beyond sampling error. Between-study heterogeneity was assessed using Higgins & Thompsonâ€™s I2 statistic, where 25% indicated low heteroge\xad neity, 50% moderate, and 75% substantial.20 We conducted subgroup analyses to explore performance variations across different factors. First, we analyzed the influence of the baseline LLM, referring to the foundation model (eg, GPT-4 or Llama2) that provides the core architec\xad ture for the system. Second, we examined data retrieval strat\xad egies, categorizing them as simple or complex. Simple strategies included fixed-length chunking and basic similarity search, and we performed a subgroup analysis to compare these with complex retrieval strategies. Third, we analyzed differences based on evaluation methods, distinguishing between human evaluations, such as Likert scale ratings for helpfulness and accuracy, and automatic evaluation metrics, including ROUGE-1 and BLEU. Finally, we conducted a sub\xad group analysis based on the type of task, classifying studies 606 Journal of the American Medical Informatics Association, 2025, Vol. 32, No. 4 Downloaded from https://academic.oup.com/jamia/article/32/4/605/7954485 by guest on 11 January 2026']","Based on the provided context, the differences between LLaMA and LLaMA 2 models in their integration with biomedical knowledge graphs (KGs) to mitigate hallucinations in large language models (LLMs), and the impact of this integration on retrieval-augmented generation (RAG) system performance as demonstrated by meta-analyses, can be summarized as follows:

### Differences in Integration of LLaMA and LLaMA 2 with Biomedical Knowledge Graphs

1. **Model Versions and Fine-tuning:**
   - **LLaMA:** Earlier versions of LLaMA have been used in biomedical QA systems that integrate KGs by translating natural language questions into graph queries (e.g., Cypher queries for Neo4j-based KGs). These systems focus on improving answer correctness and reducing hallucinations by grounding responses in structured biomedical knowledge.
   - **LLaMA 2:** More recent studies have fine-tuned LLaMA 2 specifically for biomedical question answering tasks, improving the accuracy of converting natural language questions into logical forms or graph queries. For example, Luo et al. fine-tuned LLaMA 2 to generate logical forms with high accuracy, which then guide retrieval from biomedical KGs. This fine-tuning enhances the alignment of entities and relations during retrieval, leading to superior performance compared to earlier models.

2. **Integration Approaches:**
   - **LLaMA:** Integration often involves hybrid frameworks that combine LLaMA with biomedical KGs (e.g., SPOKE) and vector stores to enhance retrieval and reasoning. These frameworks optimize retrieval to reduce token usage without sacrificing accuracy and provide evidence-based, traceable answers.
   - **LLaMA 2:** The integration with KGs in LLaMA 2-based systems tends to emphasize prompt-driven query generation and schema/entity-aware prompting, which reliably maps diverse biomedical questions to graph queries. This approach supports complex reasoning and multi-hop queries, improving the systemâ€™s ability to handle complex biomedical questions.

3. **Deployment and Privacy:**
   - LLaMA 3 (a later version related to LLaMA 2) has been deployed locally (on-premises) in biomedical settings to ensure low latency and strong data control, which is critical for privacy-sensitive clinical environments. This contrasts with some cloud-dependent deployments of other LLMs.

### Impact on Performance of RAG Systems (Meta-Analysis Findings)

1. **Performance Improvement:**
   - The meta-analysis of 20 studies comparing baseline LLMs with RAG-enhanced systems showed a statistically significant improvement in performance, with a pooled odds ratio of 1.35 (95% CI: 1.19â€“1.53, P = .001). This indicates that integrating external knowledge sources, including biomedical KGs, via RAG significantly enhances the accuracy and reliability of LLM outputs in biomedical tasks.

2. **Reduction of Hallucinations:**
   - Integrating biomedical KGs with LLaMA and LLaMA 2 models through RAG frameworks helps mitigate hallucinations by grounding generated answers in verifiable, structured biomedical knowledge. This leads to more accurate, evidence-based responses and reduces the generation of fabricated or incorrect information.

3. **Efficiency and Robustness:**
   - Frameworks like KG-RAG that combine LLaMA 2 with biomedical KGs have demonstrated robust and efficient retrieval, cutting token usage by over 50% without losing accuracy. This efficiency gain is important for practical deployment in clinical settings.

4. **Evaluation Metrics:**
   - The integrated systems have been evaluated using comprehensive metrics such as Exact Match, F1, Precision/Recall, Hits@k, and Mean Reciprocal Rank (MRR), showing superior performance across simple to complex biomedical question sets.

5. **Future Directions:**
   - The meta-analysis and literature review suggest future research should focus on:
     - System-level enhancements combining RAG with LLM-powered agents for complex reasoning.
     - Knowledge-level enhancements for deeper integration of external knowledge into LLM reasoning beyond prompt augmentation.
     - Integration-level enhancements embedding RAG systems within electronic health records (EHRs) for seamless clinical use.

### Summary

- **LLaMA and LLaMA 2 differ mainly in their fine-tuning and prompting strategies for biomedical KG integration, with LLaMA 2 showing improved accuracy in query generation and retrieval alignment.**
- **Both models benefit from RAG frameworks that incorporate biomedical KGs, which significantly reduce hallucinations and improve factual accuracy.**
- **Meta-analyses confirm that RAG-enhanced LLMs, including those based on LLaMA and LLaMA 2, outperform baseline LLMs in biomedical question answering, with a notable increase in performance (odds ratio ~1.35).**
- **These integrations enable more transparent, evidence-based, and efficient biomedical AI applications, with ongoing research aimed at deeper knowledge integration and clinical system embedding.**","LLaMA-based models in the biomedical domain have been enhanced through various approaches, including fine-tuning and prompt engineering, to reduce hallucinations and improve performance. The knowledge graphâ€“based thought (KGT) framework integrates LLMs like LLaMA with biomedical knowledge graphs by using LLMs to reason on the schema of knowledge graphs rather than directly retrieving factual information. This method generates an optimal subgraph based on key information extracted from the question and inferred schema data, which is then used by LLMs to infer answers. KGT requires no fine-tuning, supports seamless integration with multiple LLMs including LLaMA, and is plug-and-play, facilitating deployment and generalizability across diverse biomedical knowledge graphs. On the other hand, Llama2 serves as a foundational model in retrieval-augmented generation (RAG) systems, where meta-analyses have quantified the effect size of RAG-enhanced LLM performance compared to baseline LLMs. These analyses, using metrics such as Cohenâ€™s d and log-odds ratios, demonstrate that integrating retrieval mechanisms with Llama2 improves generation accuracy and usefulness. Subgroup analyses further reveal that the choice of baseline LLM, including Llama2, and the complexity of data retrieval strategies influence performance outcomes. Thus, while LLaMA models benefit from knowledge graph integration via frameworks like KGT to mitigate hallucinations and enhance reasoning, Llama2â€™s role as a base model in RAG systems is supported by meta-analytic evidence showing improved generation performance when retrieval is incorporated.",1.0,0.9500620001308421,0.999999999975,1.0
"How does the integration of single-cell RNA sequencing data from the cellxgene Atlas enhance the co-expression networks in STRING version 12.5, and what advantages does this expansion provide for understanding gene expression dynamics across different organisms?","['Nucleic Acids Research , 2025, Vol. 53, Database issue \nD 733 \nFigure 1. Illustration of the new â€˜regulatory networkâ€™ mode in STRING, where the network edges visually indicate the direction, confidence and sources \nof each regulatory interaction. Clicking on an edge within the network brings up a pop-up window with a detailed o v ervie w of the a v ailable e vidence and \nthe annotated type of regulation. Users can further explore the data behind the interactions by accessing the specific evidence viewers linked within the \npop-up, including the text-mining evidence viewer. This viewer presents excerpts from literature pertaining to the inspected interactions, as well as an \nautomatically generated large language model summary of the presented e x cerpts. \nrylation , Catalysis of Small Molecule Conjugation and Cataly- \nsis of Small Molecule Removal . To enhance the interoperabil- \nity of the annotations, these chosen relation types align with \nGene Ontology ( 32 ). \nTo identify and classify a wide array of interaction types, we \nfine-tuned the RoBERT a-large-PM-M3-V oc language model, \na transformer-based model well suited for biological tasks. \nThis model was trained on the RegulaTome dataset for multi- \nlabel extraction of the directed, typed and signed interactions \nmentioned above, and achieved an average F 1 score of 73.5% \n(with a precision of 75.2% and a recall of 71.8%) on a held- \nout test set, although the exact performance varied across dif- \nferent types of relationships. This level of accuracy is substan- \ntial, considering the complexity of the underlying task. Using \nthis model, we processed over 1.2 billion sentence-level pairs \nextracted from all available PubMed abstracts and PMC Open \nAccess full-text documents, and assigned 3.5% of them ( âˆ¼43 \nmillion, of which âˆ¼18 million in human) with at least one \npositive label indicating directed (72.9%) or signed (33.1%) \nrelationships among the proteins. Furthermore, we made use \nof the existing curated knowledge of regulatory interactions in \nthe database channel, by parsing regulatory information from \nthe SIGNOR, KEGG and Reactome databases. \nTo integrate these interactions into STRING, we followed \na similar score aggregation and benchmarking approach as \nfor physical interactions ( 21 ). Here, we perform benchmark- \ning in five categories for which we can derive gold stan- \ndard datasets of known human regulatory interactions from \nSIGNOR: regulation , upregulation , downregulation , tran- \nscriptional regulation and phosphorylation . The resulting \ncalibration functions are then applied to extracted regula- \ntions of all types to produce the final regulatory confidence \nscores. \nThe new regulatory network has been seamlessly integrated \ninto the existing STRING framework, complementing the \nâ€˜functionalâ€™ and â€˜physicalâ€™ network types. Directional edges \nwithin this network are visually represented by arrows, depict- \ning both bidirectional and unidirectional relationships. The \nnetwork visualization can show varying confidence levels be- \ntween directions (in the â€˜confidenceâ€™ view) or the specific di- \nrectionality of the sources (in the â€˜evidenceâ€™ view). The user \ninterface retains its intuitive design, enabling users to access \nthe type of regulatory event and the evidence for the interac- \ntion by clicking on the edge (Figure 1 ). Additionally, all API \nfunctions have been updated to fully support the new net- \nwork type, which can be accessed by specifying the parameter \nnetw or k_type = regulatory in the API call. \nExpanded co-expression networks \nThe co-expression channel in STRING identifies genes with \nsimilar expression profiles across various tissues and condi- \ntions, revealing their involvement in shared biological pro- \ncesses. It compiles data from large-scale gene expression stud- \nies, analyzing transcript and protein levels to detect gene pairs \nwith coordinated expression. This method highlights poten- \ntial functional relationships between genes. Importantly, the \nanalysis is free from study bias, as it directly uses raw omics \ndata instead of relying on manually curated or experimental \ndatasets, providing a more precise and objective view of gene \ninteractions. \nFor the upcoming version 12.5, we are expanding the \nco-expression channel by generating additional networks \nthrough the application of FAVA (Functional Association of \nVariational Autoencoders) ( 25 ) on a wider range of single- \ncell RNA sequencing data. Specifically, we are incorporating \nsingle-cell data from the cellxgene Atlas ( 33 ) to enhance the \nhuman and mouse networks and from the EBI Single Cell Ex- \npression Atlas to extend coverage to more model organisms \n( 34 ). This expansion will provide more detailed, organism- \nspecific co-expression maps, enabling more precise identifi- \ncation of gene co-expression patterns and cross-species com- \nparisons, particularly for development, immune response and \ndisease-related gene expression dynamics. \nImpro v ed enric hment analysis \nGene set enrichment analysis is a critical component of high- \nthroughput biological studies, providing key insights into the \nbiological characteristics of datasets. This analysis is pivotal in \nidentifying essential aspects such as biological processes, path- \nways and phenotypes, which are crucial for understanding the \nbiological implications of experiments. Typically, overrepre- \nsentation analysis is employed, wherein a subset of data ex- \nceeding a specific cutoff (empirical or customary) is compared \nagainst an experimental background. By default, STRING \nDownloaded from https://academic.oup.com/nar/article/53/D1/D730/7903368 by guest on 11 January 2026'
 'D 732 \nNucleic Acids Research , 2025, Vol. 53, Database issue \ncific network modes are fully consistent with the functional \nnetwork, such that if an interaction is present in either the \nphysical or regulatory network, it will, by definition, also be \npresent in the full functional association network (with an \nequal or higher confidence score). To compute the various con- \nfidence scores, the available evidence supporting a given inter- \naction is first separated into â€˜evidence channelsâ€™, by the type \nof evidence. For the basic functional association confidence \nscore, seven such channels are used. These include three that \nare based on genomic context predictionsâ€”neighborhood, fu- \nsion and gene co-occurrenceâ€”as well as channels dedicated to \nco-expression, experimental data, curated databases and text \nmining. \nThe genomic context channels focus on associations in- \nferred from genome sequences alone, detecting evolution- \nary patterns such as gene proximity, fusion events and co- \noccurrence across different organisms. For instance, the neigh- \nborhood channel assigns an association score to gene pairs \nthat are located close to each other on the chromosome in \nthe same orientation (in prokaryotic genomes) ( 22 ). The fu- \nsion channel identifies open reading frames that result from \ngene fusion events ( 23 ), while the co-occurrence channel ex- \namines whether genes have a nontrivial but shared distribu- \ntion across genomes ( 24 ), implying a shared history of hori- \nzontal transfers, losses or duplication events and thus likely \nrelated functions. The co-expression channel compiles data \nfrom gene expression studies, analyzing both transcript and \nprotein abundances across various conditions. By comparing \nexpression profiles, it identifies gene pairs with similar expres- \nsion patterns, suggesting functional linkage ( 10 ,25 ). The ex- \nperiments channel aggregates interaction evidence from labo- \nratory assays, including biochemical, biophysical and genetic \nassays. Data are imported from primary repositories such as \nBioGRID ( 5 ) and the IMEx consortium ( 26 ), and the confi- \ndence scores are estimated by globally benchmarking the accu- \nracy of annotated experimental techniques, as well as within- \ndataset performance and consistency for the case of high- \nthroughput experiments ( 10 ). The database channel is based \non well-described, curated proteinâ€“protein associations from \nexpert-compiled resources, such as KEGG ( 4 ), Reactome ( 3 ) \nand Gene Ontology Complexes ( 16 ). These resources provide \nwell-established pathways, complexes and functional relation- \nships, offering a high level of reliability. Unlike other chan- \nnels, the database channel assigns a uniformly high confidence \nscore to associations, reflecting their established nature. Fi- \nnally, the text-mining channel utilizes a large corpus of sci- \nentific literature, including PubMed abstracts and full-text ar- \nticles, to identify co-mentions of protein names ( 27 ). By statis- \ntically analyzing the frequency of these co-mentions in various \ntextual contexts, this channel uncovers potential associations \nthat may not be evident from structured data alone. In addi- \ntion, for the more specific association modes, dedicated large \nlanguage models are employed to detect sentences supporting \neither physical ( 21 ) or regulatory associations (see below). \nFor each evidence channel, the available interaction evi- \ndence is translated into a confidence score by first quantifying \nthe evidence using channel-specific metrics and then convert- \ning these metrics into likelihoods using calibration curves de- \nrived from prior knowledge (from pathway-map memberships \nin KEGG). For the more specific association modes â€˜physicalâ€™ \nand â€˜regulatoryâ€™, only channels and evidence that are appli- \ncable to these modes are considered. After this, all channel \nscores that have been computed for a given protein pair in \na given organism are transferred onto related protein pairs \nin other organisms, based on the â€˜interologâ€™ concept ( 27 ,28 ). \nLastly, a final, combined confidence score is computed by inte- \ngrating the channel-specific subscores probabilistically, under \nthe assumption that evidence in different channels is largely \nindependent. Users of STRING can directly rely on this com- \nbined score for browsing and filtering networks, or they can \nalternatively customize their analyses by enabling or disabling \nspecific channels separately, after which the combined score is \nrecomputed according to their chosen settings. \nAll primary evidence underlying a given interaction can \nbe inspected interactively, in dedicated evidence viewers on- \nline. Furthermore, accessory information is available for \neach protein, such as its three-dimensional structure, do- \nmain composition, annotation and cross-references. Apart \nfrom its core protein-network functionality, STRING also \nimplements features that allow extensive analysis of up- \nloaded user data. These include functional enrichment de- \ntection ( 29 ), experimental bias detection ( 21 ), homology \nsearches, and clustering and organizing large query pro- \ntein sets. All data contained in STRING are freely avail- \nable for download, under a Creative Commons BY 4.0 li- \ncense. Apart from its website, STRING can also be accessed \nvia a dedicated Cytoscape plugin ( 30 ), through an applica- \ntion programming interface (API) ( 31 ), as well as through \nan R / Bioconductor package ( https://www.bioconductor.org/ \npackages/ release/ bioc/ html/ STRINGdb.html ). \nRegulat ory netw orks \nMolecular networks, like those available through STRING, \nhave become invaluable tools in biomedical research, offering \npowerful insights into how molecules work together within \ncells. The functional association networks, while useful for \nmany applications, do not specify the interaction type (e.g. \ncomplex formation or transcriptional regulation) nor the di- \nrection of interaction (i.e. who regulates whom). Constructing \nnetworks with more detailed interaction types and directions \ncan significantly expand their utility, particularly for building \nmathematical models of biological systems or for making log- \nical inferences. The directionality of interactions is essential \nfor interpretation of omics data, which typically reveals more \nabout downstream effects of conditions such as diseases than \nit does about the upstream events that led to it and thus could \nbe targeted therapeutically. \nTo incorporate interactions with specific directionality, sign \nand type into STRING, we leveraged the novel dataset and \ndeep learning-based relation extraction method described in \nRegulaTome ( 32 ). The RegulaTome corpus provides a rich \nand diverse dataset that includes 16 961 relations between \n54 951 entities annotated across over 2500 documents. This \ncorpus is utilized to train a deep learning-based method used \nto annotate and extract the regulatory events throughout the \nentire available literature. \nOn top of undirected physical interactionsâ€”which are al- \nready included in STRINGâ€”the following types of directed \ninteractions can be extracted using the system developed for \nRegulaTome: Regulation , with the signed subtypes of Posi- \ntive Regulation and Negative Regulation , Regulation of Gene \nExpression , Regulation of Degradation and Catalysis of Post- \nTranslational Modifications , covering six subtypes: Catalysis \nof Small Protein Conjugation , Catalysis of Small Protein Re- \nmoval , Catalysis of Phosphorylation , Catalysis of Dephospho- \nDownloaded from https://academic.oup.com/nar/article/53/D1/D730/7903368 by guest on 11 January 2026'
 'D 734 \nNucleic Acids Research , 2025, Vol. 53, Database issue \nuses the whole genome / proteome background; however, it \nis recommended that users provide a custom background \nlist representing a more realistic universe of genes / proteins \ndetected by their assay ( 35 ). Alternatively, the entire sorted \ndataset can be analyzed using gene set enrichment analysis to \ndetermine whether the distribution of genes at the top, bottom \nor both ends deviates significantly from what would be ex- \npected by chance. STRING supports both of these enrichment \nmethods. The most recent update to STRING introduces vari- \nous improvements, including a new false discovery rate (FDR) \ncorrection, enhanced filtering options and a new graphical vi- \nsualization. \nFDR correction \nIn its analyses, STRING performs a test for each path- \nway (â€˜termâ€™) within a given pathway collection (â€˜categoryâ€™). \nLike many other enrichment tools, STRING employs the \nBenjaminiâ€“Hochberg correction ( 36 ) to adjust for multiple \nhypothesis testing. This statistical correction is essential for \nmanaging the FDR effectively. However, the larger the term \ncount and the greater the diversity of the terms, the higher \nthe required correction and the lower the chance of exceed- \ning the alpha level (significance threshold) for a given term. \nOne method used to address this issue involves restricting the \nanalysis to a subset of terms, such as â€˜GO slimsâ€™ that focus \non broad, high-level terms with a simplified ontology struc- \nture ( 16 ). However, this approach has some disadvantages: it \nmight exclude terms particularly relevant to a tested gene set, \nit may not cover all functions by design and it is mainly ap- \nplicable to hierarchical classification systems such as the Gene \nOntology. \nSTRING utilizes several hierarchical ontologies, such as \nthe Brenda Tissue Ontology, Disease Ontology, Human Phe- \nnotype Ontology and the aforementioned Gene Ontology; \nthey typically contain many more leaf-ward nodes than nodes \ncloser to the root. The parental nodes must contain all the \ngenes included in the child terms, creating an imbalance in \nthe ontology with many more smaller terms and fewer larger \nterms. Removing smaller terms can substantially increase sta- \ntistical power . However , simply eliminating all small terms \ncould negatively impact discoverability, especially for small \nquery gene sets where the smaller terms might be the most rel- \nevant for biological interpretation. The ontologies are usually \nlarge, with > 10 000 terms (and therefore tests) in the Biolog- \nical Process branch of the Gene Ontology tree. Such strong \ncorrections have the effect that for larger query gene sets, or \nfor backgrounds that cover only part of the proteome, it might \nnot be statistically possible to detect enrichments for a subset \nof the terms; these can therefore be removed from considera- \ntion. \nIn its new version, STRING takes a more flexible \napproachâ€”by only testing terms that have a priori any statis- \ntical potential for enrichment. This decision is based on sev- \neral parameters: the size of the term, the size of the query set, \nthe background size and the number of tests conducted. By \ncalculating the smallest and largest term sizes that could the- \noretically be enriched given the FDR correction, STRING can \ndetermine which terms are viable for inclusion in the anal- \nysis. Terms outside these bounds are excluded from testing. \nThis methodical exclusion significantly enhances the statisti- \ncal power of the enrichment analysis, without omitting any \nterms that could be enriched. For smaller query sets, this strat- \negy will maintain all terms, as even the smallest tested sets \n(term size = 2) might still be theoretically enriched. How- \never , for larger , less specific sets or for tests against a cus- \ntom smaller background, this approach markedly improves \nthe statistical power, enabling STRING to perform more fo- \ncused analyses without the drawbacks of increased false posi- \ntive rates or omitting potentially biologically interesting terms. \nAlthough this method leverages the characteristics of ontolo- \ngies, it does not require a hierarchy (parentâ€“child relationships \nbetween the terms) to function and can be applied to any \npathway / term collection with an imbalance of small terms, \nsuch as PMID-derived gene sets and STRING neighborhood \nclusters. \nTerm filtering \nOne key insight from our online user surveys is that the of- \nten overwhelming number of enrichment results can make it \nchallenging to discern the most relevant enriched terms. We \nhave thus enhanced the display of enrichment results, allow- \ning users to filter and sort them based on (i) FDR, (ii) strength, \n(iii) signal, (iv) term size and (v) term similarity. \nThe newest addition to our filtering options are the â€˜term \nsimilarityâ€™ and â€˜signalâ€™ filters. The â€˜term similarityâ€™ filter uses \nthe Jaccard index to measure the similarity between the gene \nsets of terms within the same category. The method proceeds \nby sorting the terms according to their P -values, prioritizing \nthose with the strongest statistical significance. The algorithm \nthen iterates through the list of terms; any term that exhibits \na similarity exceeding a predetermined, user-defined threshold \nrelative to terms already included in the results is subsequently \nexcluded. This approach requires no additional information \nbeyond what is already available in the dataset and can be \nuniformly applied across all categories of terms. The â€˜signalâ€™ \nfilter is defined as a weighted harmonic mean between the ra- \ntio of observed to expected gene occurrences in an enriched \nterm and its âˆ’log(FDR), respectively. This filter balances the \nimpact of both metrics and provides a more intuitive ordering \nof enriched terms and has been implemented as the default \nsorting criterion. \nThese filters are designed to exclude potentially less relevant \nresults, such as terms that are marginally significant, small or \nshow significant overlap in terms of gene content. This en- \nhancement streamlines the search for relevant observations, \nhelping users focus on the most meaningful results. \nGraphical visualization \nIn response to user feedback requesting more intuitive, graph- \nical visualizations of the detected enrichments, STRING now \nhas adopted an interactive dot plot as a primary display \nmethod for enrichment analysis outcomes. We chose the dot \nplot for its clarity in representing enriched functions across \nthree dimensions: (i) enrichment signal along the X -axis, (ii) \nthe FDR indicated through the color coding of the dots and \n(iii) the termâ€™s protein count in the network represented by the \nsize of each dot. The terms (listed on the Y -axis) are ranked \nby their enrichment signal or, if grouped by similarity, by the \nenrichment signal of their group first (Figure 2 ). This visual- \nization reflects all filter settings chosen by the user to manage \nthe volume of results displayed. These plots can be addition- \nally customized in the interface and the resulting graphs can be \ndownloaded as PNG for immediate use or as SVG for further \nmodifications. \nDownloaded from https://academic.oup.com/nar/article/53/D1/D730/7903368 by guest on 11 January 2026'
 'Databases and ontologies\nBioThings Explorer: a query engine for a federated\nknowledge graph of biomedical APIs\nJackson Callaghan\n1,â€ , Colleen H. Xu\n1,â€ , Jiwen Xin1,â€ , Marco Alvarado Cano1,\nAnders Riutta\n2, Eric Zhou1, Rohan Juneja1, Yao Yao1, Madhumita Narayan1,\nKristina Hanspers2, Ayushi Agrawal\n2, Alexander R. Pico2, Chunlei Wu\n1,*, Andrew I. Su\n1,*\n1Department of Integrative Structural and Computational Biology, The Scripps Research Institute, La Jolla, CA 92037, United States\n2Data Science and Biotechnology, Gladstone Institutes, University of California, San Francisco, CA 94158, United States\n*Corresponding authors. Department of Integrative Structural and Computational Biology, The Scripps Research Institute, La Jolla, CA 92037, United States.\nE-mails: cwu@scripps.edu (C.W.) and asu@scripps.edu (A.I.S.)\nâ€ These authors contributed equally to this work.\nAssociate Editor: Peter N. Robinson\nAbstract\nSummary: Knowledge graphs are an increasingly common data structure for representing biomedical information. These knowledge graphs can\neasily represent heterogeneous types of information, and many algorithms and tools exist for querying and analyzing graphs. Biomedical knowl-\nedge graphs have been used in a variety of applications, including drug repurposing, identiï¬cation of drug targets, prediction of drug side effects,\nand clinical decision support. Typically, knowledge graphs are constructed by centralization and integration of data from multiple disparate sour-\nces. Here, we describe BioThings Explorer, an application that can query a virtual, federated knowledge graph derived from the aggregated infor-\nmation in a network of biomedical web services. BioThings Explorer leverages semantically precise annotations of the inputs and outputs for\neach resource, and automates the chaining of web service calls to execute multi-step graph queries. Because there is no large, centralized\nknowledge graph to maintain, BioThings Explorer is distributed as a lightweight application that dynamically retrieves information at query time.\nAvailability and implementation: More information can be found at https://explorer.biothings.io and code is available at https://github.com/bio\nthings/biothings_explorer.\n1 Introduction\nWhile downloadable files are the most common way to share\nbiomedical data, application programming interfaces (APIs)\nare another popular and powerful mechanism for data dis-\nsemination. Accessing data through APIs has many comple-\nmentary advantages relative to downloading local copies of\ndata. APIs typically allow users to query for specific subsets\nof the data that are of interest. API queries are often highly\nindexed, leading to efficient data retrieval. Finally, API access\nallows for easy incorporation of the most up-to-date data into\nother computational applications or workflows.\nWhile APIs offer many advantages in terms of data acces-\nsibility, these advantages do not immediately translate into\nefficient data integration. APIs generally follow some com-\nmon architectures and protocols [e.g. representational state\ntransfer (REST), output in JavaScript Object Notation\n(JSON)], but alignment at this technical level does not guar-\nantee either syntactic or semantic interoperability. For ex-\nample, APIs can use different identifiers for the same gene,\ndifferent data structures to represent gene attributes, and\ndifferent terms to describe the relationships between bio-\nmedical entities.\nThere have been some efforts to define and enforce seman-\ntic and syntactic standards to achieve data interoperability.\nExamples of this approach include the Beacon API from the\nGA4GH consortium (Rambla et al. 2022) and the DAS speci-\nfication for sharing annotations of genomic features (Dowell\net al. 2001). These efforts rely on the active participation of\nAPI developers in adopting a community standard for their\nAPI endpoints.\nHere, we explore the use of semantically precise API anno-\ntations as a complementary approach to achieving API inter-\noperability. We divided this work into two components. First,\nwe created an extension of the widely used OpenAPI annota-\ntion standard (http://spec.openapis.org/oas/v3.1.0) to seman-\ntically describe the APIsâ€™ inputs and outputs, and a registry to\norganize these API annotations. Second, we created an appli-\ncation called BioThings Explorer to consume the API meta-\ndata and to execute multi-hop graph queries that span\nmultiple APIs. Together, this architecture allows users to\nquery a large, federated knowledge graph based on an inter-\nconnected network of biomedical APIs. This federated design\noffers a unique approach for creating knowledge graphs that\nis complementary to the more common strategy of centraliza-\ntion and local data integration.\nReceived: 18 April 2023; Revised: 18 August 2023; Editorial Decision: 11 September 2023; Accepted: 12 September 2023\nV\nC The Author(s) 2023. Published by Oxford University Press.\nThis is an Open Access article distributed under the terms of the Creative Commons Attribution License (https://creativecommons.org/licenses/by/4.0/), which\npermits unrestricted reuse, distribution, and reproduction in any medium, provided the original work is properly cited.\nBioinformatics, 2023, 39(9), btad570\nhttps://doi.org/10.1093/bioinformatics/btad570\nAdvance Access Publication Date: 14 September 2023\nApplications Note\nDownloaded from https://academic.oup.com/bioinformatics/article/39/9/btad570/7273783 by guest on 11 January 2026'
 'Nucleic Acids Research, 2016, Vol. 44, Database issue D381\nlows programmatic access, including the ability to disam-\nbiguate queries, modify all network parameters and gen-\nerate images. In order to enable large-scale analysis, which\nmay not be feasible through web-interface or API, the pre-\ncomputed network and the supplementary information are\nfreely available for download.\nSOURCES OF INTERACTIONS\nAlthough there is a plethora of data available from which\nproteinâ€“chemical networks could be derived, their dis-\npersed nature, different precision, name-space and focus\nmake it cumbersome to assemble a full picture of all avail-\nable knowledge. The STITCH pipeline aggregates high-\nthroughput experiments data, manually curated datasets\nand the results of several prediction methods into a single\nglobal network of proteinâ€“protein and proteinâ€“chemical in-\nteractions. This does not expose the user to the heterogene-\nity of the underlying data, yet, at the same time, keeps all\nthe primary evidence of the interaction readily accessible.\nA large part of the known interactions comes from man-\nually curated datasets such as DrugBank (19), GPCR-\nligand database (GLIDA) (20), Matador (21), the Thera-\npeutic Targets Database (TTD) (22) and the Comparative\nToxicogenomics Database (CTD) (23), and several path-\nway databases including the Kyoto Encyclopedia of Genes\nand Genomes (KEGG) (12), NCI/Nature Pathway Inter-\naction Database (24), Reactome (25) and BioCyc (26). As\nthere can be overlap between different manually curated\ndatasets, we do not consider multiple reports of identi-\ncal interactions as being independent from each other. In-\nstead, we count redundant interactions only once and do\nnot increase the confidence level. Other large sources of\nproteinâ€“chemical links are the datasets of experimentally\nvalidated interactions, which include ChEMBL (27), PDSP\nKi Database (28), Protein Data Bank (PDB) (29) and two\nhigh-throughput kinaseâ€“ligand interactions studies (30,31).\nAlso in this case, interactions may be reported in different\ndatabases and with different binding affinities. To compute\nthe final confidence score, we only take the strongest re-\nported affinity into account.\nThe sources of verified proteinâ€“chemical interactions are\ncomplemented by automated text mining and a structure-\nbased prediction method (18). The text-mining pipeline in-\nclude co-occurrence text-mining and natural language pro-\ncessing of all MEDLINE abstracts as well as available\nPubMed Central open-access full-text articles (32). The\nnewest addition to the text-mining sources are NIH Re-\nPORTER grant abstracts (https://projectreporter.nih.gov/).\nConsidering co-occurring terms, adding the RePORTER\ndata increased the number of high-confidence interactions\nbetween human proteins and chemicals from 2740 to 4740.\nExtensive benchmarking of each data source allows us to\nprovide unified confidence score for every interaction while\ntaking into account the sourcesâ€™ predicted precision.\nDISPLAY OF BINDING AFFINITIES IN THE NETWORK\nVIEW\nSmall molecules that activate or inhibit proteins such as en-\nzymes or receptors are among the most studied classes of\nexogenous small molecules. In order to assess the effect and\nconfidence of proteinâ€“ligand binding, as well as variabil-\nity in the affinity of known ligands, it is essential to know\nthe binding affinity between the compound and its target.\nUsually, this binding affinity is quantified as the inhibition\nconstant Ki. In some cases, Ki values are not available, but\nother values such as the IC50 or EC50 (half of the maxi-\nmal inhibitory concentration) can serve as an approxima-\ntion. Ki values of drugs vary greatly, from nanomolar in-\nhibition constants to relatively high values, such as 52 \x02M\nbetween aspirin and cyclooxygenase 2 (27). Therefore, for\nany given drug, it is not so much the absolute value of the\nKi, but rather the relative binding affinities that determine\nthe impact on the interaction network.\nIn previous versions of STITCH, Ki values from primary\nsources (27,28) were accessible to the user through the web-\ninterface. In the new release of STITCH, the user can now\nchoose to switch the network view to show the binding\naffinities of all proteinâ€“chemical interactions for which this\nvalue is known (Figure 1). This new network view is similar\nto the STITCHâ€™s confidence view: the thickness of the edge\nbetween nodes scales with the Ki value. If a Ki is not avail-\nable, EC50 or IC50 will be used to determine the depicted\nstrength of the interaction. If there are multiple measure-\nments available, the lowest value (i.e. highest reported affin-\nity) will be used to determine the thickness of the edge.\nDATA AND FILTERING FOR TISSUE SPECIFICITY\nThe proteinâ€“chemical network in STITCH is global and\nas such considers interactions anywhere in an organism.\nHowever, in multicellular organisms such as humans, not\nall proteins are present in every tissue. STITCH 5 addresses\nthis through a new feature that allows users to filter a\nhuman interaction network so that only the proteins be-\nlieved to be present in a specified tissue are shown (Fig-\nure 2). To provide this feature, STITCH now integrates\ntissue-specific protein expression patterns from two data\nsources. First, the TISSUES resource (33), which combines\nevidence from UniProt annotations, systematic large-scale\ntranscriptomics and proteomics studies, and co-occurrence\ntext mining. For use in STITCH, the text-mining evidence\nwas recomputed based on the same texts used elsewhere in\nSTITCH. Second, STITCH incorporates baseline expres-\nsion patterns from tissues deposited in the Expression At-\nlas (34). Before augmenting the network with tissues data,\nusers have to choose if they want to use data from TISSUES\nor Expression Atlas. The TISSUES resource contains con-\nfidence levels ranging from one (lowest confidence) to five\n(highest confidence). Accordingly, on the STITCH website\nusers can select a tissue and a minimum confidence level.\nIn contrast, datasets from the Expression Atlas are trans-\nformed into percentiles. The confidence score for a proteinâ€“\nprotein interaction in the given tissue is then multiplied with\nthe geometric mean of the two proteinsâ€™ expression per-\ncentiles. For proteinâ€“chemical interactions, the confidence\nscore is multiplied with the proteinâ€™s expression percentile.\nTo access the tissue expression patterns, users can search for\ntissues either by typing parts of the tissue names or by select-\ning a tissue from a list. Then, users can submit the changed\nsettings to STITCH. In return, an updated network will be\nDownloaded from https://academic.oup.com/nar/article/44/D1/D380/2503089 by guest on 11 January 2026']","['<1-hop>\n\nNucleic Acids Research , 2025, Vol. 53, Database issue D 737 networks in all domains of life, supporting directed links and tissue-specificity. J. Mol. Biol., 433 , 166835. 13. Kim, C.Y. , Baek, S. , Cha, J. , Yang, S. , Kim, E. , Marcotte, E.M. , Hart, T. and Lee,I. (2022) HumanNet v3: an improved database of human gene networks for disease research. Nucleic Acids Res., 50 , D632â€“D639. 14. Huang, J.K. , Carlin, D.E. , Yu, M.K. , Zhang, W. , Kreisberg, J.F. , Tamayo, P. and Ideker, T. (2018) Systematic evaluation of molecular networks for discovery of disease genes. Cell Syst. , 6 , 484â€“495.e5. 15. Wright, S.N. , Colton, S. , Schaffer, L.V. , Pillich, R.T. , Churas, C. , Pratt, D. and Ideker, T. (2024) State of the interactomes: an evaluation of molecular networks for generating biological insights. bioRxiv doi: https:// doi.org/ 10.1101/ 2024.04.26.587073 , 29 April 2024, preprint: not peer reviewed. 16. Ontology Consortium, G. , Aleksander, S.A. , Balhoff, J. , Carbon, S. , Cherry, J.M. , Drabkin, H.J. , Ebert, D. , Feuermann, M. , Gaudet, P. , Harris, N.L. , et al. (2023) The Gene Ontology knowledgebase in 2023. Genetics , 224 , iyad031. 17. Liberzon, A. , Birger, C. , ThorvaldsdÃ³ttir, H. , Ghandi, M. , Mesirov, J.P. and Tamayo, P. (2015) The Molecular Signatures Database hallmark gene set collection. Cell Syst. , 1 , 417â€“425. 18. Amberger, J.S. , Bocchini, C.A. , Schiettecatte, F. , Scott, A.F. and Hamosh,A. (2015) OMIM.org: Online Mendelian Inheritance in Man (OMIM Â®), an online catalog of human genes and genetic disorders. Nucleic Acids Res. , 43 , D789â€“D798. 19. Snel, B. , Bork, P. and Huynen, M.A. (2002) The identification of functional modules from the genomic association of genes. Proc. Natl Acad. Sci. U.S.A., 99 , 5890â€“5895. 20. Guala, D. , Ogris, C. , MÃ¼ller, N. and Sonnhammer, E.L.L. (2020) Genome-wide functional association networks: background, data & state-of-the-art resources. Brief. Bioinform., 21 , 1224. 21. Szklarczyk, D. , Gable, A.L. , Nastou, K.C. , Lyon, D. , Kirsch, R. , Pyysalo, S. , Doncheva, N.T. , Legeay, M. , Fang, T. , Bork, P. , et al. (2021) The STRING database in 2021: customizable proteinâ€“protein networks, and functional characterization of user-uploaded gene / measurement sets. Nucleic Acids Res., 49 , D605â€“D612. 22. Mering,C.V. (2003) STRING: a database of predicted functional associations between proteins. Nucleic Acids Res. , 31 , 258â€“261. 23. Enright, A.J. , Iliopoulos, I. , Kyrpides, N.C. and Ouzounis, C.A. (1999) Protein interaction maps for complete genomes based on gene fusion events. Nature , 402 , 86â€“90. 24. Franceschini, A. , Lin, J. , von Mering, C. and Jensen, L.J. (2016) SVD-phy: improved prediction of protein functional associations through singular value decomposition of phylogenetic profiles. Bioinformatics , 32 , 1085â€“1087. 25. Koutrouli, M. , Nastou, K. , Piera LÃ­ndez, P. , Bouwmeester, R. , Rasmussen, S. , Martens, L. and Jensen, L.J. (2024) FAVA: high-quality functional association networks inferred from scRNA-seq and proteomics data. Bioinformatics , 40 , btae010. 26. Porras, P. , Barrera, E. , Bridge, A. , Del-Toro, N. , Cesareni, G. , Duesbury, M. , Hermjakob, H. , Iannuccelli, M. , Jurisica, I. , Kotlyar, M. , et al. (2020) Towards a unified open access dataset of molecular interactions. Nat. Commun., 11 , 6144. 27. Franceschini, A. , Szklarczyk, D. , Frankild, S. , Kuhn, M. , Simonovic, M. , Roth, A. , Lin, J. , Minguez, P. , Bork, P. , von Mering, C. , et al. (2013) STRING v9.1: proteinâ€“protein interaction networks, with increased coverage and integration. Nucleic Acids Res., 41 , D808â€“D815. 28. Yu, H. , Luscombe, N.M. , Lu, H.X. , Zhu, X. , Xia, Y. , Han, J .-D.J ., Bertin, N. , Chung, S. , V idal, M. and Gerstein, M. (2004) Annotation transfer between genomes: proteinâ€“protein interologs and proteinâ€“DNA regulogs. Genome Res. , 14 , 1107â€“1118. 29. Szklarczyk, D. , Gable, A.L. , Lyon, D. , Junge, A. , Wyder, S. , Huerta-Cepas, J. , Simonovic, M. , Doncheva, N.T. , Morris, J.H. , Bork, P. , et al. (2019) STRING v11: proteinâ€“protein association networks with increased coverage, supporting functional discovery in genome-wide experimental datasets. Nucleic Acids Res., 47 , D607â€“D613. 30. Doncheva, N.T. , Morris, J.H. , Gorodkin, J. and Jensen, L.J. (2019) Cytoscape StringApp: network analysis and visualization of proteomics data. J. Proteome Res., 18 , 623â€“632. 31. Jensen, L.J. , Kuhn, M. , Stark, M. , Chaffron, S. , Creevey, C. , Muller, J. , Doerks, T. , Julien, P. , Roth, A. , Simonovic, M. , et al. (2009) STRING 8â€”a global view on proteins and their functional interactions in 630 organisms. Nucleic Acids Res. , 37 , D412â€“D416. 32. Nastou, K. , Mehryary, F. , Ohta, T. , Luoma, J. , Pyysalo, S. and Jensen,L.J. (2024) RegulaTome: a corpus of typed, directed, and signed relations between biomedical entities in the scientific literature. Database , 2024 , baae095. 33. Megill, C. , Martin, B. , Weaver, C. , Bell, S. , Prins, L. , Badajoz, S. , McCandless, B. , Pisco, A.O. , Kinsella, M. , Griffin, F. , et al. (2021) cellxgene: a performant, scalable exploration platform for high dimensional sparse matrices. bioRxiv doi: https:// doi.org/ 10.1101/ 2021.04.05.438318 , 6 April 2021, preprint: not peer reviewed. 34. Moreno, P. , Fexova, S. , George, N. , Manning, J.R. , Miao, Z. , Mohammed, S. , MuÃ±oz-Pomer, A. , Fullgrabe, A. , Bi, Y. , Bush, N. , et al. (2022) Expression Atlas update: gene and protein expression in multiple species. Nucleic Acids Res. , 50 , D129â€“D140. 35. T immons, J.A. , Szkop, K.J. and Gallagher, I.J. (2015) Multiple sources of bias confound functional enrichment analysis of global -omics data. Genome Biol. , 16 , 186. 36. Benjamini, Y. and Hochberg, Y. (1995) Controlling the false discovery rate: a practical and powerful approach to multiple testing. J. R. Stat. Soc. Ser. B Stat. Methodol., 57 ,'
 '<2-hop>\n\nExpanded co-expression networks The co-expression channel in STRING identifies genes with similar expression profiles across various tissues and condi- tions, revealing their involvement in shared biological pro- cesses. It compiles data from large-scale gene expression stud- ies, analyzing transcript and protein levels to detect gene pairs with coordinated expression. This method highlights poten- tial functional relationships between genes. Importantly, the analysis is free from study bias, as it directly uses raw omics data instead of relying on manually curated or experimental datasets, providing a more precise and objective view of gene interactions. For the upcoming version 12.5, we are expanding the co-expression channel by generating additional networks through the application of FAVA (Functional Association of Variational Autoencoders) ( 25 ) on a wider range of single- cell RNA sequencing data. Specifically, we are incorporating single-cell data from the cellxgene Atlas ( 33 ) to enhance the human and mouse networks and from the EBI Single Cell Ex- pression Atlas to extend coverage to more model organisms ( 34 ). This expansion will provide more detailed, organism- specific co-expression maps, enabling more precise identifi- cation of gene co-expression patterns and cross-species com- parisons, particularly for development, immune response and disease-related gene expression dynamics. Impro v ed enric hment analysis Gene set enrichment analysis is a critical component of high- throughput biological studies, providing key insights into the biological characteristics of datasets. This analysis is pivotal in identifying essential aspects such as biological processes, path- ways and phenotypes, which are crucial for understanding the biological implications of experiments. Typically, overrepre- sentation analysis is employed, wherein a subset of data ex- ceeding a specific cutoff (empirical or customary) is compared against an experimental background. By default, STRING Downloaded from https://academic.oup.com/nar/article/53/D1/D730/7903368 by guest on 11 January 2026']","The integration of single-cell RNA sequencing (scRNA-seq) data from the cellxgene Atlas into STRING version 12.5 significantly enhances the co-expression networks by providing more detailed and organism-specific gene co-expression maps. This is achieved through the application of FAVA (Functional Association of Variational Autoencoders) on a broader range of single-cell RNA sequencing datasets, including those from the cellxgene Atlas for human and mouse, and from the EBI Single Cell Expression Atlas for additional model organisms.

**Enhancements to Co-expression Networks:**

1. **Increased Data Resolution and Detail:**  
   Incorporating single-cell RNA sequencing data allows STRING to capture gene expression profiles at the resolution of individual cells rather than bulk tissue samples. This finer granularity enables the detection of more precise co-expression patterns that reflect cellular heterogeneity and specific cell states.

2. **Organism-Specific Networks:**  
   By integrating data from the cellxgene Atlas (human and mouse) and the EBI Single Cell Expression Atlas (other model organisms), STRING expands its coverage to multiple species. This results in co-expression networks that are tailored to the biology of each organism, improving the relevance and accuracy of inferred gene associations.

3. **Improved Identification of Functional Relationships:**  
   The enhanced co-expression networks better reveal genes with coordinated expression across various tissues, developmental stages, immune responses, and disease conditions. This helps in identifying genes involved in shared biological processes with higher confidence.

4. **Cross-Species Comparisons:**  
   The expansion facilitates comparative analyses of gene expression dynamics across different organisms, which is particularly valuable for studying conserved biological mechanisms and evolutionary relationships.

**Advantages for Understanding Gene Expression Dynamics:**

- **More Precise Functional Insights:**  
  The detailed co-expression maps enable researchers to pinpoint gene modules and pathways active in specific cell types or conditions, leading to improved understanding of cellular functions and regulatory mechanisms.

- **Enhanced Study of Development, Immunity, and Disease:**  
  The ability to analyze gene expression at single-cell resolution across species aids in dissecting complex biological processes such as development, immune responses, and disease progression.

- **Reduction of Study Bias:**  
  Since the co-expression analysis uses raw omics data directly rather than relying on curated or experimental datasets, it provides a more objective and unbiased view of gene interactions.

In summary, the integration of single-cell RNA sequencing data from the cellxgene Atlas into STRING 12.5 enriches the co-expression networks by adding high-resolution, organism-specific expression data. This expansion improves the precision and scope of gene co-expression analyses, enabling deeper insights into gene expression dynamics across different organisms and biological contexts.","The integration of single-cell RNA sequencing data from the cellxgene Atlas into STRING version 12.5 enhances the co-expression networks by generating additional networks through the application of FAVA (Functional Association of Variational Autoencoders) on a wider range of single-cell data. This incorporation specifically improves the human and mouse networks and, together with data from the EBI Single Cell Expression Atlas, extends coverage to more model organisms. This expansion provides more detailed, organism-specific co-expression maps, enabling more precise identification of gene co-expression patterns and facilitating cross-species comparisons. These improvements are particularly valuable for studying development, immune response, and disease-related gene expression dynamics, offering a more precise and objective view of gene interactions by directly using raw omics data free from study bias.",0.8035714285714286,0.9750522119959157,0.9999999999,1.0
"How do the improvements introduced in Vol. 53 of Nucleic Acids Research enhance gene set enrichment analysis, and how does the RAG system described in Vol. 32 of JAMIA apply retrieval strategies in clinical decision-making?","['offer global information based on user query, offering \nbetter performance than naÃ¯ve RAG on the traditional \nvector databases.50\n6) Implement few-shot learning with CoT for complex clin\xad\nical reasoning. \nFew-shot learning has been shown to enhance LLMsâ€™ \nreasoning capabilities by teaching specific reasoning that \nmay not have been included in their original training. \nSimilarly, CoT techniques can improve complex reason\xad\ning in clinical tasks.51,52 Researchers should generate \nhigh-quality examples and incorporate CoT strategies \ninto the final query to refine specialized reasoning. \n7) Evaluate all stages of the RAG system development. \nMost current studies focus only on final performance, \noverlooking the importance of evaluating each stage of \ndevelopment. It is crucial to formally assess and report \nperformance at the pre-retrieval, retrieval, and post- \nretrieval stages. Evaluating the knowledge boundaries of \nthe baseline LLM, potential conflicts between the LLM \nand external knowledge, and the accuracy and coverage \nof retrieved information helps ensure replicability and \ntransparency. This level of evaluation enables other \nresearchers to understand why a RAG system works (or \ndoes not) and facilitates reproducibility. \n8) Consider cost and time constraints. \nAdvanced retrieval strategies can improve performance \nbut often increase processing time and computational \ncosts. For example, graph-based RAG requires substan\xad\ntial resources for developing knowledge graphs, and \nresponses from global summaries may take longer than \nwith naÃ¯ve RAG methods.50 Another example is to fine- \ntune LLMs to evaluate the needs and the quality of \nretrieval.34 In terms of computational cost, this process \nis expensive, especially when scaling the method to \nlarger datasets or deploying it in a real-time system. \nAlso, a set of extra processes will make the whole speed \nslow. The long response time might have a nonignorable \nnegative impact in situations that need a quick answer, \nespecially common in clinical settings. Researchers \nshould balance performance improvements with time \nand cost considerations. Only 1 study in our review, \nwhich focused on gastrointestinal radiology diagnosis \nbased on imaging descriptions, compared the cost and \nresponse time between LLMs and LLMs with RAG.31\nThe mean response time was 29.8 s for LLM with RAG \nvs 15.7s for LLM alone, with costs of $0.15 and $0.02 \nper case, respectively. Another study used EHR to pre\xad\ndict cognitive decline only reported cost, with LLM: \n$4.49; RAG: $12.51. Another study that used EHR \ndata to predict cognitive decline reported costs of $4.49 \nfor LLM alone and $12.51 for LLM with RAG.53\n9) Establish monitoring and knowledge update strategies. \nAn important concept in AI applications in healthcare, \nalgorithmovigilance, which defined as â€œscientific meth\xad\nods and activities relating to the evaluation, monitoring, \nunderstanding, and prevention of adverse effects of \nalgorithms in health care,â€ 54 should also be considered \nin the RAG applications. Researchers need to develop \nlong-term monitoring strategies for the RAG system per\xad\nformance, especially in clinical applications. In addition, \ncurrent studies use fixed external datasets. Researchers \nshould update external knowledge sources as latest \ninformation becomes available. Clear strategies for \nupdating knowledge should be defined, specifying when \nand how updates will occur. \nFor future studies, the first direction could be the system- \nlevel enhancement, the combination of RAG and LLM- \npowered agents. LLM-powered agents are AI systems that \nuse LLMs with complex reasoning and planning capabilities, \nmemory management, interactive capabilities with the envi\xad\nronment, and actions to execute tasks.55,56 Recent research \npoints to the emerging trend of combination of RAG and \nLLM-powered agents, where agents can assist in planning \nand decision making for complex tasks, rather than simple \nretrieval.57 For example, clinicians and patients have diverse \ninformation access needs, some needing to analyze text from \na knowledge base, others needing to incorporate structured \ndata from an EHR. RAG will eventually only become one of \nthe methods for agents to access information. Moreover, \nfuture research could focus on the usage of internal and \nexternal functions and tools, long-term and short-term mem\xad\nory module, self-learning module. For example, a study \ndeveloped an agent to answer questions related to rare dis\xad\neases by expanding beyond RAG with additional tool func\xad\ntions, such as querying phenotypes and performing web \nsearches. This approach improved the overall correctness \nfrom 0.48 to 0.75 compared to the GPT-4 baseline LLM.58\nThe second future direction could focus on the knowledge- \nlevel enhancement: deep integration of external knowledge \ninto LLM. LLM exhibits the knowledge boundaries. RAG \napproaches retrieving external knowledge and then integrates \nit into LLMs in the forms of prompts for the final generation \nto enhance the capabilities of LLMs in perceiving knowledge \nboundaries.59 However, the integration of external knowl\xad\nedge into LLM reasoning is typically limited to providing the \nretrieved data as additional context for the LLMâ€™s query dur\xad\ning generation. This approach keeps retrieval and generation \nloosely connected, and the LLMâ€™s output can still be influ\xad\nenced by its inherent knowledge boundaries or by noise in the \nretrieved text, leading to incorrect answers. Additionally, \nwhen the external knowledge source is EHR data, this \nenhancement becomes even more important. Current EHR \ndata is organized in a â€œproblem-oriented medical recordâ€ \n(POMR) format, which collects and displays information in a \nstructured manner.60 LLMs excel in free-form contexts, and \ntheir ability to perform clinical tasks depends on access to \nunstructured text that provides a comprehensive view of the \npatient. Achieving this within the structured POMR format \nin modern EHR systems poses a significant challenge.61\nTherefore, investigating how to realize the deep integration \nof external knowledge with LLM reasoning is an important \ndirection for future research in clinical applications.\nThe final direction is the integration-level enhancement, \nfocusing on integrating RAG systems within EHRs. Current \nresearch has primarily focused on development and testing \noutside of EHR systems. To seamlessly provide support for \nhealthcare providers and patients, future efforts should pri\xad\noritize embedding RAG systems into EHR interfaces. This \nrequires collaboration with EHR vendors to ensure the neces\xad\nsary infrastructure is available. Researchers also can facilitate \nthis integration using data exchange frameworks, such as \nSMART on FHIR.62\n612                                                                                                      \nJournal of the American Medical Informatics Association, 2025, Vol. 32, No. 4 \nDownloaded from https://academic.oup.com/jamia/article/32/4/605/7954485 by guest on 11 January 2026'
 'Nucleic Acids Research , 2025, Vol. 53, Database issue \nD 731 \nIntroduction \nThe function of every living cell is primarily governed by a \ncomplex network of interacting proteins, with each proteinâ€™s \nrole determined not only by its molecular activities but also \nby its position within this network ( 1 ,2 ). Connected proteins \nwork together to contribute to common biological processes \nthrough various interaction types, such as physical binding, \ngenetic interactions and regulatory influences. These interac- \ntions can collectively be categorized as functional associa- \ntions , which serve as fundamental operational units within bi- \nological systems. Unraveling protein networks in their various \nmodalities remains a significant research focus. Consequently, \nnumerous databases have been developed over the years to \nmeet specific research needs. These range from carefully cu- \nrated pathway databases such as Reactome ( 3 ) and KEGG \n( 4 ), to databases of experimental interaction evidence curated \nfrom literature such as BioGRID ( 5 ), IntAct ( 6 ) and MINT \n( 7 ), and to those centered on specific modes of interactions, \nincluding Complex Portal for protein co-complexes ( 8 ) and \nSIGNOR for regulatory interactions ( 9 ). Finally, composite \ndatabases such as STRING ( 10 ), GeneMANIA ( 11 ), FunCoup \n( 12 ) and HumanNet ( 13 ) not only incorporate data from these \nsources but also employ an array of computational methods \nto predict additional associations, striving to provide the most \ncomprehensive views of the interactome. \nAmong these databases, STRING is notable for its many \nsources of evidence, its robust scoring system, user-friendly \ninterface and comprehensive suite of enrichment features. It \nis dedicated to assembling a broad set of associations among \nproteins for the complete proteomes of thousands of organ- \nisms across all domains of life. STRING charts interactions \nranging from highly confident, well-documented associations \nto more speculative ones, which are crucial for exploratory \nand computational analyses. The scoring system ensures that \ndata from diverse sourcesâ€”including automatic text mining, \nhigh- and low-throughput experimental data, and compu- \ntational predictionsâ€”are directly comparable and weighted \nconsistently, regardless of their origin. Additionally, STRING \nextends and maps networks across species by predicting in- \nterologs, thereby broadening the scope of functional associa- \ntions to encompass a diverse range of organisms. This includes \nuncultured novel bacterial species derived from metagenomic \nsamples, and even unpublished proteomes uploaded by users. \nThe development of interaction databases has so far been a \ntrade-off between detailing the exact mode of an interactionâ€”\nwhether physical or functional, signaling or structural, stable \nor transientâ€”and maintaining a comprehensive set of interac- \ntions. Given the limitations of the available data and method- \nologies, STRING has historically focused on broadly defined \nfunctional associations, which provided the most useful set \nof interactions for proteome-wide analysis. While these net- \nworks continue to perform well in downstream tasks ( 14 ,15 ), \nthey often lack fine-grained resolution, leaving more special- \nized databases better equipped to offer detailed insights into \nspecific interactions if needed. However, the growing vol- \nume of data and advancements in text-mining technologies \nhave since enabled the composite, general-purpose databases \nto catch up, offering more detailed maps of the interac- \ntion space. Recently, the STRING database introduced a co- \ncomplex (physical) interaction network mode, which details \nmulti-protein assemblies. With the latest update, the database \nnow also includes the largest set of regulatory (directed) inter- \nactions, highlighting the flow of information within cells. This \nmajor enhancement greatly expands the analytical capabilities \nof the database, enabling new types of studies that were not \npreviously possible with nondirected interaction networks. \nInteraction and pathway databases are frequently used to \ngain insights into the functional context of individual pro- \nteins or to understand the biological organization of an entire \nprotein dataset. Many of these databases enhance the inter- \npretation of protein datasets beyond simple interaction anno- \ntations by incorporating an enrichment analysis toolset. This \ntype of analysis involves comparing observed data against ex- \npected distributions, enabling researchers to identify statisti- \ncally significant features or patterns. Enrichment tools typi- \ncally utilize established ontologies and annotations, such as \nGene Ontology ( 16 ), MSigDB hallmark sets ( 17 ) or OMIM \n( 18 ). A few databases also leverage their annotated datasets \nto help understand the functional context of a userâ€™s input. \nFor example, KEGG ( 4 ) employs pathway and module ab- \nstractions to organize genes into datasets for overrepresenta- \ntion analysis, while Reactome maps interconnected proteins \ninto hierarchically nested modules. Uniquely, STRING uses \nan unsupervised procedure to hierarchically precluster its en- \ntire proteome-wide networks into functional modules. Incor- \nporating these network-derived gene sets into the enrichment \nanalysis facilitates the identification of novel modules, espe- \ncially in areas of the proteome where manual curation has \nbeen less comprehensive so far. \nDatabase content \nThe basic interaction scope in STRING is that of a â€˜functional \nassociationâ€™ between pairs of proteins. A functional associa- \ntion is defined as a contribution of two non-identical proteins \nto a common function ( 19 ,20 ). This can take many forms; \nfunctionally associated proteins can be in physical proxim- \nity to each other, regulate each other, exhibit genetic epistasis \nor even work antagonistically (as long as this occurs in the \ncontext of a common function). For the purpose of defining \nfunctional associations, the concept of a common function is \ncrucial, but difficult to defineâ€”it should broadly be thought \nof as corresponding to the notion of a â€˜pathwayâ€™ or â€˜function \nmoduleâ€™. In practice, STRING roughly follows the functional \ngranularity of the pathway maps in the KEGG database ( 4 ). \nFrom the set of all functional associations in STRING, sub- \nsets are derived that are more specifically annotated with re- \ngard to their mechanism of association. Currently, two such \nmore specific subsets are implemented: â€˜physicalâ€™ and â€˜regula- \ntoryâ€™. The physical mode refers to pairs of proteins that either \nbind directly or are at least subunits of the same complex ( 21 ). \nThe regulatory mode refers to associated protein pairs that are \nknown to regulate each otherâ€™s activity in at least one direc- \ntion. This mode is described in more detail further below; for \nthe first time in STRING, such connections are annotated not \nonly with confidence scores but also with a directionality. \nAll proteinâ€“protein associations in STRING are annotated \nwith â€˜confidence scoresâ€™. These scores are fully precomputed; \nthey scale between 0 and 1 and describe the estimated like- \nlihood of a postulated association being correct, given the \navailable evidence. Separate confidence scores are provided \nfor physical and regulatory modesâ€”these scores provide es- \ntimates of the likelihood that the proposed association is tak- \ning place and is indeed of the postulated type. The two spe- \nDownloaded from https://academic.oup.com/nar/article/53/D1/D730/7903368 by guest on 11 January 2026'
 'was asked about medications for peripheral artery disease \npatients without increased bleeding risk, it initially omitted \nlow-dose rivaroxaban. After integrating retrieved text from \nthe 2024 American College of Cardiology / American Heart \nAssociation Guideline for the Management of Lower Extrem\xad\nity Peripheral Artery Disease,10 the model correctly recom\xad\nmended rivaroxaban.\nSeveral guidelines exist for evaluating Artificial Intelligence \n(AI) applications and LLMs in healthcare, including \nDECIDE-AI (Developmental and Exploratory Clinical Inves\xad\ntigations of DEcision support systems driven by Artificial \nIntelligence),11 CLAIM (Checklist for Artificial Intelligence in \nMedical Imaging),12\nand CONSORT-AI (Consolidated \nStandards of Reporting Trials-AI).13 In addition, Tam et al. \nintroduced QUEST, a framework specifically for human eval\xad\nuation of LLMs in healthcare.14 However, these guidelines \ndo not cover RAG applications in clinical settings, emphasiz\xad\ning the need for a more specific guideline.\nDespite the promise of RAG in improving LLM performance \nin clinical settings, there is limited understanding of its overall \neffectiveness comparing with the baseline LLM, adoption in \nclinical domains, and optimal strategies for its development in \nbiomedical applications. The aim of this study is to synthesize \nfindings from recent research of RAG and LLM in biomedicine \nand provide clinical development guidelines to improve effec\xad\ntiveness as well as transparency in future research.\nMaterials and methods\nStudy design\nWe conducted a systematic literature review. The report was \ncreated in adherence to the Preferred Reporting Items for Sys\xad\ntematic Reviews and Meta-Analyses 2020 analysis checklist \n(File S1).15 We also performed a meta-analysis following the \nCochrane Handbook guidelines.16\nScreening papers\nWe searched in 3 databases (PubMed, Embase, and Psy\xad\ncINFO), using terms related to â€œretrieval augmented gener\xad\nationâ€ and â€œlarge language model.â€ PubMed and Embase \nwere selected based on recommendations from the Cochrane \nHandbook, as they are widely recognized for their extensive \ncoverage of biomedical literature.17 PsycINFO was included \nto capture articles from the psychological and behavioral sci\xad\nences. To maintain a high standard of quality and reliability, \nwe focused on peer-reviewed articles and excluded preprints. \nThe specific search terms used for each database are provided \nin File S2. Given that ChatGPT was released on November \n30, 2022, we set the publication filter to search papers pub\xad\nlished in 2023 and 2024. The search was performed on \nDecember 12, 2024. The inclusion criteria were: (1) the study \nmust compare baseline LLM performance with RAG per\xad\nformance and (2) the study must address a biomedical ques\xad\ntion. The exclusion criteria were: (1) literature reviews, \neditorial comments, or viewpoint papers, (2) studies focusing \non LLMs in languages other than English, or (3) studies cen\xad\ntered on a multi-agent system without a focus on RAG. SL \nscreened titles and abstracts, then conducted a full-text \nreview of papers meeting the criteria.\nData extraction\nFor each included study, we extracted the following informa\xad\ntion: author, title, publication year, journal, clinical task, and \nspecialty. Regarding RAG techniques, we gathered details \nabout the baseline LLM, retrieval sources, and strategies used \nin the pre-retrieval, retrieval, and post-retrieval stages. For \nevaluation, we extracted the evaluation method (human, \nautomated, or a combination of both), the number of evalua\xad\ntors, the evaluation dataset, and the evaluation metrics.\nMeta-analysis\nEffect size was defined as a metric quantifying the relation\xad\nship between variables, including both direction and magni\xad\ntude.18 For each included study, we calculated the effect size \nbetween baseline LLM performance and RAG-enhanced \nLLM performance. The outcomes focused on the perform\xad\nance of generation results, such as accuracy and usefulness. \nMetrics related to the retrieval process, cost, or speed were \nnot included as outcomes in the meta-analysis. For continu\xad\nous outcomes, we used Cohenâ€™s d, standardized between- \ngroup mean difference (SMD), calculated as the difference in \nmeans divided by the pooled standard deviation. The stand\xad\nard error (SE) of SMD was calculated using the following for\xad\nmula (1), where n1 and n2 represent the sample sizes of each \ngroup.19 For dichotomous measurements, we calculated the \nlog-odds ratio, obtained by transforming the odds ratio (OR) \nwith the natural logarithm, and the associated SE was calcu\xad\nlated using formula (2), where a, b, c, and d represent the \nnumber of successful and failed events in the baseline LLM \nand RAG-enhanced LLM approaches. For studies reporting \nmultiple outcomes, we used the overall outcome to calculate \neffect size. If no overall outcome was reported, we averaged \nthe effect sizes of all reported outcomes. We excluded out\xad\ncomes with a sample size of less than 30 to avoid small- \nsample bias. \nSESMD Â¼\nffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi\nn1 Ã¾ n2\nn1n2\nÃ¾\nSMD2\n2 n1 Ã¾ n2\nÃ°\nÃž\ns\n(1) \nSElogOR Â¼\nffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi\n1\na Ã¾ 1\nb Ã¾ 1\nc Ã¾ 1\nd\nr\n(2) \nWe developed a random-effect meta-analysis model, \nbecause of the variability in RAG architectures and evalua\xad\ntion datasets among the included studies. The random-effect \nmodel was used when individual study effects likely con\xad\ntained additional sources of variance beyond sampling error. \nBetween-study heterogeneity was assessed using Higgins & \nThompsonâ€™s I2 statistic, where 25% indicated low heteroge\xad\nneity, 50% moderate, and 75% substantial.20\nWe conducted subgroup analyses to explore performance \nvariations across different factors. First, we analyzed the \ninfluence of the baseline LLM, referring to the foundation \nmodel (eg, GPT-4 or Llama2) that provides the core architec\xad\nture for the system. Second, we examined data retrieval strat\xad\negies, categorizing them as simple or complex. Simple \nstrategies included fixed-length chunking and basic similarity \nsearch, and we performed a subgroup analysis to compare \nthese with complex retrieval strategies. Third, we analyzed \ndifferences based on evaluation methods, distinguishing \nbetween human evaluations, such as Likert scale ratings for \nhelpfulness and accuracy, and automatic evaluation metrics, \nincluding ROUGE-1 and BLEU. Finally, we conducted a sub\xad\ngroup analysis based on the type of task, classifying studies \n606                                                                                                      \nJournal of the American Medical Informatics Association, 2025, Vol. 32, No. 4 \nDownloaded from https://academic.oup.com/jamia/article/32/4/605/7954485 by guest on 11 January 2026'
 'D 732 \nNucleic Acids Research , 2025, Vol. 53, Database issue \ncific network modes are fully consistent with the functional \nnetwork, such that if an interaction is present in either the \nphysical or regulatory network, it will, by definition, also be \npresent in the full functional association network (with an \nequal or higher confidence score). To compute the various con- \nfidence scores, the available evidence supporting a given inter- \naction is first separated into â€˜evidence channelsâ€™, by the type \nof evidence. For the basic functional association confidence \nscore, seven such channels are used. These include three that \nare based on genomic context predictionsâ€”neighborhood, fu- \nsion and gene co-occurrenceâ€”as well as channels dedicated to \nco-expression, experimental data, curated databases and text \nmining. \nThe genomic context channels focus on associations in- \nferred from genome sequences alone, detecting evolution- \nary patterns such as gene proximity, fusion events and co- \noccurrence across different organisms. For instance, the neigh- \nborhood channel assigns an association score to gene pairs \nthat are located close to each other on the chromosome in \nthe same orientation (in prokaryotic genomes) ( 22 ). The fu- \nsion channel identifies open reading frames that result from \ngene fusion events ( 23 ), while the co-occurrence channel ex- \namines whether genes have a nontrivial but shared distribu- \ntion across genomes ( 24 ), implying a shared history of hori- \nzontal transfers, losses or duplication events and thus likely \nrelated functions. The co-expression channel compiles data \nfrom gene expression studies, analyzing both transcript and \nprotein abundances across various conditions. By comparing \nexpression profiles, it identifies gene pairs with similar expres- \nsion patterns, suggesting functional linkage ( 10 ,25 ). The ex- \nperiments channel aggregates interaction evidence from labo- \nratory assays, including biochemical, biophysical and genetic \nassays. Data are imported from primary repositories such as \nBioGRID ( 5 ) and the IMEx consortium ( 26 ), and the confi- \ndence scores are estimated by globally benchmarking the accu- \nracy of annotated experimental techniques, as well as within- \ndataset performance and consistency for the case of high- \nthroughput experiments ( 10 ). The database channel is based \non well-described, curated proteinâ€“protein associations from \nexpert-compiled resources, such as KEGG ( 4 ), Reactome ( 3 ) \nand Gene Ontology Complexes ( 16 ). These resources provide \nwell-established pathways, complexes and functional relation- \nships, offering a high level of reliability. Unlike other chan- \nnels, the database channel assigns a uniformly high confidence \nscore to associations, reflecting their established nature. Fi- \nnally, the text-mining channel utilizes a large corpus of sci- \nentific literature, including PubMed abstracts and full-text ar- \nticles, to identify co-mentions of protein names ( 27 ). By statis- \ntically analyzing the frequency of these co-mentions in various \ntextual contexts, this channel uncovers potential associations \nthat may not be evident from structured data alone. In addi- \ntion, for the more specific association modes, dedicated large \nlanguage models are employed to detect sentences supporting \neither physical ( 21 ) or regulatory associations (see below). \nFor each evidence channel, the available interaction evi- \ndence is translated into a confidence score by first quantifying \nthe evidence using channel-specific metrics and then convert- \ning these metrics into likelihoods using calibration curves de- \nrived from prior knowledge (from pathway-map memberships \nin KEGG). For the more specific association modes â€˜physicalâ€™ \nand â€˜regulatoryâ€™, only channels and evidence that are appli- \ncable to these modes are considered. After this, all channel \nscores that have been computed for a given protein pair in \na given organism are transferred onto related protein pairs \nin other organisms, based on the â€˜interologâ€™ concept ( 27 ,28 ). \nLastly, a final, combined confidence score is computed by inte- \ngrating the channel-specific subscores probabilistically, under \nthe assumption that evidence in different channels is largely \nindependent. Users of STRING can directly rely on this com- \nbined score for browsing and filtering networks, or they can \nalternatively customize their analyses by enabling or disabling \nspecific channels separately, after which the combined score is \nrecomputed according to their chosen settings. \nAll primary evidence underlying a given interaction can \nbe inspected interactively, in dedicated evidence viewers on- \nline. Furthermore, accessory information is available for \neach protein, such as its three-dimensional structure, do- \nmain composition, annotation and cross-references. Apart \nfrom its core protein-network functionality, STRING also \nimplements features that allow extensive analysis of up- \nloaded user data. These include functional enrichment de- \ntection ( 29 ), experimental bias detection ( 21 ), homology \nsearches, and clustering and organizing large query pro- \ntein sets. All data contained in STRING are freely avail- \nable for download, under a Creative Commons BY 4.0 li- \ncense. Apart from its website, STRING can also be accessed \nvia a dedicated Cytoscape plugin ( 30 ), through an applica- \ntion programming interface (API) ( 31 ), as well as through \nan R / Bioconductor package ( https://www.bioconductor.org/ \npackages/ release/ bioc/ html/ STRINGdb.html ). \nRegulat ory netw orks \nMolecular networks, like those available through STRING, \nhave become invaluable tools in biomedical research, offering \npowerful insights into how molecules work together within \ncells. The functional association networks, while useful for \nmany applications, do not specify the interaction type (e.g. \ncomplex formation or transcriptional regulation) nor the di- \nrection of interaction (i.e. who regulates whom). Constructing \nnetworks with more detailed interaction types and directions \ncan significantly expand their utility, particularly for building \nmathematical models of biological systems or for making log- \nical inferences. The directionality of interactions is essential \nfor interpretation of omics data, which typically reveals more \nabout downstream effects of conditions such as diseases than \nit does about the upstream events that led to it and thus could \nbe targeted therapeutically. \nTo incorporate interactions with specific directionality, sign \nand type into STRING, we leveraged the novel dataset and \ndeep learning-based relation extraction method described in \nRegulaTome ( 32 ). The RegulaTome corpus provides a rich \nand diverse dataset that includes 16 961 relations between \n54 951 entities annotated across over 2500 documents. This \ncorpus is utilized to train a deep learning-based method used \nto annotate and extract the regulatory events throughout the \nentire available literature. \nOn top of undirected physical interactionsâ€”which are al- \nready included in STRINGâ€”the following types of directed \ninteractions can be extracted using the system developed for \nRegulaTome: Regulation , with the signed subtypes of Posi- \ntive Regulation and Negative Regulation , Regulation of Gene \nExpression , Regulation of Degradation and Catalysis of Post- \nTranslational Modifications , covering six subtypes: Catalysis \nof Small Protein Conjugation , Catalysis of Small Protein Re- \nmoval , Catalysis of Phosphorylation , Catalysis of Dephospho- \nDownloaded from https://academic.oup.com/nar/article/53/D1/D730/7903368 by guest on 11 January 2026'
 ""integrates over 40 publicly available biomedical knowledge \nsources across separate domains, such as genes, proteins, \ndrugs, compounds, and diseases, along with their known \nrelationships.32 Two studies used textbooks, such as Harri\xad\nson's Principles of Internal Medicine, while 3 others utilized \nelectronic health record (EHR) data. Additionally, Zakka \net al. added over 500 markdown files from MDCalc to \nimprove clinical calculation capabilities in LLM.28 Two stud\xad\nies employed real-time online browsing to search academic \nsites, such as PubMed and UpToDate. The amount of \nretrieval resources varied across studies, ranging from a small \ndataset specific to 6 osteoarthritis guidelines to a large dataset \nof EHR data from 7 hospitals.\nRetrieval strategies\nIdentified retrieval strategies were grouped based on the RAG \nstages: pre-retrieval, retrieval, and post-retrieval. Figure 3 \npresents an example of how RAG is applied and lists identi\xad\nfied strategies within each stage.\nIn the pre-retrieval stage, 50% of studies (n Â¼ 10) reported \nstrategies, such as query rewriting, document preprocessing, \nand assessing the necessity of retrieval. Zakka et al. simplified \nqueries by rephrasing text into search terms that are better \nsuited for website browsing,28 while Wang et al. focused on \ntechniques such as correcting errors, expanding abbrevia\xad\ntions, and matching synonyms in user queries.30 Soman et al. \nextracted disease entities in queries and retrieved correspond\xad\ning nodes from a knowledge graph.33 Document preprocess\xad\ning involved removing non-textual elements from PMC \npapers (eg, figures, references, and author disclosures),30\nextracted tables from PDFs using pdfplumber, structured the \ncontent with pydantic for seamless integration.25 In addition \nto query modification and document preprocessing, Jeong \net al. fine-tuned a model to determine whether retrieval was \nnecessary for a given query.34\nDuring the data retrieval stage, 85% of studies (n Â¼ 17) \nreported strategies regarding indexing, aligning queries with \ndocuments, and ranking retrieval chunks. Chunking methods \nranged from fixed-size chunks35 to recursive splits.36 Embed\xad\nding models such as Text-embedding-ada-002,24,28â€“30,36,37\nMiniLM, and PubMedBERT33 were commonly used to con\xad\nvert sentences into vectors. Cosine similarity was the primary \nmetric for measuring query-document alignment. Two stud\xad\nies adopted Maximal Marginal Relevance for search and \nhighlighted its improved performance over similarity-based \nmethods.24,35 A domain-specific retriever, MedCPT, was \nused in one study.34 Another study used the multi-vector \nretriever that leveraged summarized document sections to \nidentify the original content for final answer generation.25\nThe retrieval cutoff parameters varied widely, with probabil\xad\nity thresholds up to 0.83 and the number of retrieved chunks \nranging from 3 to 90.28,36,38 Vector databases like FAISS and \nChroma were frequently reported, and LangChain was \nwidely used for document processing and retrieval.23,25,35,38\nIn the subgroup analysis, 12 studies used simple data retrieval \nstrategies (OR 1.30, 95% CI [1.16, 1.45]), while 5 studies \nused complex data retrieval strategies (OR 1.30, 95% CI \n[1.07, 1.24]), with no statistically significant difference \nobserved between the 2 approaches.\nIn the post-retrieval stage, 65% of studies (n Â¼ 13) imple\xad\nmented specific strategies to refine outputs. Murugan et al. \ntailored prompts by providing clear context, defining roles \n(eg, distinguishing between healthcare providers and patients \nto deliver appropriately detailed information), and incorpo\xad\nrating relevant citations from retrieval sources such as the \nClinical \nPharmacogenetics \nImplementation \nConsortium \nguidelines and Food and Drug Administration (FDA) label\xad\ning.24 Soman et al. utilized prompt engineering to integrate \naccurate knowledge sources and statistical evidence, such as \nP-values and z-scores, from the SPOKE knowledge graph \ninto their outputs.33 Wang et al. outlined a detailed process \nin the post-retrieval stage using prompt engineering, which \ninvolved decomposing retrieved text into individual claims, \nverifying each claim with external knowledge sources, \nFigure 3. An example of using RAG in clinical applications, with identified strategies in 3 stages: pre-retrieval, retrieval, and post-retrieval. The user query \nseeks recommended medications for symptomatic peripheral artery disease without increased bleeding risk. The system retrieves evidence from clinical \nguidelines, processes it through embeddings and a vector database, and outputs a response, including rivaroxaban with low-dose aspirin, as recommended \nby the retrieved guideline information. In this example, GPT-4 suggested a dose of 75 mg Aspirin, but the common low-dose Aspirin is 81 mg.\nJournal of the American Medical Informatics Association, 2025, Vol. 32, No. 4                                                                                                      \n609 \nDownloaded from https://academic.oup.com/jamia/article/32/4/605/7954485 by guest on 11 January 2026""]","['<1-hop>\n\nD 734 Nucleic Acids Research , 2025, Vol. 53, Database issue uses the whole genome / proteome background; however, it is recommended that users provide a custom background list representing a more realistic universe of genes / proteins detected by their assay ( 35 ). Alternatively, the entire sorted dataset can be analyzed using gene set enrichment analysis to determine whether the distribution of genes at the top, bottom or both ends deviates significantly from what would be ex- pected by chance. STRING supports both of these enrichment methods. The most recent update to STRING introduces vari- ous improvements, including a new false discovery rate (FDR) correction, enhanced filtering options and a new graphical vi- sualization.'
 '<2-hop>\n\nFigure 3. An example of using RAG in clinical applications, with identified strategies in 3 stages: pre-retrieval, retrieval, and post-retrieval. The user query seeks recommended medications for symptomatic peripheral artery disease without increased bleeding risk. The system retrieves evidence from clinical guidelines, processes it through embeddings and a vector database, and outputs a response, including rivaroxaban with low-dose aspirin, as recommended by the retrieved guideline information. In this example, GPT-4 suggested a dose of 75 mg Aspirin, but the common low-dose Aspirin is 81 mg. Journal of the American Medical Informatics Association, 2025, Vol. 32, No. 4 609 Downloaded from https://academic.oup.com/jamia/article/32/4/605/7954485 by guest on 11 January 2026']","The improvements introduced in Volume 53 of *Nucleic Acids Research* enhance gene set enrichment analysis primarily through the expansion and refinement of the STRING database, which is a comprehensive resource for protein-protein interactions and functional associations. Key enhancements include:

1. **Expanded Interaction Types and Directionality**: STRING now includes the largest set of regulatory (directed) interactions alongside physical (co-complex) interactions. This allows for more detailed and biologically meaningful network analyses, as regulatory interactions specify direction and type (e.g., positive or negative regulation), which is crucial for understanding cellular information flow and for building mathematical or logical models of biological systems.

2. **Integration of Multiple Evidence Channels**: The database integrates over 40 biomedical knowledge sources and uses seven evidence channels (genomic context, co-expression, experimental data, curated databases, text mining, etc.) to assign confidence scores to interactions. This comprehensive and probabilistic integration improves the reliability and coverage of functional associations.

3. **Hierarchical Preclustering into Functional Modules**: STRING employs an unsupervised procedure to hierarchically precluster the entire proteome into functional modules, which are then used as gene sets in enrichment analyses. This network-derived gene set approach facilitates the discovery of novel functional modules, especially in less well-characterized areas of the proteome, enhancing the interpretability and depth of gene set enrichment results.

4. **User-Friendly Access and Analysis Tools**: STRING provides extensive tools for enrichment detection, bias detection, clustering, and homology searches, accessible via web interface, Cytoscape plugin, API, and R/Bioconductor package, making it easier for researchers to perform comprehensive enrichment analyses.

Together, these improvements enable more nuanced, accurate, and comprehensive gene set enrichment analyses by leveraging detailed, high-confidence, and directionally annotated protein interaction networks.

---

Regarding the RAG (Retrieval-Augmented Generation) system described in Volume 32 of *JAMIA*, it applies retrieval strategies in clinical decision-making through a structured multi-stage process:

1. **Pre-Retrieval Stage**: This involves query rewriting and document preprocessing to optimize the input for retrieval. Techniques include simplifying queries, correcting errors, expanding abbreviations, matching synonyms, and determining whether retrieval is necessary for a given query. For example, disease entities may be extracted to retrieve relevant nodes from a knowledge graph.

2. **Retrieval Stage**: The system retrieves relevant information from various biomedical knowledge sources (e.g., clinical guidelines, EHR data, textbooks) using indexing, chunking (fixed-size or recursive splits), and embedding models (e.g., Text-embedding-ada-002, MiniLM, PubMedBERT) to convert text into vectors. Cosine similarity and advanced methods like Maximal Marginal Relevance are used to align queries with documents and rank retrieved chunks. Vector databases such as FAISS and Chroma facilitate efficient retrieval.

3. **Post-Retrieval Stage**: Retrieved information is refined and integrated into the final output using prompt engineering. This includes providing clear context, defining user roles (e.g., clinician vs. patient), incorporating citations from authoritative sources, decomposing retrieved text into claims, and verifying claims with external knowledge. This stage ensures that the generated clinical recommendations are accurate, contextually appropriate, and evidence-based.

The RAG system thus enhances clinical decision-making by combining large language models with targeted retrieval of up-to-date, domain-specific knowledge, improving the accuracy and relevance of generated clinical advice. Additionally, the system development emphasizes evaluation at all stages (pre-retrieval, retrieval, post-retrieval), balancing performance with cost and response time, and planning for ongoing monitoring and knowledge updates to maintain clinical reliability.

---

**In summary:**

- **Vol. 53 of Nucleic Acids Research** improves gene set enrichment analysis by providing a richer, directionally annotated, and confidence-scored protein interaction network (STRING), enabling more detailed and novel functional module detection.

- **Vol. 32 of JAMIA** describes the RAG system's application in clinical decision-making through sophisticated retrieval strategies at pre-retrieval, retrieval, and post-retrieval stages, integrating diverse biomedical knowledge sources to generate accurate, context-aware clinical recommendations.","Vol. 53 of Nucleic Acids Research introduces improvements to STRING, including a new false discovery rate (FDR) correction, enhanced filtering options, and a new graphical visualization, which collectively enhance gene set enrichment analysis by allowing users to analyze either a whole genome/proteome background or a custom background list more realistically representing detected genes or proteins. Meanwhile, Vol. 32 of the Journal of the American Medical Informatics Association describes the use of a Retrieval-Augmented Generation (RAG) system in clinical applications, employing three stagesâ€”pre-retrieval, retrieval, and post-retrievalâ€”to answer queries such as recommending medications for symptomatic peripheral artery disease without increased bleeding risk. This system retrieves evidence from clinical guidelines, processes it through embeddings and a vector database, and outputs responses like rivaroxaban with low-dose aspirin, demonstrating how retrieval strategies support accurate clinical decision-making.",1.0,0.9361200499424985,0.0,0.6666666666666666
