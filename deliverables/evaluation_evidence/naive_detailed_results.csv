user_input,retrieved_contexts,reference_contexts,response,reference,faithfulness,answer_relevancy,context_precision,context_recall
Who is Omar and what contributions are associated with him in biomedical knowledge graph research?,"['Omar & Mohammed / Journal of Applied Science and Technology Trends Vol. 06, No. 02, pp. 342 –357 (2025) \n \n345 \nrecord-high accuracy, while open-source models achieved \nimpressive gains through prompt optimization. \nFeng et al.[22] developed the Knowledge Graph-based \nThought (KGT) framework that integrated LLMs with a pan-\ncancer knowledge graph for biomedical question answering. \nKGT was designed to reason on the knowledge graph schema and \nidentify optimal subgraphs to use for directing accurate answer \ngeneration, all without fine-tuning the LLMs. The framework is \nbenchmarked against a new dataset (PcQA) designed specifically \nfor pan-cancer KGQA tasks and has outperformed all existing \nstate-of-the-art approaches by a rather large margin. KGT’s \npracticality in biomedical issues was highlighted through case \nstudies for drug repositioning, drug resistance, and biomarker \ndiscovery. Their approach exhibited robust adaptability among \nvarious LLMs. \nRezaei et al.[26] developed AMG-RAG, a dynamic \nframework that utilizes autonomous LLM agents with medical \nsearch tools in the continuous construction and real-time updating \nof Medical Knowledge Graphs (MKGs). Their system \nincorporated confidence scoring and multi-hop reasoning to \nimprove accuracy and interpretability in medical question \nanswering. AMG-RAG outperformed size models on both very \nhard MEDQA benchmarks and more accessible MedMCQA \nones, proving that it could conduct efficient reasoning based on \ncurrent structured medical knowledge. They also used Neo4j to \nmanage the knowledge graphs while adding external searches to \nensure the latest data. \nTiwari et al.[24] presented Auto-Cypher, a recent automated \npipeline for producing high-quality synthetic data for training \nLLMs by mapping natural language to Cypher queries for graph \ndatabases like Neo4j. The pipeline deployed the novelty of LLM-\nas-database-filler to synthesize Neo4j databases for the execution \nof generated queries to ensure their correctness. A sizable dataset \ncalled SynthCypher was created, spanning multiple domains and \ncomplex queries, leading to a 40% improvement in LLM \nperformance on Cypher generation. The datasets were used to \nfine-tune open-source models such as LLaMA, Mistral, and \nQwen, and the SPIDER benchmark was adapted for evaluation \npurposes. \nMohammed et al.[29] proposed a hybridized GraphRAG \nframework combining Neo4j-based UMLS knowledge graphs \nwith a vector store for medical textbooks to create an improved \nU.S.M.L.E.-style clinical question-answering approach. The \nproject integrated symbolic reasoning from knowledge graphs \nwith semantic retrieval performed on text embeddings to enhance \nrelevance and accuracy via adaptive re-ranking and query \nexpansion. The system had the answers produced by GPT-4o-\nMini, with different prompting strategies encouraging evidence-\nbased and traceable responses grounded in verified medical \nknowledge. Experiments showed that the hybrid approach \nimproved factual accuracy and citation fidelity as compared to \nthe L.L.M.-only approach, enhancing transparency and \nreliability. It is shown that binding both structured and \nunstructured medical knowledge sources could aid in \nameliorating hallucinations and hence improve clinical \ntrustworthiness in AI-driven medical QA. \nYang et al.[30] articulated sepsis knowledge graph was \ncrafted by combining multicenter clinical data from over 10,000 \npatients with the help of GPT-4 for entity recognition and \nrelationship extraction. Real-world data were collected from \nthree hospitals and integrated with clinical guidelines and \ndatabases from the public domain. The knowledge graph \ncontained 1,894 nodes and 2,021 relationships pertaining to \ndiseases, symptoms, biomarkers, treatments, and complications. \nGPT outperformed other models in every resolution on sepsis-\nspecific datasets to obtain high F1-score results. The constructed \ngraph highlighted complex interactions in sepsis for assisting \nclinical decision-making and was implemented on Neo4j. \nGuan et al.[20] proposed a novel method for constructing a \nlocal knowledge graph from retrieved biomedical documents by \nextracting propositional claims. They carried out layer wise \nsummarization on this graph to capture multi-document \nrelationships and provide comprehensive contextual information \nto a language model for question-answering purposes. The \nmethod resolved issues in multi-document biomedical QA, such \nas noise cancellation and efficient context usage. They then tested \ntheir method on several benchmarks for biomedical question \nanswering, achieving performance at least comparable to, if not \nbetter than, existing retrieval-augmented generation (RAG) \nbaselines. The study established enhanced reasoning and answer \naccuracy of the model achieved through structured graph \nsummarization.   \nPrevious studies have improved biomedical QA using KGs \nand LLMs, but important gaps remain. Most systems lack \ntransparent, graph-based justifications, rely on limited evaluation \nmethods, or depend on cloud resources that reduce privacy and \nreproducibility. Our framework addresses these gaps by \nproviding visible Cypher queries with evidence subgraphs, \napplying comprehensive performance metrics across difficulty \nlevels, and ensuring fully local, privacy-preserving deployment. \nTable I summarizes key previous studies on biomedical \nknowledge graphs and question answering, outlining their \nmethods, datasets, and main limitations. \nIII. PRELIMINARIES \nThis section outlines the fundamental concepts required to \nunderstand the proposed system. It introduces biomedical \nknowledge graphs, explains how Neo4j stores data in graph \nform, and describes the use of Cypher for querying. It also \nprovides a brief overview of large language models (LLMs) and \ntheir role in interpreting natural language. \n \n \nA. Biomedical Knowledge Graphs \nBiomedical Knowledge Graphs (BKGs) provide a structured \nrepresentation of complex biomedical information by modeling \ndiverse medical entities, such as diseases, drugs, symptoms, and \nbiological pathways, as interconnected nodes within a graph \nstructure. The edges in these graphs represent the semantic \nrelationships between these entities, including ’treats’, ’causes’, \n’interacts with’ and many others, as illustrated in Fig 1. This \nform of representation enables the integration of heterogeneous \nbiomedical data from a wide range of sources, including'
 'Nucleic Acids Research , 2025, Vol. 53, Database issue \nD 737 \nnetworks in all domains of life, supporting directed links and \ntissue-specificity. J. Mol. Biol., 433 , 166835.\n13. Kim, C.Y. , Baek, S. , Cha, J. , Yang, S. , Kim, E. , Marcotte, E.M. , Hart, T. \nand Lee,I. (2022) HumanNet v3: an improved database of human \ngene networks for disease research. Nucleic Acids Res., 50 , \nD632–D639.\n14. Huang, J.K. , Carlin, D.E. , Yu, M.K. , Zhang, W. , Kreisberg, J.F. , \nTamayo, P. and Ideker, T. (2018) Systematic evaluation of molecular \nnetworks for discovery of disease genes. Cell Syst. , 6 , 484–495.e5. \n15. Wright, S.N. , Colton, S. , Schaffer, L.V. , Pillich, R.T. , Churas, C. , \nPratt, D. and Ideker, T. (2024) State of the interactomes: an \nevaluation of molecular networks for generating biological \ninsights. bioRxiv doi: \nhttps:// doi.org/ 10.1101/ 2024.04.26.587073 , 29 April 2024, \npreprint: not peer reviewed.\n16. Ontology Consortium, G. , Aleksander, S.A. , Balhoff, J. , Carbon, S. , \nCherry, J.M. , Drabkin, H.J. , Ebert, D. , Feuermann, M. , Gaudet, P. , \nHarris, N.L. , et al. (2023) The Gene Ontology knowledgebase in \n2023. Genetics , 224 , iyad031.\n17. Liberzon, A. , Birger, C. , Thorvaldsdóttir, H. , Ghandi, M. , \nMesirov, J.P. and Tamayo, P. (2015) The Molecular Signatures \nDatabase hallmark gene set collection. Cell Syst. , 1 , 417–425. \n18. Amberger, J.S. , Bocchini, C.A. , Schiettecatte, F. , Scott, A.F. and \nHamosh,A. (2015) OMIM.org: Online Mendelian Inheritance in \nMan (OMIM ®), an online catalog of human genes and genetic \ndisorders. Nucleic Acids Res. , 43 , D789–D798. \n19. Snel, B. , Bork, P. and Huynen, M.A. (2002) The identification of \nfunctional modules from the genomic association of genes. Proc. \nNatl Acad. Sci. U.S.A., 99 , 5890–5895.\n20. Guala, D. , Ogris, C. , Müller, N. and Sonnhammer, E.L.L. (2020) \nGenome-wide functional association networks: background, data \n& state-of-the-art resources. Brief. Bioinform., 21 , 1224.\n21. Szklarczyk, D. , Gable, A.L. , Nastou, K.C. , Lyon, D. , Kirsch, R. , \nPyysalo, S. , Doncheva, N.T. , Legeay, M. , Fang, T. , Bork, P. , et al. \n(2021) The STRING database in 2021: customizable \nprotein–protein networks, and functional characterization of \nuser-uploaded gene / measurement sets. Nucleic Acids Res., 49 , \nD605–D612.\n22. Mering,C.V. (2003) STRING: a database of predicted functional \nassociations between proteins. Nucleic Acids Res. , 31 , 258–261. \n23. Enright, A.J. , Iliopoulos, I. , Kyrpides, N.C. and Ouzounis, C.A. \n(1999) Protein interaction maps for complete genomes based on \ngene fusion events. Nature , 402 , 86–90.\n24. Franceschini, A. , Lin, J. , von Mering, C. and Jensen, L.J. (2016) \nSVD-phy: improved prediction of protein functional associations \nthrough singular value decomposition of phylogenetic profiles. \nBioinformatics , 32 , 1085–1087.\n25. Koutrouli, M. , Nastou, K. , Piera Líndez, P. , Bouwmeester, R. , \nRasmussen, S. , Martens, L. and Jensen, L.J. (2024) FAVA: \nhigh-quality functional association networks inferred from \nscRNA-seq and proteomics data. Bioinformatics , 40 , btae010.\n26. Porras, P. , Barrera, E. , Bridge, A. , Del-Toro, N. , Cesareni, G. , \nDuesbury, M. , Hermjakob, H. , Iannuccelli, M. , Jurisica, I. , \nKotlyar, M. , et al. (2020) Towards a unified open access dataset of \nmolecular interactions. Nat. Commun., 11 , 6144.\n27. Franceschini, A. , Szklarczyk, D. , Frankild, S. , Kuhn, M. , \nSimonovic, M. , Roth, A. , Lin, J. , Minguez, P. , Bork, P. , von Mering, C. , \net al. (2013) STRING v9.1: protein–protein interaction networks, \nwith increased coverage and integration. Nucleic Acids Res., 41 , \nD808–D815.\n28. Yu, H. , Luscombe, N.M. , Lu, H.X. , Zhu, X. , Xia, Y. , Han, J .-D.J ., \nBertin, N. , Chung, S. , V idal, M. and Gerstein, M. (2004) Annotation \ntransfer between genomes: protein–protein interologs and \nprotein–DNA regulogs. Genome Res. , 14 , 1107–1118. \n29. Szklarczyk, D. , Gable, A.L. , Lyon, D. , Junge, A. , Wyder, S. , \nHuerta-Cepas, J. , Simonovic, M. , Doncheva, N.T. , Morris, J.H. , \nBork, P. , et al. (2019) STRING v11: protein–protein association \nnetworks with increased coverage, supporting functional discovery \nin genome-wide experimental datasets. Nucleic Acids Res., 47 , \nD607–D613.\n30. Doncheva, N.T. , Morris, J.H. , Gorodkin, J. and Jensen, L.J. (2019) \nCytoscape StringApp: network analysis and visualization of \nproteomics data. J. Proteome Res., 18 , 623–632.\n31. Jensen, L.J. , Kuhn, M. , Stark, M. , Chaffron, S. , Creevey, C. , Muller, J. , \nDoerks, T. , Julien, P. , Roth, A. , Simonovic, M. , et al. (2009) STRING \n8—a global view on proteins and their functional interactions in \n630 organisms. Nucleic Acids Res. , 37 , D412–D416. \n32. Nastou, K. , Mehryary, F. , Ohta, T. , Luoma, J. , Pyysalo, S. and \nJensen,L.J. (2024) RegulaTome: a corpus of typed, directed, and \nsigned relations between biomedical entities in the scientific \nliterature. Database , 2024 , baae095.\n33. Megill, C. , Martin, B. , Weaver, C. , Bell, S. , Prins, L. , Badajoz, S. , \nMcCandless, B. , Pisco, A.O. , Kinsella, M. , Griffin, F. , et al. (2021) \ncellxgene: a performant, scalable exploration platform for high \ndimensional sparse matrices. bioRxiv doi: \nhttps:// doi.org/ 10.1101/ 2021.04.05.438318 , 6 April 2021, \npreprint: not peer reviewed.\n34. Moreno, P. , Fexova, S. , George, N. , Manning, J.R. , Miao, Z. , \nMohammed, S. , Muñoz-Pomer, A. , Fullgrabe, A. , Bi, Y. , Bush, N. , \net al. (2022) Expression Atlas update: gene and protein expression \nin multiple species. Nucleic Acids Res. , 50 , D129–D140. \n35. T immons, J.A. , Szkop, K.J. and Gallagher, I.J. (2015) Multiple \nsources of bias confound functional enrichment analysis of global \n-omics data. Genome Biol. , 16 , 186. \n36. Benjamini, Y. and Hochberg, Y. (1995) Controlling the false \ndiscovery rate: a practical and powerful approach to multiple \ntesting. J. R. Stat. Soc. Ser. B Stat. Methodol., 57 , 289–300.\n37. Novikov,A. (2019) PyClustering: data mining library. J. Open \nSource Softw., 4 , 1230.\n38. Van Dongen,S. (2008) Graph clustering via a discrete uncoupling \nprocess. SIAM J. Matrix Anal. Appl., 30 , 121–141.\n39. Elnaggar, A. , Heinzinger, M. , Dallago, C. , Rehawi, G. , Wang, Y. , \nJones, L. , Gibbs, T. , Feher, T. , Angerer, C. , Steinegger, M. , et al. \n(2022) ProtTrans: toward understanding the language of life \nthrough self-supervised learning. IEEE Trans. Pattern Anal. Mach. \nIntell., 44 , 7112–7127.\n40. Baumgartner, M. , Dell’Aglio, D. , Paulheim, H. and Bernstein, A. \n(2023) Towards the Web of Embeddings: integrating multiple \nknowledge graph embedding spaces with FedCoder. J. Web \nSemant., 75 , 100741.\nReceived: September 15, 2024. Revised: October 18, 2024. Editorial Decision: October 21, 2024. Accepted: October 29, 2024 \n© The Author(s) 2024. Published by Oxford University Press on behalf of Nucleic Acids Research. \nThis is an Open Access article distributed under the terms of the Creative Commons Attribution License (https: // creativecommons.org / licenses / by / 4.0 / ), which permits unrestricted reuse, \ndistribution, and reproduction in any medium, provided the original work is properly cited. \nDownloaded from https://academic.oup.com/nar/article/53/D1/D730/7903368 by guest on 11 January 2026'
 'format. Second, because the entire federated KG is never in-\nstantiated in a single place, reasoning and scoring methods\nthat rely on having the entire knowledge graph in memory\ncannot be used with BioThings Explorer.\nIn sum, we believe that knowledge graphs enable many ex-\nciting use cases in biomedical research (Nicholson and Greene\n2020), and that BioThings Explorer is complementary to\nexisting approaches for assembling knowledge graphs, offer-\ning powerful and unique capabilities for both scientific data\nanalysts and tool developers.\nSupplementary data\nSupplementary data are available at Bioinformatics online.\nConflict of interest\nNone declared.\nFunding\nSupport for this work was provided by the National Center\nfor Advancing Translational Sciences, National Institutes of\nHealth, through the Biomedical Data Translator program,\nawards OT2TR003427 and OT2TR003445.\nData availability\nBioThings Explorer is implemented as a NodeJS application.\nThe primary repository for the BioThings Explorer project is\nat https://github.com/biothings/biothings_explorer, which in\nturn links to and incorporates other repositories as sub-\nmodules. All code is released under the Apache 2.0 open-\nsource software license.\nReferences\nCilibrasi RL, Vitanyi PMB. The google similarity distance. IEEE Trans\nKnowl Data Eng 2007;19:370–83.\nDavis AP, Wiegers TC, Johnson RJ et al. Comparative toxicogenomics\ndatabase\n(CTD):\nupdate\n2023.\nNucleic\nAcids\nRes\n2023;51:\nD1257–62.\nDowell RD, Jokerst RM, Day A et al. The distributed annotation system.\nBMC Bioinformatics 2001;2:7.\nFecho K, Bizon C, Miller F et al. A biomedical knowledge graph system\nto propose mechanistic hypotheses for real-world environmental\nhealth observations: cohort study and informatics application. JMIR\nMed Inform 2021;9:e26714.\nFecho K, Thessen AE, Baranzini SE et al. Progress toward a universal\nbiomedical data translator. Clin Transl Sci 2022;15:1838–47.\nHimmelstein DS, Lizee A, Hessler C et al. Systematic integration\nof biomedical knowledge prioritizes drugs for repurposing. Elife\n2017;6:e26726.\nLelong S, Zhou X, Afrasiabi C et al. BioThings SDK: a toolkit for build-\ning\nhigh-performance\ndata\nAPIs\nin\nbiomedical\nresearch.\nBioinformatics 2022;38:2077–9.\nMayers M, Tu R, Steinecke D et al. Design and application of a knowl-\nedge network for automatic prioritization of drug mechanisms.\nBioinformatics 2022;38:2880–91.\nMorris JH, Soman K, Akbas RE et al. The scalable precision medicine\nopen knowledge engine (SPOKE): a massive knowledge graph of bio-\nmedical information. Bioinformatics 2023;39(2):btad080.\nMungall CJ, McMurry JA, Ko¨hler S et al. The monarch initiative: an in-\ntegrative data and analytic platform connecting phenotypes to geno-\ntypes across species. Nucleic Acids Res 2017;45:D712–22.\nNicholson DN, Greene CS. Constructing knowledge graphs and their\nbiomedical applications. Comput Struct Biotechnol J 2020;18:\n1414–28.\nRambla J, Baudis M, Ariosa R et al. Beacon v2 and Beacon networks: a\n‘lingua franca’ for federated data discovery in biomedical genomics,\nand beyond. Hum Mutat 2022;43:791–9.\nUnni DR, Moxon SAT, Bada M et al. Biolink Model: a universal schema\nfor knowledge graphs in clinical, biomedical, and translational sci-\nence. Clin Transl Sci 2022;15:1848–55.\nWood EC, Glen AK, Kvarfordt LG et al. RTX-KG2: a system for build-\ning a semantically standardized knowledge graph for translational\nbiomedicine. BMC Bioinformatics 2022;23:400.\nZaveri A, Dastgheib S, Wu C et al. smartAPI: towards a more intelligent\nnetwork of web APIs. In: Blomqvist, E., Maynard, D., Gangemi, A.,\net al. (eds) The Semantic Web. ESWC 2017. Lecture Notes in\nComputer Science, 2017, 154–69. https://link.springer.com/chapter/\n10.1007/978-3-319-58451-5_11#chapter-info.\n4\nCallaghan et al.\nDownloaded from https://academic.oup.com/bioinformatics/article/39/9/btad570/7273783 by guest on 11 January 2026'
 'Omar & Mohammed / Journal of Applied Science and Technology Trends Vol. 06, No. 02, pp. 342 –357 (2025) \n \n343 \nfor visualizing and validating LLM outputs [16], and MedKA for \nKG-enhanced question answering [17]. \nTo address these challenges, several recent works have \nexplored the integration of large language models with \nbiomedical knowledge graphs (KGs). A biomedical KG is a \nstructured network that connects entities such as diseases, drugs, \nand symptoms using defined relationships [18, 19]. These graphs \nstore verified medical knowledge from trusted databases, \nallowing for more accurate and explainable responses [12]. KGs \nare especially useful in multi-step reasoning tasks, where finding \nan answer requires connecting different pieces of information \n[20]. These entities and relationships can be visually represented \nin a biomedical knowledge graph, as shown in Fig. 1, where \nnodes represent medical concepts such as drugs, diseases, \nsymptoms, and pathways, and edges denote their semantic \nrelationships. \n \nFig. 1. Overview of Biomedical Knowledge Graph Entities and Relationships \n \nOne example of a widely used biomedical KG is SPOKE, \nwhich includes millions of nodes and relationships from over 40 \nbiomedical databases [12]. Integrating an LLM with a KG allows \nthe strengths of both technologies to work together: the LLM \nprovides language understanding, and the KG provides \nstructured, factual knowledge [21, 22]. A common method is \nretrieval-augmented generation (RAG), where the LLM retrieves \ninformation from the KG and uses it to generate more accurate \nresponses [13, 23]. In more advanced setups, the LLM can even \ngenerate queries like Cypher to fetch specific data from the graph \n[24, 25]. Neo4j is a popular graph database that supports fast and \nflexible storage and querying of knowledge graphs using Cypher \n[21]. It is well-suited for biomedical applications because it \nallows easy exploration of complex medical relationships. Recent \nwork has shown that combining Neo4j with LLMs can lead to \nbetter accuracy, fewer hallucinations, and more explainable \nresults [24, 26]. \nDespite improvements, building a reliable hybrid system that \ncombines an LLM with a biomedical KG remains a technical \nchallenge. Some approaches require complex pipelines or large \ntraining datasets, while others rely on fine-tuning specific to a \nnarrow set of questions [27, 28]. There is still a need for systems \nthat are both accurate and easy to scale, especially in domains like \nmedicine where the cost of errors is high [22]. \nRecent advances in KG-augmented LLMs have improved \nperformance, yet biomedical QA continues to face three practical \ngaps: \n1. Traceability: LLM-only or text-retrieval-only pipelines \nrarely provide graph-grounded justifications; users lack the \nability to inspect the exact nodes and edges that support an \nanswer. \n2. Evaluation: Prior work often judges quality via surface-\nform checks (e.g., matching a Cypher template), which fails to \ncapture end-to-end answer correctness or ranking quality \nacross different difficulty levels. \n3. Deployment: Many solutions assume cloud resources or \ndomain-specific fine-tuning, yet biomedical contexts typically \ndemand a local, privacy-preserving system with low latency \nand reproducible behavior. \nTimestamp-aware execution and periodic KG refresh help \navoid deprecated or contraindicated links, making the tool better \nsuited for safety-critical clinical contexts (e.g., drug–drug \ninteractions). \nTo address these limitations, Our work introduces a locally \ndeployable pipeline that translates biomedical questions into \nexecutable Cypher queries over a Neo4j knowledge graph. The \nsystem returns answers with supporting nodes and edges, and is'
 'Omar & Mohammed / Journal of Applied Science and Technology Trends Vol. 06, No. 02, pp. 342 –357 (2025) \n \n357 \n[4] Z. Ji, N. Lee, R. Frieske, T. Yu, D. Su, Y. Xu, E. Ishii, Y. J. Bang, A. Madotto, \nand P. Fung, “Survey of hallucination in natural language generation,” \nACM Computing Surveys, vol. 55, no. 12, pp. 1–38, 2023. \n[5] C. Malaviya, S. Lee, S. Chen, E. Sieber, M. Yatskar, and D. Roth, “Expertqa: \nexpert-curated questions and attributed answers,” arXiv preprint \narXiv:2309.07852, 2023. \n[6] L. Pusch and T. O. Conrad, “Combining llms and knowledge graphs to reduce \nhallucinations \nin \nquestion \nanswering,” \narXiv \npreprint \narXiv:2409.04181, 2024. \n[7] H. Nori, N. King, S. M. McKinney, D. Carignan, and E. Horvitz, \n“Capabilities of gpt-4 on medical challenge problems,” arXiv preprint \narXiv:2303.13375, 2023. \n[8] T. Sekar, Kushal, S. Shankar, S. Mohammed, and J. Fiaidhi, “Investigations \non using evidence-based graphrag pipeline using llm tailored for usmle \nstyle questions,” medRxiv, pp. 2025–05, 2025. \n[9] S. Farquhar, J. Kossen, L. Kuhn, and Y. Gal, “Detecting hallucinations in \nlarge language models using semantic entropy,” Nature, vol. 630, no. \n8017, pp. 625–630, 2024. \n[10] E. Asgari, N. Montaña-Brown, M. Dubois, S. Khalil, J. Balloch, J. A. \nYeung, and D. Pimenta, “A framework to assess clinical safety and \nhallucination rates of llms for medical text summarisation,” npj Digital \nMedicine, vol. 8, no. 1, p. 274, 2025. \n[11] C. Su, Y. Hou, M. Zhou, S. Rajendran, J. R. Maasch, Z. Abedi, H. Zhang, \nZ. Bai, A. Cuturrufo, W. Guo, et al., “Biomedical discovery through the \nintegrative biomedical knowledge hub (ibkh),” Iscience, vol. 26, no. 4, \n2023. \n[12] J. H. Morris, K. Soman, R. E. Akbas, X. Zhou, B. Smith, E. C. Meng, C. C. \nHuang, G. Cerono, G. Schenk, A. Rizk-Jackson, et al., “The scalable \nprecision medicine open knowledge engine (spoke): a massive \nknowledge graph of biomedical information,” Bioinformatics, vol. 39, \nno. 2, p. btad080, 2023. \n[13] K. Soman, P. W. Rose, J. H. Morris, R. E. Akbas, B. Smith, B. Peetoom, C. \nVillouta-Reyes, G. Cerono, Y. Shi, A. Rizk-Jackson, et al., “Biomedical \nknowledge graph-optimized prompt generation for large language \nmodels,” Bioinformatics, vol. 40, no. 9, p. btae560, 2024. \n[14] F. Frau, P. Loustalot, M. Törnqvist, N. Temam, J. Cupe, M. Montmerle, and \nF. Augé, “Connecting electronic health records to a biomedical \nknowledge graph to link clinical phenotypes and molecular endotypes \nin atopic dermatitis,” Scientific Reports, vol. 15, no. 1, p. 3082, 2025. \n[15] Y. Gao, R. Li, E. Croxford, J. Caskey, B. W. Patterson, M. Churpek, T. \nMiller, D. Dligach, and M. Afshar, “Leveraging medical knowledge \ngraphs into large language models for diagnosis prediction: design and \napplication study,” Jmir AI, vol. 4, p. e58670, 2025. \n[16] Y. Yan, Y. Hou, Y. Xiao, R. Zhang, and Q. Wang, “Knownet: guided health \ninformation seeking from llms via knowledge graph integration,” IEEE \nTransactions on Visualization and Computer Graphics, 2024. \n[17] Y. Deng, S. Zhao, Y. Miao, J. Zhu, and J. Li, “Medka: a knowledge graph-\naugmented approach to improve factuality in medical large language \nmodels,” Journal of Biomedical Informatics, p. 104871, 2025. \n[18] L. Ehrlinger and W. Wöß, “Towards a definition of knowledge graphs,” \nSEMANTiCS (Posters, Demos, SuCCESS), vol. 48, no. 1–4, p. 2, 2016. \n[19] E. Rajabi and S. Kafaie, “Building a disease knowledge graph,” in Caring \nis Sharing – Exploiting the Value in Data for Health and Innovation, \npp. 701–705, IOS Press, 2023. \n[20] L. Guan, Y. Huang, and J. Liu, “Biomedical question answering via multi-\nlevel summarization on a local knowledge graph,” arXiv preprint \narXiv:2504.01309, 2025. \n[21] D. Steinigen, R. Teucher, T. H. Ruland, M. Rudat, N. Flores-Herr, P. \nFischer, N. Milosevic, C. Schymura, and A. Ziletti, “Fact finder – \nenhancing domain expertise of large language models by incorporating \nknowledge graphs,” arXiv preprint arXiv:2408.03010, 2024. \n[22] Y. Feng, L. Zhou, C. Ma, Y. Zheng, R. He, and Y. Li, “Knowledge graph–\nbased thought: a knowledge graph–enhanced llm framework for pan-\ncancer question answering,” GigaScience, vol. 14, p. giae082, 2025. \n[23] H. Luo, Z. Tang, S. Peng, Y. Guo, W. Zhang, C. Ma, G. Dong, M. Song, \nW. Lin, Y. Zhu, et al., “Chatkbqa: a generate-then-retrieve framework \nfor knowledge base question answering with fine-tuned large language \nmodels,” arXiv preprint arXiv:2310.08975, 2023. \n[24] A. Tiwari, S. K. R. Malay, V. Yadav, M. Hashemi, and S. T. Madhusudhan, \n“Auto-cypher: improving llms on cypher generation via llm-supervised \ngeneration-verification framework,” in Proceedings of the 2025 \nConference of the Nations of the Americas Chapter of the Association \nfor Computational Linguistics: Human Language Technologies \n(Volume 2: Short Papers), pp. 623–640, 2025. \n[25] R. Wang, Z. Zhang, L. Rossetto, F. Ruosch, and A. Bernstein, “Nlqxform: \na language model-based question to sparql transformer,” arXiv preprint \narXiv:2311.07588, 2023. \n[26] M. R. Rezaei, R. S. Fard, J. L. Parker, R. G. Krishnan, and M. Lankarany, \n“Agentic medical knowledge graphs enhance medical question \nanswering: bridging the gap between llms and evolving medical \nknowledge,” arXiv preprint arXiv:2502.13010, 2025. \n[27] Z. Dong, B. Peng, Y. Wang, J. Fu, X. Wang, Y. Shan, and X. Zhou, “Effiqa: \nefficient question-answering with strategic multi-model collaboration \non knowledge graphs,” arXiv preprint arXiv:2406.01238, 2024. \n[28] Y. Duan, Q. Zhou, Y. Li, C. Qin, Z. Wang, H. Kan, and J. Hu, “Research \non a traditional chinese medicine case-based question-answering \nsystem integrating large language models and knowledge graphs,” \nFrontiers in Medicine, vol. 11, p. 1512329, 2025. \n[29] S. Mohammed, J. Fiaidhi, T. Sekar, K. Kushal, and S. Shankar, \n“Investigations on using evidence-based graphrag pipeline using llm \ntailored for answering usmle medical exam questions,” medRxiv, pp. \n2025–05, 2025. \n[30] H. Yang, J. Li, C. Zhang, A. P. Sierra, and B. Shen, “Large language model–\ndriven knowledge graph construction in sepsis care using multicenter \nclinical databases: development and usability study,” Journal of \nMedical Internet Research, vol. 27, p. e65537, 2025. \n[31] K.-L. Hsieh, G. Plascencia-Villa, K.-H. Lin, G. Perry, X. Jiang, and Y. Kim, \n“Synthesize heterogeneous biological knowledge via representation \nlearning for alzheimer’s disease drug repurposing,” Iscience, vol. 26, \nno. 1, 2023. \n[32] R. Angles and C. Gutierrez, “Survey of graph database models,” ACM \nComputing Surveys (CSUR), vol. 40, no. 1, pp. 1–39, 2008. \n[33] B. Chicho and A. O. Mohammed, “An empirical comparison of neo4j and \ntigergraph databases for network centrality,” Science Journal of \nUniversity of Zakho, vol. 11, no. 2, pp. 190–201, 2023. \n[34] I. Robinson, J. Webber, and E. Eifrem, Graph Databases: New \nOpportunities for Connected Data, O’Reilly Media, 2015. \n[35] A. Lysenko, I. A. Roznovăţ, M. Saqi, A. Mazein, C. J. Rawlings, and C. \nAuffray, “Representing and querying disease networks using graph \ndatabases,” BioData Mining, vol. 9, no. 1, p. 23, 2016. \n[36] M. Šestak, M. Heričko, T. W. Družovec, and M. Turkanović, “Applying k-\nvertex cardinality constraints on a neo4j graph database,” Future \nGeneration Computer Systems, vol. 115, pp. 459–474, 2021. \n[37] M. Desai, R. G. Mehta, and D. P. Rana, “An empirical analysis to identify \nthe effect of indexing on influence detection using graph databases,” \nInternational Journal of Innovative Technology and Exploring \nEngineering, vol. 8, no. 9S, pp. 414–421, 2019. \n[38] S. Beis, S. Papadopoulos, and Y. Kompatsiaris, “Benchmarking graph \ndatabases on the problem of community detection,” in New Trends in \nDatabase and Information Systems II, pp. 3–14, Springer, 2015. \n[39] R. Wang, Z. Yang, W. Zhang, and X. Lin, “An empirical study on recent \ngraph database systems,” in International Conference on Knowledge \nScience, Engineering and Management, pp. 328–340, Springer, 2020. \n[40] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “Bert: pre-training of \ndeep bidirectional transformers for language understanding,” in \nProceedings of the 2019 Conference of the North American Chapter of \nthe Association for Computational Linguistics: Human Language \nTechnologies, vol. 1, pp. 4171–4186, 2019. \n[41] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. \nNeelakantan, P. Shyam, G. Sastry, A. Askell, et al., “Language models \nare few-shot learners,” Advances in Neural Information Processing \nSystems, vol. 33, pp. 1877–1901, 2020. \n[42] J. Lee, W. Yoon, S. Kim, D. Kim, S. Kim, C. H. So, and J. Kang, “Biobert: \na pre-trained biomedical language representation model for biomedical \ntext mining,” Bioinformatics, vol. 36, no. 4, pp. 1234–1240, 2020. \n[43] E. Alsentzer, J. R. Murphy, W. Boag, W.-H. Weng, D. Jin, T. Naumann, \nand M. McDermott, “Publicly available clinical bert embeddings,” \narXiv preprint arXiv:1904.03323, 2019. \n[44] Z. He, S. Sunkara, X. Zang, Y. Xu, L. Liu, N. Wichers, G. Schubiner, R. \nLee, and J. Chen, “Actionbert: leveraging user actions for semantic \nunderstanding of user interfaces,” in Proceedings of the AAAI \nConference on Artificial Intelligence, vol. 35, pp. 5931–5938, 2021.']","['Omar & Mohammed / Journal of Applied Science and Technology Trends Vol. 06, No. 02, pp. 342 –357 (2025) 345 record-high accuracy, while open-source models achieved impressive gains through prompt optimization. Feng et al.[22] developed the Knowledge Graph-based Thought (KGT) framework that integrated LLMs with a pan- cancer knowledge graph for biomedical question answering. KGT was designed to reason on the knowledge graph schema and identify optimal subgraphs to use for directing accurate answer generation, all without fine-tuning the LLMs. The framework is benchmarked against a new dataset (PcQA) designed specifically for pan-cancer KGQA tasks and has outperformed all existing state-of-the-art approaches by a rather large margin. KGT’s practicality in biomedical issues was highlighted through case studies for drug repositioning, drug resistance, and biomarker discovery. Their approach exhibited robust adaptability among various LLMs. Rezaei et al.[26] developed AMG-RAG, a dynamic framework that utilizes autonomous LLM agents with medical search tools in the continuous construction and real-time updating of Medical Knowledge Graphs (MKGs). Their system incorporated confidence scoring and multi-hop reasoning to improve accuracy and interpretability in medical question answering. AMG-RAG outperformed size models on both very hard MEDQA benchmarks and more accessible MedMCQA ones, proving that it could conduct efficient reasoning based on current structured medical knowledge. They also used Neo4j to manage the knowledge graphs while adding external searches to ensure the latest data. Tiwari et al.[24] presented Auto-Cypher, a recent automated pipeline for producing high-quality synthetic data for training LLMs by mapping natural language to Cypher queries for graph databases like Neo4j. The pipeline deployed the novelty of LLM- as-database-filler to synthesize Neo4j databases for the execution of generated queries to ensure their correctness. A sizable dataset called SynthCypher was created, spanning multiple domains and complex queries, leading to a 40% improvement in LLM performance on Cypher generation. The datasets were used to fine-tune open-source models such as LLaMA, Mistral, and Qwen, and the SPIDER benchmark was adapted for evaluation purposes. Mohammed et al.[29] proposed a hybridized GraphRAG framework combining Neo4j-based UMLS knowledge graphs with a vector store for medical textbooks to create an improved U.S.M.L.E.-style clinical question-answering approach. The project integrated symbolic reasoning from knowledge graphs with semantic retrieval performed on text embeddings to enhance relevance and accuracy via adaptive re-ranking and query expansion. The system had the answers produced by GPT-4o- Mini, with different prompting strategies encouraging evidence- based and traceable responses grounded in verified medical knowledge. Experiments showed that the hybrid approach improved factual accuracy and citation fidelity as compared to the L.L.M.-only approach, enhancing transparency and reliability. It is shown that binding both structured and unstructured medical knowledge sources could aid in ameliorating hallucinations and hence improve clinical trustworthiness in AI-driven medical QA. Yang et al.[30] articulated sepsis knowledge graph was crafted by combining multicenter clinical data from over 10,000 patients with the help of GPT-4 for entity recognition and relationship extraction. Real-world data were collected from three hospitals and integrated with clinical guidelines and databases from the public domain. The knowledge graph contained 1,894 nodes and 2,021 relationships pertaining to diseases, symptoms, biomarkers, treatments, and complications. GPT outperformed other models in every resolution on sepsis- specific datasets to obtain high F1-score results. The constructed graph highlighted complex interactions in sepsis for assisting clinical decision-making and was implemented on Neo4j. Guan et al.[20] proposed a novel method for constructing a local knowledge graph from retrieved biomedical documents by extracting propositional claims. They carried out layer wise summarization on this graph to capture multi-document relationships and provide comprehensive contextual information to a language model for question-answering purposes. The method resolved issues in multi-document biomedical QA, such as noise cancellation and efficient context usage. They then tested their method on several benchmarks for biomedical question answering, achieving performance at least comparable to, if not better than, existing retrieval-augmented generation (RAG) baselines. The study established enhanced reasoning and answer accuracy of the model achieved through structured graph summarization. Previous studies have improved biomedical QA using KGs and LLMs, but important gaps remain. Most systems lack transparent, graph-based justifications, rely on limited evaluation methods, or depend on cloud resources that reduce privacy and reproducibility. Our framework addresses these gaps by providing visible Cypher queries with evidence subgraphs, applying comprehensive performance metrics across difficulty levels, and ensuring fully local, privacy-preserving deployment. Table I summarizes key previous studies on biomedical knowledge graphs and question answering, outlining their methods, datasets, and main limitations.']","Omar is a researcher who, along with Mohammed, has contributed to the field of biomedical knowledge graph (KG) research, particularly in integrating large language models (LLMs) with biomedical knowledge graphs for improved question answering (QA) in medical contexts.

**Contributions associated with Omar in biomedical knowledge graph research include:**

1. **Development of a Hybridized GraphRAG Framework:**
   - Omar and Mohammed proposed a hybridized GraphRAG framework that combines Neo4j-based UMLS (Unified Medical Language System) knowledge graphs with a vector store containing medical textbooks.
   - This approach integrates symbolic reasoning from knowledge graphs with semantic retrieval from text embeddings.
   - The system enhances clinical question answering, specifically targeting U.S.M.L.E.-style medical exam questions.
   - It uses GPT-4o-Mini for answer generation with prompting strategies that encourage evidence-based, traceable, and medically verified responses.
   - The hybrid method improves factual accuracy, citation fidelity, transparency, and reliability compared to LLM-only approaches.
   - This integration helps reduce hallucinations in AI-generated medical answers, thereby improving clinical trustworthiness.

2. **Focus on Evidence-Based, Explainable Biomedical QA:**
   - Their work emphasizes providing answers grounded in verified medical knowledge with visible Cypher queries and supporting evidence subgraphs.
   - The system is designed for local deployment, ensuring privacy, low latency, and reproducibility, which are critical in biomedical applications.

3. **Addressing Practical Gaps in Biomedical QA:**
   - Omar’s research addresses key challenges such as traceability (providing graph-grounded justifications), comprehensive evaluation across difficulty levels, and deployment constraints (local, privacy-preserving systems).

4. **Publication and Recognition:**
   - Their work is documented in the Journal of Applied Science and Technology Trends (Vol. 06, No. 02, 2025), highlighting their contributions to biomedical KG and LLM integration for medical question answering.

In summary, Omar’s contributions lie in advancing hybrid biomedical QA systems that effectively combine structured knowledge graphs with large language models to produce accurate, explainable, and clinically trustworthy answers, particularly for medical exam-style questions.","Omar is a co-author mentioned in the context of biomedical knowledge graph research, specifically in the publication by Omar & Mohammed in the Journal of Applied Science and Technology Trends. Their work is referenced alongside other significant studies on integrating large language models with biomedical knowledge graphs for question answering, highlighting advances in frameworks that improve accuracy, reasoning, and clinical trustworthiness in medical QA systems.",1.0,0.9497673118408749,0.6999999999766667,1.0
what Cypher do for biomedical data?,"[""Omar & Mohammed / Journal of Applied Science and Technology Trends Vol. 06, No. 02, pp. 342 –357 (2025) \n \n352 \nThe dataset contains 60 questions divided into three difficulty \nlevels based on how complex the language is and how deep the \nbiomedical reasoning needs to be: \n• Level 1: 25 simple questions focusing mostly on easy-to-\nunderstand facts, such as symptoms of certain diseases or \ndrugs used to treat specific conditions. \n• Level 2: 20 medium-level questions that involve more \ndetailed relationships, such as drug interactions and \nSideEffect. \n• Level 3: 15 hard questions requiring multi-step reasoning \nacross multiple biomedical concepts or biological \npathways, similar to the complexity found in real clinical \ncases. \nThe set of evaluation questions was designed to span a wide \nrange of common biomedical topics and to reflect clinically \nrelevant query types reported in prior literature. Each question is \npaired with its corresponding gold standard cypher query and \ncategorized by difficulty level, as summarized in Table V, where \nthree illustrative examples are shown. The questions were derived \nfrom publicly available biomedical QA benchmarks and adapted \nfrom established knowledge bases (e.g., iBKH schema relations), \nensuring both coverage and diversity across diseases, drugs, \nsymptoms, and pathways.\nTABLE V.  QUESTION DIFFICULTY LEVELS AND SAMPLE CYPHER QUERIES \nLevel \nDefinition \nExample \nCypher Query \n1 \nSingle-hop question using \none relationship \nWhat are the symptoms of \nAlzheimer? \nMATCH (d:Disease)-[:HAS_SYMPTOM]->(s:Symptom) \nWHERE toLower(d.name) CONTAINS 'alzheimer' \nRETURN s.name AS symptom \n \n2 \nQuestions requiring one \nrelationship \nWhat are the side effects of \ndrugs used to treat asthma? \nWITH toLower('asthma') AS disease_name \nMATCH (d:Disease)<-[:TREATS]-(dr:Drug) \nWHERE toLower(d.name) CONTAINS disease_name \nMATCH (dr:Drug)-[:CAUSES]->(se:SideEffect) \nRETURN DISTINCT se.name AS side_effect, dr.name AS drug \n \n3 \nQuestions requiring two or \nmore relationships \nWhat cholesterol medications \ncause side effects, and what are \nsome of those effects? \nWITH toLower('cholesterol') AS disease_name \nMATCH (d:Disease)<-[:TREATS]-(dr:Drug) \nWHERE toLower(d.name) CONTAINS disease_name \nMATCH (dr:Drug)-[:CAUSES]->(se:SideEffect) \nRETURN DISTINCT dr.name AS drug, se.name AS side_effect \n \n \n \nFig. 4. Graphical User Interface of the Biomedical Knowledge Explorer System \nB. Quantitative Evaluation \nThe performance of the proposed system is evaluated on the \niBKH knowledge graph using Exact Match, Precision@k, \nRecall@k, F1@k, Hits@k, MRR, and total response time across \nsimple, medium, and complex biomedical queries. These metrics \nwere measured for each difficulty level (simple, medium, and \ncomplex) to see how performance changes as questions become \nmore challenging.\n \nTABLE VI.  QUERY GENERATION EXACT MATCH BY DIFFICULTY LEVEL""
 'Omar & Mohammed / Journal of Applied Science and Technology Trends Vol. 06, No. 02, pp. 342 –357 (2025) \n \n355 \nFor the question ’What are the symptoms of brain cancer?’ \nThe system generated a Cypher query that correctly followed the \nHAS_SYMPTOM relationship from disease nodes to symptoms \nnodes, filtering by the specified disease name. The results \nretrieved included terms such as aphasia (Broca, Wernicke), \nanomia, agraphia, agnosia, amnesia (including retrograde \namnesia), olfaction disorders, and apnea symptoms consistent \nwith established neurological manifestations of brain tumors. In \nNeo4j, the data formed a clear center-and-spoke pattern, with \nbrain cancer at the center and its associated nodes of symptoms \nradiating outward, as shown in Fig. 5. \nCypher: \nWITH toLower(""brain cancer"") AS disease_name \nMATCH (d:Disease)-[:HAS_SYMPTOM]->(s:Symptom) \nWHERE toLower(d.name) CONTAINS disease_name \nRETURN DISTINCT s,d \nLIMIT 10; \n \n \nFig. 5. Graphical user interface displaying Cypher query and results for breast \ncancer treatment \n \n \nA second query, What are the side effects of insulin?, \nproduced a Cypher statement starting from the drug node for \ninsulin and traversing the CAUSES relationship to the connected \nSideEffect nodes. The resulting list included dizziness, diarrhea, \ncough, back pain, weakness, rash/dermatitis, and hypersensitivity \nside effects well documented in insulin safety profiles. In the \nvisual representation in Fig. 6, insulin appeared centrally \nconnected to these side effect nodes, further strengthening the \ncorrectness of the relationship mapping of the system. \n \nCypher: \nWITH toLower(""insulin"") AS drug_name  \nMATCH (d:Drug)-[:CAUSES]->(se:SideEffect)  \nWHERE toLower(d.name) CONTAINS drug_name  \nRETURN se,d \nLIMIT 20; \n \nFig. 6. Knowledge Graph Visualization of Insulin and Its Related Side Effects \n \n \nThese examples demonstrate the system’s ability to interpret \nplain English biomedical questions, generate correct Cypher \nqueries, and return results that are clinically plausible, easy to \ninterpret, and directly traceable to specific graph relationships. \nThis supports both the accuracy and the transparency of the \nproposed approach. \nTo illustrate how biomedical questions can be translated into \ngraph queries, consider the natural language question: ’What are \nthe side effects of drugs that treat epilepsy?’. The following \nCypher query demonstrates how this question is mapped in the \nknowledge graph, where diseases are connected to drugs through \nthe TREATS relation and drugs are connected to side effects \nthrough the CAUSES relation. \n \nCypher: \nMATCH (d:Disease)<-[:TREATS]-(dr:Drug)  \nWHERE toLower(d.name) CONTAINS ""epilepsy"" \nMATCH (dr:Drug)-[:CAUSES]->(se:SideEffect) \nRETURN DISTINCT se,d,dr \nlimit 10'
 'Omar & Mohammed / Journal of Applied Science and Technology Trends Vol. 06, No. 02, pp. 342 –357 (2025) \n \n350 \ndocumentation, indexes “enable quicker and more efficient \npattern matching” by allowing the query planner to rapidly locate \nnodes by label and property. \nWith the schema in place, data was imported using Cypher’s \nLOAD CSV commands. For each vocabulary file, rows with \nnonnull primary IDs were processed: the code used MERGE to \ncreate (or match existing) nodes with the given label and primary \nproperty, and then SET the remaining attributes from the CSV \ncolumns. For example, in importing disease_vocab.csv, each row \nproduced a node (:Disease primary: <id>) with properties name, \ndo_id, kegg_id, and umls_cui set from the row (empty strings \nwere turned into null). Similarly, drug_vocab.csv rows produced \ndrug nodes with properties for DrugBank, KEGG, PharmGKB \nIDs, etc. This approach follows best practice: using MERGE on \nthe unique ID ensures that nodes are not duplicated during \nmultiple passes. \nAfter all nodes were created, the relationship CSVs were \nloaded. Each row in those files was matched to the corresponding \nsource and target nodes by their primary IDs, and conditional \nlogic was used to create edges. For example, the disease-\nsymptom file (Di_Sy_res.csv) was processed by matching a \ndisease node and a symptom node for each row, then executing \nMERGE (d)-[r:HAS_SYMPTOM]-(s) if the present column is \nnonzero; the edge was given a present property and a source \nproperty from the CSV. The disease-disease file (di_di_res.csv) \nwas processed by matching disease nodes d1 and d2:If is_a = 1, \na (:Disease)-[:IS_A]->(:Disease) edge was merged; if Resemble \n= 1, a (:Disease)-[:RESEMBLES]->(:Disease) edge was merged. \nDrug-disease relationships were handled similarly: the script \nmatched a Drug node and a Disease node for each row of \nD_Di_res.csv, then for each flag that was 1, it merged the \ncorresponding edge label (such as TREATS, PALLIATES, \nASSOCIATED_WITH, etc.) from the drug to the disease. Drug–\nside-effect rows produced (:Drug)-[:CAUSES]->(:SideEffect) \nedges with the source noted, and drug–drug rows produced either \nINTERACTS_WITH or RESEMBLES edges between matched \ndrug pairs. Finally, the drug-pathway and disease-pathway files \neach produced:ASSOCIATED_WITH edges linking drugs or \ndiseases to pathway nodes. \nIn this graph model, most relationships are directional (for \nexample, a drug TREATS a disease; a disease does not TREAT \na drug). This follows common practice in biomedical KGs. The \nsame relations (such as RESEMBLES or INTERACTS_WITH) \nare inherently symmetric, but were stored as two directed edges \nor one undirected edge depending on implementation. All \nrelationship creation steps used Cypher’s MERGE so that \nrepeated loads or out-of-order imports did not create duplicate \nedges. \nThis study used a static KG snapshot for reproducibility, but \nthe system also supports incremental updates through the Neo4j \nMERGE and batch import functions. A summary of the different \ntypes of nodes and relationships is provided, together with their \ncounts, descriptions, and examples in Table III.\n \nTABLE III.  DETAILED SUMMARY OF KNOWLEDGE GRAPH COMPONENTS IN IBKH SUBSET \nComponent Type \nEntity/Relationship \nCount \nDescription \nExamples \n \nDisease \n19,236 \nMedical conditions and disorders \nDiabetes, Hypertension, Cancer \n \nDrug \n37,997 \nPharmaceutical compounds \nAspirin, Insulin, Amoxicillin \nNodes \nSymptom \n1,361 \nClinical signs and symptoms \nHeadache, Fever, Nausea \n \nSideEffect \n4,251 \nNegative outcomes of drugs \nNausea, Drowsiness \n \nPathway \n2,983 \nBiological processes and pathways \nApoptosis, Glycolysis \n \nASSOCIATED_WITH \n101,534 \nGeneral associations (disease–\npathway, drug–pathway, etc.) \nDiabetes ASSOCIATED_WITH Pathway \n \nCAUSES \n145,321 \nDrug-SideEffect relationships \nAspirin CAUSES Gastric_Bleeding \n \nHAS_SYMPTOM \n3,357 \nDisease-symptom links \nCOVID-19 HAS_SYMPTOM Fever \n \nHAS_BIOMARKER \n412 \nBiomarkers linked to disease \nPSA HAS_BIOMARKER Prostate_Cancer \nRelationships \nINHIBITS_CELL_GROWTH \n1,508 \nDrugs inhibiting cell growth \nChemo INHIBITS_CELL_GROWTH Tumor \n \nINTERACTS_WITH \n2,682,142 \nDrug-drug interactions \nAspirin INTERACTS_WITH Warfarin \n \nIS_A \n10,529 \nSubtype hierarchy \nFlu IS_A Viral_Infection \n \nPALLIATES \n388 \nDrug palliates disease \nMorphine PALLIATES Cancer \n \nPREVENTS_SUPPRESSES \n859 \nPreventive links \nVaccine PREVENTS_SUPPRESSES Measles \n \nRESEMBLES \n7,000 \nSimilarity relationships \nDrugA RESEMBLES DrugB \n \nTREATMENT_THERAPY \n44,852 \nTherapy relationships \nRadiotherapy TREATMENT_THERAPY Tumor \n \nTREATS \n5,491 \nDrug-disease links \nInsulin TREATS Diabetes \n \nALLEVIATES_REDUCES \n∼180,000 \nSymptom relief \nParacetamol ALLEVIATES_REDUCES Fever \nTotal \nNodes \n65,828 \nTotal biomedical entities \n— \n \nRelationships \n3,004,166 \nTotal knowledge graph links \n— \nC. Natural Language to Cypher Query Translation \nA key feature of the system is its ability to accept questions \nwritten in plain English and automatically generate the \ncorresponding Cypher queries. This is accomplished using \nMeta’s LLaMA 3 large language model, which runs entirely on \na local machine through the open-source llama.cpp framework. \nRunning the model locally ensures low-latency execution and \nkeeps sensitive queries within the user’s environment. \nTo generate a Cypher query, LLaMA 3 is prompted with \nexamples of natural language questions along with their correct \nCypher translations. The prompt also includes instructions on \nhow to navigate the structure of the graph schema. When a user \nenters a question (e.g., ’What are the symptoms of Alzheimer’s \ndisease?’), the system inserts it into the prompt and asks LLaMA \n3 to produce a corresponding query. For example, the model may \nreturn: \nMATCH (d:Disease)-[:HAS_SYMPTOM]->(s:Symptom) \nWHERE toLower(d.name) CONTAINS ""alzheimer"" \nRETURN s.name'
 'Omar & Mohammed / Journal of Applied Science and Technology Trends Vol. 06, No. 02, pp. 342 –357 (2025) \n \n351 \nThis query searches for a disease node whose name contains \n’alzheimer’ and follows HAS_SYMPTOM edges to list related \nsymptom names. The system then executes this cypher to retrieve \nanswers. The prompts (such as few-shot examples and schema \nhints) were carefully designed to help LLaMA 3 generate correct \nCypher queries. The model learns how to use the graph’s labels \nand relationships properly. For example, if a user asks, ’Which \ndrugs treat diabetes?’, LLaMA might respond with a suitable \nCypher query: \n \nMATCH (d:Drug)-[:TREATS]->(di:Disease) \nWHERE toLower(di.name) CONTAINS ""diabetes"" \nRETURN d.name  \n \nThis queries for drug nodes that have a TREATS edge to a \ndiabetes disease node. By leveraging LLaMA 3 in this way, our \nsystem can flexibly handle many phrasing variations without \nmanual mapping rules. \nD. Model Configuration & Decoding \nWe run a local LLaMA 3.2-3B model in GGUF format (llama-\n3.2-3b-instruct-q4_k_m.gguf) via llama.cpp, as shown in Table \nIV. \nTABLE IV.  MODEL RUNTIME AND DECODING SETTINGS \nRuntime settings \nDecoding settings \nn_ctx = 1024 \ntemperature = 0.2 \nn_threads = 12 \ntop_p = 0.95 \nn_gpu_layers = 33 \ntop_k = 40 \nn_batch = 512 \nrepeat_penalty = 1.1 \n \nmax_tokens = 80 \n \nseed = 42 \n \nE. Graph Subset and Versioning \nWe use an iBKH derived subgraph (≈65.8k nodes; ≈3.0M \nedges) spanning DRUG, DISEASE, SYMPTOM, PATHWAY. IDs are \nnormalized to CURIEs and duplicates collapsed across \nUMLS/DrugBank/DisGeNET/SIDER/KEGG. Each edge stores \nprovenance/licensing metadata (source, source_version, license, \nretrieved_at, \nevidence_pmids/urls) \nand, \nwhen \navailable, \ntemporal fields (valid_from, valid_to). We report coverage as \nthe \npercentage \nof \nevaluated \nquestions \nwhose \ngold \nentities/relations are present. \nF. Query Execution and Reliability \nAfter a Cypher query is generated, it is executed on the Neo4j \ndatabase through the official Neo4j Python driver, which \nmanages the secure connection and returns the results. Instead of \nrestricting the output with a fixed LIMIT (e.g., LIMIT 5), the \nsystem retrieves candidate results and evaluates them using \nstandardized retrieval metrics such as Hits@1, Hits@5, and \nHits@10. This approach ensures that the system remains \nresponsive while providing a fair assessment of ranking quality \nacross different cutoff levels, rather than depending solely on a \nfixed number of returned items. Neo4j’s indexing on key node \nproperties, such as name and primary identifiers, also helps speed \nup lookups as the knowledge graph grows. In cases where the \nlanguage model generates an incomplete or incorrect query, such \nas referencing nodes or relationships that do not exist, the system \ncatches the error and either retries with a simpler prompt or \ninforms the user. Together, these steps make sure that queries run \nquickly, return valid results, and keep the overall experience \nsmooth and reliable for biomedical question-answering. \nG. User Interface for Query Transparency \nThe system includes a lightweight, cross-platform graphical \nuser interface (GUI) implemented as a web application using the \nFlask framework in Python, with HTML and Bootstrap for \ninteractive visualization. The interface is designed to make the \nquestion-answering process transparent and accessible to users \nwithout technical expertise. It consists of three main panels: \n1. Input Panel: Where the user can enter a biomedical \nquestion in natural language. \n2. Query Panel: Which displays the Cypher query generated \nby the language model, allowing users to verify how their \nquestion was interpreted. \n3. Results Panel: Which presents the retrieved answers in a \nclear, readable format, accompanied by a brief natural \nlanguage explanation generated by the system. \nBy showing both the query and the answer, the GUI promotes \nuser trust and enables validation of the system’s reasoning \nprocess. The interface is lightweight enough to run smoothly on \nstandard desktop machines without additional dependencies, \nmaking it practical for local deployments in clinical or research \nsettings. Fig. 4 illustrates the overall layout. \nV. EXPERIMENTAL RESULTS \nIn this section, the proposed system is evaluated based on its \nability to translate natural-language biomedical questions into \nexecutable Cypher queries over the iBKH knowledge graph. The \nassessment is conducted end-to-end and focuses on three main \naspects: (i) the accuracy of query generation, measured by the \nproportion of correctly produced Cypher queries; (ii) system \nefficiency, quantified through total response time from question \nsubmission to final answer delivery, covering both query \ngeneration and execution; and (iii) the quality of retrieved \nanswers, evaluated using standardized information retrieval \nmetrics including Precision@k, Recall@k, F1@k, Hits@k, and \nMean Reciprocal Rank (MRR). For clarity, all these metrics are \nformally defined in the following subsections, with their \ncorresponding equations, and will be used consistently \nthroughout the results section. Together, these dimensions \nprovide a comprehensive view of both correctness and efficiency \nacross simple, medium, and complex biomedical queries. \nA. Description of the Experimental Data \nTo evaluate the proposed system, this work used a carefully \nselected set of biomedical questions designed to test how well the \nsystem understands natural language and converts it into Cypher \nqueries for a biomedical knowledge graph. \nTo evaluate the system, a benchmark of 60 biomedical \nquestions was constructed by the authors, guided by the schema \nand relations in iBKH. The questions were grouped into simple, \nmedium, and complex levels to assess performance across \ndifferent reasoning requirements. Gold-standard answers were \nmanually prepared for each question to enable quantitative \nevaluation using Exact Match, Precision@k, Recall@k, F1@k, \nHits@k, and MRR. The complete set of 60 questions is available \nat this link https://drive.google.com/drive/my-drive.'
 'Omar & Mohammed / Journal of Applied Science and Technology Trends Vol. 06, No. 02, pp. 342 –357 (2025) \n \n346 \nscientific literature, clinical records, genomic databases, and \nexperimental findings [19, 31]. \nSuch integration creates a comprehensive biomedical \nknowledge base that supports advanced analytics and discovery. \nFor example, biomedical knowledge graphs can reveal previously \nunknown relationships (e.g., between drugs and diseases) and \nhelp prioritize potential biomarkers for complex conditions. The \nIntegrative Biomedical Knowledge Hub (iBKH) is one such \nlarge-scale graph that consolidates diverse biomedical resources \ninto a unified hub, enabling discovery at scale [11]. Beyond \niBKH, large biomedical knowledge graphs such as SPOKE \nfurther illustrate how graph integration accelerates research and \nsupports precision-medicine use cases [12]. Overall, these graphs \nserve as foundational resources for data-driven and personalized \nmedicine. These knowledge graphs serve as foundational \nresources for precision medicine, where treatment can be tailored \nto the individual’s biological profile, improving outcomes and \nminimizing side effects [19, 31]. \n \nTABLE I.  SUMMARY OF RELATED RESEARCH ON BIOMEDICAL KGS AND QUESTION ANSWERING \nRef. \nYear \nData/Graph \nMethod \nBaselines \nKey Metric \nLimitation \n[11] \n2023 \niBKH (18 biomedical \nsources, Neo4j) \nIntegrative KG + Graph \nlearning; drug repurposing \ncase \nKnown drugs, \nAlzheimer’s \nstudy \nPredictive performance \n(drug repurposing) \nLimited to Alzheimer’s case \nstudy; scalability and updates not \ndetailed \n[19] \n2023 \nDisease KG (9,400 \nnodes, 45,000 relations \nin Neo4j) \nCypher queries for disease–\ndrug–symptom reasoning \nCross-referential \ndisease DB \nNew relation inference; \ncomplex query \nanswering \nLimited to single domain; lacks \nlarge-scale evaluation \n[3] \n2023 \nBKGs vs. GPT-3.5/4 \nComparative QA study: \nLLMs vs. KGs \nGPT-3.5, GPT-4, \nKG reasoning \nAccuracy, Novelty, \nReasoning \nGPT-4 inconsistent in \nreasoning/citations; KG less fluent \nbut more reliable \n[13] \n2024 \nSPOKE KG + \nLLaMA2, GPT-3.5, \nGPT-4 \nKG-optimized retrieval for \nLLMs (RAG) \nOther retrieval \nmethods \nAccuracy, token \nreduction >50% \nFocus on retrieval optimization, \nnot KG construction \n[23] \n2024 \nBenchmark KB datasets \nGenerate-then-retrieve \n(LLM → logical form → \nKB retrieval) \nPrior KBQA \nmethods \nAccuracy (highest to \ndate) \nRisk of error in logical form \ngeneration \n[6] \n2024 \nPrimeKG \nLLM + KG hybrid, Cypher \nquery validation, RAG \nGPT-4 Turbo, \nLLaMA 3 \nAccuracy, \nExplainability \nDependent on KG coverage; \ncomputationally intensive \n[22] \n2025 \nPan-cancer KG (PcQA \ndataset) \nKG-enhanced reasoning \n(subgraph selection) \nSOTA KGQA \nmethods \nOutperformed SOTA on \nPcQA \nLimited to pan-cancer focus; no \nfine-tuning explored \n[26] \n2025 \nDynamic Medical KG + \nNeo4j \nLLM agents + multi-hop \nreasoning \nMEDQA, \nMedMCQA \nbaselines \nAccuracy, \nInterpretability \nHigh system complexity; requires \ncontinuous updating \n[24] \n2025 \nSynthCypher dataset \n(Neo4j + synthetic \nqueries) \nLLM-supervised Cypher \ngeneration and verification \nSPIDER \nbenchmark \nCypher accuracy 40% \nSynthetic dataset may not capture \nall real-world cases \n[29] \n2025 \nUMLS KG + Neo4j \nHybrid GraphRAG \nLLM-only QA \nAccuracy, Citation \nfidelity \nMore complex pipeline; relies on \nexternal vector store \n[30] \n2025 \nClinical data (10k \npatients, 1,894 nodes, \nNeo4j) \nKG construction using \nGPT-4 for entity/relation \nextraction \nOther KG \nconstruction \nmethods \nHigh F1-scores \nFocus only on sepsis; limited \ngeneralization \n[20] \n2025 \nLocal KG from \nbiomedical documents \nMulti-level summarization \nover KG for QA \nRAG baselines \nQA accuracy, reasoning \nTested mainly on document QA; \nscalability not proven \n \nB. Neo4j Graph Database \nTo manage the complexity and large size of biomedical \nknowledge graphs, specialized graph databases are needed. \nNeo4j is one of the most popular graph databases designed to \nstore and query data structured as nodes (entities) and \nrelationships (edges), both of which can have descriptive \nproperties [32, 33]. It uses the property graph model, which \nmakes it easy to represent complex, connected biomedical data \nsuch as drug-gene interactions or disease pathways. Neo4j’s \nCypher query language is especially advantageous because it \nallows users to write expressive and efficient queries to explore \nmulti-step connections in the data [34]. \nNeo4j works well for biomedical data because it can quickly \nrun complicated queries over highly interconnected datasets. This \nis important in biology and medicine, where relationships \nbetween entities like proteins, diseases, and drugs are often \ncomplex and layered. Studies have shown that Neo4j handles \nlarge biomedical graphs efficiently, making it a favorite among \nresearchers and industry users alike [33, 35, 36]. Its indexing and \ncaching mechanisms also help speed up query processing and \ndata retrieval [37]. \nMoreover, \nNeo4j \nintegrates \nsmoothly \nwith \nmany \nprogramming languages and analytics tools, which makes it \neasier to build interactive biomedical applications and clinical \ndecision support systems that can turn complex graph data into \nuseful insights [38, 39]. \nC. Large Language Models (LLMs) in Biomedical Question \nAnswering \nLarge Language Models (LLMs) are powerful AI systems \ntrained on vast amounts of text data. They learn the structure and \npatterns of language, enabling them to understand questions, \ngenerate responses, summarize information, and perform other \ncomplex language tasks. Well-known models such as LLaMA \nand GPT-3 have greatly advanced the field of natural language \nprocessing by showing strong performance across many tasks \n[40, 41].']","['III. PRELIMINARIES This section outlines the fundamental concepts required to understand the proposed system. It introduces biomedical knowledge graphs, explains how Neo4j stores data in graph form, and describes the use of Cypher for querying. It also provides a brief overview of large language models (LLMs) and their role in interpreting natural language. A. Biomedical Knowledge Graphs Biomedical Knowledge Graphs (BKGs) provide a structured representation of complex biomedical information by modeling diverse medical entities, such as diseases, drugs, symptoms, and biological pathways, as interconnected nodes within a graph structure. The edges in these graphs represent the semantic relationships between these entities, including ’treats’, ’causes’, ’interacts with’ and many others, as illustrated in Fig 1. This form of representation enables the integration of heterogeneous biomedical data from a wide range of sources, including']","Cypher is used in biomedical data to query and explore complex biomedical knowledge graphs stored in the Neo4j graph database. Specifically, Cypher enables the translation of natural language biomedical questions into executable graph queries that retrieve clinically relevant information from a large-scale biomedical knowledge graph (such as the Integrative Biomedical Knowledge Hub, iBKH). 

Key roles of Cypher for biomedical data include:

1. **Querying Biomedical Knowledge Graphs:**  
   Cypher is used to write expressive and efficient queries that navigate nodes (entities like diseases, drugs, symptoms, pathways) and relationships (such as TREATS, CAUSES, HAS_SYMPTOM) in the biomedical knowledge graph. This allows retrieval of detailed biomedical facts, relationships, and multi-hop connections.

2. **Supporting Multi-level Question Answering:**  
   The system translates natural language questions of varying complexity (simple fact retrieval, medium-level relationship queries, and complex multi-step reasoning) into Cypher queries. For example:  
   - Simple: ""What are the symptoms of Alzheimer’s?""  
   - Medium: ""What are the side effects of drugs used to treat asthma?""  
   - Complex: ""What cholesterol medications cause side effects, and what are some of those effects?""

3. **Data Import and Graph Construction:**  
   Cypher commands (e.g., LOAD CSV, MERGE) are used to import biomedical data from vocabularies and relationship files into the Neo4j graph, creating nodes and edges with properties while avoiding duplication.

4. **Enabling Efficient and Transparent Query Execution:**  
   Cypher queries are executed on Neo4j to retrieve answers quickly and reliably. The system includes a GUI that displays both the generated Cypher query and the results, promoting transparency and user trust.

5. **Integration with AI Models for Natural Language Understanding:**  
   Large Language Models (e.g., LLaMA 3) generate Cypher queries from natural language questions, allowing flexible and accurate querying without manual rule-based mapping.

In summary, Cypher acts as the core query language that bridges natural language biomedical questions and the underlying graph-structured biomedical data, enabling accurate, efficient, and interpretable biomedical question answering and knowledge discovery.","Cypher is used for querying biomedical knowledge graphs stored in Neo4j, enabling retrieval and manipulation of complex biomedical information represented as interconnected nodes and relationships.",1.0,0.9302549181788341,0.99999999998,1.0
Wht is the role of the Journal of Applied Science and Technology Trends in biomedical data analysis?,"['Databases and ontologies\nBioThings Explorer: a query engine for a federated\nknowledge graph of biomedical APIs\nJackson Callaghan\n1,†, Colleen H. Xu\n1,†, Jiwen Xin1,†, Marco Alvarado Cano1,\nAnders Riutta\n2, Eric Zhou1, Rohan Juneja1, Yao Yao1, Madhumita Narayan1,\nKristina Hanspers2, Ayushi Agrawal\n2, Alexander R. Pico2, Chunlei Wu\n1,*, Andrew I. Su\n1,*\n1Department of Integrative Structural and Computational Biology, The Scripps Research Institute, La Jolla, CA 92037, United States\n2Data Science and Biotechnology, Gladstone Institutes, University of California, San Francisco, CA 94158, United States\n*Corresponding authors. Department of Integrative Structural and Computational Biology, The Scripps Research Institute, La Jolla, CA 92037, United States.\nE-mails: cwu@scripps.edu (C.W.) and asu@scripps.edu (A.I.S.)\n†These authors contributed equally to this work.\nAssociate Editor: Peter N. Robinson\nAbstract\nSummary: Knowledge graphs are an increasingly common data structure for representing biomedical information. These knowledge graphs can\neasily represent heterogeneous types of information, and many algorithms and tools exist for querying and analyzing graphs. Biomedical knowl-\nedge graphs have been used in a variety of applications, including drug repurposing, identiﬁcation of drug targets, prediction of drug side effects,\nand clinical decision support. Typically, knowledge graphs are constructed by centralization and integration of data from multiple disparate sour-\nces. Here, we describe BioThings Explorer, an application that can query a virtual, federated knowledge graph derived from the aggregated infor-\nmation in a network of biomedical web services. BioThings Explorer leverages semantically precise annotations of the inputs and outputs for\neach resource, and automates the chaining of web service calls to execute multi-step graph queries. Because there is no large, centralized\nknowledge graph to maintain, BioThings Explorer is distributed as a lightweight application that dynamically retrieves information at query time.\nAvailability and implementation: More information can be found at https://explorer.biothings.io and code is available at https://github.com/bio\nthings/biothings_explorer.\n1 Introduction\nWhile downloadable files are the most common way to share\nbiomedical data, application programming interfaces (APIs)\nare another popular and powerful mechanism for data dis-\nsemination. Accessing data through APIs has many comple-\nmentary advantages relative to downloading local copies of\ndata. APIs typically allow users to query for specific subsets\nof the data that are of interest. API queries are often highly\nindexed, leading to efficient data retrieval. Finally, API access\nallows for easy incorporation of the most up-to-date data into\nother computational applications or workflows.\nWhile APIs offer many advantages in terms of data acces-\nsibility, these advantages do not immediately translate into\nefficient data integration. APIs generally follow some com-\nmon architectures and protocols [e.g. representational state\ntransfer (REST), output in JavaScript Object Notation\n(JSON)], but alignment at this technical level does not guar-\nantee either syntactic or semantic interoperability. For ex-\nample, APIs can use different identifiers for the same gene,\ndifferent data structures to represent gene attributes, and\ndifferent terms to describe the relationships between bio-\nmedical entities.\nThere have been some efforts to define and enforce seman-\ntic and syntactic standards to achieve data interoperability.\nExamples of this approach include the Beacon API from the\nGA4GH consortium (Rambla et al. 2022) and the DAS speci-\nfication for sharing annotations of genomic features (Dowell\net al. 2001). These efforts rely on the active participation of\nAPI developers in adopting a community standard for their\nAPI endpoints.\nHere, we explore the use of semantically precise API anno-\ntations as a complementary approach to achieving API inter-\noperability. We divided this work into two components. First,\nwe created an extension of the widely used OpenAPI annota-\ntion standard (http://spec.openapis.org/oas/v3.1.0) to seman-\ntically describe the APIs’ inputs and outputs, and a registry to\norganize these API annotations. Second, we created an appli-\ncation called BioThings Explorer to consume the API meta-\ndata and to execute multi-hop graph queries that span\nmultiple APIs. Together, this architecture allows users to\nquery a large, federated knowledge graph based on an inter-\nconnected network of biomedical APIs. This federated design\noffers a unique approach for creating knowledge graphs that\nis complementary to the more common strategy of centraliza-\ntion and local data integration.\nReceived: 18 April 2023; Revised: 18 August 2023; Editorial Decision: 11 September 2023; Accepted: 12 September 2023\nV\nC The Author(s) 2023. Published by Oxford University Press.\nThis is an Open Access article distributed under the terms of the Creative Commons Attribution License (https://creativecommons.org/licenses/by/4.0/), which\npermits unrestricted reuse, distribution, and reproduction in any medium, provided the original work is properly cited.\nBioinformatics, 2023, 39(9), btad570\nhttps://doi.org/10.1093/bioinformatics/btad570\nAdvance Access Publication Date: 14 September 2023\nApplications Note\nDownloaded from https://academic.oup.com/bioinformatics/article/39/9/btad570/7273783 by guest on 11 January 2026'
 ""integrates over 40 publicly available biomedical knowledge \nsources across separate domains, such as genes, proteins, \ndrugs, compounds, and diseases, along with their known \nrelationships.32 Two studies used textbooks, such as Harri\xad\nson's Principles of Internal Medicine, while 3 others utilized \nelectronic health record (EHR) data. Additionally, Zakka \net al. added over 500 markdown files from MDCalc to \nimprove clinical calculation capabilities in LLM.28 Two stud\xad\nies employed real-time online browsing to search academic \nsites, such as PubMed and UpToDate. The amount of \nretrieval resources varied across studies, ranging from a small \ndataset specific to 6 osteoarthritis guidelines to a large dataset \nof EHR data from 7 hospitals.\nRetrieval strategies\nIdentified retrieval strategies were grouped based on the RAG \nstages: pre-retrieval, retrieval, and post-retrieval. Figure 3 \npresents an example of how RAG is applied and lists identi\xad\nfied strategies within each stage.\nIn the pre-retrieval stage, 50% of studies (n ¼ 10) reported \nstrategies, such as query rewriting, document preprocessing, \nand assessing the necessity of retrieval. Zakka et al. simplified \nqueries by rephrasing text into search terms that are better \nsuited for website browsing,28 while Wang et al. focused on \ntechniques such as correcting errors, expanding abbrevia\xad\ntions, and matching synonyms in user queries.30 Soman et al. \nextracted disease entities in queries and retrieved correspond\xad\ning nodes from a knowledge graph.33 Document preprocess\xad\ning involved removing non-textual elements from PMC \npapers (eg, figures, references, and author disclosures),30\nextracted tables from PDFs using pdfplumber, structured the \ncontent with pydantic for seamless integration.25 In addition \nto query modification and document preprocessing, Jeong \net al. fine-tuned a model to determine whether retrieval was \nnecessary for a given query.34\nDuring the data retrieval stage, 85% of studies (n ¼ 17) \nreported strategies regarding indexing, aligning queries with \ndocuments, and ranking retrieval chunks. Chunking methods \nranged from fixed-size chunks35 to recursive splits.36 Embed\xad\nding models such as Text-embedding-ada-002,24,28–30,36,37\nMiniLM, and PubMedBERT33 were commonly used to con\xad\nvert sentences into vectors. Cosine similarity was the primary \nmetric for measuring query-document alignment. Two stud\xad\nies adopted Maximal Marginal Relevance for search and \nhighlighted its improved performance over similarity-based \nmethods.24,35 A domain-specific retriever, MedCPT, was \nused in one study.34 Another study used the multi-vector \nretriever that leveraged summarized document sections to \nidentify the original content for final answer generation.25\nThe retrieval cutoff parameters varied widely, with probabil\xad\nity thresholds up to 0.83 and the number of retrieved chunks \nranging from 3 to 90.28,36,38 Vector databases like FAISS and \nChroma were frequently reported, and LangChain was \nwidely used for document processing and retrieval.23,25,35,38\nIn the subgroup analysis, 12 studies used simple data retrieval \nstrategies (OR 1.30, 95% CI [1.16, 1.45]), while 5 studies \nused complex data retrieval strategies (OR 1.30, 95% CI \n[1.07, 1.24]), with no statistically significant difference \nobserved between the 2 approaches.\nIn the post-retrieval stage, 65% of studies (n ¼ 13) imple\xad\nmented specific strategies to refine outputs. Murugan et al. \ntailored prompts by providing clear context, defining roles \n(eg, distinguishing between healthcare providers and patients \nto deliver appropriately detailed information), and incorpo\xad\nrating relevant citations from retrieval sources such as the \nClinical \nPharmacogenetics \nImplementation \nConsortium \nguidelines and Food and Drug Administration (FDA) label\xad\ning.24 Soman et al. utilized prompt engineering to integrate \naccurate knowledge sources and statistical evidence, such as \nP-values and z-scores, from the SPOKE knowledge graph \ninto their outputs.33 Wang et al. outlined a detailed process \nin the post-retrieval stage using prompt engineering, which \ninvolved decomposing retrieved text into individual claims, \nverifying each claim with external knowledge sources, \nFigure 3. An example of using RAG in clinical applications, with identified strategies in 3 stages: pre-retrieval, retrieval, and post-retrieval. The user query \nseeks recommended medications for symptomatic peripheral artery disease without increased bleeding risk. The system retrieves evidence from clinical \nguidelines, processes it through embeddings and a vector database, and outputs a response, including rivaroxaban with low-dose aspirin, as recommended \nby the retrieved guideline information. In this example, GPT-4 suggested a dose of 75 mg Aspirin, but the common low-dose Aspirin is 81 mg.\nJournal of the American Medical Informatics Association, 2025, Vol. 32, No. 4                                                                                                      \n609 \nDownloaded from https://academic.oup.com/jamia/article/32/4/605/7954485 by guest on 11 January 2026""
 'Joy et. al \n1 \n \nFederated Knowledge Retrieval Elevates Large Language Model Performance on \nBiomedical Benchmarks \nJanet Joy1,2, Andrew I. Su1,2 \n1Department of Integrative Structural and Computational Biology, Scripps Research, La \nJolla, CA, USA. \n2Correspondence: Janet Joy (jjoy@scripps.edu) and Andrew I. Su (asu@scripps.edu) \nMain Figures: 4 \nSupplemental Figures: 11 \nSupplemental Table: 1 \nAbstract \n \nBackground \nLarge language models (LLMs) have significantly advanced natural language \nprocessing in biomedical research, however, their reliance on implicit, statistical \nrepresentations often results in factual inaccuracies or hallucinations, posing significant \nconcerns in high-stakes biomedical contexts. \nResults \nTo overcome these limitations, we developed BTE-RAG, a retrieval-augmented \ngeneration framework that integrates the reasoning capabilities of advanced language \nmodels with explicit mechanistic evidence sourced from BioThings Explorer, an API \nfederation of more than sixty authoritative biomedical knowledge sources. We \nsystematically evaluated BTE-RAG in comparison to traditional LLM-only methods \nacross three benchmark datasets that we created from DrugMechDB. These datasets \nspecifically targeted gene-centric mechanisms (798 questions), metabolite effects (201 \nquestions), and drug–biological process relationships (842 questions). On the gene-\ncentric task, BTE-RAG increased accuracy from 51% to 75.8% for GPT-4o mini and \nfrom 69.8% to 78.6% for GPT-4o. In metabolite-focused questions, the proportion of \nresponses with cosine similarity scores of at least 0.90 rose by 82% for GPT-4o mini \nand 77% for GPT-4o. While overall accuracy was consistent in the drug–biological \nprocess benchmark, the retrieval method enhanced response concordance, producing a \ngreater than 10% increase in high-agreement answers (from 129 to 144) using GPT-4o. \nConclusion \nFederated knowledge retrieval provides transparent improvements in accuracy for large \nlanguage models, establishing BTE-RAG as a valuable and practical tool for \nmechanistic exploration and translational biomedical research. \n.\nCC-BY 4.0 International license\navailable under a\n(which was not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made \nThe copyright holder for this preprint\nthis version posted August 2, 2025. \n; \nhttps://doi.org/10.1101/2025.08.01.668022\ndoi: \nbioRxiv preprint'
 'format. Second, because the entire federated KG is never in-\nstantiated in a single place, reasoning and scoring methods\nthat rely on having the entire knowledge graph in memory\ncannot be used with BioThings Explorer.\nIn sum, we believe that knowledge graphs enable many ex-\nciting use cases in biomedical research (Nicholson and Greene\n2020), and that BioThings Explorer is complementary to\nexisting approaches for assembling knowledge graphs, offer-\ning powerful and unique capabilities for both scientific data\nanalysts and tool developers.\nSupplementary data\nSupplementary data are available at Bioinformatics online.\nConflict of interest\nNone declared.\nFunding\nSupport for this work was provided by the National Center\nfor Advancing Translational Sciences, National Institutes of\nHealth, through the Biomedical Data Translator program,\nawards OT2TR003427 and OT2TR003445.\nData availability\nBioThings Explorer is implemented as a NodeJS application.\nThe primary repository for the BioThings Explorer project is\nat https://github.com/biothings/biothings_explorer, which in\nturn links to and incorporates other repositories as sub-\nmodules. All code is released under the Apache 2.0 open-\nsource software license.\nReferences\nCilibrasi RL, Vitanyi PMB. The google similarity distance. IEEE Trans\nKnowl Data Eng 2007;19:370–83.\nDavis AP, Wiegers TC, Johnson RJ et al. Comparative toxicogenomics\ndatabase\n(CTD):\nupdate\n2023.\nNucleic\nAcids\nRes\n2023;51:\nD1257–62.\nDowell RD, Jokerst RM, Day A et al. The distributed annotation system.\nBMC Bioinformatics 2001;2:7.\nFecho K, Bizon C, Miller F et al. A biomedical knowledge graph system\nto propose mechanistic hypotheses for real-world environmental\nhealth observations: cohort study and informatics application. JMIR\nMed Inform 2021;9:e26714.\nFecho K, Thessen AE, Baranzini SE et al. Progress toward a universal\nbiomedical data translator. Clin Transl Sci 2022;15:1838–47.\nHimmelstein DS, Lizee A, Hessler C et al. Systematic integration\nof biomedical knowledge prioritizes drugs for repurposing. Elife\n2017;6:e26726.\nLelong S, Zhou X, Afrasiabi C et al. BioThings SDK: a toolkit for build-\ning\nhigh-performance\ndata\nAPIs\nin\nbiomedical\nresearch.\nBioinformatics 2022;38:2077–9.\nMayers M, Tu R, Steinecke D et al. Design and application of a knowl-\nedge network for automatic prioritization of drug mechanisms.\nBioinformatics 2022;38:2880–91.\nMorris JH, Soman K, Akbas RE et al. The scalable precision medicine\nopen knowledge engine (SPOKE): a massive knowledge graph of bio-\nmedical information. Bioinformatics 2023;39(2):btad080.\nMungall CJ, McMurry JA, Ko¨hler S et al. The monarch initiative: an in-\ntegrative data and analytic platform connecting phenotypes to geno-\ntypes across species. Nucleic Acids Res 2017;45:D712–22.\nNicholson DN, Greene CS. Constructing knowledge graphs and their\nbiomedical applications. Comput Struct Biotechnol J 2020;18:\n1414–28.\nRambla J, Baudis M, Ariosa R et al. Beacon v2 and Beacon networks: a\n‘lingua franca’ for federated data discovery in biomedical genomics,\nand beyond. Hum Mutat 2022;43:791–9.\nUnni DR, Moxon SAT, Bada M et al. Biolink Model: a universal schema\nfor knowledge graphs in clinical, biomedical, and translational sci-\nence. Clin Transl Sci 2022;15:1848–55.\nWood EC, Glen AK, Kvarfordt LG et al. RTX-KG2: a system for build-\ning a semantically standardized knowledge graph for translational\nbiomedicine. BMC Bioinformatics 2022;23:400.\nZaveri A, Dastgheib S, Wu C et al. smartAPI: towards a more intelligent\nnetwork of web APIs. In: Blomqvist, E., Maynard, D., Gangemi, A.,\net al. (eds) The Semantic Web. ESWC 2017. Lecture Notes in\nComputer Science, 2017, 154–69. https://link.springer.com/chapter/\n10.1007/978-3-319-58451-5_11#chapter-info.\n4\nCallaghan et al.\nDownloaded from https://academic.oup.com/bioinformatics/article/39/9/btad570/7273783 by guest on 11 January 2026'
 'Clinical applications of RAG\nRAG techniques have been applied across a broad range of \nmedical specialties, as shown in Table 1. These applications \ninclude clinical decision-making and medical question- \nanswering. In clinical decision making, RAG has supported \npersonalized treatment,23,24 emergency triage,25 and disease \nmanagement.26,27 For medical question-answering, RAG’s \ncapability has been explored to address complex treatment \nguidelines questions,28 as well as queries focused on specific \nareas, such as head and neck surgery-related questions,29 and \npatient questions regarding diabetes.30 In the subgroup anal\xad\nysis, 13 studies focused on clinical decision-making (OR \n1.46, 95% CI [1.16, 1.71]) and 7 studies focused on medical \nquestion-answering (OR 1.32, 95% CI [1.08, 1.63]), with no \nstatistically significant difference observed between these 2 \ngroups.\nBaseline LLMs\nThe baseline LLMs varied across studies, with GPT-4 being \nthe most common, used in 14 studies, (OR: 1.58, 95% CI: \n1.21-2.04). GPT-3.5, used in 6 studies, showed an OR of \n1.43 (95% CI: 1.06-1.93). Llama2 was applied in 5 studies \n(OR: 1.25, 95% CI: 1.08-1.44).\nRetrieval sources\nRetrieval sources were categorized as pre-stored documents \nand real-time online browsing. Regarding pre-stored docu\xad\nments, 6 studies used clinical guidelines, such as the Emer\xad\ngency Severity Index (ESI) Ver.3 Field Triage.25 Five studies \nused academic articles from sources like PubMed abstracts or \nfull texts, or document sets such as the Radiographics Top 10 \nReading List on Gastrointestinal Imaging.31 Three studies \nused specialized knowledge bases, including ChatENT, \nOncoKB, and RareDis Corpus, while one study employed a \ngeneral biomedical knowledge graph (Scalable Precision \nMedicine Open Knowledge Engine [SPOKE]). SPOKE \nTable 1. Distribution of RAG applications by medical specialty and task \ntype.\nMedical Specialty\nFrequency\nInternal medicine\n4\nGeneral medicine\n3\nOncology\n3\nEmergency medicine\n2\nGastroenterology\n2\nOtolaryngology\n1\nHepatology\n1\nRare diseases\n1\nOrthopedics\n1\nNeurology\n1\nOphthalmology\n1\nTask\nFrequency (%)\nClinical decision-making\n13 (65%)\nMedical question-answering\n7 (35%)\nFigure 2. A forest plot showing the odds ratio (OR) of total impacts of the RAG-enhanced system compared with the baseline LLM system in clinical \ntasks. The left panel lists individual studies (e.g., Zakka et al., Murugan et al., Long et al.) along with their corresponding ORs and 95% confidence \nintervals (CIs). The right panel visualizes these ORs, with square markers proportional to study weights and horizontal lines representing CIs. The overall \npooled OR is 1.35 (95% CI: 1.19–1.53), represented by a diamond at the bottom.\n608                                                                                                      \nJournal of the American Medical Informatics Association, 2025, Vol. 32, No. 4 \nDownloaded from https://academic.oup.com/jamia/article/32/4/605/7954485 by guest on 11 January 2026']","['Omar & Mohammed / Journal of Applied Science and Technology Trends Vol. 06, No. 02, pp. 342 –357 (2025) 348 drugs, diseases, symptoms) and execution returns structured data (tuples) relevant to the question. Step 5. Answer Synthesis: The structured tuples flow to Answer Synthesis, which aggregates and formats them into a concise raw answer. This raw answer is sent back to LLaMA 3 to optionally refine the phrasing while preserving the retrieved facts. Step 6. Result Presentation: LLaMA 3 produces the final answer, which the interface displays together with the executed Cypher query and an optional preview of the returned rows, improving transparency and trust. The pipeline couples LLM-based language understanding (LLaMA 3) with a schema-grounded Neo4j knowledge graph. The Cypher Query Gen refines the query formulation, Query Execution retrieves evidence and Answer Synthesis converts structured results into readable outputs that produce answers that are accurate, interpretable, and easy to audit directly from the displayed query and evidence.']","The role of the *Journal of Applied Science and Technology Trends* in biomedical data analysis, based on the provided context, is not explicitly described. However, the context includes detailed information about biomedical data analysis tools and frameworks such as BioThings Explorer and retrieval-augmented generation (RAG) techniques, which are discussed in articles published in other journals like *Bioinformatics* and the *Journal of the American Medical Informatics Association*.

From the context, the key roles related to biomedical data analysis involve:

- Development and use of federated knowledge graphs and API networks (e.g., BioThings Explorer) to integrate and query heterogeneous biomedical data sources dynamically without centralizing all data.
- Application of retrieval-augmented generation (RAG) methods that combine large language models (LLMs) with explicit biomedical knowledge retrieval to improve accuracy and reliability in biomedical question answering and clinical decision support.
- Use of semantic API annotations and federated querying to enhance interoperability and data integration across diverse biomedical databases and ontologies.

While these advances are reported in specific journals such as *Bioinformatics* and *Journal of the American Medical Informatics Association*, the *Journal of Applied Science and Technology Trends* is not directly mentioned in the provided context as playing a role in biomedical data analysis.

**Summary:**  
The provided context does not specify a role for the *Journal of Applied Science and Technology Trends* in biomedical data analysis. Instead, it highlights the contributions of other platforms and journals in advancing biomedical data integration, knowledge graph querying, and retrieval-augmented generation techniques.","The Journal of Applied Science and Technology Trends publishes research such as the work by Omar & Mohammed, which details a pipeline that combines LLM-based language understanding (LLaMA 3) with a schema-grounded Neo4j knowledge graph to produce accurate, interpretable, and auditable answers in biomedical data analysis.",0.96,0.0,0.0,0.0
How is the UMLS term utilized in the iBKH dataset for biomedical entities?,"[""Omar & Mohammed / Journal of Applied Science and Technology Trends Vol. 06, No. 02, pp. 342 –357 (2025) \n \n352 \nThe dataset contains 60 questions divided into three difficulty \nlevels based on how complex the language is and how deep the \nbiomedical reasoning needs to be: \n• Level 1: 25 simple questions focusing mostly on easy-to-\nunderstand facts, such as symptoms of certain diseases or \ndrugs used to treat specific conditions. \n• Level 2: 20 medium-level questions that involve more \ndetailed relationships, such as drug interactions and \nSideEffect. \n• Level 3: 15 hard questions requiring multi-step reasoning \nacross multiple biomedical concepts or biological \npathways, similar to the complexity found in real clinical \ncases. \nThe set of evaluation questions was designed to span a wide \nrange of common biomedical topics and to reflect clinically \nrelevant query types reported in prior literature. Each question is \npaired with its corresponding gold standard cypher query and \ncategorized by difficulty level, as summarized in Table V, where \nthree illustrative examples are shown. The questions were derived \nfrom publicly available biomedical QA benchmarks and adapted \nfrom established knowledge bases (e.g., iBKH schema relations), \nensuring both coverage and diversity across diseases, drugs, \nsymptoms, and pathways.\nTABLE V.  QUESTION DIFFICULTY LEVELS AND SAMPLE CYPHER QUERIES \nLevel \nDefinition \nExample \nCypher Query \n1 \nSingle-hop question using \none relationship \nWhat are the symptoms of \nAlzheimer? \nMATCH (d:Disease)-[:HAS_SYMPTOM]->(s:Symptom) \nWHERE toLower(d.name) CONTAINS 'alzheimer' \nRETURN s.name AS symptom \n \n2 \nQuestions requiring one \nrelationship \nWhat are the side effects of \ndrugs used to treat asthma? \nWITH toLower('asthma') AS disease_name \nMATCH (d:Disease)<-[:TREATS]-(dr:Drug) \nWHERE toLower(d.name) CONTAINS disease_name \nMATCH (dr:Drug)-[:CAUSES]->(se:SideEffect) \nRETURN DISTINCT se.name AS side_effect, dr.name AS drug \n \n3 \nQuestions requiring two or \nmore relationships \nWhat cholesterol medications \ncause side effects, and what are \nsome of those effects? \nWITH toLower('cholesterol') AS disease_name \nMATCH (d:Disease)<-[:TREATS]-(dr:Drug) \nWHERE toLower(d.name) CONTAINS disease_name \nMATCH (dr:Drug)-[:CAUSES]->(se:SideEffect) \nRETURN DISTINCT dr.name AS drug, se.name AS side_effect \n \n \n \nFig. 4. Graphical User Interface of the Biomedical Knowledge Explorer System \nB. Quantitative Evaluation \nThe performance of the proposed system is evaluated on the \niBKH knowledge graph using Exact Match, Precision@k, \nRecall@k, F1@k, Hits@k, MRR, and total response time across \nsimple, medium, and complex biomedical queries. These metrics \nwere measured for each difficulty level (simple, medium, and \ncomplex) to see how performance changes as questions become \nmore challenging.\n \nTABLE VI.  QUERY GENERATION EXACT MATCH BY DIFFICULTY LEVEL""
 'Omar & Mohammed / Journal of Applied Science and Technology Trends Vol. 06, No. 02, pp. 342 –357 (2025) \n \n351 \nThis query searches for a disease node whose name contains \n’alzheimer’ and follows HAS_SYMPTOM edges to list related \nsymptom names. The system then executes this cypher to retrieve \nanswers. The prompts (such as few-shot examples and schema \nhints) were carefully designed to help LLaMA 3 generate correct \nCypher queries. The model learns how to use the graph’s labels \nand relationships properly. For example, if a user asks, ’Which \ndrugs treat diabetes?’, LLaMA might respond with a suitable \nCypher query: \n \nMATCH (d:Drug)-[:TREATS]->(di:Disease) \nWHERE toLower(di.name) CONTAINS ""diabetes"" \nRETURN d.name  \n \nThis queries for drug nodes that have a TREATS edge to a \ndiabetes disease node. By leveraging LLaMA 3 in this way, our \nsystem can flexibly handle many phrasing variations without \nmanual mapping rules. \nD. Model Configuration & Decoding \nWe run a local LLaMA 3.2-3B model in GGUF format (llama-\n3.2-3b-instruct-q4_k_m.gguf) via llama.cpp, as shown in Table \nIV. \nTABLE IV.  MODEL RUNTIME AND DECODING SETTINGS \nRuntime settings \nDecoding settings \nn_ctx = 1024 \ntemperature = 0.2 \nn_threads = 12 \ntop_p = 0.95 \nn_gpu_layers = 33 \ntop_k = 40 \nn_batch = 512 \nrepeat_penalty = 1.1 \n \nmax_tokens = 80 \n \nseed = 42 \n \nE. Graph Subset and Versioning \nWe use an iBKH derived subgraph (≈65.8k nodes; ≈3.0M \nedges) spanning DRUG, DISEASE, SYMPTOM, PATHWAY. IDs are \nnormalized to CURIEs and duplicates collapsed across \nUMLS/DrugBank/DisGeNET/SIDER/KEGG. Each edge stores \nprovenance/licensing metadata (source, source_version, license, \nretrieved_at, \nevidence_pmids/urls) \nand, \nwhen \navailable, \ntemporal fields (valid_from, valid_to). We report coverage as \nthe \npercentage \nof \nevaluated \nquestions \nwhose \ngold \nentities/relations are present. \nF. Query Execution and Reliability \nAfter a Cypher query is generated, it is executed on the Neo4j \ndatabase through the official Neo4j Python driver, which \nmanages the secure connection and returns the results. Instead of \nrestricting the output with a fixed LIMIT (e.g., LIMIT 5), the \nsystem retrieves candidate results and evaluates them using \nstandardized retrieval metrics such as Hits@1, Hits@5, and \nHits@10. This approach ensures that the system remains \nresponsive while providing a fair assessment of ranking quality \nacross different cutoff levels, rather than depending solely on a \nfixed number of returned items. Neo4j’s indexing on key node \nproperties, such as name and primary identifiers, also helps speed \nup lookups as the knowledge graph grows. In cases where the \nlanguage model generates an incomplete or incorrect query, such \nas referencing nodes or relationships that do not exist, the system \ncatches the error and either retries with a simpler prompt or \ninforms the user. Together, these steps make sure that queries run \nquickly, return valid results, and keep the overall experience \nsmooth and reliable for biomedical question-answering. \nG. User Interface for Query Transparency \nThe system includes a lightweight, cross-platform graphical \nuser interface (GUI) implemented as a web application using the \nFlask framework in Python, with HTML and Bootstrap for \ninteractive visualization. The interface is designed to make the \nquestion-answering process transparent and accessible to users \nwithout technical expertise. It consists of three main panels: \n1. Input Panel: Where the user can enter a biomedical \nquestion in natural language. \n2. Query Panel: Which displays the Cypher query generated \nby the language model, allowing users to verify how their \nquestion was interpreted. \n3. Results Panel: Which presents the retrieved answers in a \nclear, readable format, accompanied by a brief natural \nlanguage explanation generated by the system. \nBy showing both the query and the answer, the GUI promotes \nuser trust and enables validation of the system’s reasoning \nprocess. The interface is lightweight enough to run smoothly on \nstandard desktop machines without additional dependencies, \nmaking it practical for local deployments in clinical or research \nsettings. Fig. 4 illustrates the overall layout. \nV. EXPERIMENTAL RESULTS \nIn this section, the proposed system is evaluated based on its \nability to translate natural-language biomedical questions into \nexecutable Cypher queries over the iBKH knowledge graph. The \nassessment is conducted end-to-end and focuses on three main \naspects: (i) the accuracy of query generation, measured by the \nproportion of correctly produced Cypher queries; (ii) system \nefficiency, quantified through total response time from question \nsubmission to final answer delivery, covering both query \ngeneration and execution; and (iii) the quality of retrieved \nanswers, evaluated using standardized information retrieval \nmetrics including Precision@k, Recall@k, F1@k, Hits@k, and \nMean Reciprocal Rank (MRR). For clarity, all these metrics are \nformally defined in the following subsections, with their \ncorresponding equations, and will be used consistently \nthroughout the results section. Together, these dimensions \nprovide a comprehensive view of both correctness and efficiency \nacross simple, medium, and complex biomedical queries. \nA. Description of the Experimental Data \nTo evaluate the proposed system, this work used a carefully \nselected set of biomedical questions designed to test how well the \nsystem understands natural language and converts it into Cypher \nqueries for a biomedical knowledge graph. \nTo evaluate the system, a benchmark of 60 biomedical \nquestions was constructed by the authors, guided by the schema \nand relations in iBKH. The questions were grouped into simple, \nmedium, and complex levels to assess performance across \ndifferent reasoning requirements. Gold-standard answers were \nmanually prepared for each question to enable quantitative \nevaluation using Exact Match, Precision@k, Recall@k, F1@k, \nHits@k, and MRR. The complete set of 60 questions is available \nat this link https://drive.google.com/drive/my-drive.'
 'Omar & Mohammed / Journal of Applied Science and Technology Trends Vol. 06, No. 02, pp. 342 –357 (2025) \n \n356 \n \nFig. 7. Side effects of drugs that treat epilepsy \n \nExecuting this query returns drugs associated with epilepsy \nand their corresponding side effects. For example, the query \nidentifies Pregabalin as a treatment for epilepsy and retrieves \nmultiple side effects such as anxiety, arrhythmia, gastrointestinal \npain, and others. The visualization of the graph in Fig. 7, \nhighlights this pattern, where the drug node is linked to epilepsy \nvia TREATS and to several side effect nodes via CAUSES, \nproviding an interpretable biomedical knowledge structure. \nVI. DISCUSSION \nUnder \na \nunified \nevaluation \nprotocol \nreporting \nP@k/R@k/F1@k, Hits@k, and MRR at 𝑘∈{1,5,10} , the \npipeline exhibits consistent end-to-end behavior across the three \ndifficulty tiers. The 𝑘-ablation shows strong early precision (high \nP@1 and elevated MRR), while recall increases with larger 𝑘, \nindicating that correct answers typically surface near the top yet \npersist deeper in the ranked slate. At the query-generation level, \nexact-match (EM) is high for simple questions (e.g., ∼96%) and \nlower for complex, multi-hop questions (e.g., ∼86.7%), which \naligns with increased compositionality. End-to-end latency \n(mean total ≈6.07\u2006s from question to final answer) remains within \ninteractive bounds on our local setup. Practically, a fully offline \ndeployment avoids internet connectivity, API keys, and external \ndata \ntransfer, \nstrengthening \nprivacy, \ncompliance, \nand \nreproducibility in biomedical contexts. \nSeveral challenges qualify these findings. First, the \ncomparability challenge: to our knowledge, no prior study \nevaluates iBKH using the same metric suite, making cross-paper \nnumeric comparisons not “apples to apples.” We therefore \ninterpret the results as controlled evidence about this pipeline \nunder a single, consistent protocol rather than as a cross-study \nleaderboard. Beyond comparability, performance may vary with \nother biomedical knowledge graphs; the current iBKH snapshot \nis static, limiting real-time updates; and scaling to larger or \ndynamically refreshed graphs can introduce latency and \nconsistency trade-offs. \nError analysis shows that residual failures concentrate in \ncomplex, multi-hop queries where missing relation hops or brittle \nname-based matching (synonyms, abbreviations, homonyms) \nlead to partial answers. These observations motivate concept-\nlevel normalization via biomedical identifiers (e.g., UMLS, \nSNOMED, RxNorm) with synonym expansion, as well as \nschema-constrained query generation and path-guided decoding \nto better satisfy multi-hop constraints. Finally, correctness is \nassessed primarily at the answer level (Hits@k, MRR, \nprecision/recall) and does not yet include full semantic-\nequivalence checks across alternative Cypher queries, which may \noverlook cases where different queries yield the same correct \nresults. Complementing template EM with result-set equivalence \nchecks, expanding the metric suite (e.g., nDCG/MAP) with per-\nquestion 95% bootstrap confidence intervals, and supporting \nincremental graph updates with distributed storage are promising \nsteps to enhance robustness, scalability, and external validity. \nVII. CONCLUSION \nThis study introduced a hybrid biomedical question \nanswering framework that couples the LLaMA-3B language \nmodel with a Neo4j-based iBKH knowledge graph to enable the \nautomatic generation of executable Cypher queries and to deliver \ntransparent, evidence-grounded answers through justification \nsubgraphs. Evaluation in simple, medium, and complex queries \ndemonstrated consistently high performance, with strong \nprecision, recall, F1, Hits@k, and MRR values, while \nmaintaining low latency suitable for interactive biomedical \napplications. Beyond quantitative performance, the proposed \nsystem provides a reproducible and privacy-preserving solution \nby operating fully offline, a property of particular importance in \nsensitive clinical and research contexts. However, certain \nlimitations remain. The reliance on a static iBKH snapshot \nconstrains coverage and adaptability, recall is lower for complex \nmulti-hop reasoning, and the absence of canonical entity \nnormalization (e.g., UMLS, SNOMED, RxNorm) may reduce \nsemantic robustness. Future research will therefore focus on \nintegrating standardized biomedical entity normalization, \nenabling dynamic and incremental knowledge graph updates, and \nleveraging domain-adapted or fine-tuned biomedical LLMs. \nThese directions are expected to further strengthen the \nrobustness, scalability, and applicability of the framework in real-\nworld biomedical and clinical environments. \n \nREFERENCES \n[1] S. Bubeck, V. Chandrasekaran, R. Eldan, J. Gehrke, E. Horvitz, E. Kamar, \nP. Lee, Y. T. Lee, Y. Li, S. Lundberg, et al., “Sparks of artificial general \nintelligence: \nEarly experiments with \ngpt-4,” arXiv preprint \narXiv:2303.12712, 2023. \n[2] J. Achiam, S. Adler, S. Agarwal, L. Ahmad, I. Akkaya, F. L. Aleman, D. \nAlmeida, J. Altenschmidt, S. Altman, S. Anadkat, et al., “Gpt-4 \ntechnical report,” arXiv preprint arXiv:2303.08774, 2023. \n[3] Y. Hou, J. Yeung, H. Xu, C. Su, F. Wang, and R. Zhang, “From answers to \ninsights: unveiling the strengths and limitations of chatgpt and \nbiomedical knowledge graphs,” Research Square, pp. rs–3, 2023.'
 'Omar & Mohammed / Journal of Applied Science and Technology Trends Vol. 06, No. 02, pp. 342 –357 (2025) \n \n350 \ndocumentation, indexes “enable quicker and more efficient \npattern matching” by allowing the query planner to rapidly locate \nnodes by label and property. \nWith the schema in place, data was imported using Cypher’s \nLOAD CSV commands. For each vocabulary file, rows with \nnonnull primary IDs were processed: the code used MERGE to \ncreate (or match existing) nodes with the given label and primary \nproperty, and then SET the remaining attributes from the CSV \ncolumns. For example, in importing disease_vocab.csv, each row \nproduced a node (:Disease primary: <id>) with properties name, \ndo_id, kegg_id, and umls_cui set from the row (empty strings \nwere turned into null). Similarly, drug_vocab.csv rows produced \ndrug nodes with properties for DrugBank, KEGG, PharmGKB \nIDs, etc. This approach follows best practice: using MERGE on \nthe unique ID ensures that nodes are not duplicated during \nmultiple passes. \nAfter all nodes were created, the relationship CSVs were \nloaded. Each row in those files was matched to the corresponding \nsource and target nodes by their primary IDs, and conditional \nlogic was used to create edges. For example, the disease-\nsymptom file (Di_Sy_res.csv) was processed by matching a \ndisease node and a symptom node for each row, then executing \nMERGE (d)-[r:HAS_SYMPTOM]-(s) if the present column is \nnonzero; the edge was given a present property and a source \nproperty from the CSV. The disease-disease file (di_di_res.csv) \nwas processed by matching disease nodes d1 and d2:If is_a = 1, \na (:Disease)-[:IS_A]->(:Disease) edge was merged; if Resemble \n= 1, a (:Disease)-[:RESEMBLES]->(:Disease) edge was merged. \nDrug-disease relationships were handled similarly: the script \nmatched a Drug node and a Disease node for each row of \nD_Di_res.csv, then for each flag that was 1, it merged the \ncorresponding edge label (such as TREATS, PALLIATES, \nASSOCIATED_WITH, etc.) from the drug to the disease. Drug–\nside-effect rows produced (:Drug)-[:CAUSES]->(:SideEffect) \nedges with the source noted, and drug–drug rows produced either \nINTERACTS_WITH or RESEMBLES edges between matched \ndrug pairs. Finally, the drug-pathway and disease-pathway files \neach produced:ASSOCIATED_WITH edges linking drugs or \ndiseases to pathway nodes. \nIn this graph model, most relationships are directional (for \nexample, a drug TREATS a disease; a disease does not TREAT \na drug). This follows common practice in biomedical KGs. The \nsame relations (such as RESEMBLES or INTERACTS_WITH) \nare inherently symmetric, but were stored as two directed edges \nor one undirected edge depending on implementation. All \nrelationship creation steps used Cypher’s MERGE so that \nrepeated loads or out-of-order imports did not create duplicate \nedges. \nThis study used a static KG snapshot for reproducibility, but \nthe system also supports incremental updates through the Neo4j \nMERGE and batch import functions. A summary of the different \ntypes of nodes and relationships is provided, together with their \ncounts, descriptions, and examples in Table III.\n \nTABLE III.  DETAILED SUMMARY OF KNOWLEDGE GRAPH COMPONENTS IN IBKH SUBSET \nComponent Type \nEntity/Relationship \nCount \nDescription \nExamples \n \nDisease \n19,236 \nMedical conditions and disorders \nDiabetes, Hypertension, Cancer \n \nDrug \n37,997 \nPharmaceutical compounds \nAspirin, Insulin, Amoxicillin \nNodes \nSymptom \n1,361 \nClinical signs and symptoms \nHeadache, Fever, Nausea \n \nSideEffect \n4,251 \nNegative outcomes of drugs \nNausea, Drowsiness \n \nPathway \n2,983 \nBiological processes and pathways \nApoptosis, Glycolysis \n \nASSOCIATED_WITH \n101,534 \nGeneral associations (disease–\npathway, drug–pathway, etc.) \nDiabetes ASSOCIATED_WITH Pathway \n \nCAUSES \n145,321 \nDrug-SideEffect relationships \nAspirin CAUSES Gastric_Bleeding \n \nHAS_SYMPTOM \n3,357 \nDisease-symptom links \nCOVID-19 HAS_SYMPTOM Fever \n \nHAS_BIOMARKER \n412 \nBiomarkers linked to disease \nPSA HAS_BIOMARKER Prostate_Cancer \nRelationships \nINHIBITS_CELL_GROWTH \n1,508 \nDrugs inhibiting cell growth \nChemo INHIBITS_CELL_GROWTH Tumor \n \nINTERACTS_WITH \n2,682,142 \nDrug-drug interactions \nAspirin INTERACTS_WITH Warfarin \n \nIS_A \n10,529 \nSubtype hierarchy \nFlu IS_A Viral_Infection \n \nPALLIATES \n388 \nDrug palliates disease \nMorphine PALLIATES Cancer \n \nPREVENTS_SUPPRESSES \n859 \nPreventive links \nVaccine PREVENTS_SUPPRESSES Measles \n \nRESEMBLES \n7,000 \nSimilarity relationships \nDrugA RESEMBLES DrugB \n \nTREATMENT_THERAPY \n44,852 \nTherapy relationships \nRadiotherapy TREATMENT_THERAPY Tumor \n \nTREATS \n5,491 \nDrug-disease links \nInsulin TREATS Diabetes \n \nALLEVIATES_REDUCES \n∼180,000 \nSymptom relief \nParacetamol ALLEVIATES_REDUCES Fever \nTotal \nNodes \n65,828 \nTotal biomedical entities \n— \n \nRelationships \n3,004,166 \nTotal knowledge graph links \n— \nC. Natural Language to Cypher Query Translation \nA key feature of the system is its ability to accept questions \nwritten in plain English and automatically generate the \ncorresponding Cypher queries. This is accomplished using \nMeta’s LLaMA 3 large language model, which runs entirely on \na local machine through the open-source llama.cpp framework. \nRunning the model locally ensures low-latency execution and \nkeeps sensitive queries within the user’s environment. \nTo generate a Cypher query, LLaMA 3 is prompted with \nexamples of natural language questions along with their correct \nCypher translations. The prompt also includes instructions on \nhow to navigate the structure of the graph schema. When a user \nenters a question (e.g., ’What are the symptoms of Alzheimer’s \ndisease?’), the system inserts it into the prompt and asks LLaMA \n3 to produce a corresponding query. For example, the model may \nreturn: \nMATCH (d:Disease)-[:HAS_SYMPTOM]->(s:Symptom) \nWHERE toLower(d.name) CONTAINS ""alzheimer"" \nRETURN s.name'
 'Omar & Mohammed / Journal of Applied Science and Technology Trends Vol. 06, No. 02, pp. 342 –357 (2025) \n \n345 \nrecord-high accuracy, while open-source models achieved \nimpressive gains through prompt optimization. \nFeng et al.[22] developed the Knowledge Graph-based \nThought (KGT) framework that integrated LLMs with a pan-\ncancer knowledge graph for biomedical question answering. \nKGT was designed to reason on the knowledge graph schema and \nidentify optimal subgraphs to use for directing accurate answer \ngeneration, all without fine-tuning the LLMs. The framework is \nbenchmarked against a new dataset (PcQA) designed specifically \nfor pan-cancer KGQA tasks and has outperformed all existing \nstate-of-the-art approaches by a rather large margin. KGT’s \npracticality in biomedical issues was highlighted through case \nstudies for drug repositioning, drug resistance, and biomarker \ndiscovery. Their approach exhibited robust adaptability among \nvarious LLMs. \nRezaei et al.[26] developed AMG-RAG, a dynamic \nframework that utilizes autonomous LLM agents with medical \nsearch tools in the continuous construction and real-time updating \nof Medical Knowledge Graphs (MKGs). Their system \nincorporated confidence scoring and multi-hop reasoning to \nimprove accuracy and interpretability in medical question \nanswering. AMG-RAG outperformed size models on both very \nhard MEDQA benchmarks and more accessible MedMCQA \nones, proving that it could conduct efficient reasoning based on \ncurrent structured medical knowledge. They also used Neo4j to \nmanage the knowledge graphs while adding external searches to \nensure the latest data. \nTiwari et al.[24] presented Auto-Cypher, a recent automated \npipeline for producing high-quality synthetic data for training \nLLMs by mapping natural language to Cypher queries for graph \ndatabases like Neo4j. The pipeline deployed the novelty of LLM-\nas-database-filler to synthesize Neo4j databases for the execution \nof generated queries to ensure their correctness. A sizable dataset \ncalled SynthCypher was created, spanning multiple domains and \ncomplex queries, leading to a 40% improvement in LLM \nperformance on Cypher generation. The datasets were used to \nfine-tune open-source models such as LLaMA, Mistral, and \nQwen, and the SPIDER benchmark was adapted for evaluation \npurposes. \nMohammed et al.[29] proposed a hybridized GraphRAG \nframework combining Neo4j-based UMLS knowledge graphs \nwith a vector store for medical textbooks to create an improved \nU.S.M.L.E.-style clinical question-answering approach. The \nproject integrated symbolic reasoning from knowledge graphs \nwith semantic retrieval performed on text embeddings to enhance \nrelevance and accuracy via adaptive re-ranking and query \nexpansion. The system had the answers produced by GPT-4o-\nMini, with different prompting strategies encouraging evidence-\nbased and traceable responses grounded in verified medical \nknowledge. Experiments showed that the hybrid approach \nimproved factual accuracy and citation fidelity as compared to \nthe L.L.M.-only approach, enhancing transparency and \nreliability. It is shown that binding both structured and \nunstructured medical knowledge sources could aid in \nameliorating hallucinations and hence improve clinical \ntrustworthiness in AI-driven medical QA. \nYang et al.[30] articulated sepsis knowledge graph was \ncrafted by combining multicenter clinical data from over 10,000 \npatients with the help of GPT-4 for entity recognition and \nrelationship extraction. Real-world data were collected from \nthree hospitals and integrated with clinical guidelines and \ndatabases from the public domain. The knowledge graph \ncontained 1,894 nodes and 2,021 relationships pertaining to \ndiseases, symptoms, biomarkers, treatments, and complications. \nGPT outperformed other models in every resolution on sepsis-\nspecific datasets to obtain high F1-score results. The constructed \ngraph highlighted complex interactions in sepsis for assisting \nclinical decision-making and was implemented on Neo4j. \nGuan et al.[20] proposed a novel method for constructing a \nlocal knowledge graph from retrieved biomedical documents by \nextracting propositional claims. They carried out layer wise \nsummarization on this graph to capture multi-document \nrelationships and provide comprehensive contextual information \nto a language model for question-answering purposes. The \nmethod resolved issues in multi-document biomedical QA, such \nas noise cancellation and efficient context usage. They then tested \ntheir method on several benchmarks for biomedical question \nanswering, achieving performance at least comparable to, if not \nbetter than, existing retrieval-augmented generation (RAG) \nbaselines. The study established enhanced reasoning and answer \naccuracy of the model achieved through structured graph \nsummarization.   \nPrevious studies have improved biomedical QA using KGs \nand LLMs, but important gaps remain. Most systems lack \ntransparent, graph-based justifications, rely on limited evaluation \nmethods, or depend on cloud resources that reduce privacy and \nreproducibility. Our framework addresses these gaps by \nproviding visible Cypher queries with evidence subgraphs, \napplying comprehensive performance metrics across difficulty \nlevels, and ensuring fully local, privacy-preserving deployment. \nTable I summarizes key previous studies on biomedical \nknowledge graphs and question answering, outlining their \nmethods, datasets, and main limitations. \nIII. PRELIMINARIES \nThis section outlines the fundamental concepts required to \nunderstand the proposed system. It introduces biomedical \nknowledge graphs, explains how Neo4j stores data in graph \nform, and describes the use of Cypher for querying. It also \nprovides a brief overview of large language models (LLMs) and \ntheir role in interpreting natural language. \n \n \nA. Biomedical Knowledge Graphs \nBiomedical Knowledge Graphs (BKGs) provide a structured \nrepresentation of complex biomedical information by modeling \ndiverse medical entities, such as diseases, drugs, symptoms, and \nbiological pathways, as interconnected nodes within a graph \nstructure. The edges in these graphs represent the semantic \nrelationships between these entities, including ’treats’, ’causes’, \n’interacts with’ and many others, as illustrated in Fig 1. This \nform of representation enables the integration of heterogeneous \nbiomedical data from a wide range of sources, including']","['B. Dataset and Knowledge Graph Construction 1) Dataset The integrated Biomedical Knowledge Hub (iBKH), a large biomedical knowledge base, forms the first level of the system and integrates information from various curated high-quality biomedical databases. This implies that the data set includes various types of entities, such as diseases, symptoms, drugs, biological pathways, etc. This study used the representative subset of the iBKH dataset, which contained 65828 biomedical entities. These entities are semantically interconnected through a total of 3004166 relationships, thus creating a rich knowledge graph. The iBKH dataset was originally introduced in [11], and it is freely available at (https://github.com/wcm-wanglab/iBKH). This dataset is the core semantic foundation upon which this study is built. The knowledge graph is populated from multiple tabular sources (CSV files), each listing entities or relationships. The main input files and their contents are as follows: • Disease vocabulary(disease_vocab.csv): Contains columns such as primary (a unique disease ID), name, do_id (Disease Ontology ID), kegg_id, and umls_cui (UMLS Concept ID). Each row represents a disease node with external identifiers. • Drug vocabulary (drug_vocab.csv): Includes primary (unique drug ID), name, drugbank_id, kegg_id, pharmgkb_id, umls_cui, mesh_id, iDISK_id and CID (PubChem ID). Each row defines a drug node with standard database identifiers. • Symptom vocabulary (symptom_vocab.csv): Contains primary (unique symptom ID), name, mesh_id, umls_cui and iDISK_id. Each row defines a symptom node. • Side effect vocabulary (side_effect_vocab.csv): Includes primary (unique side-effect ID) and name. Each row represents a side-effect node (with UMLS ID when available). • Pathway vocabulary (pathway_vocab.csv): Contains primary (unique pathway ID), name, reactome_id, go_id, and kegg_id. Each row defines a biological pathway node. Relationship files (each row typically contains two entity IDs and one or more boolean flags or codes) include: • Disease–Symptom links (Di_Sy_res.csv): Rows include Disease and Symptom IDs, a presence flag (1 or 0) and a data source. If Present = 1, a HAS_SYMPTOM edge is created from the disease to the symptom, with properties for presence and source. • Disease–Disease links (di_di_res.csv): Rows include Disease_1 and Disease_2 IDs with binary flags for is_a and Resemble. If is_a = 1, an (IS_A) edge is created (Disease_1 → Disease_2); if Resemble = 1, a RESEMBLES edge is created. The source field is used for provenance. • Drug–Disease links (D_Di_res.csv): Includes Drug and Disease IDs with several binary flags. If a flag equals 1, a corresponding edge is created: o TREATS (Treats = 1) o PALLIATES (Palliates = 1) o ASSOCIATED_WITH (Associate = 1) o ALLEVIATES_REDUCES (alleviates = 1) o TREATMENT_THERAPY (treatment/therapy = 1) o INHIBITS_CELL_GROWTH (inhibits cell growth = 1) o HAS_BIOMARKER (biomarkers = 1) o PREVENTS_SUPPRESSES (prevents/suppresses = 1) o ROLE_IN_PATHOGENESIS (role in disease pathogenesis = 1) • Drug–SideEffect links (D_SE_res.csv): Contains Drug and SideEffect IDs with a Source column. Each row creates a CAUSES edge from the drug to the side effect, with source as an edge property. • Drug–Drug interactions (D_D_res.csv): Rows include Drug_1 and Drug_2 IDs with flags for Interaction and Resemble. If Interaction = 1, an INTERACTS_WITH edge is created (bidirectional). If Resemble = 1, a RESEMBLES edge is added. • Drug–Pathway links (D_Pwy_res.csv): Includes Drug ID and Pathway ID. Each row generates an ASSOCIATED_WITH edge from the drug to the pathway. • Disease–Pathway links (Di_Pwy_res.csv): Contains Disease ID and Pathway ID. Each row creates an ASSOCIATED_WITH edge from the disease to the pathway. 2) Data Upload Performance The time required to upload different types of entities and relationships into the Neo4j biomedical knowledge graph, measured in seconds. These measurements reflect both the size and complexity of the data being processed. As shown in Table II, the longest upload time is for Drug- Drug Relationships, which takes approximately 190 seconds due to the large number of edges (over 3 million). Following this, Disease-Disease and Drug-Disease Relationships also require considerable time for loading. On the other hand, individual']","The UMLS term (specifically the UMLS Concept Unique Identifier, or UMLS CUI) is utilized in the iBKH dataset as a key property for biomedical entity normalization and integration. During the construction of the iBKH knowledge graph, entity identifiers from multiple biomedical vocabularies and databases (such as UMLS, DrugBank, DisGeNET, SIDER, KEGG) are normalized and merged to avoid duplication. For example, when importing disease vocabulary data, each disease node is created with properties including the UMLS CUI (uml_cui), alongside other identifiers like DOID and KEGG IDs. Empty strings in these fields are converted to null to maintain data consistency.

This normalization using UMLS CUIs helps unify different biomedical entities referring to the same concept across various data sources, improving semantic robustness and enabling more accurate multi-source integration. It also supports synonym expansion and concept-level matching, which are important for handling challenges such as synonyms, abbreviations, and homonyms in biomedical queries.

In summary, the UMLS term in the iBKH dataset serves as a canonical biomedical identifier property on nodes (e.g., diseases), facilitating entity normalization, deduplication, and integration across heterogeneous biomedical data sources within the knowledge graph.","In the iBKH dataset, the UMLS Concept Unique Identifier (UMLS CUI) is used as an external identifier for multiple biomedical entities. Specifically, it appears as a column in the disease vocabulary, drug vocabulary, and symptom vocabulary files, linking each entity to its corresponding UMLS concept.",1.0,0.9788347559752296,0.6388888888675925,1.0
How does the KGT framework leverage biomedical knowledge graphs to enhance large language models' reasoning capabilities and improve biomedical question answering performance?,"['Omar & Mohammed / Journal of Applied Science and Technology Trends Vol. 06, No. 02, pp. 342 –357 (2025) \n \n346 \nscientific literature, clinical records, genomic databases, and \nexperimental findings [19, 31]. \nSuch integration creates a comprehensive biomedical \nknowledge base that supports advanced analytics and discovery. \nFor example, biomedical knowledge graphs can reveal previously \nunknown relationships (e.g., between drugs and diseases) and \nhelp prioritize potential biomarkers for complex conditions. The \nIntegrative Biomedical Knowledge Hub (iBKH) is one such \nlarge-scale graph that consolidates diverse biomedical resources \ninto a unified hub, enabling discovery at scale [11]. Beyond \niBKH, large biomedical knowledge graphs such as SPOKE \nfurther illustrate how graph integration accelerates research and \nsupports precision-medicine use cases [12]. Overall, these graphs \nserve as foundational resources for data-driven and personalized \nmedicine. These knowledge graphs serve as foundational \nresources for precision medicine, where treatment can be tailored \nto the individual’s biological profile, improving outcomes and \nminimizing side effects [19, 31]. \n \nTABLE I.  SUMMARY OF RELATED RESEARCH ON BIOMEDICAL KGS AND QUESTION ANSWERING \nRef. \nYear \nData/Graph \nMethod \nBaselines \nKey Metric \nLimitation \n[11] \n2023 \niBKH (18 biomedical \nsources, Neo4j) \nIntegrative KG + Graph \nlearning; drug repurposing \ncase \nKnown drugs, \nAlzheimer’s \nstudy \nPredictive performance \n(drug repurposing) \nLimited to Alzheimer’s case \nstudy; scalability and updates not \ndetailed \n[19] \n2023 \nDisease KG (9,400 \nnodes, 45,000 relations \nin Neo4j) \nCypher queries for disease–\ndrug–symptom reasoning \nCross-referential \ndisease DB \nNew relation inference; \ncomplex query \nanswering \nLimited to single domain; lacks \nlarge-scale evaluation \n[3] \n2023 \nBKGs vs. GPT-3.5/4 \nComparative QA study: \nLLMs vs. KGs \nGPT-3.5, GPT-4, \nKG reasoning \nAccuracy, Novelty, \nReasoning \nGPT-4 inconsistent in \nreasoning/citations; KG less fluent \nbut more reliable \n[13] \n2024 \nSPOKE KG + \nLLaMA2, GPT-3.5, \nGPT-4 \nKG-optimized retrieval for \nLLMs (RAG) \nOther retrieval \nmethods \nAccuracy, token \nreduction >50% \nFocus on retrieval optimization, \nnot KG construction \n[23] \n2024 \nBenchmark KB datasets \nGenerate-then-retrieve \n(LLM → logical form → \nKB retrieval) \nPrior KBQA \nmethods \nAccuracy (highest to \ndate) \nRisk of error in logical form \ngeneration \n[6] \n2024 \nPrimeKG \nLLM + KG hybrid, Cypher \nquery validation, RAG \nGPT-4 Turbo, \nLLaMA 3 \nAccuracy, \nExplainability \nDependent on KG coverage; \ncomputationally intensive \n[22] \n2025 \nPan-cancer KG (PcQA \ndataset) \nKG-enhanced reasoning \n(subgraph selection) \nSOTA KGQA \nmethods \nOutperformed SOTA on \nPcQA \nLimited to pan-cancer focus; no \nfine-tuning explored \n[26] \n2025 \nDynamic Medical KG + \nNeo4j \nLLM agents + multi-hop \nreasoning \nMEDQA, \nMedMCQA \nbaselines \nAccuracy, \nInterpretability \nHigh system complexity; requires \ncontinuous updating \n[24] \n2025 \nSynthCypher dataset \n(Neo4j + synthetic \nqueries) \nLLM-supervised Cypher \ngeneration and verification \nSPIDER \nbenchmark \nCypher accuracy 40% \nSynthetic dataset may not capture \nall real-world cases \n[29] \n2025 \nUMLS KG + Neo4j \nHybrid GraphRAG \nLLM-only QA \nAccuracy, Citation \nfidelity \nMore complex pipeline; relies on \nexternal vector store \n[30] \n2025 \nClinical data (10k \npatients, 1,894 nodes, \nNeo4j) \nKG construction using \nGPT-4 for entity/relation \nextraction \nOther KG \nconstruction \nmethods \nHigh F1-scores \nFocus only on sepsis; limited \ngeneralization \n[20] \n2025 \nLocal KG from \nbiomedical documents \nMulti-level summarization \nover KG for QA \nRAG baselines \nQA accuracy, reasoning \nTested mainly on document QA; \nscalability not proven \n \nB. Neo4j Graph Database \nTo manage the complexity and large size of biomedical \nknowledge graphs, specialized graph databases are needed. \nNeo4j is one of the most popular graph databases designed to \nstore and query data structured as nodes (entities) and \nrelationships (edges), both of which can have descriptive \nproperties [32, 33]. It uses the property graph model, which \nmakes it easy to represent complex, connected biomedical data \nsuch as drug-gene interactions or disease pathways. Neo4j’s \nCypher query language is especially advantageous because it \nallows users to write expressive and efficient queries to explore \nmulti-step connections in the data [34]. \nNeo4j works well for biomedical data because it can quickly \nrun complicated queries over highly interconnected datasets. This \nis important in biology and medicine, where relationships \nbetween entities like proteins, diseases, and drugs are often \ncomplex and layered. Studies have shown that Neo4j handles \nlarge biomedical graphs efficiently, making it a favorite among \nresearchers and industry users alike [33, 35, 36]. Its indexing and \ncaching mechanisms also help speed up query processing and \ndata retrieval [37]. \nMoreover, \nNeo4j \nintegrates \nsmoothly \nwith \nmany \nprogramming languages and analytics tools, which makes it \neasier to build interactive biomedical applications and clinical \ndecision support systems that can turn complex graph data into \nuseful insights [38, 39]. \nC. Large Language Models (LLMs) in Biomedical Question \nAnswering \nLarge Language Models (LLMs) are powerful AI systems \ntrained on vast amounts of text data. They learn the structure and \npatterns of language, enabling them to understand questions, \ngenerate responses, summarize information, and perform other \ncomplex language tasks. Well-known models such as LLaMA \nand GPT-3 have greatly advanced the field of natural language \nprocessing by showing strong performance across many tasks \n[40, 41].'
 'Omar & Mohammed / Journal of Applied Science and Technology Trends Vol. 06, No. 02, pp. 342 –357 (2025) \n \n345 \nrecord-high accuracy, while open-source models achieved \nimpressive gains through prompt optimization. \nFeng et al.[22] developed the Knowledge Graph-based \nThought (KGT) framework that integrated LLMs with a pan-\ncancer knowledge graph for biomedical question answering. \nKGT was designed to reason on the knowledge graph schema and \nidentify optimal subgraphs to use for directing accurate answer \ngeneration, all without fine-tuning the LLMs. The framework is \nbenchmarked against a new dataset (PcQA) designed specifically \nfor pan-cancer KGQA tasks and has outperformed all existing \nstate-of-the-art approaches by a rather large margin. KGT’s \npracticality in biomedical issues was highlighted through case \nstudies for drug repositioning, drug resistance, and biomarker \ndiscovery. Their approach exhibited robust adaptability among \nvarious LLMs. \nRezaei et al.[26] developed AMG-RAG, a dynamic \nframework that utilizes autonomous LLM agents with medical \nsearch tools in the continuous construction and real-time updating \nof Medical Knowledge Graphs (MKGs). Their system \nincorporated confidence scoring and multi-hop reasoning to \nimprove accuracy and interpretability in medical question \nanswering. AMG-RAG outperformed size models on both very \nhard MEDQA benchmarks and more accessible MedMCQA \nones, proving that it could conduct efficient reasoning based on \ncurrent structured medical knowledge. They also used Neo4j to \nmanage the knowledge graphs while adding external searches to \nensure the latest data. \nTiwari et al.[24] presented Auto-Cypher, a recent automated \npipeline for producing high-quality synthetic data for training \nLLMs by mapping natural language to Cypher queries for graph \ndatabases like Neo4j. The pipeline deployed the novelty of LLM-\nas-database-filler to synthesize Neo4j databases for the execution \nof generated queries to ensure their correctness. A sizable dataset \ncalled SynthCypher was created, spanning multiple domains and \ncomplex queries, leading to a 40% improvement in LLM \nperformance on Cypher generation. The datasets were used to \nfine-tune open-source models such as LLaMA, Mistral, and \nQwen, and the SPIDER benchmark was adapted for evaluation \npurposes. \nMohammed et al.[29] proposed a hybridized GraphRAG \nframework combining Neo4j-based UMLS knowledge graphs \nwith a vector store for medical textbooks to create an improved \nU.S.M.L.E.-style clinical question-answering approach. The \nproject integrated symbolic reasoning from knowledge graphs \nwith semantic retrieval performed on text embeddings to enhance \nrelevance and accuracy via adaptive re-ranking and query \nexpansion. The system had the answers produced by GPT-4o-\nMini, with different prompting strategies encouraging evidence-\nbased and traceable responses grounded in verified medical \nknowledge. Experiments showed that the hybrid approach \nimproved factual accuracy and citation fidelity as compared to \nthe L.L.M.-only approach, enhancing transparency and \nreliability. It is shown that binding both structured and \nunstructured medical knowledge sources could aid in \nameliorating hallucinations and hence improve clinical \ntrustworthiness in AI-driven medical QA. \nYang et al.[30] articulated sepsis knowledge graph was \ncrafted by combining multicenter clinical data from over 10,000 \npatients with the help of GPT-4 for entity recognition and \nrelationship extraction. Real-world data were collected from \nthree hospitals and integrated with clinical guidelines and \ndatabases from the public domain. The knowledge graph \ncontained 1,894 nodes and 2,021 relationships pertaining to \ndiseases, symptoms, biomarkers, treatments, and complications. \nGPT outperformed other models in every resolution on sepsis-\nspecific datasets to obtain high F1-score results. The constructed \ngraph highlighted complex interactions in sepsis for assisting \nclinical decision-making and was implemented on Neo4j. \nGuan et al.[20] proposed a novel method for constructing a \nlocal knowledge graph from retrieved biomedical documents by \nextracting propositional claims. They carried out layer wise \nsummarization on this graph to capture multi-document \nrelationships and provide comprehensive contextual information \nto a language model for question-answering purposes. The \nmethod resolved issues in multi-document biomedical QA, such \nas noise cancellation and efficient context usage. They then tested \ntheir method on several benchmarks for biomedical question \nanswering, achieving performance at least comparable to, if not \nbetter than, existing retrieval-augmented generation (RAG) \nbaselines. The study established enhanced reasoning and answer \naccuracy of the model achieved through structured graph \nsummarization.   \nPrevious studies have improved biomedical QA using KGs \nand LLMs, but important gaps remain. Most systems lack \ntransparent, graph-based justifications, rely on limited evaluation \nmethods, or depend on cloud resources that reduce privacy and \nreproducibility. Our framework addresses these gaps by \nproviding visible Cypher queries with evidence subgraphs, \napplying comprehensive performance metrics across difficulty \nlevels, and ensuring fully local, privacy-preserving deployment. \nTable I summarizes key previous studies on biomedical \nknowledge graphs and question answering, outlining their \nmethods, datasets, and main limitations. \nIII. PRELIMINARIES \nThis section outlines the fundamental concepts required to \nunderstand the proposed system. It introduces biomedical \nknowledge graphs, explains how Neo4j stores data in graph \nform, and describes the use of Cypher for querying. It also \nprovides a brief overview of large language models (LLMs) and \ntheir role in interpreting natural language. \n \n \nA. Biomedical Knowledge Graphs \nBiomedical Knowledge Graphs (BKGs) provide a structured \nrepresentation of complex biomedical information by modeling \ndiverse medical entities, such as diseases, drugs, symptoms, and \nbiological pathways, as interconnected nodes within a graph \nstructure. The edges in these graphs represent the semantic \nrelationships between these entities, including ’treats’, ’causes’, \n’interacts with’ and many others, as illustrated in Fig 1. This \nform of representation enables the integration of heterogeneous \nbiomedical data from a wide range of sources, including'
 'Omar & Mohammed / Journal of Applied Science and Technology Trends Vol. 06, No. 02, pp. 342 –357 (2025) \n \n344 \nevaluated using Exact Match, Precision, Recall, F1, Hits@k, \nMRR, and latency across simple, medium, and complex question \nsets. Unlike prior template-based methods, our approach enables \ntraceable, outcome-level validation. In summary, the main \ncontributions of this work are as follows: \n• Hybrid LLM to Cypher QA: A system that translates \nnatural language questions into accurate, executable \nCypher over a Neo4j drug, disease, and symptom KG. \n• Prompt-driven query generation: Schema,  entity \naware prompting that reliably maps diverse biomedical \nquestions to graph queries. \n• Evidence transparency: Along with each answer, we \nsurface the generated Cypher and the supporting \njustification subgraph (nodes, edges) plus a brief natural \nlanguage rationale. \n• Answer-level evaluation: End-to-end assessment using \nExact Match, F1, Precision/Recall, Hits@k, MRR and \nlatency across simple, medium and complex tiers. \n• Local, reproducible deployment: On-prem LLaMA 3 \ninference (no cloud dependency) suitable for biomedical \nsettings requiring low latency and strong data control. \n \nThe remainder of this paper is organized as follows: Section \n2 reviews related work on biomedical knowledge graphs and \nLLM-based QA systems. Section 3 provides background on \nknowledge graphs, large language models, and question \nanswering frameworks. Section 4 details the proposed \nmethodology, \nincluding \nsystem \narchitecture, \ndataset \nconstruction, and query translation. Section 5 presents the \nexperimental results through both quantitative metrics and \nqualitative case studies. Section 6 discusses the findings, analyzes \nlimitations, and compares performance against baseline models. \nFinally, Section 7 concludes this paper and outlines directions for \nfuture work. \nII. RELATED WORK \nRecently, studies have concentrated on the integration of \nclinical and medical knowledge graphs (LLM) to improve the \nanswer to medical questions. Researchers have derived several \nbiomedical KGs using Neo4j and incorporated the application of \nLLMs like LLaMA and GPT to convert natural language \nquestions into graph queries. Improvements in answer \ncorrectness, reduction of hallucination errors, one-to-many \nrelationships, and support for complex reasoning were the \nobjectives of these efforts. Some frameworks also adopted \nretrieval methods to ground responses in secure data. \nSu et al.[11] developed an integrative Biomedical Knowledge \nHub (iBKH), a huge biomedical knowledge graph that comprises \n18 of the very best data sources. The deployment of the iBKH in \nNeo4j allows for a user-friendly web portal to allow fast and \ninteractive knowledge retrieval. The system implemented \nadvanced graph learning techniques to enable the discovery of \nbiomedical knowledge, illustrated by an example of repurposing \nin silico drugs for Alzheimer’s disease. iBKH achieved \npromising predictive performance for known drugs and proposed \npossible new drug candidates. \nRajabi and Kafaie[19] proposed a disease knowledge graph \nusing a cross-referential disease database comprising diseases, \nsymptoms, and drugs interconnected with relationships. They \ntransferred the data into Neo4j to create a graph of 9,400 nodes \nand 45,000 relationships representing the semantic links between \nmedical concepts. Applying Cypher queries enabled answering \ncomplex medical questions regarding identifying drugs that may \ncause certain diseases; it was demonstrated that the graph inferred \nnew relationships not explicitly existing in the original data. The \nconclusion was that disease knowledge graphs sped up clinical \ndiscovery and contributed to understanding complex medical \nrelationships. \nHou et al.[3] assessed and contrasted ChatGPT (both GPT-\n3.5 and GPT-4) and the biomedical knowledge graphs (BKGs) \nconcerning their ability to answer biomedical questions, generate \nnew knowledge, and reason. Their datasets were focused on \ndietary supplements and drugs, while evaluation criteria entailed \naccuracy, novelty, and reasoning ability. The results indicate that \nwhile GPT-4 surpassed GPT-3.5 and BKGs in knowledge \nprovision, it proved inconsistent with regard to citations and \nreasoning. Compared to them, BKGs scored higher in accuracy \nand reliability, especially in discovering novel links as well as \nwithin structured reasoning. \nSoman et al.[13] presented a novel framework called KG-\nRAG that integrates a large biomedical knowledge graph \n(SPOKE) with LLaMA 2, GPT-3.5, and GPT-4 (LLMs) to \nproduce accurate biomedical text. They optimized the retrieval of \nrelevant graph context to cut over 50% tokens without losing \naccuracy. It aided LLMs in performing better on biomedical \nquestion answering with very high accuracy boosts, especially in \nthe case of LLaMA 2. They compared KG-RAG to other retrieval \nmethods and indicated its comparatively more robust and \nefficient results. The framework produced reliable evidence-\nbased answers grounded in biomedical knowledge. \nLuo et al.[23] created ChatKBQA, a new framework with a \nquestion-and-answer approach over knowledge bases that first \ngenerates logical forms with the help of fine-tuned LLMs and \nthen retrieves the relevant entities and relations. This generate-\nthen-retrieve approach is supposed to handle a couple of issues \nwith the earlier methods concerning tedious retrieval and error \npropagation. They fine-tuned open-source LLMs like LLaMA 2 \nto change natural-language questions into logical forms with high \naccuracy. The retrieval phase uses unsupervised phrase-level \nsemantic matching in a way that enhances the alignment of \nentities and relations. Experiments on benchmark datasets \nindicate ChatKBQA to be superior to its predecessors, with the \nhighest accuracy to date. \nPusch and Conrad[6] conducted work under a hybrid \napproach conflating LLMs and biomedical Knowledge Graphs \n(KGs) to suppress hallucination errors in question-answering. \nThey proposed query-checking algorithms for validating, \ncorrecting, and executing the KG Cypher queries that LLMs \ngenerated, thereby attaining accurate and understandable \nanswers. The system used retrieval-augmented generation (RAG) \nto ground answers within KG data. The methodology was \nvalidated on a biomedical KG called PrimeKG using 50 \nbenchmark questions, assessing models like GPT-4 Turbo and \nLLaMA 3. Commercially available GPT-4 Turbo obtained'
 'Omar & Mohammed / Journal of Applied Science and Technology Trends Vol. 06, No. 02, pp. 342 –357 (2025) \n \n343 \nfor visualizing and validating LLM outputs [16], and MedKA for \nKG-enhanced question answering [17]. \nTo address these challenges, several recent works have \nexplored the integration of large language models with \nbiomedical knowledge graphs (KGs). A biomedical KG is a \nstructured network that connects entities such as diseases, drugs, \nand symptoms using defined relationships [18, 19]. These graphs \nstore verified medical knowledge from trusted databases, \nallowing for more accurate and explainable responses [12]. KGs \nare especially useful in multi-step reasoning tasks, where finding \nan answer requires connecting different pieces of information \n[20]. These entities and relationships can be visually represented \nin a biomedical knowledge graph, as shown in Fig. 1, where \nnodes represent medical concepts such as drugs, diseases, \nsymptoms, and pathways, and edges denote their semantic \nrelationships. \n \nFig. 1. Overview of Biomedical Knowledge Graph Entities and Relationships \n \nOne example of a widely used biomedical KG is SPOKE, \nwhich includes millions of nodes and relationships from over 40 \nbiomedical databases [12]. Integrating an LLM with a KG allows \nthe strengths of both technologies to work together: the LLM \nprovides language understanding, and the KG provides \nstructured, factual knowledge [21, 22]. A common method is \nretrieval-augmented generation (RAG), where the LLM retrieves \ninformation from the KG and uses it to generate more accurate \nresponses [13, 23]. In more advanced setups, the LLM can even \ngenerate queries like Cypher to fetch specific data from the graph \n[24, 25]. Neo4j is a popular graph database that supports fast and \nflexible storage and querying of knowledge graphs using Cypher \n[21]. It is well-suited for biomedical applications because it \nallows easy exploration of complex medical relationships. Recent \nwork has shown that combining Neo4j with LLMs can lead to \nbetter accuracy, fewer hallucinations, and more explainable \nresults [24, 26]. \nDespite improvements, building a reliable hybrid system that \ncombines an LLM with a biomedical KG remains a technical \nchallenge. Some approaches require complex pipelines or large \ntraining datasets, while others rely on fine-tuning specific to a \nnarrow set of questions [27, 28]. There is still a need for systems \nthat are both accurate and easy to scale, especially in domains like \nmedicine where the cost of errors is high [22]. \nRecent advances in KG-augmented LLMs have improved \nperformance, yet biomedical QA continues to face three practical \ngaps: \n1. Traceability: LLM-only or text-retrieval-only pipelines \nrarely provide graph-grounded justifications; users lack the \nability to inspect the exact nodes and edges that support an \nanswer. \n2. Evaluation: Prior work often judges quality via surface-\nform checks (e.g., matching a Cypher template), which fails to \ncapture end-to-end answer correctness or ranking quality \nacross different difficulty levels. \n3. Deployment: Many solutions assume cloud resources or \ndomain-specific fine-tuning, yet biomedical contexts typically \ndemand a local, privacy-preserving system with low latency \nand reproducible behavior. \nTimestamp-aware execution and periodic KG refresh help \navoid deprecated or contraindicated links, making the tool better \nsuited for safety-critical clinical contexts (e.g., drug–drug \ninteractions). \nTo address these limitations, Our work introduces a locally \ndeployable pipeline that translates biomedical questions into \nexecutable Cypher queries over a Neo4j knowledge graph. The \nsystem returns answers with supporting nodes and edges, and is'
 'Vol. 06, No. 02, pp. 342 –357 (2025) \nISSN: 2708-0757 \n \nJOURNAL OF APPLIED SCIENCE AND TECHNOLOGY TRENDS \n \nwww.jastt.org  \n \n                                                          342 \ndoi: 10.38094/jastt62404  \nA Hybrid LLM–Knowledge Graph Framework for Accurate \nBiomedical Question Answering \n \nHavraz Y. Omar1,²*, Abdulhakeem O. Mohammed³  \n \n¹Department of Information Technology, Technical College of Duhok, Duhok Polytechnic University, Duhok, Kurdistan Region, \nIraq. havraz.omar@dpu.edu.krd  \n² Department of Information Technology, Technical College of Informatics – Akre, Akre University for Applied Sciences, Akre, \nKurdistan Region, Iraq. \n³ Department of Computer Science, College of Science, University of Zakho, Zakho, Kurdistan Region, Iraq. \na.mohammed@uoz.edu.krd \n \n*Correspondence: havraz.omar@dpu.edu.krd \n \nAbstract \nBiomedical question answering requires accurate and interpretable systems; however, existing approaches often face challenges such as \nlanguage model hallucinations and limited reasoning when relying solely on standalone knowledge graphs. To address these limitations, \nthis study proposes a hybrid framework that integrates the LLaMA-3B language model with a Neo4j-based drug–disease–symptom \nknowledge graph. The system translates natural language questions into executable Cypher queries, operates on an iBKH-derived graph \ncomprising over 65,000 entities and 3 million relationships, and returns answers with supporting evidence through a transparent interface. \nExperiments conducted on 60 biomedical questions across three levels of difficulty demonstrate the robustness of the approach: 96% \nexact match for simple queries, 95% for medium queries, and 86.7% for complex queries. Overall, the system achieves Precision@5 of \n96.1%, Recall@5 of 89.0%, F1@5 of 91.0%, Hits@k of 96.1%, and an MRR of 94.4%, while maintaining an average response time of \nonly 6.07 seconds. These results indicate that the system retrieves nearly all relevant answers, ranks them correctly, and delivers them \nwith latency low enough for interactive use. Moreover, unlike cloud-based APIs such as ChatGPT, which require internet connectivity \nand external data transmission, the proposed framework operates fully offline, ensuring privacy, reproducibility, and compliance with \nbiomedical data governance. Overall, this pipeline provides an accurate, efficient, and privacy-preserving solution for biomedical question \nanswering, making it a practical alternative to cloud-dependent approaches in sensitive healthcare contexts. \n \nKeywords: Knowledge Graph, LLM, Question Answering, Neo4j, Biomedical Informatics, Healthcare AI, LLaMA 3. \n \nReceived: August 14th, 2025 / Revised: October 10th, 2025 / Accepted: October 16th, 2025 / Online: October 20th, 2025 \n \nI. INTRODUCTION \nAnswering questions in the biomedical field is a difficult task \ndue to the complexity of medical knowledge and the need for \nprecision. In recent years, large language models  (LLMs) like \nLLaMA, GPT-4 have made progress in understanding and \ngenerating human-like responses to medical questions  [1,  2]. \nThese models can process large amounts of information and \nrespond in natural language, which makes them helpful in \nhealthcare settings [3]. However, they often struggle to provide \naccurate answers when dealing with specialized biomedical \ncontent [4, 5]. \nOne major issue with LLMs is a problem called hallucination, \nwhere the model generates information that sounds right but is \nactually incorrect or unsupported [6]. In medical applications, \nthis can be dangerous, as healthcare professionals rely on precise \nand trustworthy information  [7]. Therefore, researchers are \nexploring ways to combine LLMs with structured sources of \nknowledge to improve their reliability [8]. \nLLM-only systems in biomedicine still hallucinate and are \nhard to verify, limiting safe use [9, 10]. Biomedical knowledge \ngraphs (BKGs) such as iBKH and SPOKE curate multi-source \nfacts and enable multi-hop reasoning, yet they neither interpret \nfree text nor generate answers [11, 12]. Recent hybrids (KG-\naware RAG) improve grounding but often lack explicit path-level \njustifications and robust end-to-end answer evaluation [13, 14]. \nRecent studies have increasingly integrated Knowledge \nGraphs (KGs) with Large Language Models (LLMs) to improve \nfactual accuracy, reasoning, and reduce hallucinations. Notable \nexamples include DR.KNOWS, which combines UMLS-based \nKGs with LLMs for better diagnostic reasoning [15], KnowNet']","['<1-hop>\n\n8 | GigaScience , 2025, Vol. 14 Table 5. Ablation study of the KGT fr ame work under Code-Llama-13B ROUGE (%) Method GPT-4 Eval (%) BERTScore (%) Recall Precision F1 score KGT (ours) 92.4 97.7 87.4 87.7 86.8 Without GSBI 71.8 95.5 68.1 69.8 66.8 Without QA&GSBI 69.7 94.7 55.0 66.3 58.2 Without QA&GSBI&SC 24.7 77.4 14.8 12.3 12.2 compared to the complete KGT, indicating that each of the 3 mod- ules is vital for the final performance . Furthermore , such obser- vations confirm that our KGT can indeed le v er a ge knowledge to enhance the final performance of LLMs. Implementation settings Our knowledge gr a ph is quite large, with a complex schema, and typicall y involv es input tokens within 1,300. Our experiment does not r equir e fine-tuning, and the infer ence time is r elated to the model size and computational resources. For example, when us- ing our method, KGT, with the Code-Llama-13B model on an 80 GB A100 GPU, it occupies 33 GB of VRAM. Without any acceleration fr ame works, the infer ence r equir es 4 passes, eac h taking ar ound 20 seconds. Case studies Drug repositioning Drug r epositioning emer ges as a pr omising str ategy to acceler ate the process of drug development. This approach involves identi- fying new therapeutic uses for existing drugs, thereby saving time and r esources typicall y r equir ed for bringing a ne w drug to market [ 45 ]. Our system is capable of investigating the potential reposi- tioning of carteolol for the treatment of hemangiomas . T he exam- ple is shown in Supplementary Table S2 and r elational dia gr am is shown in Fig. 4 A. Utilizing the system’s knowledge gr a ph, a r ela- tional chain is delineated, illustrating that propranolol, another inhibitor of ADRB1, is effectiv el y emplo y ed in the treatment of hemangiomas . T he system harnesses this insight to formulate a hypothesis that carteolol, by virtue of its similar mechanism of inhibition, could be potentially repositioning for treating heman- giomas [ 46 ]. This hypothesis would serve as a precursor to clin- ical trials and r esearc h, potentiall y expediting the availability of an additional ther a peutic option for patients with hemangiomas. Drug resistance resear c h Drug resistance in cancer treatment poses a significant challenge in clinical oncology. Understanding the genetic basis of resistance can lead to mor e effectiv e tr eatment str ategies and personalized medicine a ppr oac hes. Researc h in drug r esistance involv es deter- mining why certain cancer-carrying mutated genes are not re- sponsive to specific drugs and finding ways to overcome this re- sistance [ 47 ]. Our system is capable of exploring drug resistance in cancer. The example is shown in Supplementary Table S3 , and a relational diagram is shown in Fig. 4 B. The KG data indicate that the ALK-p.L1196M m utation, whic h is associated with gas- tric cancer, has a known resistance to nalatinib [ 48 , 49 ]. The LLM processes this information and infers that due to this resistance, nalatinib might not be an effective medication for treating can- cers caused by the ALK-p.L1196M mutation. The case highlights the critical importance of understanding specific gene–drug in- teractions in drug resistance research. It demonstrates how cer- tain gene mutations could render a drug ineffective, which in turn could guide oncologists in choosing alternative treatments or de- v eloping ne w drugs that can bypass or tar get the r esistance mec h- anisms. By accelerating the process of understanding drug resis- tance, these artificial intelligence–driven systems can contribute to impr ov ed patient outcomes and the optimization of cancer tr eatment pr otocols.'
 '<2-hop>\n\nIndividualized treatment Details on individualized tr eatment ar e pr ovided in Supplementary Case Studies A . It is important to note that this example is included solely to illustrate the technical capabilities of the proposed method. The output generated in this example has not been validated for clinical use, and further validation in clinical settings would be r equir ed befor e an y suc h a pplication. Selection and understanding of biomarkers Details on selection and understanding of biomarkers are pro- vided in Supplementary Case Studies B . Discussion In this article, we introduce a novel framework KGT, which em- ploys LLMs for reasoning on the schema of KGs, to enhance the reasoning abilities of LLMs in areas with missing domain data by utilizing domain-specific knowledge gr a phs, suc h as oncology knowledge gr a phs, ther eby addr essing the issue of factual hal- lucinations in LLMs. Our method excels in extr acting, v alidat- ing, and refining factual knowledge throughout the LLMs’ rea- soning process. It seamlessly integrates with various LLMs, in- cluding open-source models like Code-Llama, and enhances the capabilities of LLMs solely through prompt engineering and in- context learning without any fine-tuning. This grants it significant generalizability. We possess an extensive oncology knowledge graph and have established a benchmark based on it to e v aluate the ca pabilities of various methods. When tested on PcQA using various open-source LLMs, the KGT fr ame work performs exceptionall y well, sur passing the current best methods by 33%. This significant improvement positions our a ppr oac h as a pioneering benchmark in biomedical KGQA, setting a new standard that advances beyond previously established best pr actices. Additionall y, thr ough case studies, our a ppr oac h has been shown to effectiv el y pr ovide ther a peutic plans, gener ate v aluable hypotheses for drug r epositioning, identify po- tential drug targets, and study drug resistance . T his underscores the pr actical v alue of the KGT fr ame work in deliv ering insight- ful contributions that aid in the de v elopment and optimization of treatment strategies. Each case study’s conclusions are further v alidated by e vidence fr om pr e viousl y published r esearc h pa pers, enhancing the credibility and impact of our findings. Ho w e v er, it is important to note that the constructed QA dataset and the corresponding published subset of the SOKG Downloaded from https://academic.oup.com/gigascience/article/doi/10.1093/gigascience/giae082/7943459 by guest on 11 January 2026'
 '<3-hop>\n\nOmar & Mohammed / Journal of Applied Science and Technology Trends Vol. 06, No. 02, pp. 342 –357 (2025) 345 record-high accuracy, while open-source models achieved impressive gains through prompt optimization. Feng et al.[22] developed the Knowledge Graph-based Thought (KGT) framework that integrated LLMs with a pan- cancer knowledge graph for biomedical question answering. KGT was designed to reason on the knowledge graph schema and identify optimal subgraphs to use for directing accurate answer generation, all without fine-tuning the LLMs. The framework is benchmarked against a new dataset (PcQA) designed specifically for pan-cancer KGQA tasks and has outperformed all existing state-of-the-art approaches by a rather large margin. KGT’s practicality in biomedical issues was highlighted through case studies for drug repositioning, drug resistance, and biomarker discovery. Their approach exhibited robust adaptability among various LLMs. Rezaei et al.[26] developed AMG-RAG, a dynamic framework that utilizes autonomous LLM agents with medical search tools in the continuous construction and real-time updating of Medical Knowledge Graphs (MKGs). Their system incorporated confidence scoring and multi-hop reasoning to improve accuracy and interpretability in medical question answering. AMG-RAG outperformed size models on both very hard MEDQA benchmarks and more accessible MedMCQA ones, proving that it could conduct efficient reasoning based on current structured medical knowledge. They also used Neo4j to manage the knowledge graphs while adding external searches to ensure the latest data. Tiwari et al.[24] presented Auto-Cypher, a recent automated pipeline for producing high-quality synthetic data for training LLMs by mapping natural language to Cypher queries for graph databases like Neo4j. The pipeline deployed the novelty of LLM- as-database-filler to synthesize Neo4j databases for the execution of generated queries to ensure their correctness. A sizable dataset called SynthCypher was created, spanning multiple domains and complex queries, leading to a 40% improvement in LLM performance on Cypher generation. The datasets were used to fine-tune open-source models such as LLaMA, Mistral, and Qwen, and the SPIDER benchmark was adapted for evaluation purposes. Mohammed et al.[29] proposed a hybridized GraphRAG framework combining Neo4j-based UMLS knowledge graphs with a vector store for medical textbooks to create an improved U.S.M.L.E.-style clinical question-answering approach. The project integrated symbolic reasoning from knowledge graphs with semantic retrieval performed on text embeddings to enhance relevance and accuracy via adaptive re-ranking and query expansion. The system had the answers produced by GPT-4o- Mini, with different prompting strategies encouraging evidence- based and traceable responses grounded in verified medical knowledge. Experiments showed that the hybrid approach improved factual accuracy and citation fidelity as compared to the L.L.M.-only approach, enhancing transparency and reliability. It is shown that binding both structured and unstructured medical knowledge sources could aid in ameliorating hallucinations and hence improve clinical trustworthiness in AI-driven medical QA. Yang et al.[30] articulated sepsis knowledge graph was crafted by combining multicenter clinical data from over 10,000 patients with the help of GPT-4 for entity recognition and relationship extraction. Real-world data were collected from three hospitals and integrated with clinical guidelines and databases from the public domain. The knowledge graph contained 1,894 nodes and 2,021 relationships pertaining to diseases, symptoms, biomarkers, treatments, and complications. GPT outperformed other models in every resolution on sepsis- specific datasets to obtain high F1-score results. The constructed graph highlighted complex interactions in sepsis for assisting clinical decision-making and was implemented on Neo4j. Guan et al.[20] proposed a novel method for constructing a local knowledge graph from retrieved biomedical documents by extracting propositional claims. They carried out layer wise summarization on this graph to capture multi-document relationships and provide comprehensive contextual information to a language model for question-answering purposes. The method resolved issues in multi-document biomedical QA, such as noise cancellation and efficient context usage. They then tested their method on several benchmarks for biomedical question answering, achieving performance at least comparable to, if not better than, existing retrieval-augmented generation (RAG) baselines. The study established enhanced reasoning and answer accuracy of the model achieved through structured graph summarization. Previous studies have improved biomedical QA using KGs and LLMs, but important gaps remain. Most systems lack transparent, graph-based justifications, rely on limited evaluation methods, or depend on cloud resources that reduce privacy and reproducibility. Our framework addresses these gaps by providing visible Cypher queries with evidence subgraphs, applying comprehensive performance metrics across difficulty levels, and ensuring fully local, privacy-preserving deployment. Table I summarizes key previous studies on biomedical knowledge graphs and question answering, outlining their methods, datasets, and main limitations.'
 '<4-hop>\n\nIII. PRELIMINARIES This section outlines the fundamental concepts required to understand the proposed system. It introduces biomedical knowledge graphs, explains how Neo4j stores data in graph form, and describes the use of Cypher for querying. It also provides a brief overview of large language models (LLMs) and their role in interpreting natural language. A. Biomedical Knowledge Graphs Biomedical Knowledge Graphs (BKGs) provide a structured representation of complex biomedical information by modeling diverse medical entities, such as diseases, drugs, symptoms, and biological pathways, as interconnected nodes within a graph structure. The edges in these graphs represent the semantic relationships between these entities, including ’treats’, ’causes’, ’interacts with’ and many others, as illustrated in Fig 1. This form of representation enables the integration of heterogeneous biomedical data from a wide range of sources, including']","The KGT (Knowledge Graph-based Thought) framework leverages biomedical knowledge graphs to enhance large language models' (LLMs) reasoning capabilities and improve biomedical question answering (QA) performance through the following key mechanisms, as detailed in the provided context:

1. **Integration of LLMs with Biomedical Knowledge Graphs (KGs):**  
   KGT integrates LLMs with a large-scale, domain-specific biomedical knowledge graph—in this case, a pan-cancer KG—to combine the language understanding strengths of LLMs with the structured, factual, and relational knowledge encoded in the KG. This hybrid approach allows the system to utilize verified biomedical facts and relationships, which are difficult for LLMs alone to reliably generate.

2. **Reasoning on Knowledge Graph Schema and Subgraph Selection:**  
   The framework is designed to reason explicitly over the KG schema and identify optimal subgraphs relevant to the input question. By selecting pertinent subgraphs, KGT directs the LLM to focus on the most relevant biomedical entities and relationships, enabling multi-hop reasoning that connects disparate pieces of information necessary for complex biomedical queries.

3. **Translation of Natural Language Questions into Executable Graph Queries:**  
   KGT translates natural language biomedical questions into executable Cypher queries that run on the Neo4j graph database hosting the biomedical KG. This translation enables precise retrieval of structured data from the KG, ensuring that answers are grounded in verified biomedical knowledge rather than generated solely from language patterns.

4. **Answer Generation Without Fine-Tuning LLMs:**  
   The framework achieves accurate answer generation without requiring fine-tuning of the underlying LLMs. Instead, it leverages the KG to guide reasoning and retrieval, which reduces dependency on large labeled datasets and enhances adaptability across different LLMs.

5. **Transparent Evidence and Justification:**  
   KGT provides transparent outputs by returning not only the answer but also the supporting evidence subgraph (nodes and edges) from the KG along with the generated Cypher query and a natural language rationale. This transparency allows users to inspect the exact biomedical facts and relationships that support the answer, improving trust and interpretability.

6. **Robust Performance and Adaptability:**  
   Benchmarking on the PcQA dataset (focused on pan-cancer questions) shows that KGT outperforms state-of-the-art KGQA methods by a significant margin. The framework demonstrates robustness across various LLMs and practical biomedical use cases such as drug repositioning, drug resistance, and biomarker discovery.

7. **Local, Privacy-Preserving Deployment:**  
   Unlike cloud-based LLM APIs, KGT operates fully offline with local deployment of LLMs and Neo4j KG, ensuring privacy, reproducibility, and compliance with biomedical data governance. This is critical in sensitive healthcare contexts where data security and low latency are essential.

In summary, the KGT framework enhances biomedical question answering by combining the natural language understanding of LLMs with the structured, multi-hop reasoning capabilities of biomedical knowledge graphs. It translates questions into precise graph queries, retrieves and reasons over relevant subgraphs, and generates accurate, evidence-backed answers with transparent justifications—all without fine-tuning the LLMs and while maintaining privacy and efficiency.","The KGT framework enhances large language models' (LLMs) reasoning capabilities by employing biomedical knowledge graphs (KGs) to provide structured domain-specific knowledge. It reasons on the schema of these KGs to identify optimal subgraphs that guide accurate answer generation without requiring fine-tuning of the LLMs. This approach addresses the issue of factual hallucinations by extracting, validating, and refining factual knowledge during the reasoning process. The framework integrates seamlessly with various LLMs, including open-source models like Code-Llama, and improves performance solely through prompt engineering and in-context learning. Evaluations on a pan-cancer question answering benchmark (PcQA) demonstrate that KGT surpasses existing state-of-the-art methods by 33%, establishing a new standard in biomedical KGQA. Additionally, case studies show KGT's practical value in generating therapeutic plans, drug repositioning hypotheses, identifying drug resistance mechanisms, and biomarker discovery, all supported by evidence from prior research. Thus, by combining LLMs with biomedical knowledge graphs, KGT significantly improves the accuracy, interpretability, and efficiency of biomedical question answering.",1.0,0.9857663812835605,0.99999999998,0.5714285714285714
"What evaluation methods and performance metrics have been used to compare RAG-enhanced LLMs with baseline LLMs in clinical tasks, and how do these methods reflect the accuracy and usefulness of the models?","['was asked about medications for peripheral artery disease \npatients without increased bleeding risk, it initially omitted \nlow-dose rivaroxaban. After integrating retrieved text from \nthe 2024 American College of Cardiology / American Heart \nAssociation Guideline for the Management of Lower Extrem\xad\nity Peripheral Artery Disease,10 the model correctly recom\xad\nmended rivaroxaban.\nSeveral guidelines exist for evaluating Artificial Intelligence \n(AI) applications and LLMs in healthcare, including \nDECIDE-AI (Developmental and Exploratory Clinical Inves\xad\ntigations of DEcision support systems driven by Artificial \nIntelligence),11 CLAIM (Checklist for Artificial Intelligence in \nMedical Imaging),12\nand CONSORT-AI (Consolidated \nStandards of Reporting Trials-AI).13 In addition, Tam et al. \nintroduced QUEST, a framework specifically for human eval\xad\nuation of LLMs in healthcare.14 However, these guidelines \ndo not cover RAG applications in clinical settings, emphasiz\xad\ning the need for a more specific guideline.\nDespite the promise of RAG in improving LLM performance \nin clinical settings, there is limited understanding of its overall \neffectiveness comparing with the baseline LLM, adoption in \nclinical domains, and optimal strategies for its development in \nbiomedical applications. The aim of this study is to synthesize \nfindings from recent research of RAG and LLM in biomedicine \nand provide clinical development guidelines to improve effec\xad\ntiveness as well as transparency in future research.\nMaterials and methods\nStudy design\nWe conducted a systematic literature review. The report was \ncreated in adherence to the Preferred Reporting Items for Sys\xad\ntematic Reviews and Meta-Analyses 2020 analysis checklist \n(File S1).15 We also performed a meta-analysis following the \nCochrane Handbook guidelines.16\nScreening papers\nWe searched in 3 databases (PubMed, Embase, and Psy\xad\ncINFO), using terms related to “retrieval augmented gener\xad\nation” and “large language model.” PubMed and Embase \nwere selected based on recommendations from the Cochrane \nHandbook, as they are widely recognized for their extensive \ncoverage of biomedical literature.17 PsycINFO was included \nto capture articles from the psychological and behavioral sci\xad\nences. To maintain a high standard of quality and reliability, \nwe focused on peer-reviewed articles and excluded preprints. \nThe specific search terms used for each database are provided \nin File S2. Given that ChatGPT was released on November \n30, 2022, we set the publication filter to search papers pub\xad\nlished in 2023 and 2024. The search was performed on \nDecember 12, 2024. The inclusion criteria were: (1) the study \nmust compare baseline LLM performance with RAG per\xad\nformance and (2) the study must address a biomedical ques\xad\ntion. The exclusion criteria were: (1) literature reviews, \neditorial comments, or viewpoint papers, (2) studies focusing \non LLMs in languages other than English, or (3) studies cen\xad\ntered on a multi-agent system without a focus on RAG. SL \nscreened titles and abstracts, then conducted a full-text \nreview of papers meeting the criteria.\nData extraction\nFor each included study, we extracted the following informa\xad\ntion: author, title, publication year, journal, clinical task, and \nspecialty. Regarding RAG techniques, we gathered details \nabout the baseline LLM, retrieval sources, and strategies used \nin the pre-retrieval, retrieval, and post-retrieval stages. For \nevaluation, we extracted the evaluation method (human, \nautomated, or a combination of both), the number of evalua\xad\ntors, the evaluation dataset, and the evaluation metrics.\nMeta-analysis\nEffect size was defined as a metric quantifying the relation\xad\nship between variables, including both direction and magni\xad\ntude.18 For each included study, we calculated the effect size \nbetween baseline LLM performance and RAG-enhanced \nLLM performance. The outcomes focused on the perform\xad\nance of generation results, such as accuracy and usefulness. \nMetrics related to the retrieval process, cost, or speed were \nnot included as outcomes in the meta-analysis. For continu\xad\nous outcomes, we used Cohen’s d, standardized between- \ngroup mean difference (SMD), calculated as the difference in \nmeans divided by the pooled standard deviation. The stand\xad\nard error (SE) of SMD was calculated using the following for\xad\nmula (1), where n1 and n2 represent the sample sizes of each \ngroup.19 For dichotomous measurements, we calculated the \nlog-odds ratio, obtained by transforming the odds ratio (OR) \nwith the natural logarithm, and the associated SE was calcu\xad\nlated using formula (2), where a, b, c, and d represent the \nnumber of successful and failed events in the baseline LLM \nand RAG-enhanced LLM approaches. For studies reporting \nmultiple outcomes, we used the overall outcome to calculate \neffect size. If no overall outcome was reported, we averaged \nthe effect sizes of all reported outcomes. We excluded out\xad\ncomes with a sample size of less than 30 to avoid small- \nsample bias. \nSESMD ¼\nffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi\nn1 þ n2\nn1n2\nþ\nSMD2\n2 n1 þ n2\nð\nÞ\ns\n(1) \nSElogOR ¼\nffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi\n1\na þ 1\nb þ 1\nc þ 1\nd\nr\n(2) \nWe developed a random-effect meta-analysis model, \nbecause of the variability in RAG architectures and evalua\xad\ntion datasets among the included studies. The random-effect \nmodel was used when individual study effects likely con\xad\ntained additional sources of variance beyond sampling error. \nBetween-study heterogeneity was assessed using Higgins & \nThompson’s I2 statistic, where 25% indicated low heteroge\xad\nneity, 50% moderate, and 75% substantial.20\nWe conducted subgroup analyses to explore performance \nvariations across different factors. First, we analyzed the \ninfluence of the baseline LLM, referring to the foundation \nmodel (eg, GPT-4 or Llama2) that provides the core architec\xad\nture for the system. Second, we examined data retrieval strat\xad\negies, categorizing them as simple or complex. Simple \nstrategies included fixed-length chunking and basic similarity \nsearch, and we performed a subgroup analysis to compare \nthese with complex retrieval strategies. Third, we analyzed \ndifferences based on evaluation methods, distinguishing \nbetween human evaluations, such as Likert scale ratings for \nhelpfulness and accuracy, and automatic evaluation metrics, \nincluding ROUGE-1 and BLEU. Finally, we conducted a sub\xad\ngroup analysis based on the type of task, classifying studies \n606                                                                                                      \nJournal of the American Medical Informatics Association, 2025, Vol. 32, No. 4 \nDownloaded from https://academic.oup.com/jamia/article/32/4/605/7954485 by guest on 11 January 2026'
 'conducting safety checks by applying 24 predefined rules to \nensure ethical and factual accuracy, and summarizing the \nresults.30 Glicksberg et al. developed an ensemble model that \ncombined structured and unstructured data to predict hospi\xad\ntal admission probabilities. These predicted probabilities, \nalong with similar historical cases, were incorporated into the \nprompt to enhance the performance of LLM.37 Chen et al. \nused Chain-of-Thought (CoT) prompting to improve LLM \nreasoning capabilities.39 Kresevic et al. customized prompts \nto help the model interpret structured guidelines, combined \nwith few-shot learning using 54 question-answer pairs.27\nJeong et al. fine-tuned LLMs to assess the relevance of \nretrieved evidence, ensure all statements were evidence-based, \nand confirm that the response effectively addressed the \nquery.34\nEvaluation\nNine studies used human evaluation, 8 relied on automated \nevaluation (eg, similarity comparisons between generated \nsentences and original answers), and 3 used a mix of both. \nOutcomes from human evaluation showed an overall OR of \n1.65 (95% CI: 1.36-2.03), while automatic evaluation \nresulted in an OR of 1.20 (95% CI: 1.1-1.41). The differen\xad\nces between the 2 were statistically significant (P < .01). \nThere were 4 human evaluators on average, with the range \nspanning from 1 to 10. Most human evaluators were \nphysicians from relevant specialties according to the \nstudy focus. In one case, 3 diabetic patients were involved in \nevaluating the understandability of diabetes-related patient \nqueries.30\nTwelve studies used self-curated datasets focused on \nresearch tasks. Examples included the ClinicalQA bench\xad\nmark, which comprised 314 open-ended questions about \ntreatment guidelines and clinical calculations generated by \nphysicians,28 and 43 diabetes-related questions sourced from \nthe National Institute of Diabetes and Digestive and Kidney \nDiseases website.30 Simulated cases from medical examina\xad\ntions were also utilized.25\nThree studies used EHR \ndata.35,37,40 Six studies used public benchmark datasets, such \nUS board exam practice questions, MedMCQA29,34 and \nlongform question-answering benchmarks (eg, LiveQA, Med\xad\nicationQA).34 The self-curated datasets averaged 76 ques\xad\ntions, ranging from 7 to 314. The length of public benchmark \ndatasets varied significantly, from 102 questions in the \nLiveQA dataset28 to 194 000 questions in the MedMCQA \ndataset.34\nMost studies reported evaluation metrics for the final \nresponse generation, while 4 (25%) also included specific \nmetrics to evaluate the retrieval process. For instance, 1 study \nmeasured recall in context retrieval,24 another evaluated \nretrieval accuracy,33 and a fine-tuned LLM was developed to \nassess the relevance of retrieved information to the user’s \nquery.34 Additionally, 1 study evaluated the accuracy of \nusing LLMs to extract text from figures and tables during \ndocument preprocessing.27 The final evaluation metrics \nfocused on the generated responses, consistent with those \nused in LLM-only systems. These metrics could be catego\xad\nrized as accuracy, completeness, user perception, safety, hal\xad\nlucination, citation, bias, and language. Accuracy was the \nmost frequently reported metric, covering Likert scale rat\xad\nings, match rates, correct treatment percentages,9 AUC, \nAUPRC, and F1 scores, as well as text similarity metrics like \nROUGE \n(ROUGE-1, \nROUGE-2, \nROUGE-L), \nBLEU, \nMETEOR, and BERTScore,21\nwhich compared LLM- \ngenerated responses to expert-provided answers. Complete\xad\nness metrics assessed whether responses included all neces\xad\nsary information, typically using Likert scales. User \nperception captured subjective feedback from both healthcare \nproviders and patients on understandability, helpfulness, and \nwhether responses met user intent, usually using Likert scales. \nSafety metrics focused both on user-related and system- \nrelated aspects. These metrics assessed potential harm, adver\xad\nsarial safety, and risk management,24 ensuring that outputs \nwere free of harmful content or risks. Scientific validity and \nadherence to evidence were also evaluated.29 One study used \nadversarial prompting, defined as intentionally adding harm\xad\nful directives to a prompt, to evaluate the safety of the RAG \nsystem.28 Hallucinations were primarily identified through \nmanual review, with definitions varying across studies. Some \nstudies defined hallucinations as nonfactual information, \nwhile one study added 2 other types of hallucinations: input- \nconflicting (content deviating from user-provided input) and \ncontextual-conflicting (content conflicting with previously \ngenerated information).27,41 Citation metrics measured the \naccuracy of provided references, with valid references consid\xad\nered those that pointed to established publications, guide\xad\nlines, or research. Bias and language were evaluated for \nclarity and neutrality, ensuring responses were unbiased and \nempathetic to patient concerns.24\nDiscussion\nThis study presents a systematic review of current research \non RAG for clinical tasks. Overall, RAG implementation \nincreased outcomes by 1.35 times compared to baseline \nLLM. We analyzed clinical tasks, baseline LLMs, retrieval \nsources and strategies, as well as evaluation methods. Despite \nthe potential benefits of RAG systems, there remains room \nfor improvement. Building on our literature review, we devel\xad\noped GUIDE-RAG (Guidelines for Unified Implementation \nand Development of Enhanced LLM Applications with RAG \nin Clinical Settings) for future clinical applications using \nRAG (Figure 4).\nGUIDE-RAG:\n1) Define clear clinical tasks and evaluation datasets. \nFuture research should clearly define clinical tasks and \nquestions to maximize the effectiveness of RAGs. Ambi\xad\nguity in questions can hinder performance, particularly \nin less powerful LLMs, making it challenging to achieve \nsignificant improvements in responses generation, even \nwith improved knowledge selection.42 For example, one \nstudy in the review constructed the evaluation dataset \nusing 30 case reports on rare diseases from PubMed.38\nThe authors did not report human performance on the \nself-build dataset. The questions themselves might have \nbeen inherently challenging or ambiguous. As expected, \nthe reported performance showed modest improvement, \nwith an OR of 1.31. \n2) Identify appropriate external resources for specific clini\xad\ncal tasks. \nThe first step in developing a RAG-based clinical system \nis to identify external resources that fill the knowledge \ngaps of the baseline LLM in relation to specific clinical \ntasks. The external knowledge should complement the \nLLM’s existing capabilities to effectively address task \n610                                                                                                      \nJournal of the American Medical Informatics Association, 2025, Vol. 32, No. 4 \nDownloaded from https://academic.oup.com/jamia/article/32/4/605/7954485 by guest on 11 January 2026'
 'Clinical applications of RAG\nRAG techniques have been applied across a broad range of \nmedical specialties, as shown in Table 1. These applications \ninclude clinical decision-making and medical question- \nanswering. In clinical decision making, RAG has supported \npersonalized treatment,23,24 emergency triage,25 and disease \nmanagement.26,27 For medical question-answering, RAG’s \ncapability has been explored to address complex treatment \nguidelines questions,28 as well as queries focused on specific \nareas, such as head and neck surgery-related questions,29 and \npatient questions regarding diabetes.30 In the subgroup anal\xad\nysis, 13 studies focused on clinical decision-making (OR \n1.46, 95% CI [1.16, 1.71]) and 7 studies focused on medical \nquestion-answering (OR 1.32, 95% CI [1.08, 1.63]), with no \nstatistically significant difference observed between these 2 \ngroups.\nBaseline LLMs\nThe baseline LLMs varied across studies, with GPT-4 being \nthe most common, used in 14 studies, (OR: 1.58, 95% CI: \n1.21-2.04). GPT-3.5, used in 6 studies, showed an OR of \n1.43 (95% CI: 1.06-1.93). Llama2 was applied in 5 studies \n(OR: 1.25, 95% CI: 1.08-1.44).\nRetrieval sources\nRetrieval sources were categorized as pre-stored documents \nand real-time online browsing. Regarding pre-stored docu\xad\nments, 6 studies used clinical guidelines, such as the Emer\xad\ngency Severity Index (ESI) Ver.3 Field Triage.25 Five studies \nused academic articles from sources like PubMed abstracts or \nfull texts, or document sets such as the Radiographics Top 10 \nReading List on Gastrointestinal Imaging.31 Three studies \nused specialized knowledge bases, including ChatENT, \nOncoKB, and RareDis Corpus, while one study employed a \ngeneral biomedical knowledge graph (Scalable Precision \nMedicine Open Knowledge Engine [SPOKE]). SPOKE \nTable 1. Distribution of RAG applications by medical specialty and task \ntype.\nMedical Specialty\nFrequency\nInternal medicine\n4\nGeneral medicine\n3\nOncology\n3\nEmergency medicine\n2\nGastroenterology\n2\nOtolaryngology\n1\nHepatology\n1\nRare diseases\n1\nOrthopedics\n1\nNeurology\n1\nOphthalmology\n1\nTask\nFrequency (%)\nClinical decision-making\n13 (65%)\nMedical question-answering\n7 (35%)\nFigure 2. A forest plot showing the odds ratio (OR) of total impacts of the RAG-enhanced system compared with the baseline LLM system in clinical \ntasks. The left panel lists individual studies (e.g., Zakka et al., Murugan et al., Long et al.) along with their corresponding ORs and 95% confidence \nintervals (CIs). The right panel visualizes these ORs, with square markers proportional to study weights and horizontal lines representing CIs. The overall \npooled OR is 1.35 (95% CI: 1.19–1.53), represented by a diamond at the bottom.\n608                                                                                                      \nJournal of the American Medical Informatics Association, 2025, Vol. 32, No. 4 \nDownloaded from https://academic.oup.com/jamia/article/32/4/605/7954485 by guest on 11 January 2026'
 'Research and Applications\nImproving large language model applications in \nbiomedicine with retrieval-augmented generation: a \nsystematic review, meta-analysis, and clinical \ndevelopment guidelines\nSiru Liu, PhD\x03,1,2, Allison B. McCoy \n, PhD1, Adam Wright \n, PhD1,3 \n1Department of Biomedical Informatics, Vanderbilt University Medical Center, Nashville, TN 37212, United States, 2Department of Computer \nScience, Vanderbilt University, Nashville, TN 37212, United States, 3Department of Medicine, Vanderbilt University Medical Center, \nNashville, TN 37212, United States\n\x03Corresponding author: Siru Liu, PhD, Department of Biomedical Informatics, Vanderbilt University Medical Center, 2525 West End Ave #1475, Nashville, \nTN 37212, United States (siru.liu@vumc.org)\nAbstract\nObjective: The objectives of this study are to synthesize findings from recent research of retrieval-augmented generation (RAG) and large lan\xad\nguage models (LLMs) in biomedicine and provide clinical development guidelines to improve effectiveness.\nMaterials and Methods: We conducted a systematic literature review and a meta-analysis. The report was created in adherence to the Pre\xad\nferred Reporting Items for Systematic Reviews and Meta-Analyses 2020 analysis. Searches were performed in 3 databases (PubMed, Embase, \nPsycINFO) using terms related to “retrieval augmented generation” and “large language model,” for articles published in 2023 and 2024. We \nselected studies that compared baseline LLM performance with RAG performance. We developed a random-effect meta-analysis model, using \nodds ratio as the effect size.\nResults: Among 335 studies, 20 were included in this literature review. The pooled effect size was 1.35, with a 95% confidence interval of \n1.19-1.53, indicating a statistically significant effect (P ¼ .001). We reported clinical tasks, baseline LLMs, retrieval sources and strategies, as \nwell as evaluation methods.\nDiscussion: Building on our literature review, we developed Guidelines for Unified Implementation and Development of Enhanced LLM Appli\xad\ncations with RAG in Clinical Settings to inform clinical applications using RAG.\nConclusion: Overall, RAG implementation showed a 1.35 odds ratio increase in performance compared to baseline LLMs. Future research \nshould focus on (1) system-level enhancement: the combination of RAG and agent, (2) knowledge-level enhancement: deep integration of \nknowledge into LLM, and (3) integration-level enhancement: integrating RAG systems within electronic health records.\nKey words: large language model; retrieval augmented generation; systematic review; meta-analysis. \nIntroduction\nLarge language models (LLMs) have reported remarkable \nperformance in question-answering, summarization, and text \ngeneration.1 Given this, researchers have explored its poten\xad\ntial in biomedical areas.2 For example, several studies \nreported the ability of using LLM to answer patient mes\xad\nsages,3 to analyze alert logic in clinical decision support,4 and \nto make discharge summaries more readable to patients.5\nHowever, several challenges remain.\nLLMs are trained on fixed datasets, which restrict their \nknowledge to information available up to the training cut-off \ndate. For example, GPT-4o’s training data only includes \ninformation up to October 2023, making it unable to \nrespond accurately to findings that emerged afterward. LLM \ntraining datasets are also generally broad and lack the specif\xad\nicity required for biomedical applications. Finally, not all \nsources used to train the LLMs are reliable and trustworthy. \nTo address these limitations, researchers have performed \nfine-tuning and retrieval-augmented generation (RAG) tech\xad\nniques. Fine-tuning can adapt LLMs to specific domains, but \nit is resource-intensive and does not allow for real-time \nupdates. In contrast, RAG maintains the original LLM archi\xad\ntecture while incorporating relevant context directly into \nqueries, offering more flexibility and control. In addition, \nRAG’s unique advantage in biomedical applications lies in its \nability to adapt to dynamic environments by delivering up-to- \ndate information and efficiently integrating external knowl\xad\nedge sources with high interpretability.6\nAnother limitation of using LLMs directly is the risk of hal\xad\nlucination, where the model generates incorrect or fabricated \ninformation.7 To mitigate such issues, researchers have pro\xad\nposed RAG as a solution that integrates up-to-date, relevant \ninformation, enhancing both the accuracy and reliability of \nLLM generated responses.8,9 For example, when ChatGPT \nReceived: November 19, 2024; Revised: December 17, 2024; Editorial Decision: January 2, 2025; Accepted: January 3, 2025 \n© The Author(s) 2025. Published by Oxford University Press on behalf of the American Medical Informatics Association.  \nThis is an Open Access article distributed under the terms of the Creative Commons Attribution-NonCommercial License (https://creativecommons.org/licenses/ \nby-nc/4.0/), which permits non-commercial re-use, distribution, and reproduction in any medium, provided the original work is properly cited. For commercial \nre-use, please contact reprints@oup.com for reprints and translation rights for reprints. All other permissions can be obtained through our RightsLink service via \nthe Permissions link on the article page on our site—for further information please contact journals.permissions@oup.com. \nJournal of the American Medical Informatics Association, 2025, 32(4), 605–615 \nhttps://doi.org/10.1093/jamia/ocaf008 \nAdvance access publication 15 January 2025 \nResearch and Applications \nDownloaded from https://academic.oup.com/jamia/article/32/4/605/7954485 by guest on 11 January 2026'
 'Limitations\nThis study was limited to peer-reviewed publications avail\xad\nable in biomedical databases (eg, PubMed, Embase), exclud\xad\ning \npreprint \narticles \nfrom \nrepositories \nlike \nArXiv. \nAdditionally, only studies in English language were included, \nwhich might have excluded relevant studies in other lan\xad\nguages. We did not include sources such as IEEE Xplore or \nGoogle Scholar, which might have additional relevant stud\xad\nies. However, our focus was on biomedicine, and we priori\xad\ntized databases specifically tailored to biomedical research to \nmaintain the relevance and quality of the included studies. \nFurthermore, we used free-text searches in the databases, \nwhich activated automatic mapping to Medical Subject \nHeadings (MeSH) and Emtree terms, improving retrieval \naccuracy. However, the limitations of automatic term map\xad\nping cannot be ignored, as it may introduce variability if the \nunderlying algorithms change. To address this, we have docu\xad\nmented all identified papers from our search. The title, publi\xad\ncation year, PMID, PUI, and database source for each study \nare provided in File S3.\nConclusion\nWe conducted a systematic literature review of studies \nexploring the use of RAG and LLM in clinical tasks. RAG \nimplementation showed a 1.35 odds ratio increase in per\xad\nformance compared to baseline LLMs. To improve perform\xad\nance and transparency in future studies, we developed \nguidelines for improving clinical RAG applications based on \ncurrent research findings. Future research could focus on \nthese 3 directions: (1) system-level enhancement: the combi\xad\nnation of RAG and agent, (2) knowledge-level enhancements: \ndeep integration of knowledge into LLM, and (3) integration- \nlevel enhancements: integrating RAG systems within EHRs.\nAuthor contributions\nSiru Liu (Conceptualization, Data curation, Formal analysis, \nInvestigation, Methodology, Software, Visualization, Writing \n– original draft), Allison B. McCoy (Conceptualization, Writ\xad\ning – review & editing), Adam Wright (Conceptualization, \nWriting – review & editing).\nSupplementary material\nSupplementary material is available at Journal of the Ameri\xad\ncan Medical Informatics Association online.\nFunding\nThis work was supported by National Institutes of Health \ngrants: R00LM014097-02 and R01LM013995-01.\nConflicts of interest\nThe authors do not have conflicts of interest related to this \nstudy.\nData availability\nThe characteristics and outcomes for each included study \nwere reported in the File S2.\nReferences\n01. Raiaan MAK, Mukta MSH, Fatema K, et al. A review on large lan\xad\nguage models: architectures, applications, taxonomies, open issues \nand challenges. IEEE Access. 2024;12:26839-26874. https://doi. \norg/10.1109/ACCESS.2024.3365742\n02. Thirunavukarasu AJ, Ting DSJ, Elangovan K, et al. Large language \nmodels in medicine. Nat Med. 2023;29:1930-1940. https://doi. \norg/10.1038/s41591-023-02448-8\n03. Ayers JW, Poliak A, Dredze M, et al. Comparing physician and arti\xad\nficial intelligence Chatbot responses to patient questions posted to a \npublic social media forum. JAMA Intern Med. 2023;183:589-596. \nhttps://doi.org/10.1001/jamainternmed.2023.1838\n04. Liu S, Wright AP, Patterson BL, et al. Using AI-generated sugges\xad\ntions from ChatGPT to optimize clinical decision support. J Am \nMed Inform Assoc. 2023;30:1237-1245. https://doi.org/10.1093/ \njamia/ocad072\n05. Zaretsky J, Kim JM, Baskharoun S, et al. Generative artificial intel\xad\nligence to transform inpatient discharge summaries to patient- \nfriendly language and format. JAMA Netw Open. 2024;7: \ne240357. https://doi.org/10.1001/jamanetworkopen.2024.0357\n06. Gao Y, Xiong Y, Gao X, et al. Retrieval-augmented generation for \nlarge language models: a survey. 2023, preprint: not peer reviewed. \nhttps://arxiv.org/abs/2312.10997\n07. Xu Z, Jain S, Kankanhalli M. Hallucination is inevitable: an innate \nlimitation of large language models. January 22, 2024, preprint: \nnot peer reviewed. https://arxiv.org/abs/2401.11817\n08. Shuster K, Poff S, Chen M, et al. Retrieval augmentation reduces \nhallucination in conversation. Findings of the Association for Com\xad\nputational Linguistics, Findings of ACL: EMNLP 2021. Associa\xad\ntion for Computational Linguistics (ACL); 2021:3784-3803.\n09. Ayala  O, Bechard P. Reducing hallucination in structured outputs \nvia Retrieval-Augmented Generation. Proceedings of the 2024 \nConference of the North American Chapter of the Association for \nComputational Linguistics: Human Language Technologies. \nAssociation for Computational Linguistics; 2024;228-38. https:// \ndoi.org/10.18653/v1/2024.naacl-industry.19\n10. Gornik HL, Aronow HD, Goodney PP, et al. 2024 ACC/AHA/ \nAACVPR/APMA/ABC/SCAI/SVM/SVN/SVS/SIR/VESS guideline \nfor the management of lower extremity peripheral artery disease: a \nreport of the American College of Cardiology/American Heart \nAssociation Joint Committee on Clinical Practice Guidelines. Cir\xad\nculation. 2024;149:e1313-e1410. https://doi.org/10.1161/CIR. \n0000000000001251\n11. Vasey B, Nagendran M, Campbell B, et al. Reporting guideline for \nthe early stage clinical evaluation of decision support systems \ndriven by artificial intelligence: DECIDE-AI. BMJ. 2022;377: \ne070904. https://doi.org/10.1136/bmj-2022-070904\n12. Mongan J, Moy L, Kahn CE. Checklist for Artificial Intelligence in \nMedical Imaging (CLAIM): a guide for authors and reviewers. \nRadiol Artif Intell. 2020;2:e200029. https://doi.org/10.1148/ryai. \n2020200029\n13. Martindale APL, Ng B, Ngai V, et al. Concordance of randomised \ncontrolled trials for artificial intelligence interventions with the \nCONSORT-AI \nreporting \nguidelines. \nNat \nCommun. \n2024;15:6376-6311. https://doi.org/10.1038/s41467-024-45355-3\n14. Tam TYC, Sivarajkumar S, Kapoor S, et al. A framework for \nhuman evaluation of large language models in healthcare derived \nfrom literature review. NPJ Digit Med. 2024;7:258. https://doi. \norg/10.1038/s41746-024-01258-7\n15. Shamseer L, Moher D, Clarke M, et al. Preferred reporting items \nfor systematic review and meta-analysis protocols (PRISMA-p) \n2015: elaboration and explanation. BMJ. 2015;350:g7647. \nhttps://doi.org/10.1136/bmj.g7647\n16. Higgins JPT, Thomas J, Chandler J, et al. (editors). Cochrane Hand\xad\nbook for Systematic Reviews of Interventions version 6.5 (updated \nAugust 2024). Cochrane, 2024. Available from www.training. \ncochrane.org/handbook. Date accessed December 11, 2024.\nJournal of the American Medical Informatics Association, 2025, Vol. 32, No. 4                                                                                                      \n613 \nDownloaded from https://academic.oup.com/jamia/article/32/4/605/7954485 by guest on 11 January 2026']","['<1-hop>\n\nLimitations This study was limited to peer-reviewed publications avail\xad able in biomedical databases (eg, PubMed, Embase), exclud\xad ing preprint articles from repositories like ArXiv. Additionally, only studies in English language were included, which might have excluded relevant studies in other lan\xad guages. We did not include sources such as IEEE Xplore or Google Scholar, which might have additional relevant stud\xad ies. However, our focus was on biomedicine, and we priori\xad tized databases specifically tailored to biomedical research to maintain the relevance and quality of the included studies. Furthermore, we used free-text searches in the databases, which activated automatic mapping to Medical Subject Headings (MeSH) and Emtree terms, improving retrieval accuracy. However, the limitations of automatic term map\xad ping cannot be ignored, as it may introduce variability if the underlying algorithms change. To address this, we have docu\xad mented all identified papers from our search. The title, publi\xad cation year, PMID, PUI, and database source for each study are provided in File S3. Conclusion We conducted a systematic literature review of studies exploring the use of RAG and LLM in clinical tasks. RAG implementation showed a 1.35 odds ratio increase in per\xad formance compared to baseline LLMs. To improve perform\xad ance and transparency in future studies, we developed guidelines for improving clinical RAG applications based on current research findings. Future research could focus on these 3 directions: (1) system-level enhancement: the combi\xad nation of RAG and agent, (2) knowledge-level enhancements: deep integration of knowledge into LLM, and (3) integration- level enhancements: integrating RAG systems within EHRs. Author contributions Siru Liu (Conceptualization, Data curation, Formal analysis, Investigation, Methodology, Software, Visualization, Writing – original draft), Allison B. McCoy (Conceptualization, Writ\xad ing – review & editing), Adam Wright (Conceptualization, Writing – review & editing). Supplementary material Supplementary material is available at Journal of the Ameri\xad can Medical Informatics Association online.'
 '<2-hop>\n\nwas asked about medications for peripheral artery disease patients without increased bleeding risk, it initially omitted low-dose rivaroxaban. After integrating retrieved text from the 2024 American College of Cardiology / American Heart Association Guideline for the Management of Lower Extrem\xad ity Peripheral Artery Disease,10 the model correctly recom\xad mended rivaroxaban. Several guidelines exist for evaluating Artificial Intelligence (AI) applications and LLMs in healthcare, including DECIDE-AI (Developmental and Exploratory Clinical Inves\xad tigations of DEcision support systems driven by Artificial Intelligence),11 CLAIM (Checklist for Artificial Intelligence in Medical Imaging),12 and CONSORT-AI (Consolidated Standards of Reporting Trials-AI).13 In addition, Tam et al. introduced QUEST, a framework specifically for human eval\xad uation of LLMs in healthcare.14 However, these guidelines do not cover RAG applications in clinical settings, emphasiz\xad ing the need for a more specific guideline. Despite the promise of RAG in improving LLM performance in clinical settings, there is limited understanding of its overall effectiveness comparing with the baseline LLM, adoption in clinical domains, and optimal strategies for its development in biomedical applications. The aim of this study is to synthesize findings from recent research of RAG and LLM in biomedicine and provide clinical development guidelines to improve effec\xad tiveness as well as transparency in future research. Materials and methods Study design We conducted a systematic literature review. The report was created in adherence to the Preferred Reporting Items for Sys\xad tematic Reviews and Meta-Analyses 2020 analysis checklist (File S1).15 We also performed a meta-analysis following the Cochrane Handbook guidelines.16 Screening papers We searched in 3 databases (PubMed, Embase, and Psy\xad cINFO), using terms related to “retrieval augmented gener\xad ation” and “large language model.” PubMed and Embase were selected based on recommendations from the Cochrane Handbook, as they are widely recognized for their extensive coverage of biomedical literature.17 PsycINFO was included to capture articles from the psychological and behavioral sci\xad ences. To maintain a high standard of quality and reliability, we focused on peer-reviewed articles and excluded preprints. The specific search terms used for each database are provided in File S2. Given that ChatGPT was released on November 30, 2022, we set the publication filter to search papers pub\xad lished in 2023 and 2024. The search was performed on December 12, 2024. The inclusion criteria were: (1) the study must compare baseline LLM performance with RAG per\xad formance and (2) the study must address a biomedical ques\xad tion. The exclusion criteria were: (1) literature reviews, editorial comments, or viewpoint papers, (2) studies focusing on LLMs in languages other than English, or (3) studies cen\xad tered on a multi-agent system without a focus on RAG. SL screened titles and abstracts, then conducted a full-text review of papers meeting the criteria. Data extraction For each included study, we extracted the following informa\xad tion: author, title, publication year, journal, clinical task, and specialty. Regarding RAG techniques, we gathered details about the baseline LLM, retrieval sources, and strategies used in the pre-retrieval, retrieval, and post-retrieval stages. For evaluation, we extracted the evaluation method (human, automated, or a combination of both), the number of evalua\xad tors, the evaluation dataset, and the evaluation metrics.'
 '<3-hop>\n\nMeta-analysis Effect size was defined as a metric quantifying the relation\xad ship between variables, including both direction and magni\xad tude.18 For each included study, we calculated the effect size between baseline LLM performance and RAG-enhanced LLM performance. The outcomes focused on the perform\xad ance of generation results, such as accuracy and usefulness. Metrics related to the retrieval process, cost, or speed were not included as outcomes in the meta-analysis. For continu\xad ous outcomes, we used Cohen’s d, standardized between- group mean difference (SMD), calculated as the difference in means divided by the pooled standard deviation. The stand\xad ard error (SE) of SMD was calculated using the following for\xad mula (1), where n1 and n2 represent the sample sizes of each group.19 For dichotomous measurements, we calculated the log-odds ratio, obtained by transforming the odds ratio (OR) with the natural logarithm, and the associated SE was calcu\xad lated using formula (2), where a, b, c, and d represent the number of successful and failed events in the baseline LLM and RAG-enhanced LLM approaches. For studies reporting multiple outcomes, we used the overall outcome to calculate effect size. If no overall outcome was reported, we averaged the effect sizes of all reported outcomes. We excluded out\xad comes with a sample size of less than 30 to avoid small- sample bias. SESMD ¼ ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi n1 þ n2 n1n2 þ SMD2 2 n1 þ n2 ð Þ s (1) SElogOR ¼ ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi 1 a þ 1 b þ 1 c þ 1 d r (2) We developed a random-effect meta-analysis model, because of the variability in RAG architectures and evalua\xad tion datasets among the included studies. The random-effect model was used when individual study effects likely con\xad tained additional sources of variance beyond sampling error. Between-study heterogeneity was assessed using Higgins & Thompson’s I2 statistic, where 25% indicated low heteroge\xad neity, 50% moderate, and 75% substantial.20 We conducted subgroup analyses to explore performance variations across different factors. First, we analyzed the influence of the baseline LLM, referring to the foundation model (eg, GPT-4 or Llama2) that provides the core architec\xad ture for the system. Second, we examined data retrieval strat\xad egies, categorizing them as simple or complex. Simple strategies included fixed-length chunking and basic similarity search, and we performed a subgroup analysis to compare these with complex retrieval strategies. Third, we analyzed differences based on evaluation methods, distinguishing between human evaluations, such as Likert scale ratings for helpfulness and accuracy, and automatic evaluation metrics, including ROUGE-1 and BLEU. Finally, we conducted a sub\xad group analysis based on the type of task, classifying studies 606 Journal of the American Medical Informatics Association, 2025, Vol. 32, No. 4 Downloaded from https://academic.oup.com/jamia/article/32/4/605/7954485 by guest on 11 January 2026']","The evaluation methods and performance metrics used to compare Retrieval-Augmented Generation (RAG)-enhanced Large Language Models (LLMs) with baseline LLMs in clinical tasks, as described in the provided context, include the following:

### Evaluation Methods:
1. **Human Evaluation:**
   - Used in 9 studies.
   - Typically involved physicians or domain experts as evaluators (average of 4 evaluators, range 1 to 10).
   - Evaluators assessed model outputs on criteria such as accuracy, helpfulness, completeness, understandability, and whether responses met user intent.
   - Some evaluations included patient perspectives (e.g., diabetic patients assessing understandability).
   - Human evaluation often used Likert scale ratings.
   - Human evaluation showed a higher odds ratio (OR) of 1.65 (95% CI: 1.36-2.03), indicating a stronger performance improvement with RAG-enhanced LLMs compared to baseline.

2. **Automated Evaluation:**
   - Used in 8 studies.
   - Metrics included similarity comparisons between generated responses and expert-provided answers.
   - Common automated metrics: ROUGE (ROUGE-1, ROUGE-2, ROUGE-L), BLEU, METEOR, BERTScore.
   - Automated evaluation showed an OR of 1.20 (95% CI: 1.1-1.41), which was statistically significantly lower than human evaluation (P < .01).

3. **Combined Human and Automated Evaluation:**
   - Used in 3 studies.
   - Combined approaches leveraged both subjective human judgment and objective automated metrics.

### Performance Metrics:
- **Accuracy:** Most frequently reported metric, measured by:
  - Likert scale ratings of correctness.
  - Match rates with expert answers.
  - Correct treatment percentages.
  - Statistical measures such as AUC (Area Under the Curve), AUPRC (Area Under the Precision-Recall Curve), and F1 scores.
  - Text similarity metrics (ROUGE, BLEU, METEOR, BERTScore).

- **Completeness:** Assessed whether responses included all necessary information, often via Likert scales.

- **User Perception:** Subjective feedback on understandability, helpfulness, and whether responses met user intent, usually via Likert scales.

- **Safety:** Evaluated potential harm, adversarial safety, and risk management to ensure outputs were free from harmful content or risks.

- **Hallucination:** Identification of nonfactual or conflicting information, primarily through manual review.

- **Citation Accuracy:** Measured the correctness of references provided by the model, ensuring citations pointed to valid and established sources.

- **Bias and Language:** Evaluated for clarity, neutrality, and empathy to patient concerns.

### Reflection of Accuracy and Usefulness:
- The use of **human evaluation** reflects the practical usefulness and clinical relevance of the models, as expert clinicians assess whether the model outputs are accurate, complete, and helpful in real-world clinical decision-making.
- **Automated metrics** provide objective, reproducible measures of textual similarity and correctness, useful for large-scale benchmarking but may not fully capture clinical nuance or usefulness.
- The combination of both methods offers a comprehensive assessment, balancing objective accuracy with subjective clinical relevance.
- The meta-analysis showed that RAG-enhanced LLMs outperform baseline LLMs with an overall pooled odds ratio of 1.35 (95% CI: 1.19-1.53), indicating statistically significant improvement in clinical task performance.
- Subgroup analyses revealed that human evaluations tend to show larger effect sizes than automated metrics, highlighting the importance of expert judgment in assessing clinical AI tools.

### Summary:
- **Evaluation methods:** Human evaluation (expert ratings), automated evaluation (text similarity and statistical metrics), and combined approaches.
- **Performance metrics:** Accuracy (various forms), completeness, user perception, safety, hallucination, citation accuracy, bias, and language quality.
- These methods collectively assess both the **accuracy** (correctness and completeness of information) and **usefulness** (clinical relevance, understandability, safety) of RAG-enhanced LLMs compared to baseline LLMs in clinical settings.","Studies comparing RAG-enhanced LLMs with baseline LLMs in clinical tasks have employed various evaluation methods, including human evaluations such as Likert scale ratings for helpfulness and accuracy, and automated evaluation metrics like ROUGE-1 and BLEU. Several guidelines exist for evaluating AI applications and LLMs in healthcare, including DECIDE-AI, CLAIM, CONSORT-AI, and the QUEST framework for human evaluation of LLMs, though these do not specifically cover RAG applications, highlighting the need for more tailored guidelines. Performance metrics focused on generation results such as accuracy and usefulness were analyzed in a meta-analysis using effect size measures like Cohen’s d and log-odds ratio. The meta-analysis excluded metrics related to retrieval process, cost, or speed, concentrating on the quality of generated outputs. Subgroup analyses further explored variations in performance based on baseline LLM models, retrieval strategies, evaluation methods, and task types, demonstrating a comprehensive approach to assessing RAG and LLM performance in biomedical applications.",1.0,0.9540277294803875,0.8874999999778125,1.0
How does the system leverage cypher query generation and multi-hop reasoning with confidence scoring to ensure accurate and reliable biomedical question answering over the iBKH knowledge graph?,"['Omar & Mohammed / Journal of Applied Science and Technology Trends Vol. 06, No. 02, pp. 342 –357 (2025) \n \n356 \n \nFig. 7. Side effects of drugs that treat epilepsy \n \nExecuting this query returns drugs associated with epilepsy \nand their corresponding side effects. For example, the query \nidentifies Pregabalin as a treatment for epilepsy and retrieves \nmultiple side effects such as anxiety, arrhythmia, gastrointestinal \npain, and others. The visualization of the graph in Fig. 7, \nhighlights this pattern, where the drug node is linked to epilepsy \nvia TREATS and to several side effect nodes via CAUSES, \nproviding an interpretable biomedical knowledge structure. \nVI. DISCUSSION \nUnder \na \nunified \nevaluation \nprotocol \nreporting \nP@k/R@k/F1@k, Hits@k, and MRR at 𝑘∈{1,5,10} , the \npipeline exhibits consistent end-to-end behavior across the three \ndifficulty tiers. The 𝑘-ablation shows strong early precision (high \nP@1 and elevated MRR), while recall increases with larger 𝑘, \nindicating that correct answers typically surface near the top yet \npersist deeper in the ranked slate. At the query-generation level, \nexact-match (EM) is high for simple questions (e.g., ∼96%) and \nlower for complex, multi-hop questions (e.g., ∼86.7%), which \naligns with increased compositionality. End-to-end latency \n(mean total ≈6.07\u2006s from question to final answer) remains within \ninteractive bounds on our local setup. Practically, a fully offline \ndeployment avoids internet connectivity, API keys, and external \ndata \ntransfer, \nstrengthening \nprivacy, \ncompliance, \nand \nreproducibility in biomedical contexts. \nSeveral challenges qualify these findings. First, the \ncomparability challenge: to our knowledge, no prior study \nevaluates iBKH using the same metric suite, making cross-paper \nnumeric comparisons not “apples to apples.” We therefore \ninterpret the results as controlled evidence about this pipeline \nunder a single, consistent protocol rather than as a cross-study \nleaderboard. Beyond comparability, performance may vary with \nother biomedical knowledge graphs; the current iBKH snapshot \nis static, limiting real-time updates; and scaling to larger or \ndynamically refreshed graphs can introduce latency and \nconsistency trade-offs. \nError analysis shows that residual failures concentrate in \ncomplex, multi-hop queries where missing relation hops or brittle \nname-based matching (synonyms, abbreviations, homonyms) \nlead to partial answers. These observations motivate concept-\nlevel normalization via biomedical identifiers (e.g., UMLS, \nSNOMED, RxNorm) with synonym expansion, as well as \nschema-constrained query generation and path-guided decoding \nto better satisfy multi-hop constraints. Finally, correctness is \nassessed primarily at the answer level (Hits@k, MRR, \nprecision/recall) and does not yet include full semantic-\nequivalence checks across alternative Cypher queries, which may \noverlook cases where different queries yield the same correct \nresults. Complementing template EM with result-set equivalence \nchecks, expanding the metric suite (e.g., nDCG/MAP) with per-\nquestion 95% bootstrap confidence intervals, and supporting \nincremental graph updates with distributed storage are promising \nsteps to enhance robustness, scalability, and external validity. \nVII. CONCLUSION \nThis study introduced a hybrid biomedical question \nanswering framework that couples the LLaMA-3B language \nmodel with a Neo4j-based iBKH knowledge graph to enable the \nautomatic generation of executable Cypher queries and to deliver \ntransparent, evidence-grounded answers through justification \nsubgraphs. Evaluation in simple, medium, and complex queries \ndemonstrated consistently high performance, with strong \nprecision, recall, F1, Hits@k, and MRR values, while \nmaintaining low latency suitable for interactive biomedical \napplications. Beyond quantitative performance, the proposed \nsystem provides a reproducible and privacy-preserving solution \nby operating fully offline, a property of particular importance in \nsensitive clinical and research contexts. However, certain \nlimitations remain. The reliance on a static iBKH snapshot \nconstrains coverage and adaptability, recall is lower for complex \nmulti-hop reasoning, and the absence of canonical entity \nnormalization (e.g., UMLS, SNOMED, RxNorm) may reduce \nsemantic robustness. Future research will therefore focus on \nintegrating standardized biomedical entity normalization, \nenabling dynamic and incremental knowledge graph updates, and \nleveraging domain-adapted or fine-tuned biomedical LLMs. \nThese directions are expected to further strengthen the \nrobustness, scalability, and applicability of the framework in real-\nworld biomedical and clinical environments. \n \nREFERENCES \n[1] S. Bubeck, V. Chandrasekaran, R. Eldan, J. Gehrke, E. Horvitz, E. Kamar, \nP. Lee, Y. T. Lee, Y. Li, S. Lundberg, et al., “Sparks of artificial general \nintelligence: \nEarly experiments with \ngpt-4,” arXiv preprint \narXiv:2303.12712, 2023. \n[2] J. Achiam, S. Adler, S. Agarwal, L. Ahmad, I. Akkaya, F. L. Aleman, D. \nAlmeida, J. Altenschmidt, S. Altman, S. Anadkat, et al., “Gpt-4 \ntechnical report,” arXiv preprint arXiv:2303.08774, 2023. \n[3] Y. Hou, J. Yeung, H. Xu, C. Su, F. Wang, and R. Zhang, “From answers to \ninsights: unveiling the strengths and limitations of chatgpt and \nbiomedical knowledge graphs,” Research Square, pp. rs–3, 2023.'
 'Vol. 06, No. 02, pp. 342 –357 (2025) \nISSN: 2708-0757 \n \nJOURNAL OF APPLIED SCIENCE AND TECHNOLOGY TRENDS \n \nwww.jastt.org  \n \n                                                          342 \ndoi: 10.38094/jastt62404  \nA Hybrid LLM–Knowledge Graph Framework for Accurate \nBiomedical Question Answering \n \nHavraz Y. Omar1,²*, Abdulhakeem O. Mohammed³  \n \n¹Department of Information Technology, Technical College of Duhok, Duhok Polytechnic University, Duhok, Kurdistan Region, \nIraq. havraz.omar@dpu.edu.krd  \n² Department of Information Technology, Technical College of Informatics – Akre, Akre University for Applied Sciences, Akre, \nKurdistan Region, Iraq. \n³ Department of Computer Science, College of Science, University of Zakho, Zakho, Kurdistan Region, Iraq. \na.mohammed@uoz.edu.krd \n \n*Correspondence: havraz.omar@dpu.edu.krd \n \nAbstract \nBiomedical question answering requires accurate and interpretable systems; however, existing approaches often face challenges such as \nlanguage model hallucinations and limited reasoning when relying solely on standalone knowledge graphs. To address these limitations, \nthis study proposes a hybrid framework that integrates the LLaMA-3B language model with a Neo4j-based drug–disease–symptom \nknowledge graph. The system translates natural language questions into executable Cypher queries, operates on an iBKH-derived graph \ncomprising over 65,000 entities and 3 million relationships, and returns answers with supporting evidence through a transparent interface. \nExperiments conducted on 60 biomedical questions across three levels of difficulty demonstrate the robustness of the approach: 96% \nexact match for simple queries, 95% for medium queries, and 86.7% for complex queries. Overall, the system achieves Precision@5 of \n96.1%, Recall@5 of 89.0%, F1@5 of 91.0%, Hits@k of 96.1%, and an MRR of 94.4%, while maintaining an average response time of \nonly 6.07 seconds. These results indicate that the system retrieves nearly all relevant answers, ranks them correctly, and delivers them \nwith latency low enough for interactive use. Moreover, unlike cloud-based APIs such as ChatGPT, which require internet connectivity \nand external data transmission, the proposed framework operates fully offline, ensuring privacy, reproducibility, and compliance with \nbiomedical data governance. Overall, this pipeline provides an accurate, efficient, and privacy-preserving solution for biomedical question \nanswering, making it a practical alternative to cloud-dependent approaches in sensitive healthcare contexts. \n \nKeywords: Knowledge Graph, LLM, Question Answering, Neo4j, Biomedical Informatics, Healthcare AI, LLaMA 3. \n \nReceived: August 14th, 2025 / Revised: October 10th, 2025 / Accepted: October 16th, 2025 / Online: October 20th, 2025 \n \nI. INTRODUCTION \nAnswering questions in the biomedical field is a difficult task \ndue to the complexity of medical knowledge and the need for \nprecision. In recent years, large language models  (LLMs) like \nLLaMA, GPT-4 have made progress in understanding and \ngenerating human-like responses to medical questions  [1,  2]. \nThese models can process large amounts of information and \nrespond in natural language, which makes them helpful in \nhealthcare settings [3]. However, they often struggle to provide \naccurate answers when dealing with specialized biomedical \ncontent [4, 5]. \nOne major issue with LLMs is a problem called hallucination, \nwhere the model generates information that sounds right but is \nactually incorrect or unsupported [6]. In medical applications, \nthis can be dangerous, as healthcare professionals rely on precise \nand trustworthy information  [7]. Therefore, researchers are \nexploring ways to combine LLMs with structured sources of \nknowledge to improve their reliability [8]. \nLLM-only systems in biomedicine still hallucinate and are \nhard to verify, limiting safe use [9, 10]. Biomedical knowledge \ngraphs (BKGs) such as iBKH and SPOKE curate multi-source \nfacts and enable multi-hop reasoning, yet they neither interpret \nfree text nor generate answers [11, 12]. Recent hybrids (KG-\naware RAG) improve grounding but often lack explicit path-level \njustifications and robust end-to-end answer evaluation [13, 14]. \nRecent studies have increasingly integrated Knowledge \nGraphs (KGs) with Large Language Models (LLMs) to improve \nfactual accuracy, reasoning, and reduce hallucinations. Notable \nexamples include DR.KNOWS, which combines UMLS-based \nKGs with LLMs for better diagnostic reasoning [15], KnowNet'
 ""Omar & Mohammed / Journal of Applied Science and Technology Trends Vol. 06, No. 02, pp. 342 –357 (2025) \n \n352 \nThe dataset contains 60 questions divided into three difficulty \nlevels based on how complex the language is and how deep the \nbiomedical reasoning needs to be: \n• Level 1: 25 simple questions focusing mostly on easy-to-\nunderstand facts, such as symptoms of certain diseases or \ndrugs used to treat specific conditions. \n• Level 2: 20 medium-level questions that involve more \ndetailed relationships, such as drug interactions and \nSideEffect. \n• Level 3: 15 hard questions requiring multi-step reasoning \nacross multiple biomedical concepts or biological \npathways, similar to the complexity found in real clinical \ncases. \nThe set of evaluation questions was designed to span a wide \nrange of common biomedical topics and to reflect clinically \nrelevant query types reported in prior literature. Each question is \npaired with its corresponding gold standard cypher query and \ncategorized by difficulty level, as summarized in Table V, where \nthree illustrative examples are shown. The questions were derived \nfrom publicly available biomedical QA benchmarks and adapted \nfrom established knowledge bases (e.g., iBKH schema relations), \nensuring both coverage and diversity across diseases, drugs, \nsymptoms, and pathways.\nTABLE V.  QUESTION DIFFICULTY LEVELS AND SAMPLE CYPHER QUERIES \nLevel \nDefinition \nExample \nCypher Query \n1 \nSingle-hop question using \none relationship \nWhat are the symptoms of \nAlzheimer? \nMATCH (d:Disease)-[:HAS_SYMPTOM]->(s:Symptom) \nWHERE toLower(d.name) CONTAINS 'alzheimer' \nRETURN s.name AS symptom \n \n2 \nQuestions requiring one \nrelationship \nWhat are the side effects of \ndrugs used to treat asthma? \nWITH toLower('asthma') AS disease_name \nMATCH (d:Disease)<-[:TREATS]-(dr:Drug) \nWHERE toLower(d.name) CONTAINS disease_name \nMATCH (dr:Drug)-[:CAUSES]->(se:SideEffect) \nRETURN DISTINCT se.name AS side_effect, dr.name AS drug \n \n3 \nQuestions requiring two or \nmore relationships \nWhat cholesterol medications \ncause side effects, and what are \nsome of those effects? \nWITH toLower('cholesterol') AS disease_name \nMATCH (d:Disease)<-[:TREATS]-(dr:Drug) \nWHERE toLower(d.name) CONTAINS disease_name \nMATCH (dr:Drug)-[:CAUSES]->(se:SideEffect) \nRETURN DISTINCT dr.name AS drug, se.name AS side_effect \n \n \n \nFig. 4. Graphical User Interface of the Biomedical Knowledge Explorer System \nB. Quantitative Evaluation \nThe performance of the proposed system is evaluated on the \niBKH knowledge graph using Exact Match, Precision@k, \nRecall@k, F1@k, Hits@k, MRR, and total response time across \nsimple, medium, and complex biomedical queries. These metrics \nwere measured for each difficulty level (simple, medium, and \ncomplex) to see how performance changes as questions become \nmore challenging.\n \nTABLE VI.  QUERY GENERATION EXACT MATCH BY DIFFICULTY LEVEL""
 'Omar & Mohammed / Journal of Applied Science and Technology Trends Vol. 06, No. 02, pp. 342 –357 (2025) \n \n351 \nThis query searches for a disease node whose name contains \n’alzheimer’ and follows HAS_SYMPTOM edges to list related \nsymptom names. The system then executes this cypher to retrieve \nanswers. The prompts (such as few-shot examples and schema \nhints) were carefully designed to help LLaMA 3 generate correct \nCypher queries. The model learns how to use the graph’s labels \nand relationships properly. For example, if a user asks, ’Which \ndrugs treat diabetes?’, LLaMA might respond with a suitable \nCypher query: \n \nMATCH (d:Drug)-[:TREATS]->(di:Disease) \nWHERE toLower(di.name) CONTAINS ""diabetes"" \nRETURN d.name  \n \nThis queries for drug nodes that have a TREATS edge to a \ndiabetes disease node. By leveraging LLaMA 3 in this way, our \nsystem can flexibly handle many phrasing variations without \nmanual mapping rules. \nD. Model Configuration & Decoding \nWe run a local LLaMA 3.2-3B model in GGUF format (llama-\n3.2-3b-instruct-q4_k_m.gguf) via llama.cpp, as shown in Table \nIV. \nTABLE IV.  MODEL RUNTIME AND DECODING SETTINGS \nRuntime settings \nDecoding settings \nn_ctx = 1024 \ntemperature = 0.2 \nn_threads = 12 \ntop_p = 0.95 \nn_gpu_layers = 33 \ntop_k = 40 \nn_batch = 512 \nrepeat_penalty = 1.1 \n \nmax_tokens = 80 \n \nseed = 42 \n \nE. Graph Subset and Versioning \nWe use an iBKH derived subgraph (≈65.8k nodes; ≈3.0M \nedges) spanning DRUG, DISEASE, SYMPTOM, PATHWAY. IDs are \nnormalized to CURIEs and duplicates collapsed across \nUMLS/DrugBank/DisGeNET/SIDER/KEGG. Each edge stores \nprovenance/licensing metadata (source, source_version, license, \nretrieved_at, \nevidence_pmids/urls) \nand, \nwhen \navailable, \ntemporal fields (valid_from, valid_to). We report coverage as \nthe \npercentage \nof \nevaluated \nquestions \nwhose \ngold \nentities/relations are present. \nF. Query Execution and Reliability \nAfter a Cypher query is generated, it is executed on the Neo4j \ndatabase through the official Neo4j Python driver, which \nmanages the secure connection and returns the results. Instead of \nrestricting the output with a fixed LIMIT (e.g., LIMIT 5), the \nsystem retrieves candidate results and evaluates them using \nstandardized retrieval metrics such as Hits@1, Hits@5, and \nHits@10. This approach ensures that the system remains \nresponsive while providing a fair assessment of ranking quality \nacross different cutoff levels, rather than depending solely on a \nfixed number of returned items. Neo4j’s indexing on key node \nproperties, such as name and primary identifiers, also helps speed \nup lookups as the knowledge graph grows. In cases where the \nlanguage model generates an incomplete or incorrect query, such \nas referencing nodes or relationships that do not exist, the system \ncatches the error and either retries with a simpler prompt or \ninforms the user. Together, these steps make sure that queries run \nquickly, return valid results, and keep the overall experience \nsmooth and reliable for biomedical question-answering. \nG. User Interface for Query Transparency \nThe system includes a lightweight, cross-platform graphical \nuser interface (GUI) implemented as a web application using the \nFlask framework in Python, with HTML and Bootstrap for \ninteractive visualization. The interface is designed to make the \nquestion-answering process transparent and accessible to users \nwithout technical expertise. It consists of three main panels: \n1. Input Panel: Where the user can enter a biomedical \nquestion in natural language. \n2. Query Panel: Which displays the Cypher query generated \nby the language model, allowing users to verify how their \nquestion was interpreted. \n3. Results Panel: Which presents the retrieved answers in a \nclear, readable format, accompanied by a brief natural \nlanguage explanation generated by the system. \nBy showing both the query and the answer, the GUI promotes \nuser trust and enables validation of the system’s reasoning \nprocess. The interface is lightweight enough to run smoothly on \nstandard desktop machines without additional dependencies, \nmaking it practical for local deployments in clinical or research \nsettings. Fig. 4 illustrates the overall layout. \nV. EXPERIMENTAL RESULTS \nIn this section, the proposed system is evaluated based on its \nability to translate natural-language biomedical questions into \nexecutable Cypher queries over the iBKH knowledge graph. The \nassessment is conducted end-to-end and focuses on three main \naspects: (i) the accuracy of query generation, measured by the \nproportion of correctly produced Cypher queries; (ii) system \nefficiency, quantified through total response time from question \nsubmission to final answer delivery, covering both query \ngeneration and execution; and (iii) the quality of retrieved \nanswers, evaluated using standardized information retrieval \nmetrics including Precision@k, Recall@k, F1@k, Hits@k, and \nMean Reciprocal Rank (MRR). For clarity, all these metrics are \nformally defined in the following subsections, with their \ncorresponding equations, and will be used consistently \nthroughout the results section. Together, these dimensions \nprovide a comprehensive view of both correctness and efficiency \nacross simple, medium, and complex biomedical queries. \nA. Description of the Experimental Data \nTo evaluate the proposed system, this work used a carefully \nselected set of biomedical questions designed to test how well the \nsystem understands natural language and converts it into Cypher \nqueries for a biomedical knowledge graph. \nTo evaluate the system, a benchmark of 60 biomedical \nquestions was constructed by the authors, guided by the schema \nand relations in iBKH. The questions were grouped into simple, \nmedium, and complex levels to assess performance across \ndifferent reasoning requirements. Gold-standard answers were \nmanually prepared for each question to enable quantitative \nevaluation using Exact Match, Precision@k, Recall@k, F1@k, \nHits@k, and MRR. The complete set of 60 questions is available \nat this link https://drive.google.com/drive/my-drive.'
 'Omar & Mohammed / Journal of Applied Science and Technology Trends Vol. 06, No. 02, pp. 342 –357 (2025) \n \n344 \nevaluated using Exact Match, Precision, Recall, F1, Hits@k, \nMRR, and latency across simple, medium, and complex question \nsets. Unlike prior template-based methods, our approach enables \ntraceable, outcome-level validation. In summary, the main \ncontributions of this work are as follows: \n• Hybrid LLM to Cypher QA: A system that translates \nnatural language questions into accurate, executable \nCypher over a Neo4j drug, disease, and symptom KG. \n• Prompt-driven query generation: Schema,  entity \naware prompting that reliably maps diverse biomedical \nquestions to graph queries. \n• Evidence transparency: Along with each answer, we \nsurface the generated Cypher and the supporting \njustification subgraph (nodes, edges) plus a brief natural \nlanguage rationale. \n• Answer-level evaluation: End-to-end assessment using \nExact Match, F1, Precision/Recall, Hits@k, MRR and \nlatency across simple, medium and complex tiers. \n• Local, reproducible deployment: On-prem LLaMA 3 \ninference (no cloud dependency) suitable for biomedical \nsettings requiring low latency and strong data control. \n \nThe remainder of this paper is organized as follows: Section \n2 reviews related work on biomedical knowledge graphs and \nLLM-based QA systems. Section 3 provides background on \nknowledge graphs, large language models, and question \nanswering frameworks. Section 4 details the proposed \nmethodology, \nincluding \nsystem \narchitecture, \ndataset \nconstruction, and query translation. Section 5 presents the \nexperimental results through both quantitative metrics and \nqualitative case studies. Section 6 discusses the findings, analyzes \nlimitations, and compares performance against baseline models. \nFinally, Section 7 concludes this paper and outlines directions for \nfuture work. \nII. RELATED WORK \nRecently, studies have concentrated on the integration of \nclinical and medical knowledge graphs (LLM) to improve the \nanswer to medical questions. Researchers have derived several \nbiomedical KGs using Neo4j and incorporated the application of \nLLMs like LLaMA and GPT to convert natural language \nquestions into graph queries. Improvements in answer \ncorrectness, reduction of hallucination errors, one-to-many \nrelationships, and support for complex reasoning were the \nobjectives of these efforts. Some frameworks also adopted \nretrieval methods to ground responses in secure data. \nSu et al.[11] developed an integrative Biomedical Knowledge \nHub (iBKH), a huge biomedical knowledge graph that comprises \n18 of the very best data sources. The deployment of the iBKH in \nNeo4j allows for a user-friendly web portal to allow fast and \ninteractive knowledge retrieval. The system implemented \nadvanced graph learning techniques to enable the discovery of \nbiomedical knowledge, illustrated by an example of repurposing \nin silico drugs for Alzheimer’s disease. iBKH achieved \npromising predictive performance for known drugs and proposed \npossible new drug candidates. \nRajabi and Kafaie[19] proposed a disease knowledge graph \nusing a cross-referential disease database comprising diseases, \nsymptoms, and drugs interconnected with relationships. They \ntransferred the data into Neo4j to create a graph of 9,400 nodes \nand 45,000 relationships representing the semantic links between \nmedical concepts. Applying Cypher queries enabled answering \ncomplex medical questions regarding identifying drugs that may \ncause certain diseases; it was demonstrated that the graph inferred \nnew relationships not explicitly existing in the original data. The \nconclusion was that disease knowledge graphs sped up clinical \ndiscovery and contributed to understanding complex medical \nrelationships. \nHou et al.[3] assessed and contrasted ChatGPT (both GPT-\n3.5 and GPT-4) and the biomedical knowledge graphs (BKGs) \nconcerning their ability to answer biomedical questions, generate \nnew knowledge, and reason. Their datasets were focused on \ndietary supplements and drugs, while evaluation criteria entailed \naccuracy, novelty, and reasoning ability. The results indicate that \nwhile GPT-4 surpassed GPT-3.5 and BKGs in knowledge \nprovision, it proved inconsistent with regard to citations and \nreasoning. Compared to them, BKGs scored higher in accuracy \nand reliability, especially in discovering novel links as well as \nwithin structured reasoning. \nSoman et al.[13] presented a novel framework called KG-\nRAG that integrates a large biomedical knowledge graph \n(SPOKE) with LLaMA 2, GPT-3.5, and GPT-4 (LLMs) to \nproduce accurate biomedical text. They optimized the retrieval of \nrelevant graph context to cut over 50% tokens without losing \naccuracy. It aided LLMs in performing better on biomedical \nquestion answering with very high accuracy boosts, especially in \nthe case of LLaMA 2. They compared KG-RAG to other retrieval \nmethods and indicated its comparatively more robust and \nefficient results. The framework produced reliable evidence-\nbased answers grounded in biomedical knowledge. \nLuo et al.[23] created ChatKBQA, a new framework with a \nquestion-and-answer approach over knowledge bases that first \ngenerates logical forms with the help of fine-tuned LLMs and \nthen retrieves the relevant entities and relations. This generate-\nthen-retrieve approach is supposed to handle a couple of issues \nwith the earlier methods concerning tedious retrieval and error \npropagation. They fine-tuned open-source LLMs like LLaMA 2 \nto change natural-language questions into logical forms with high \naccuracy. The retrieval phase uses unsupervised phrase-level \nsemantic matching in a way that enhances the alignment of \nentities and relations. Experiments on benchmark datasets \nindicate ChatKBQA to be superior to its predecessors, with the \nhighest accuracy to date. \nPusch and Conrad[6] conducted work under a hybrid \napproach conflating LLMs and biomedical Knowledge Graphs \n(KGs) to suppress hallucination errors in question-answering. \nThey proposed query-checking algorithms for validating, \ncorrecting, and executing the KG Cypher queries that LLMs \ngenerated, thereby attaining accurate and understandable \nanswers. The system used retrieval-augmented generation (RAG) \nto ground answers within KG data. The methodology was \nvalidated on a biomedical KG called PrimeKG using 50 \nbenchmark questions, assessing models like GPT-4 Turbo and \nLLaMA 3. Commercially available GPT-4 Turbo obtained']","['<1-hop>\n\nOmar & Mohammed / Journal of Applied Science and Technology Trends Vol. 06, No. 02, pp. 342 –357 (2025) 351 This query searches for a disease node whose name contains ’alzheimer’ and follows HAS_SYMPTOM edges to list related symptom names. The system then executes this cypher to retrieve answers. The prompts (such as few-shot examples and schema hints) were carefully designed to help LLaMA 3 generate correct Cypher queries. The model learns how to use the graph’s labels and relationships properly. For example, if a user asks, ’Which drugs treat diabetes?’, LLaMA might respond with a suitable Cypher query: MATCH (d:Drug)-[:TREATS]->(di:Disease) WHERE toLower(di.name) CONTAINS ""diabetes"" RETURN d.name This queries for drug nodes that have a TREATS edge to a diabetes disease node. By leveraging LLaMA 3 in this way, our system can flexibly handle many phrasing variations without manual mapping rules. D. Model Configuration & Decoding We run a local LLaMA 3.2-3B model in GGUF format (llama- 3.2-3b-instruct-q4_k_m.gguf) via llama.cpp, as shown in Table IV. TABLE IV. MODEL RUNTIME AND DECODING SETTINGS Runtime settings Decoding settings n_ctx = 1024 temperature = 0.2 n_threads = 12 top_p = 0.95 n_gpu_layers = 33 top_k = 40 n_batch = 512 repeat_penalty = 1.1 max_tokens = 80 seed = 42 E. Graph Subset and Versioning We use an iBKH derived subgraph (≈65.8k nodes; ≈3.0M edges) spanning DRUG, DISEASE, SYMPTOM, PATHWAY. IDs are normalized to CURIEs and duplicates collapsed across UMLS/DrugBank/DisGeNET/SIDER/KEGG. Each edge stores provenance/licensing metadata (source, source_version, license, retrieved_at, evidence_pmids/urls) and, when available, temporal fields (valid_from, valid_to). We report coverage as the percentage of evaluated questions whose gold entities/relations are present. F. Query Execution and Reliability After a Cypher query is generated, it is executed on the Neo4j database through the official Neo4j Python driver, which manages the secure connection and returns the results. Instead of restricting the output with a fixed LIMIT (e.g., LIMIT 5), the system retrieves candidate results and evaluates them using standardized retrieval metrics such as Hits@1, Hits@5, and Hits@10. This approach ensures that the system remains responsive while providing a fair assessment of ranking quality across different cutoff levels, rather than depending solely on a fixed number of returned items. Neo4j’s indexing on key node properties, such as name and primary identifiers, also helps speed up lookups as the knowledge graph grows. In cases where the language model generates an incomplete or incorrect query, such as referencing nodes or relationships that do not exist, the system catches the error and either retries with a simpler prompt or informs the user. Together, these steps make sure that queries run quickly, return valid results, and keep the overall experience smooth and reliable for biomedical question-answering. G. User Interface for Query Transparency The system includes a lightweight, cross-platform graphical user interface (GUI) implemented as a web application using the Flask framework in Python, with HTML and Bootstrap for interactive visualization. The interface is designed to make the question-answering process transparent and accessible to users without technical expertise. It consists of three main panels: 1. Input Panel: Where the user can enter a biomedical question in natural language. 2. Query Panel: Which displays the Cypher query generated by the language model, allowing users to verify how their question was interpreted. 3. Results Panel: Which presents the retrieved answers in a clear, readable format, accompanied by a brief natural language explanation generated by the system. By showing both the query and the answer, the GUI promotes user trust and enables validation of the system’s reasoning process. The interface is lightweight enough to run smoothly on standard desktop machines without additional dependencies, making it practical for local deployments in clinical or research settings. Fig. 4 illustrates the overall layout.'
 '<2-hop>\n\nV. EXPERIMENTAL RESULTS In this section, the proposed system is evaluated based on its ability to translate natural-language biomedical questions into executable Cypher queries over the iBKH knowledge graph. The assessment is conducted end-to-end and focuses on three main aspects: (i) the accuracy of query generation, measured by the proportion of correctly produced Cypher queries; (ii) system efficiency, quantified through total response time from question submission to final answer delivery, covering both query generation and execution; and (iii) the quality of retrieved answers, evaluated using standardized information retrieval metrics including Precision@k, Recall@k, F1@k, Hits@k, and Mean Reciprocal Rank (MRR). For clarity, all these metrics are formally defined in the following subsections, with their corresponding equations, and will be used consistently throughout the results section. Together, these dimensions provide a comprehensive view of both correctness and efficiency across simple, medium, and complex biomedical queries. A. Description of the Experimental Data To evaluate the proposed system, this work used a carefully selected set of biomedical questions designed to test how well the system understands natural language and converts it into Cypher queries for a biomedical knowledge graph. To evaluate the system, a benchmark of 60 biomedical questions was constructed by the authors, guided by the schema and relations in iBKH. The questions were grouped into simple, medium, and complex levels to assess performance across different reasoning requirements. Gold-standard answers were manually prepared for each question to enable quantitative evaluation using Exact Match, Precision@k, Recall@k, F1@k, Hits@k, and MRR. The complete set of 60 questions is available at this link https://drive.google.com/drive/my-drive.'
 '<3-hop>\n\nOmar & Mohammed / Journal of Applied Science and Technology Trends Vol. 06, No. 02, pp. 342 –357 (2025) 345 record-high accuracy, while open-source models achieved impressive gains through prompt optimization. Feng et al.[22] developed the Knowledge Graph-based Thought (KGT) framework that integrated LLMs with a pan- cancer knowledge graph for biomedical question answering. KGT was designed to reason on the knowledge graph schema and identify optimal subgraphs to use for directing accurate answer generation, all without fine-tuning the LLMs. The framework is benchmarked against a new dataset (PcQA) designed specifically for pan-cancer KGQA tasks and has outperformed all existing state-of-the-art approaches by a rather large margin. KGT’s practicality in biomedical issues was highlighted through case studies for drug repositioning, drug resistance, and biomarker discovery. Their approach exhibited robust adaptability among various LLMs. Rezaei et al.[26] developed AMG-RAG, a dynamic framework that utilizes autonomous LLM agents with medical search tools in the continuous construction and real-time updating of Medical Knowledge Graphs (MKGs). Their system incorporated confidence scoring and multi-hop reasoning to improve accuracy and interpretability in medical question answering. AMG-RAG outperformed size models on both very hard MEDQA benchmarks and more accessible MedMCQA ones, proving that it could conduct efficient reasoning based on current structured medical knowledge. They also used Neo4j to manage the knowledge graphs while adding external searches to ensure the latest data. Tiwari et al.[24] presented Auto-Cypher, a recent automated pipeline for producing high-quality synthetic data for training LLMs by mapping natural language to Cypher queries for graph databases like Neo4j. The pipeline deployed the novelty of LLM- as-database-filler to synthesize Neo4j databases for the execution of generated queries to ensure their correctness. A sizable dataset called SynthCypher was created, spanning multiple domains and complex queries, leading to a 40% improvement in LLM performance on Cypher generation. The datasets were used to fine-tune open-source models such as LLaMA, Mistral, and Qwen, and the SPIDER benchmark was adapted for evaluation purposes. Mohammed et al.[29] proposed a hybridized GraphRAG framework combining Neo4j-based UMLS knowledge graphs with a vector store for medical textbooks to create an improved U.S.M.L.E.-style clinical question-answering approach. The project integrated symbolic reasoning from knowledge graphs with semantic retrieval performed on text embeddings to enhance relevance and accuracy via adaptive re-ranking and query expansion. The system had the answers produced by GPT-4o- Mini, with different prompting strategies encouraging evidence- based and traceable responses grounded in verified medical knowledge. Experiments showed that the hybrid approach improved factual accuracy and citation fidelity as compared to the L.L.M.-only approach, enhancing transparency and reliability. It is shown that binding both structured and unstructured medical knowledge sources could aid in ameliorating hallucinations and hence improve clinical trustworthiness in AI-driven medical QA. Yang et al.[30] articulated sepsis knowledge graph was crafted by combining multicenter clinical data from over 10,000 patients with the help of GPT-4 for entity recognition and relationship extraction. Real-world data were collected from three hospitals and integrated with clinical guidelines and databases from the public domain. The knowledge graph contained 1,894 nodes and 2,021 relationships pertaining to diseases, symptoms, biomarkers, treatments, and complications. GPT outperformed other models in every resolution on sepsis- specific datasets to obtain high F1-score results. The constructed graph highlighted complex interactions in sepsis for assisting clinical decision-making and was implemented on Neo4j. Guan et al.[20] proposed a novel method for constructing a local knowledge graph from retrieved biomedical documents by extracting propositional claims. They carried out layer wise summarization on this graph to capture multi-document relationships and provide comprehensive contextual information to a language model for question-answering purposes. The method resolved issues in multi-document biomedical QA, such as noise cancellation and efficient context usage. They then tested their method on several benchmarks for biomedical question answering, achieving performance at least comparable to, if not better than, existing retrieval-augmented generation (RAG) baselines. The study established enhanced reasoning and answer accuracy of the model achieved through structured graph summarization. Previous studies have improved biomedical QA using KGs and LLMs, but important gaps remain. Most systems lack transparent, graph-based justifications, rely on limited evaluation methods, or depend on cloud resources that reduce privacy and reproducibility. Our framework addresses these gaps by providing visible Cypher queries with evidence subgraphs, applying comprehensive performance metrics across difficulty levels, and ensuring fully local, privacy-preserving deployment. Table I summarizes key previous studies on biomedical knowledge graphs and question answering, outlining their methods, datasets, and main limitations.']","The system ensures accurate and reliable biomedical question answering over the iBKH knowledge graph by integrating **Cypher query generation** with **multi-hop reasoning** and employing **confidence scoring** mechanisms, as detailed below:

1. **Cypher Query Generation via LLaMA-3B**  
   - The system uses a locally deployed LLaMA-3B language model to translate natural language biomedical questions into executable Cypher queries tailored for the Neo4j-based iBKH knowledge graph.  
   - Prompt engineering incorporates schema awareness and entity hints, enabling the model to correctly map diverse biomedical questions to graph queries. This approach supports flexible handling of phrasing variations without manual rule crafting.  
   - The generated Cypher queries explicitly encode the relationships and constraints needed to retrieve relevant biomedical facts, including multi-hop relations (e.g., drugs treating diseases and causing side effects).  
   - Query generation accuracy is high, with exact match rates of approximately 96% for simple queries and 86.7% for complex, multi-hop queries, reflecting the system’s ability to handle compositional reasoning.

2. **Multi-hop Reasoning with Schema-Constrained Query Generation**  
   - Complex biomedical questions often require multi-hop reasoning, involving multiple relationships and entities (e.g., linking drugs to diseases and then to side effects).  
   - The system supports multi-hop queries by generating Cypher queries that traverse multiple edges in the knowledge graph, enabling retrieval of interconnected biomedical concepts.  
   - To improve robustness in multi-hop scenarios, the system employs path-guided decoding and schema constraints during query generation, ensuring that generated queries respect the graph’s structure and semantics.  
   - Error analysis identified that failures in multi-hop queries often stem from missing relation hops or brittle name-based matching; to mitigate this, the system uses concept-level normalization (e.g., UMLS, SNOMED, RxNorm) and synonym expansion to enhance semantic matching.

3. **Confidence Scoring and Evaluation Metrics**  
   - After query execution, the system evaluates retrieved answers using standardized information retrieval metrics such as Precision@k, Recall@k, F1@k, Hits@k, and Mean Reciprocal Rank (MRR). These metrics provide quantitative confidence measures about the relevance and ranking quality of answers.  
   - Instead of limiting results to a fixed number, the system retrieves candidate answers and assesses them across multiple cutoff levels (k=1,5,10), ensuring that correct answers surface near the top and persist in the ranked list.  
   - The system also performs error handling by detecting invalid or incomplete queries (e.g., referencing non-existent nodes or relationships) and either retries with simpler prompts or informs the user, maintaining reliability.  
   - Although current correctness assessment focuses on answer-level metrics, future plans include semantic equivalence checks across alternative Cypher queries and confidence intervals to further enhance robustness.

4. **Transparency and Evidence-grounded Answers**  
   - The system provides a transparent interface that displays the generated Cypher query alongside the retrieved answers and supporting justification subgraphs (nodes and edges).  
   - This transparency allows users to verify the reasoning path and increases trust in the system’s outputs, which is critical in biomedical contexts.  
   - The justification subgraphs serve as evidence, grounding answers in explicit graph data rather than opaque model outputs.

5. **Offline, Privacy-preserving Deployment**  
   - Operating fully offline without reliance on cloud APIs ensures data privacy, compliance, and reproducibility, which are essential in sensitive biomedical and clinical environments.  
   - The system maintains low latency (~6 seconds average response time) suitable for interactive use, balancing efficiency with thorough multi-hop reasoning.

**In summary**, the system leverages LLaMA-3B to generate precise, schema-aware Cypher queries that enable multi-hop reasoning over the richly interconnected iBKH knowledge graph. Confidence in answer correctness is established through rigorous retrieval metrics and error handling, while transparency via query and evidence visualization fosters user trust. This hybrid approach effectively mitigates hallucination risks inherent in LLM-only systems and delivers accurate, reliable biomedical question answering suitable for real-world clinical and research applications.","The system uses LLaMA 3 to generate Cypher queries by learning to utilize the graph's labels and relationships properly, enabling flexible handling of various natural language biomedical questions without manual mapping rules. After generating a Cypher query, it is executed on the Neo4j database, which manages secure connections and returns results efficiently. The system evaluates retrieved answers using standardized metrics such as Hits@k and Mean Reciprocal Rank (MRR) to assess ranking quality rather than relying on fixed limits, ensuring responsiveness and fairness. Additionally, the system catches errors from incomplete or incorrect queries and retries with simpler prompts or informs the user, maintaining reliability. Multi-hop reasoning and confidence scoring are incorporated as described in related frameworks like AMG-RAG, which utilize autonomous LLM agents and medical search tools to improve accuracy and interpretability in medical question answering. This combination of transparent Cypher query generation, multi-hop reasoning, and confidence scoring over the iBKH knowledge graph enables accurate, efficient, and trustworthy biomedical question answering.",0.8987341772151899,0.9486962571841895,0.8874999999778125,0.6666666666666666
How does LLaMA 3 utilize the Integrated Biomedical Knowledge Hub (iBKH) dataset to improve the accuracy and transparency of biomedical question answering in the described system?,"['Omar & Mohammed / Journal of Applied Science and Technology Trends Vol. 06, No. 02, pp. 342 –357 (2025) \n \n347 \nIn biomedical research and clinical settings, LLMs help \ntranslate natural language questions from doctors, researchers, or \npatients into precise, structured queries that can be executed on \nbiomedical knowledge graphs and databases. This makes it easier \nto retrieve detailed biomedical information like drug interactions, \ngene-disease associations, and symptom descriptions [42, 43]. \nDespite their power, LLMs can sometimes generate incorrect \nor fabricated responses, a phenomenon known as hallucination, \nwhich poses risks in sensitive biomedical contexts. These \nhallucinations occur because the models generate plausible \nanswers based on patterns learned from data rather than verified \nknowledge. To mitigate this, researchers integrate LLMs with \nbiomedical knowledge graphs to ground answers in factual data, \nsignificantly improving accuracy and reducing misinformation \n[4]. Further improvements come from fine-tuning LLMs on \nbiomedical corpora and carefully engineering prompts, which \nenhance their reliability and relevance in medical question \nanswering. \nAdditionally, combining LLMs with knowledge graphs and \nreasoning techniques is an active area of research that promises \nto increase the interpretability and trustworthiness of AI systems \nin biomedicine. These advances are critical for developing tools \nthat assist clinical decision-making and accelerate biomedical \ndiscoveries [43, 44]. \nIV. METHODS AND MATERIALS \nThis section describes the methodology used to build a \nbiomedical question-answer system. The proposed method \nconsists of three main stages; First, a biomedical knowledge \ngraph is constructed in the data ingestion phase, using structured \ndata sources (e.g., diseases, drugs, symptoms). Second, a \nlanguage model (LLaMA 3) interprets the user’s question \nwritten in English in the user interaction phase and converts it \ninto a Cypher query. Third, a graphical user interface allows \nusers to type questions and view the results interactively. \nA. System Architecture \nThe proposed framework is organized as a step-by-step \npipeline that integrates a local large language model (LLM) with \na biomedical knowledge graph stored in Neo4j. The overall \nworkflow is illustrated in Fig. 2. Each module performs a \nspecific function, and together they ensure that the system \ndelivers accurate, reliable, and explainable answers. \n \n \nFig. 2. Workflow of the LLaMA 3 and Neo4j-Based QA System \n \nStep 1. User Input (Flask Web Interface): A user submits \na biomedical question in natural language through a Flask-based \nweb interface. The same interface will later display the answer, \nthe executed query, and a compact preview of the retrieved rows. \nStep 2. LLM Processing and Initial Cypher: The text query \nis forwarded to a local LLaMA 3 module, which interprets the \nintent and drafts an initial Cypher pattern suitable for querying \nthe graph. \nStep 3. Cypher Query Generation: The Cypher Query Gen \nblock receives the initial pattern from LLaMA 3, canonicalizes \nand completes it (projection, DISTINCT, filters), and returns a \nfinalized Cypher query to the model. \nStep 4. Query execution on Neo4j: LLaMA 3 passes the \nfinalized query to the Query execution component (inside the \n’Answer modules’ box), which runs it against the Neo4j \nKnowledge Graph. KG supplies knowledge graph data (e.g.,'
 'Omar & Mohammed / Journal of Applied Science and Technology Trends Vol. 06, No. 02, pp. 342 –357 (2025) \n \n351 \nThis query searches for a disease node whose name contains \n’alzheimer’ and follows HAS_SYMPTOM edges to list related \nsymptom names. The system then executes this cypher to retrieve \nanswers. The prompts (such as few-shot examples and schema \nhints) were carefully designed to help LLaMA 3 generate correct \nCypher queries. The model learns how to use the graph’s labels \nand relationships properly. For example, if a user asks, ’Which \ndrugs treat diabetes?’, LLaMA might respond with a suitable \nCypher query: \n \nMATCH (d:Drug)-[:TREATS]->(di:Disease) \nWHERE toLower(di.name) CONTAINS ""diabetes"" \nRETURN d.name  \n \nThis queries for drug nodes that have a TREATS edge to a \ndiabetes disease node. By leveraging LLaMA 3 in this way, our \nsystem can flexibly handle many phrasing variations without \nmanual mapping rules. \nD. Model Configuration & Decoding \nWe run a local LLaMA 3.2-3B model in GGUF format (llama-\n3.2-3b-instruct-q4_k_m.gguf) via llama.cpp, as shown in Table \nIV. \nTABLE IV.  MODEL RUNTIME AND DECODING SETTINGS \nRuntime settings \nDecoding settings \nn_ctx = 1024 \ntemperature = 0.2 \nn_threads = 12 \ntop_p = 0.95 \nn_gpu_layers = 33 \ntop_k = 40 \nn_batch = 512 \nrepeat_penalty = 1.1 \n \nmax_tokens = 80 \n \nseed = 42 \n \nE. Graph Subset and Versioning \nWe use an iBKH derived subgraph (≈65.8k nodes; ≈3.0M \nedges) spanning DRUG, DISEASE, SYMPTOM, PATHWAY. IDs are \nnormalized to CURIEs and duplicates collapsed across \nUMLS/DrugBank/DisGeNET/SIDER/KEGG. Each edge stores \nprovenance/licensing metadata (source, source_version, license, \nretrieved_at, \nevidence_pmids/urls) \nand, \nwhen \navailable, \ntemporal fields (valid_from, valid_to). We report coverage as \nthe \npercentage \nof \nevaluated \nquestions \nwhose \ngold \nentities/relations are present. \nF. Query Execution and Reliability \nAfter a Cypher query is generated, it is executed on the Neo4j \ndatabase through the official Neo4j Python driver, which \nmanages the secure connection and returns the results. Instead of \nrestricting the output with a fixed LIMIT (e.g., LIMIT 5), the \nsystem retrieves candidate results and evaluates them using \nstandardized retrieval metrics such as Hits@1, Hits@5, and \nHits@10. This approach ensures that the system remains \nresponsive while providing a fair assessment of ranking quality \nacross different cutoff levels, rather than depending solely on a \nfixed number of returned items. Neo4j’s indexing on key node \nproperties, such as name and primary identifiers, also helps speed \nup lookups as the knowledge graph grows. In cases where the \nlanguage model generates an incomplete or incorrect query, such \nas referencing nodes or relationships that do not exist, the system \ncatches the error and either retries with a simpler prompt or \ninforms the user. Together, these steps make sure that queries run \nquickly, return valid results, and keep the overall experience \nsmooth and reliable for biomedical question-answering. \nG. User Interface for Query Transparency \nThe system includes a lightweight, cross-platform graphical \nuser interface (GUI) implemented as a web application using the \nFlask framework in Python, with HTML and Bootstrap for \ninteractive visualization. The interface is designed to make the \nquestion-answering process transparent and accessible to users \nwithout technical expertise. It consists of three main panels: \n1. Input Panel: Where the user can enter a biomedical \nquestion in natural language. \n2. Query Panel: Which displays the Cypher query generated \nby the language model, allowing users to verify how their \nquestion was interpreted. \n3. Results Panel: Which presents the retrieved answers in a \nclear, readable format, accompanied by a brief natural \nlanguage explanation generated by the system. \nBy showing both the query and the answer, the GUI promotes \nuser trust and enables validation of the system’s reasoning \nprocess. The interface is lightweight enough to run smoothly on \nstandard desktop machines without additional dependencies, \nmaking it practical for local deployments in clinical or research \nsettings. Fig. 4 illustrates the overall layout. \nV. EXPERIMENTAL RESULTS \nIn this section, the proposed system is evaluated based on its \nability to translate natural-language biomedical questions into \nexecutable Cypher queries over the iBKH knowledge graph. The \nassessment is conducted end-to-end and focuses on three main \naspects: (i) the accuracy of query generation, measured by the \nproportion of correctly produced Cypher queries; (ii) system \nefficiency, quantified through total response time from question \nsubmission to final answer delivery, covering both query \ngeneration and execution; and (iii) the quality of retrieved \nanswers, evaluated using standardized information retrieval \nmetrics including Precision@k, Recall@k, F1@k, Hits@k, and \nMean Reciprocal Rank (MRR). For clarity, all these metrics are \nformally defined in the following subsections, with their \ncorresponding equations, and will be used consistently \nthroughout the results section. Together, these dimensions \nprovide a comprehensive view of both correctness and efficiency \nacross simple, medium, and complex biomedical queries. \nA. Description of the Experimental Data \nTo evaluate the proposed system, this work used a carefully \nselected set of biomedical questions designed to test how well the \nsystem understands natural language and converts it into Cypher \nqueries for a biomedical knowledge graph. \nTo evaluate the system, a benchmark of 60 biomedical \nquestions was constructed by the authors, guided by the schema \nand relations in iBKH. The questions were grouped into simple, \nmedium, and complex levels to assess performance across \ndifferent reasoning requirements. Gold-standard answers were \nmanually prepared for each question to enable quantitative \nevaluation using Exact Match, Precision@k, Recall@k, F1@k, \nHits@k, and MRR. The complete set of 60 questions is available \nat this link https://drive.google.com/drive/my-drive.'
 'Vol. 06, No. 02, pp. 342 –357 (2025) \nISSN: 2708-0757 \n \nJOURNAL OF APPLIED SCIENCE AND TECHNOLOGY TRENDS \n \nwww.jastt.org  \n \n                                                          342 \ndoi: 10.38094/jastt62404  \nA Hybrid LLM–Knowledge Graph Framework for Accurate \nBiomedical Question Answering \n \nHavraz Y. Omar1,²*, Abdulhakeem O. Mohammed³  \n \n¹Department of Information Technology, Technical College of Duhok, Duhok Polytechnic University, Duhok, Kurdistan Region, \nIraq. havraz.omar@dpu.edu.krd  \n² Department of Information Technology, Technical College of Informatics – Akre, Akre University for Applied Sciences, Akre, \nKurdistan Region, Iraq. \n³ Department of Computer Science, College of Science, University of Zakho, Zakho, Kurdistan Region, Iraq. \na.mohammed@uoz.edu.krd \n \n*Correspondence: havraz.omar@dpu.edu.krd \n \nAbstract \nBiomedical question answering requires accurate and interpretable systems; however, existing approaches often face challenges such as \nlanguage model hallucinations and limited reasoning when relying solely on standalone knowledge graphs. To address these limitations, \nthis study proposes a hybrid framework that integrates the LLaMA-3B language model with a Neo4j-based drug–disease–symptom \nknowledge graph. The system translates natural language questions into executable Cypher queries, operates on an iBKH-derived graph \ncomprising over 65,000 entities and 3 million relationships, and returns answers with supporting evidence through a transparent interface. \nExperiments conducted on 60 biomedical questions across three levels of difficulty demonstrate the robustness of the approach: 96% \nexact match for simple queries, 95% for medium queries, and 86.7% for complex queries. Overall, the system achieves Precision@5 of \n96.1%, Recall@5 of 89.0%, F1@5 of 91.0%, Hits@k of 96.1%, and an MRR of 94.4%, while maintaining an average response time of \nonly 6.07 seconds. These results indicate that the system retrieves nearly all relevant answers, ranks them correctly, and delivers them \nwith latency low enough for interactive use. Moreover, unlike cloud-based APIs such as ChatGPT, which require internet connectivity \nand external data transmission, the proposed framework operates fully offline, ensuring privacy, reproducibility, and compliance with \nbiomedical data governance. Overall, this pipeline provides an accurate, efficient, and privacy-preserving solution for biomedical question \nanswering, making it a practical alternative to cloud-dependent approaches in sensitive healthcare contexts. \n \nKeywords: Knowledge Graph, LLM, Question Answering, Neo4j, Biomedical Informatics, Healthcare AI, LLaMA 3. \n \nReceived: August 14th, 2025 / Revised: October 10th, 2025 / Accepted: October 16th, 2025 / Online: October 20th, 2025 \n \nI. INTRODUCTION \nAnswering questions in the biomedical field is a difficult task \ndue to the complexity of medical knowledge and the need for \nprecision. In recent years, large language models  (LLMs) like \nLLaMA, GPT-4 have made progress in understanding and \ngenerating human-like responses to medical questions  [1,  2]. \nThese models can process large amounts of information and \nrespond in natural language, which makes them helpful in \nhealthcare settings [3]. However, they often struggle to provide \naccurate answers when dealing with specialized biomedical \ncontent [4, 5]. \nOne major issue with LLMs is a problem called hallucination, \nwhere the model generates information that sounds right but is \nactually incorrect or unsupported [6]. In medical applications, \nthis can be dangerous, as healthcare professionals rely on precise \nand trustworthy information  [7]. Therefore, researchers are \nexploring ways to combine LLMs with structured sources of \nknowledge to improve their reliability [8]. \nLLM-only systems in biomedicine still hallucinate and are \nhard to verify, limiting safe use [9, 10]. Biomedical knowledge \ngraphs (BKGs) such as iBKH and SPOKE curate multi-source \nfacts and enable multi-hop reasoning, yet they neither interpret \nfree text nor generate answers [11, 12]. Recent hybrids (KG-\naware RAG) improve grounding but often lack explicit path-level \njustifications and robust end-to-end answer evaluation [13, 14]. \nRecent studies have increasingly integrated Knowledge \nGraphs (KGs) with Large Language Models (LLMs) to improve \nfactual accuracy, reasoning, and reduce hallucinations. Notable \nexamples include DR.KNOWS, which combines UMLS-based \nKGs with LLMs for better diagnostic reasoning [15], KnowNet'
 'Omar & Mohammed / Journal of Applied Science and Technology Trends Vol. 06, No. 02, pp. 342 –357 (2025) \n \n344 \nevaluated using Exact Match, Precision, Recall, F1, Hits@k, \nMRR, and latency across simple, medium, and complex question \nsets. Unlike prior template-based methods, our approach enables \ntraceable, outcome-level validation. In summary, the main \ncontributions of this work are as follows: \n• Hybrid LLM to Cypher QA: A system that translates \nnatural language questions into accurate, executable \nCypher over a Neo4j drug, disease, and symptom KG. \n• Prompt-driven query generation: Schema,  entity \naware prompting that reliably maps diverse biomedical \nquestions to graph queries. \n• Evidence transparency: Along with each answer, we \nsurface the generated Cypher and the supporting \njustification subgraph (nodes, edges) plus a brief natural \nlanguage rationale. \n• Answer-level evaluation: End-to-end assessment using \nExact Match, F1, Precision/Recall, Hits@k, MRR and \nlatency across simple, medium and complex tiers. \n• Local, reproducible deployment: On-prem LLaMA 3 \ninference (no cloud dependency) suitable for biomedical \nsettings requiring low latency and strong data control. \n \nThe remainder of this paper is organized as follows: Section \n2 reviews related work on biomedical knowledge graphs and \nLLM-based QA systems. Section 3 provides background on \nknowledge graphs, large language models, and question \nanswering frameworks. Section 4 details the proposed \nmethodology, \nincluding \nsystem \narchitecture, \ndataset \nconstruction, and query translation. Section 5 presents the \nexperimental results through both quantitative metrics and \nqualitative case studies. Section 6 discusses the findings, analyzes \nlimitations, and compares performance against baseline models. \nFinally, Section 7 concludes this paper and outlines directions for \nfuture work. \nII. RELATED WORK \nRecently, studies have concentrated on the integration of \nclinical and medical knowledge graphs (LLM) to improve the \nanswer to medical questions. Researchers have derived several \nbiomedical KGs using Neo4j and incorporated the application of \nLLMs like LLaMA and GPT to convert natural language \nquestions into graph queries. Improvements in answer \ncorrectness, reduction of hallucination errors, one-to-many \nrelationships, and support for complex reasoning were the \nobjectives of these efforts. Some frameworks also adopted \nretrieval methods to ground responses in secure data. \nSu et al.[11] developed an integrative Biomedical Knowledge \nHub (iBKH), a huge biomedical knowledge graph that comprises \n18 of the very best data sources. The deployment of the iBKH in \nNeo4j allows for a user-friendly web portal to allow fast and \ninteractive knowledge retrieval. The system implemented \nadvanced graph learning techniques to enable the discovery of \nbiomedical knowledge, illustrated by an example of repurposing \nin silico drugs for Alzheimer’s disease. iBKH achieved \npromising predictive performance for known drugs and proposed \npossible new drug candidates. \nRajabi and Kafaie[19] proposed a disease knowledge graph \nusing a cross-referential disease database comprising diseases, \nsymptoms, and drugs interconnected with relationships. They \ntransferred the data into Neo4j to create a graph of 9,400 nodes \nand 45,000 relationships representing the semantic links between \nmedical concepts. Applying Cypher queries enabled answering \ncomplex medical questions regarding identifying drugs that may \ncause certain diseases; it was demonstrated that the graph inferred \nnew relationships not explicitly existing in the original data. The \nconclusion was that disease knowledge graphs sped up clinical \ndiscovery and contributed to understanding complex medical \nrelationships. \nHou et al.[3] assessed and contrasted ChatGPT (both GPT-\n3.5 and GPT-4) and the biomedical knowledge graphs (BKGs) \nconcerning their ability to answer biomedical questions, generate \nnew knowledge, and reason. Their datasets were focused on \ndietary supplements and drugs, while evaluation criteria entailed \naccuracy, novelty, and reasoning ability. The results indicate that \nwhile GPT-4 surpassed GPT-3.5 and BKGs in knowledge \nprovision, it proved inconsistent with regard to citations and \nreasoning. Compared to them, BKGs scored higher in accuracy \nand reliability, especially in discovering novel links as well as \nwithin structured reasoning. \nSoman et al.[13] presented a novel framework called KG-\nRAG that integrates a large biomedical knowledge graph \n(SPOKE) with LLaMA 2, GPT-3.5, and GPT-4 (LLMs) to \nproduce accurate biomedical text. They optimized the retrieval of \nrelevant graph context to cut over 50% tokens without losing \naccuracy. It aided LLMs in performing better on biomedical \nquestion answering with very high accuracy boosts, especially in \nthe case of LLaMA 2. They compared KG-RAG to other retrieval \nmethods and indicated its comparatively more robust and \nefficient results. The framework produced reliable evidence-\nbased answers grounded in biomedical knowledge. \nLuo et al.[23] created ChatKBQA, a new framework with a \nquestion-and-answer approach over knowledge bases that first \ngenerates logical forms with the help of fine-tuned LLMs and \nthen retrieves the relevant entities and relations. This generate-\nthen-retrieve approach is supposed to handle a couple of issues \nwith the earlier methods concerning tedious retrieval and error \npropagation. They fine-tuned open-source LLMs like LLaMA 2 \nto change natural-language questions into logical forms with high \naccuracy. The retrieval phase uses unsupervised phrase-level \nsemantic matching in a way that enhances the alignment of \nentities and relations. Experiments on benchmark datasets \nindicate ChatKBQA to be superior to its predecessors, with the \nhighest accuracy to date. \nPusch and Conrad[6] conducted work under a hybrid \napproach conflating LLMs and biomedical Knowledge Graphs \n(KGs) to suppress hallucination errors in question-answering. \nThey proposed query-checking algorithms for validating, \ncorrecting, and executing the KG Cypher queries that LLMs \ngenerated, thereby attaining accurate and understandable \nanswers. The system used retrieval-augmented generation (RAG) \nto ground answers within KG data. The methodology was \nvalidated on a biomedical KG called PrimeKG using 50 \nbenchmark questions, assessing models like GPT-4 Turbo and \nLLaMA 3. Commercially available GPT-4 Turbo obtained'
 'Omar & Mohammed / Journal of Applied Science and Technology Trends Vol. 06, No. 02, pp. 342 –357 (2025) \n \n343 \nfor visualizing and validating LLM outputs [16], and MedKA for \nKG-enhanced question answering [17]. \nTo address these challenges, several recent works have \nexplored the integration of large language models with \nbiomedical knowledge graphs (KGs). A biomedical KG is a \nstructured network that connects entities such as diseases, drugs, \nand symptoms using defined relationships [18, 19]. These graphs \nstore verified medical knowledge from trusted databases, \nallowing for more accurate and explainable responses [12]. KGs \nare especially useful in multi-step reasoning tasks, where finding \nan answer requires connecting different pieces of information \n[20]. These entities and relationships can be visually represented \nin a biomedical knowledge graph, as shown in Fig. 1, where \nnodes represent medical concepts such as drugs, diseases, \nsymptoms, and pathways, and edges denote their semantic \nrelationships. \n \nFig. 1. Overview of Biomedical Knowledge Graph Entities and Relationships \n \nOne example of a widely used biomedical KG is SPOKE, \nwhich includes millions of nodes and relationships from over 40 \nbiomedical databases [12]. Integrating an LLM with a KG allows \nthe strengths of both technologies to work together: the LLM \nprovides language understanding, and the KG provides \nstructured, factual knowledge [21, 22]. A common method is \nretrieval-augmented generation (RAG), where the LLM retrieves \ninformation from the KG and uses it to generate more accurate \nresponses [13, 23]. In more advanced setups, the LLM can even \ngenerate queries like Cypher to fetch specific data from the graph \n[24, 25]. Neo4j is a popular graph database that supports fast and \nflexible storage and querying of knowledge graphs using Cypher \n[21]. It is well-suited for biomedical applications because it \nallows easy exploration of complex medical relationships. Recent \nwork has shown that combining Neo4j with LLMs can lead to \nbetter accuracy, fewer hallucinations, and more explainable \nresults [24, 26]. \nDespite improvements, building a reliable hybrid system that \ncombines an LLM with a biomedical KG remains a technical \nchallenge. Some approaches require complex pipelines or large \ntraining datasets, while others rely on fine-tuning specific to a \nnarrow set of questions [27, 28]. There is still a need for systems \nthat are both accurate and easy to scale, especially in domains like \nmedicine where the cost of errors is high [22]. \nRecent advances in KG-augmented LLMs have improved \nperformance, yet biomedical QA continues to face three practical \ngaps: \n1. Traceability: LLM-only or text-retrieval-only pipelines \nrarely provide graph-grounded justifications; users lack the \nability to inspect the exact nodes and edges that support an \nanswer. \n2. Evaluation: Prior work often judges quality via surface-\nform checks (e.g., matching a Cypher template), which fails to \ncapture end-to-end answer correctness or ranking quality \nacross different difficulty levels. \n3. Deployment: Many solutions assume cloud resources or \ndomain-specific fine-tuning, yet biomedical contexts typically \ndemand a local, privacy-preserving system with low latency \nand reproducible behavior. \nTimestamp-aware execution and periodic KG refresh help \navoid deprecated or contraindicated links, making the tool better \nsuited for safety-critical clinical contexts (e.g., drug–drug \ninteractions). \nTo address these limitations, Our work introduces a locally \ndeployable pipeline that translates biomedical questions into \nexecutable Cypher queries over a Neo4j knowledge graph. The \nsystem returns answers with supporting nodes and edges, and is']","['<1-hop>\n\nOmar & Mohammed / Journal of Applied Science and Technology Trends Vol. 06, No. 02, pp. 342 –357 (2025) 348 drugs, diseases, symptoms) and execution returns structured data (tuples) relevant to the question. Step 5. Answer Synthesis: The structured tuples flow to Answer Synthesis, which aggregates and formats them into a concise raw answer. This raw answer is sent back to LLaMA 3 to optionally refine the phrasing while preserving the retrieved facts. Step 6. Result Presentation: LLaMA 3 produces the final answer, which the interface displays together with the executed Cypher query and an optional preview of the returned rows, improving transparency and trust. The pipeline couples LLM-based language understanding (LLaMA 3) with a schema-grounded Neo4j knowledge graph. The Cypher Query Gen refines the query formulation, Query Execution retrieves evidence and Answer Synthesis converts structured results into readable outputs that produce answers that are accurate, interpretable, and easy to audit directly from the displayed query and evidence.'
 '<2-hop>\n\nB. Dataset and Knowledge Graph Construction 1) Dataset The integrated Biomedical Knowledge Hub (iBKH), a large biomedical knowledge base, forms the first level of the system and integrates information from various curated high-quality biomedical databases. This implies that the data set includes various types of entities, such as diseases, symptoms, drugs, biological pathways, etc. This study used the representative subset of the iBKH dataset, which contained 65828 biomedical entities. These entities are semantically interconnected through a total of 3004166 relationships, thus creating a rich knowledge graph. The iBKH dataset was originally introduced in [11], and it is freely available at (https://github.com/wcm-wanglab/iBKH). This dataset is the core semantic foundation upon which this study is built. The knowledge graph is populated from multiple tabular sources (CSV files), each listing entities or relationships. The main input files and their contents are as follows: • Disease vocabulary(disease_vocab.csv): Contains columns such as primary (a unique disease ID), name, do_id (Disease Ontology ID), kegg_id, and umls_cui (UMLS Concept ID). Each row represents a disease node with external identifiers. • Drug vocabulary (drug_vocab.csv): Includes primary (unique drug ID), name, drugbank_id, kegg_id, pharmgkb_id, umls_cui, mesh_id, iDISK_id and CID (PubChem ID). Each row defines a drug node with standard database identifiers. • Symptom vocabulary (symptom_vocab.csv): Contains primary (unique symptom ID), name, mesh_id, umls_cui and iDISK_id. Each row defines a symptom node. • Side effect vocabulary (side_effect_vocab.csv): Includes primary (unique side-effect ID) and name. Each row represents a side-effect node (with UMLS ID when available). • Pathway vocabulary (pathway_vocab.csv): Contains primary (unique pathway ID), name, reactome_id, go_id, and kegg_id. Each row defines a biological pathway node. Relationship files (each row typically contains two entity IDs and one or more boolean flags or codes) include: • Disease–Symptom links (Di_Sy_res.csv): Rows include Disease and Symptom IDs, a presence flag (1 or 0) and a data source. If Present = 1, a HAS_SYMPTOM edge is created from the disease to the symptom, with properties for presence and source. • Disease–Disease links (di_di_res.csv): Rows include Disease_1 and Disease_2 IDs with binary flags for is_a and Resemble. If is_a = 1, an (IS_A) edge is created (Disease_1 → Disease_2); if Resemble = 1, a RESEMBLES edge is created. The source field is used for provenance. • Drug–Disease links (D_Di_res.csv): Includes Drug and Disease IDs with several binary flags. If a flag equals 1, a corresponding edge is created: o TREATS (Treats = 1) o PALLIATES (Palliates = 1) o ASSOCIATED_WITH (Associate = 1) o ALLEVIATES_REDUCES (alleviates = 1) o TREATMENT_THERAPY (treatment/therapy = 1) o INHIBITS_CELL_GROWTH (inhibits cell growth = 1) o HAS_BIOMARKER (biomarkers = 1) o PREVENTS_SUPPRESSES (prevents/suppresses = 1) o ROLE_IN_PATHOGENESIS (role in disease pathogenesis = 1) • Drug–SideEffect links (D_SE_res.csv): Contains Drug and SideEffect IDs with a Source column. Each row creates a CAUSES edge from the drug to the side effect, with source as an edge property. • Drug–Drug interactions (D_D_res.csv): Rows include Drug_1 and Drug_2 IDs with flags for Interaction and Resemble. If Interaction = 1, an INTERACTS_WITH edge is created (bidirectional). If Resemble = 1, a RESEMBLES edge is added. • Drug–Pathway links (D_Pwy_res.csv): Includes Drug ID and Pathway ID. Each row generates an ASSOCIATED_WITH edge from the drug to the pathway. • Disease–Pathway links (Di_Pwy_res.csv): Contains Disease ID and Pathway ID. Each row creates an ASSOCIATED_WITH edge from the disease to the pathway. 2) Data Upload Performance The time required to upload different types of entities and relationships into the Neo4j biomedical knowledge graph, measured in seconds. These measurements reflect both the size and complexity of the data being processed. As shown in Table II, the longest upload time is for Drug- Drug Relationships, which takes approximately 190 seconds due to the large number of edges (over 3 million). Following this, Disease-Disease and Drug-Disease Relationships also require considerable time for loading. On the other hand, individual']","LLaMA 3 utilizes the Integrated Biomedical Knowledge Hub (iBKH) dataset in the described system to improve the accuracy and transparency of biomedical question answering through the following key mechanisms:

1. **Grounding Answers in a Large, Structured Biomedical Knowledge Graph**  
   - The system uses an iBKH-derived subgraph stored in Neo4j, containing approximately 65,800 nodes and 3 million edges covering entities such as drugs, diseases, symptoms, and pathways.  
   - This knowledge graph integrates multiple trusted biomedical data sources (e.g., UMLS, DrugBank, DisGeNET, SIDER, KEGG) with normalized identifiers and provenance metadata, ensuring that the information is factual, up-to-date, and well-structured.  
   - By querying this graph, the system grounds LLaMA 3’s responses in verified biomedical knowledge rather than relying solely on the language model’s learned patterns, thereby reducing hallucinations and misinformation.

2. **Translation of Natural Language Questions into Executable Cypher Queries**  
   - LLaMA 3 interprets user-submitted biomedical questions in natural language and converts them into Cypher queries that can be executed on the Neo4j iBKH graph.  
   - The system uses prompt engineering with schema and entity-aware hints to help LLaMA 3 generate accurate and complete Cypher queries, which improves the correctness of query generation (e.g., 96% exact match for simple queries).  
   - A query canonicalization step further refines these queries before execution, ensuring they are syntactically and semantically valid.

3. **Execution and Retrieval of Verified Answers from iBKH**  
   - The generated Cypher queries are executed against the iBKH knowledge graph using the Neo4j Python driver, which efficiently retrieves relevant biomedical facts.  
   - The system evaluates retrieved results using standardized information retrieval metrics (Precision@k, Recall@k, F1@k, Hits@k, MRR) to ensure high-quality and relevant answers.  
   - This approach ensures that answers are not only accurate but also ranked and presented effectively.

4. **Transparency Through User Interface and Evidence Presentation**  
   - The system’s graphical user interface (GUI) displays the original natural language question, the generated Cypher query, and the retrieved answers along with a brief natural language explanation.  
   - It also surfaces the supporting subgraph (nodes and edges) from iBKH that justifies each answer, enabling users to verify the reasoning path and source of information.  
   - This transparency fosters user trust and allows validation of the system’s outputs, addressing the traceability gap common in LLM-only systems.

5. **Local, Privacy-Preserving Deployment**  
   - The entire pipeline, including LLaMA 3 inference and Neo4j querying of the iBKH dataset, runs locally without cloud dependency.  
   - This ensures compliance with biomedical data governance, preserves patient privacy, and provides reproducible and low-latency responses suitable for clinical and research environments.

**In summary**, LLaMA 3 leverages the iBKH dataset by translating natural language biomedical questions into precise Cypher queries executed on a richly curated biomedical knowledge graph. This hybrid approach grounds answers in verified data, reduces hallucinations, and enhances transparency by exposing the query and evidence supporting each answer, thereby significantly improving the accuracy and trustworthiness of biomedical question answering.","LLaMA 3 is integrated into a biomedical question answering pipeline that leverages the Integrated Biomedical Knowledge Hub (iBKH) dataset as its core semantic foundation. The iBKH dataset contains 65,828 biomedical entities interconnected by over 3 million relationships, forming a rich knowledge graph with entities such as diseases, symptoms, drugs, and biological pathways. In the system, structured data relevant to a biomedical question is retrieved from this knowledge graph using Cypher queries refined by the Query Generation component. The retrieved structured tuples are then aggregated and formatted into a concise raw answer, which is sent to LLaMA 3. LLaMA 3 optionally refines the phrasing of this answer while preserving the retrieved facts, producing the final answer. This final output is presented alongside the executed Cypher query and an optional preview of the returned rows, enhancing transparency and trust. Thus, LLaMA 3 couples large language model-based understanding with the schema-grounded iBKH knowledge graph to produce answers that are accurate, interpretable, and easy to audit.",1.0,0.9901972799173234,0.99999999998,0.7142857142857143
"How does the PharmGKB identifier integration in the drug vocabulary contribute to the construction and semantic richness of the iBKH biomedical knowledge graph, particularly in relation to drug-disease and drug-pathway relationships?","['Omar & Mohammed / Journal of Applied Science and Technology Trends Vol. 06, No. 02, pp. 342 –357 (2025) \n \n350 \ndocumentation, indexes “enable quicker and more efficient \npattern matching” by allowing the query planner to rapidly locate \nnodes by label and property. \nWith the schema in place, data was imported using Cypher’s \nLOAD CSV commands. For each vocabulary file, rows with \nnonnull primary IDs were processed: the code used MERGE to \ncreate (or match existing) nodes with the given label and primary \nproperty, and then SET the remaining attributes from the CSV \ncolumns. For example, in importing disease_vocab.csv, each row \nproduced a node (:Disease primary: <id>) with properties name, \ndo_id, kegg_id, and umls_cui set from the row (empty strings \nwere turned into null). Similarly, drug_vocab.csv rows produced \ndrug nodes with properties for DrugBank, KEGG, PharmGKB \nIDs, etc. This approach follows best practice: using MERGE on \nthe unique ID ensures that nodes are not duplicated during \nmultiple passes. \nAfter all nodes were created, the relationship CSVs were \nloaded. Each row in those files was matched to the corresponding \nsource and target nodes by their primary IDs, and conditional \nlogic was used to create edges. For example, the disease-\nsymptom file (Di_Sy_res.csv) was processed by matching a \ndisease node and a symptom node for each row, then executing \nMERGE (d)-[r:HAS_SYMPTOM]-(s) if the present column is \nnonzero; the edge was given a present property and a source \nproperty from the CSV. The disease-disease file (di_di_res.csv) \nwas processed by matching disease nodes d1 and d2:If is_a = 1, \na (:Disease)-[:IS_A]->(:Disease) edge was merged; if Resemble \n= 1, a (:Disease)-[:RESEMBLES]->(:Disease) edge was merged. \nDrug-disease relationships were handled similarly: the script \nmatched a Drug node and a Disease node for each row of \nD_Di_res.csv, then for each flag that was 1, it merged the \ncorresponding edge label (such as TREATS, PALLIATES, \nASSOCIATED_WITH, etc.) from the drug to the disease. Drug–\nside-effect rows produced (:Drug)-[:CAUSES]->(:SideEffect) \nedges with the source noted, and drug–drug rows produced either \nINTERACTS_WITH or RESEMBLES edges between matched \ndrug pairs. Finally, the drug-pathway and disease-pathway files \neach produced:ASSOCIATED_WITH edges linking drugs or \ndiseases to pathway nodes. \nIn this graph model, most relationships are directional (for \nexample, a drug TREATS a disease; a disease does not TREAT \na drug). This follows common practice in biomedical KGs. The \nsame relations (such as RESEMBLES or INTERACTS_WITH) \nare inherently symmetric, but were stored as two directed edges \nor one undirected edge depending on implementation. All \nrelationship creation steps used Cypher’s MERGE so that \nrepeated loads or out-of-order imports did not create duplicate \nedges. \nThis study used a static KG snapshot for reproducibility, but \nthe system also supports incremental updates through the Neo4j \nMERGE and batch import functions. A summary of the different \ntypes of nodes and relationships is provided, together with their \ncounts, descriptions, and examples in Table III.\n \nTABLE III.  DETAILED SUMMARY OF KNOWLEDGE GRAPH COMPONENTS IN IBKH SUBSET \nComponent Type \nEntity/Relationship \nCount \nDescription \nExamples \n \nDisease \n19,236 \nMedical conditions and disorders \nDiabetes, Hypertension, Cancer \n \nDrug \n37,997 \nPharmaceutical compounds \nAspirin, Insulin, Amoxicillin \nNodes \nSymptom \n1,361 \nClinical signs and symptoms \nHeadache, Fever, Nausea \n \nSideEffect \n4,251 \nNegative outcomes of drugs \nNausea, Drowsiness \n \nPathway \n2,983 \nBiological processes and pathways \nApoptosis, Glycolysis \n \nASSOCIATED_WITH \n101,534 \nGeneral associations (disease–\npathway, drug–pathway, etc.) \nDiabetes ASSOCIATED_WITH Pathway \n \nCAUSES \n145,321 \nDrug-SideEffect relationships \nAspirin CAUSES Gastric_Bleeding \n \nHAS_SYMPTOM \n3,357 \nDisease-symptom links \nCOVID-19 HAS_SYMPTOM Fever \n \nHAS_BIOMARKER \n412 \nBiomarkers linked to disease \nPSA HAS_BIOMARKER Prostate_Cancer \nRelationships \nINHIBITS_CELL_GROWTH \n1,508 \nDrugs inhibiting cell growth \nChemo INHIBITS_CELL_GROWTH Tumor \n \nINTERACTS_WITH \n2,682,142 \nDrug-drug interactions \nAspirin INTERACTS_WITH Warfarin \n \nIS_A \n10,529 \nSubtype hierarchy \nFlu IS_A Viral_Infection \n \nPALLIATES \n388 \nDrug palliates disease \nMorphine PALLIATES Cancer \n \nPREVENTS_SUPPRESSES \n859 \nPreventive links \nVaccine PREVENTS_SUPPRESSES Measles \n \nRESEMBLES \n7,000 \nSimilarity relationships \nDrugA RESEMBLES DrugB \n \nTREATMENT_THERAPY \n44,852 \nTherapy relationships \nRadiotherapy TREATMENT_THERAPY Tumor \n \nTREATS \n5,491 \nDrug-disease links \nInsulin TREATS Diabetes \n \nALLEVIATES_REDUCES \n∼180,000 \nSymptom relief \nParacetamol ALLEVIATES_REDUCES Fever \nTotal \nNodes \n65,828 \nTotal biomedical entities \n— \n \nRelationships \n3,004,166 \nTotal knowledge graph links \n— \nC. Natural Language to Cypher Query Translation \nA key feature of the system is its ability to accept questions \nwritten in plain English and automatically generate the \ncorresponding Cypher queries. This is accomplished using \nMeta’s LLaMA 3 large language model, which runs entirely on \na local machine through the open-source llama.cpp framework. \nRunning the model locally ensures low-latency execution and \nkeeps sensitive queries within the user’s environment. \nTo generate a Cypher query, LLaMA 3 is prompted with \nexamples of natural language questions along with their correct \nCypher translations. The prompt also includes instructions on \nhow to navigate the structure of the graph schema. When a user \nenters a question (e.g., ’What are the symptoms of Alzheimer’s \ndisease?’), the system inserts it into the prompt and asks LLaMA \n3 to produce a corresponding query. For example, the model may \nreturn: \nMATCH (d:Disease)-[:HAS_SYMPTOM]->(s:Symptom) \nWHERE toLower(d.name) CONTAINS ""alzheimer"" \nRETURN s.name'
 'D384 Nucleic Acids Research, 2016, Vol. 44, Database issue\net al. (2013) The BioGRID interaction database: 2013 update.\nNucleic Acids Res., 41, D816–D823.\n14. Kjærulff,S.K., Wich,L., Kringelum,J., Jacobsen,U.P.,\nKouskoumvekaki,I., Audouze,K., Lund,O., Brunak,S., Oprea,T.I.\nand Taboureau,O. (2013) ChemProt-2.0: visual navigation in a\ndisease chemical biology database. Nucleic Acids Res., 41,\nD464–D469.\n15. Hopkins,A.L., Groom,C.R. and Alex,A. (2004) Ligand efficiency: a\nuseful metric for lead selection. Drug Discov. Today, 9, 430–431.\n16. Geeleher,P., Cox,N.J. and Huang,R.S. (2014) Clinical drug response\ncan be predicted using baseline gene expression levels and in vitro\ndrug sensitivity in cell lines. Genome Biol., 15, R47.\n17. Szklarczyk,D., Franceschini,A., Wyder,S., Forslund,K., Heller,D.,\nHuerta-Cepas,J., Simonovic,M., Roth,A., Santos,A., Tsafou,K.P.\net al. (2015) STRING v10: protein–protein interaction networks,\nintegrated over the tree of life. Nucleic Acids Res., 43, D447–D452.\n18. Kuhn,M., Szklarczyk,D., Pletscher-Frankild,S., Blicher,T.H., von\nMering,C., Jensen,L.J. and Bork,P. (2014) STITCH 4: integration of\nprotein–chemical interactions with user data. Nucleic Acids Res., 42,\nD401–D407.\n19. Law,V., Knox,C., Djoumbou,Y., Jewison,T., Guo,A.C., Liu,Y.,\nMaciejewski,A., Arndt,D., Wilson,M., Neveu,V. et al. (2014)\nDrugBank 4.0: shedding new light on drug metabolism. Nucleic\nAcids Res., 42, D1091–D1097.\n20. Okuno,Y., Tamon,A., Yabuuchi,H., Niijima,S., Minowa,Y.,\nTonomura,K., Kunimoto,R. and Feng,C. (2008) GLIDA:\nGPCR-ligand database for chemical genomics drug\ndiscovery––database and tools update. Nucleic Acids Res., 36,\nD907–D912.\n21. G¨unther,S., Kuhn,M., Dunkel,M., Campillos,M., Senger,C.,\nPetsalaki,E., Ahmed,J., Urdiales,E.G., Gewiess,A., Jensen,L.J. et al.\n(2008) SuperTarget and Matador: resources for exploring drug-target\nrelationships. Nucleic Acids Res., 36, D919–D922.\n22. Zhu,F., Shi,Z., Qin,C., Tao,L., Liu,X., Xu,F., Zhang,L., Song,Y.,\nLiu,X., Zhang,J. et al. (2012) Therapeutic target database update\n2012: a resource for facilitating target-oriented drug discovery.\nNucleic Acids Res., 40, D1128–D1136.\n23. Davis,A.P., Grondin,C.J., Lennon-Hopkins,K.,\nSaraceni-Richards,C., Sciaky,D., King,B.L., Wiegers,T.C. and\nMattingly,C.J. (2015) The Comparative Toxicogenomics Database’s\n10th year anniversary: update 2015. Nucleic Acids Res., 43,\nD914–D920.\n24. Schaefer,C.F., Anthony,K., Krupa,S., Buchoff,J., Day,M.,\nHannay,T. and Buetow,K.H. (2009) PID: the Pathway Interaction\nDatabase. Nucleic Acids Res., 37, D674–D679.\n25. Croft,D., Mundo,A.F., Haw,R., Milacic,M., Weiser,J., Wu,G.,\nCaudy,M., Garapati,P., Gillespie,M., Kamdar,M.R. et al. (2014) The\nReactome pathway knowledgebase. Nucleic Acids Res., 42,\nD472–D477.\n26. Caspi,R., Altman,T., Billington,R., Dreher,K., Foerster,H.,\nFulcher,C.A., Holland,T.A., Keseler,I.M., Kothari,A., Kubo,A.\net al. (2014) The MetaCyc database of metabolic pathways and\nenzymes and the BioCyc collection of Pathway/Genome Databases.\nNucleic Acids Res., 42, D459–D471.\n27. Bento,A.P., Gaulton,A., Hersey,A., Bellis,L.J., Chambers,J.,\nDavies,M., Kr¨uger,F.A., Light,Y., Mak,L., McGlinchey,S. et al.\n(2014) The ChEMBL bioactivity database: an update. Nucleic Acids\nRes., 42, D1083–D1090.\n28. Roth,B.L., Lopez,E., Patel,S. and Kroeze,W.K. (2000) The\nmultiplicity of serotonin receptors: uselessly diverse molecules or an\nembarrassment of riches? Neuroscientist, 6, 252–262.\n29. Rose,P.W., Prli´c,A., Bi,C., Bluhm,W.F., Christie,C.H., Dutta,S.,\nGreen,R.K., Goodsell,D.S., Westbrook,J.D., Woo,J. et al. (2015) The\nRCSB Protein Data Bank: views of structural biology for basic and\napplied research and education. Nucleic Acids Res., 43, D345–D356.\n30. Anastassiadis,T., Deacon,S.W., Devarajan,K., Ma,H. and\nPeterson,J.R. (2011) Comprehensive assay of kinase catalytic activity\nreveals features of kinase inhibitor selectivity. Nat. Biotechnol., 29,\n1039–1045.\n31. Davis,M.I., Hunt,J.P., Herrgard,S., Ciceri,P., Wodicka,L.M.,\nPallares,G., Hocker,M., Treiber,D.K. and Zarrinkar,P.P. (2011)\nComprehensive analysis of kinase inhibitor selectivity. Nat.\nBiotechnol., 29, 1046–1051.\n32. Franceschini,A., Szklarczyk,D., Frankild,S., Kuhn,M.,\nSimonovic,M., Roth,A., Lin,J., Minguez,P., Bork,P., von Mering,C.\net al. (2013) STRING v9.1: protein-protein interaction networks,\nwith increased coverage and integration. Nucleic Acids Res., 41,\nD808–D815.\n33. Santos,A., Tsafou,K., Stolte,C., Pletscher-Frankild,S.,\nO’Donoghue,S.I. and Jensen,L.J. (2015) Comprehensive comparison\nof large-scale tissue expression datasets. PeerJ, 3, e1054.\n34. Petryszak,R., Burdett,T., Fiorelli,B., Fonseca,N.A.,\nGonzalez-Porta,M., Hastings,E., Huber,W., Jupp,S., Keays,M.,\nKryvych,N. et al. (2014) Expression Atlas update–a database of gene\nand transcript expression from microarray- and sequencing-based\nfunctional genomics experiments. Nucleic Acids Res., 42,\nD926–D932.\n35. O’Reilly,L.P., Long,O.S., Cobanoglu,M.C., Benson,J.A., Luke,C.J.,\nMiedel,M.T., Hale,P., Perlmutter,D.H., Bahar,I., Silverman,G.A.\net al. (2014) A genome-wide RNAi screen identifies potential drug\ntargets in a C. elegans model of \x031-antitrypsin deficiency. Hum. Mol.\nGenet., 23, 5123–5132.\n36. Kumar,K.K., Lowe,E.W. Jr, Aboud,A.A., Neely,M.D., Redha,R.,\nBauer,J.A., Odak,M., Weaver,C.D., Meiler,J., Aschner,M. et al.\n(2014) Cellular manganese content is developmentally regulated in\nhuman dopaminergic neurons. Sci. Rep., 4, 6801.\n37. Ligeti,B., P´enzv´alt´o,Z., Vera,R., Gy˝orffy,B. and Pongor,S. (2015) A\nnetwork-based target overlap score for characterizing drug\ncombinations: high correlation with cancer clinical trial results.\nPLoS One, 10, e0129267.\n38. Vogt,I., Prinz,J. and Campillos,M. (2014) Molecularly and clinically\nrelated drugs and diseases are enriched in phenotypically similar\ndrug-disease pairs. Genome Med., 6, 52.\n39. Tung,C.W. (2015) ChemDIS: a chemical-disease inference system\nbased on chemical-protein interactions. J. Cheminform., 7, 25.\n40. Zhou,H., Gao,M. and Skolnick,J. (2015) Comprehensive prediction\nof drug-protein interactions and side effects for the human proteome.\nSci. Rep., 5, 11090.\nDownloaded from https://academic.oup.com/nar/article/44/D1/D380/2503089 by guest on 11 January 2026'
 'Knowledge gr a ph–based thought | 9 \nA\nB\nC\nD\nFigure 4: (A), (B), (C), and (D) r espectiv el y r epr esent the r elational dia gr ams of drug r epositioning, drug r esistance r esearc h, individualized tr eatment, \nand selection and understanding of biomarkers. \nwer e specificall y designed to v alidate the effectiv eness of the KGT \nfr ame work within this study. While the dataset is highly relevant \nto biomedical applications, its scope is primarily focused on vali- \ndating the proposed method. T herefore , it ma y not co ver all poten- \ntial use cases. Additionally, our system curr entl y has the dr awbac k \nof not performing fuzzy matching; if a drug name is misspelled by \ne v en 1 letter, it fails to r etrie v e information from the knowledge \ngr a ph. Ther efor e, we plan to impr ov e this aspect in the future to \nenhance the system’s usability and reliability. Our ultimate goal \nis to create a robust framework applicable to the rapidly evolving \ndomain of medical knowledge, supporting health care profession- \nals in delivering personalized, precise medication tailored to the \nindividual needs of each patient. \nFinally, we affirm that this study serves as a proof of concept, \naiming to showcase the technical feasibility and initial efficacy of \nthe method, which has not been validated in actual clinical prac- \ntice. In any clinical or medical decision-making, reliance should \nalways be placed on the judgment and guidance of professional \nhealth care practitioners. \nAdditional Files \nSupplementary Table S1. Comparison of PcQA with MetaQA and \nFACTKG in multihop tasks . T he types of intermediate entities are \nindicated in bold. \nSupplementary Table S2. Example of drug repositioning. \nSupplementary Table S3. Example of drug resistance research. \nSupplementary Table S4. Example of individualized treatment. \nSupplementary Table S5. Example of selection and understand- \ning of biomarkers. \nSupplementary Table S6. Prompts for Base and CoT&ICL. \nSupplementary Fig. S1. Prompt for k e y information extraction. \nSupplementary Fig. S2. Prompt for query statement generation. \nSupplementary Fig. S3. Prompt for LLM inference and output. \nSupplementary Fig. S4. Prompt for GPT-4–based evaluation. \nSupplementary Fig. S5. (A), (B), (C), and (D) r espectiv el y r epr esent \nthe r elational dia gr ams of drug r epositioning, drug r esistance r e- \nsearc h, individualized tr eatment, and selection and understand- \ning of biomarkers. \nDownloaded from https://academic.oup.com/gigascience/article/doi/10.1093/gigascience/giae082/7943459 by guest on 11 January 2026'
 'Omar & Mohammed / Journal of Applied Science and Technology Trends Vol. 06, No. 02, pp. 342 –357 (2025) \n \n346 \nscientific literature, clinical records, genomic databases, and \nexperimental findings [19, 31]. \nSuch integration creates a comprehensive biomedical \nknowledge base that supports advanced analytics and discovery. \nFor example, biomedical knowledge graphs can reveal previously \nunknown relationships (e.g., between drugs and diseases) and \nhelp prioritize potential biomarkers for complex conditions. The \nIntegrative Biomedical Knowledge Hub (iBKH) is one such \nlarge-scale graph that consolidates diverse biomedical resources \ninto a unified hub, enabling discovery at scale [11]. Beyond \niBKH, large biomedical knowledge graphs such as SPOKE \nfurther illustrate how graph integration accelerates research and \nsupports precision-medicine use cases [12]. Overall, these graphs \nserve as foundational resources for data-driven and personalized \nmedicine. These knowledge graphs serve as foundational \nresources for precision medicine, where treatment can be tailored \nto the individual’s biological profile, improving outcomes and \nminimizing side effects [19, 31]. \n \nTABLE I.  SUMMARY OF RELATED RESEARCH ON BIOMEDICAL KGS AND QUESTION ANSWERING \nRef. \nYear \nData/Graph \nMethod \nBaselines \nKey Metric \nLimitation \n[11] \n2023 \niBKH (18 biomedical \nsources, Neo4j) \nIntegrative KG + Graph \nlearning; drug repurposing \ncase \nKnown drugs, \nAlzheimer’s \nstudy \nPredictive performance \n(drug repurposing) \nLimited to Alzheimer’s case \nstudy; scalability and updates not \ndetailed \n[19] \n2023 \nDisease KG (9,400 \nnodes, 45,000 relations \nin Neo4j) \nCypher queries for disease–\ndrug–symptom reasoning \nCross-referential \ndisease DB \nNew relation inference; \ncomplex query \nanswering \nLimited to single domain; lacks \nlarge-scale evaluation \n[3] \n2023 \nBKGs vs. GPT-3.5/4 \nComparative QA study: \nLLMs vs. KGs \nGPT-3.5, GPT-4, \nKG reasoning \nAccuracy, Novelty, \nReasoning \nGPT-4 inconsistent in \nreasoning/citations; KG less fluent \nbut more reliable \n[13] \n2024 \nSPOKE KG + \nLLaMA2, GPT-3.5, \nGPT-4 \nKG-optimized retrieval for \nLLMs (RAG) \nOther retrieval \nmethods \nAccuracy, token \nreduction >50% \nFocus on retrieval optimization, \nnot KG construction \n[23] \n2024 \nBenchmark KB datasets \nGenerate-then-retrieve \n(LLM → logical form → \nKB retrieval) \nPrior KBQA \nmethods \nAccuracy (highest to \ndate) \nRisk of error in logical form \ngeneration \n[6] \n2024 \nPrimeKG \nLLM + KG hybrid, Cypher \nquery validation, RAG \nGPT-4 Turbo, \nLLaMA 3 \nAccuracy, \nExplainability \nDependent on KG coverage; \ncomputationally intensive \n[22] \n2025 \nPan-cancer KG (PcQA \ndataset) \nKG-enhanced reasoning \n(subgraph selection) \nSOTA KGQA \nmethods \nOutperformed SOTA on \nPcQA \nLimited to pan-cancer focus; no \nfine-tuning explored \n[26] \n2025 \nDynamic Medical KG + \nNeo4j \nLLM agents + multi-hop \nreasoning \nMEDQA, \nMedMCQA \nbaselines \nAccuracy, \nInterpretability \nHigh system complexity; requires \ncontinuous updating \n[24] \n2025 \nSynthCypher dataset \n(Neo4j + synthetic \nqueries) \nLLM-supervised Cypher \ngeneration and verification \nSPIDER \nbenchmark \nCypher accuracy 40% \nSynthetic dataset may not capture \nall real-world cases \n[29] \n2025 \nUMLS KG + Neo4j \nHybrid GraphRAG \nLLM-only QA \nAccuracy, Citation \nfidelity \nMore complex pipeline; relies on \nexternal vector store \n[30] \n2025 \nClinical data (10k \npatients, 1,894 nodes, \nNeo4j) \nKG construction using \nGPT-4 for entity/relation \nextraction \nOther KG \nconstruction \nmethods \nHigh F1-scores \nFocus only on sepsis; limited \ngeneralization \n[20] \n2025 \nLocal KG from \nbiomedical documents \nMulti-level summarization \nover KG for QA \nRAG baselines \nQA accuracy, reasoning \nTested mainly on document QA; \nscalability not proven \n \nB. Neo4j Graph Database \nTo manage the complexity and large size of biomedical \nknowledge graphs, specialized graph databases are needed. \nNeo4j is one of the most popular graph databases designed to \nstore and query data structured as nodes (entities) and \nrelationships (edges), both of which can have descriptive \nproperties [32, 33]. It uses the property graph model, which \nmakes it easy to represent complex, connected biomedical data \nsuch as drug-gene interactions or disease pathways. Neo4j’s \nCypher query language is especially advantageous because it \nallows users to write expressive and efficient queries to explore \nmulti-step connections in the data [34]. \nNeo4j works well for biomedical data because it can quickly \nrun complicated queries over highly interconnected datasets. This \nis important in biology and medicine, where relationships \nbetween entities like proteins, diseases, and drugs are often \ncomplex and layered. Studies have shown that Neo4j handles \nlarge biomedical graphs efficiently, making it a favorite among \nresearchers and industry users alike [33, 35, 36]. Its indexing and \ncaching mechanisms also help speed up query processing and \ndata retrieval [37]. \nMoreover, \nNeo4j \nintegrates \nsmoothly \nwith \nmany \nprogramming languages and analytics tools, which makes it \neasier to build interactive biomedical applications and clinical \ndecision support systems that can turn complex graph data into \nuseful insights [38, 39]. \nC. Large Language Models (LLMs) in Biomedical Question \nAnswering \nLarge Language Models (LLMs) are powerful AI systems \ntrained on vast amounts of text data. They learn the structure and \npatterns of language, enabling them to understand questions, \ngenerate responses, summarize information, and perform other \ncomplex language tasks. Well-known models such as LLaMA \nand GPT-3 have greatly advanced the field of natural language \nprocessing by showing strong performance across many tasks \n[40, 41].'
 'format. Second, because the entire federated KG is never in-\nstantiated in a single place, reasoning and scoring methods\nthat rely on having the entire knowledge graph in memory\ncannot be used with BioThings Explorer.\nIn sum, we believe that knowledge graphs enable many ex-\nciting use cases in biomedical research (Nicholson and Greene\n2020), and that BioThings Explorer is complementary to\nexisting approaches for assembling knowledge graphs, offer-\ning powerful and unique capabilities for both scientific data\nanalysts and tool developers.\nSupplementary data\nSupplementary data are available at Bioinformatics online.\nConflict of interest\nNone declared.\nFunding\nSupport for this work was provided by the National Center\nfor Advancing Translational Sciences, National Institutes of\nHealth, through the Biomedical Data Translator program,\nawards OT2TR003427 and OT2TR003445.\nData availability\nBioThings Explorer is implemented as a NodeJS application.\nThe primary repository for the BioThings Explorer project is\nat https://github.com/biothings/biothings_explorer, which in\nturn links to and incorporates other repositories as sub-\nmodules. All code is released under the Apache 2.0 open-\nsource software license.\nReferences\nCilibrasi RL, Vitanyi PMB. The google similarity distance. IEEE Trans\nKnowl Data Eng 2007;19:370–83.\nDavis AP, Wiegers TC, Johnson RJ et al. Comparative toxicogenomics\ndatabase\n(CTD):\nupdate\n2023.\nNucleic\nAcids\nRes\n2023;51:\nD1257–62.\nDowell RD, Jokerst RM, Day A et al. The distributed annotation system.\nBMC Bioinformatics 2001;2:7.\nFecho K, Bizon C, Miller F et al. A biomedical knowledge graph system\nto propose mechanistic hypotheses for real-world environmental\nhealth observations: cohort study and informatics application. JMIR\nMed Inform 2021;9:e26714.\nFecho K, Thessen AE, Baranzini SE et al. Progress toward a universal\nbiomedical data translator. Clin Transl Sci 2022;15:1838–47.\nHimmelstein DS, Lizee A, Hessler C et al. Systematic integration\nof biomedical knowledge prioritizes drugs for repurposing. Elife\n2017;6:e26726.\nLelong S, Zhou X, Afrasiabi C et al. BioThings SDK: a toolkit for build-\ning\nhigh-performance\ndata\nAPIs\nin\nbiomedical\nresearch.\nBioinformatics 2022;38:2077–9.\nMayers M, Tu R, Steinecke D et al. Design and application of a knowl-\nedge network for automatic prioritization of drug mechanisms.\nBioinformatics 2022;38:2880–91.\nMorris JH, Soman K, Akbas RE et al. The scalable precision medicine\nopen knowledge engine (SPOKE): a massive knowledge graph of bio-\nmedical information. Bioinformatics 2023;39(2):btad080.\nMungall CJ, McMurry JA, Ko¨hler S et al. The monarch initiative: an in-\ntegrative data and analytic platform connecting phenotypes to geno-\ntypes across species. Nucleic Acids Res 2017;45:D712–22.\nNicholson DN, Greene CS. Constructing knowledge graphs and their\nbiomedical applications. Comput Struct Biotechnol J 2020;18:\n1414–28.\nRambla J, Baudis M, Ariosa R et al. Beacon v2 and Beacon networks: a\n‘lingua franca’ for federated data discovery in biomedical genomics,\nand beyond. Hum Mutat 2022;43:791–9.\nUnni DR, Moxon SAT, Bada M et al. Biolink Model: a universal schema\nfor knowledge graphs in clinical, biomedical, and translational sci-\nence. Clin Transl Sci 2022;15:1848–55.\nWood EC, Glen AK, Kvarfordt LG et al. RTX-KG2: a system for build-\ning a semantically standardized knowledge graph for translational\nbiomedicine. BMC Bioinformatics 2022;23:400.\nZaveri A, Dastgheib S, Wu C et al. smartAPI: towards a more intelligent\nnetwork of web APIs. In: Blomqvist, E., Maynard, D., Gangemi, A.,\net al. (eds) The Semantic Web. ESWC 2017. Lecture Notes in\nComputer Science, 2017, 154–69. https://link.springer.com/chapter/\n10.1007/978-3-319-58451-5_11#chapter-info.\n4\nCallaghan et al.\nDownloaded from https://academic.oup.com/bioinformatics/article/39/9/btad570/7273783 by guest on 11 January 2026']","['<1-hop>\n\nOmar & Mohammed / Journal of Applied Science and Technology Trends Vol. 06, No. 02, pp. 342 –357 (2025) 350 documentation, indexes “enable quicker and more efficient pattern matching” by allowing the query planner to rapidly locate nodes by label and property. With the schema in place, data was imported using Cypher’s LOAD CSV commands. For each vocabulary file, rows with nonnull primary IDs were processed: the code used MERGE to create (or match existing) nodes with the given label and primary property, and then SET the remaining attributes from the CSV columns. For example, in importing disease_vocab.csv, each row produced a node (:Disease primary: <id>) with properties name, do_id, kegg_id, and umls_cui set from the row (empty strings were turned into null). Similarly, drug_vocab.csv rows produced drug nodes with properties for DrugBank, KEGG, PharmGKB IDs, etc. This approach follows best practice: using MERGE on the unique ID ensures that nodes are not duplicated during multiple passes. After all nodes were created, the relationship CSVs were loaded. Each row in those files was matched to the corresponding source and target nodes by their primary IDs, and conditional logic was used to create edges. For example, the disease- symptom file (Di_Sy_res.csv) was processed by matching a disease node and a symptom node for each row, then executing MERGE (d)-[r:HAS_SYMPTOM]-(s) if the present column is nonzero; the edge was given a present property and a source property from the CSV. The disease-disease file (di_di_res.csv) was processed by matching disease nodes d1 and d2:If is_a = 1, a (:Disease)-[:IS_A]->(:Disease) edge was merged; if Resemble = 1, a (:Disease)-[:RESEMBLES]->(:Disease) edge was merged. Drug-disease relationships were handled similarly: the script matched a Drug node and a Disease node for each row of D_Di_res.csv, then for each flag that was 1, it merged the corresponding edge label (such as TREATS, PALLIATES, ASSOCIATED_WITH, etc.) from the drug to the disease. Drug– side-effect rows produced (:Drug)-[:CAUSES]->(:SideEffect) edges with the source noted, and drug–drug rows produced either INTERACTS_WITH or RESEMBLES edges between matched drug pairs. Finally, the drug-pathway and disease-pathway files each produced:ASSOCIATED_WITH edges linking drugs or diseases to pathway nodes. In this graph model, most relationships are directional (for example, a drug TREATS a disease; a disease does not TREAT a drug). This follows common practice in biomedical KGs. The same relations (such as RESEMBLES or INTERACTS_WITH) are inherently symmetric, but were stored as two directed edges or one undirected edge depending on implementation. All relationship creation steps used Cypher’s MERGE so that repeated loads or out-of-order imports did not create duplicate edges. This study used a static KG snapshot for reproducibility, but the system also supports incremental updates through the Neo4j MERGE and batch import functions. A summary of the different types of nodes and relationships is provided, together with their counts, descriptions, and examples in Table III. TABLE III. DETAILED SUMMARY OF KNOWLEDGE GRAPH COMPONENTS IN IBKH SUBSET Component Type Entity/Relationship Count Description Examples Disease 19,236 Medical conditions and disorders Diabetes, Hypertension, Cancer Drug 37,997 Pharmaceutical compounds Aspirin, Insulin, Amoxicillin'
 '<2-hop>\n\nB. Dataset and Knowledge Graph Construction 1) Dataset The integrated Biomedical Knowledge Hub (iBKH), a large biomedical knowledge base, forms the first level of the system and integrates information from various curated high-quality biomedical databases. This implies that the data set includes various types of entities, such as diseases, symptoms, drugs, biological pathways, etc. This study used the representative subset of the iBKH dataset, which contained 65828 biomedical entities. These entities are semantically interconnected through a total of 3004166 relationships, thus creating a rich knowledge graph. The iBKH dataset was originally introduced in [11], and it is freely available at (https://github.com/wcm-wanglab/iBKH). This dataset is the core semantic foundation upon which this study is built. The knowledge graph is populated from multiple tabular sources (CSV files), each listing entities or relationships. The main input files and their contents are as follows: • Disease vocabulary(disease_vocab.csv): Contains columns such as primary (a unique disease ID), name, do_id (Disease Ontology ID), kegg_id, and umls_cui (UMLS Concept ID). Each row represents a disease node with external identifiers. • Drug vocabulary (drug_vocab.csv): Includes primary (unique drug ID), name, drugbank_id, kegg_id, pharmgkb_id, umls_cui, mesh_id, iDISK_id and CID (PubChem ID). Each row defines a drug node with standard database identifiers. • Symptom vocabulary (symptom_vocab.csv): Contains primary (unique symptom ID), name, mesh_id, umls_cui and iDISK_id. Each row defines a symptom node. • Side effect vocabulary (side_effect_vocab.csv): Includes primary (unique side-effect ID) and name. Each row represents a side-effect node (with UMLS ID when available). • Pathway vocabulary (pathway_vocab.csv): Contains primary (unique pathway ID), name, reactome_id, go_id, and kegg_id. Each row defines a biological pathway node. Relationship files (each row typically contains two entity IDs and one or more boolean flags or codes) include: • Disease–Symptom links (Di_Sy_res.csv): Rows include Disease and Symptom IDs, a presence flag (1 or 0) and a data source. If Present = 1, a HAS_SYMPTOM edge is created from the disease to the symptom, with properties for presence and source. • Disease–Disease links (di_di_res.csv): Rows include Disease_1 and Disease_2 IDs with binary flags for is_a and Resemble. If is_a = 1, an (IS_A) edge is created (Disease_1 → Disease_2); if Resemble = 1, a RESEMBLES edge is created. The source field is used for provenance. • Drug–Disease links (D_Di_res.csv): Includes Drug and Disease IDs with several binary flags. If a flag equals 1, a corresponding edge is created: o TREATS (Treats = 1) o PALLIATES (Palliates = 1) o ASSOCIATED_WITH (Associate = 1) o ALLEVIATES_REDUCES (alleviates = 1) o TREATMENT_THERAPY (treatment/therapy = 1) o INHIBITS_CELL_GROWTH (inhibits cell growth = 1) o HAS_BIOMARKER (biomarkers = 1) o PREVENTS_SUPPRESSES (prevents/suppresses = 1) o ROLE_IN_PATHOGENESIS (role in disease pathogenesis = 1) • Drug–SideEffect links (D_SE_res.csv): Contains Drug and SideEffect IDs with a Source column. Each row creates a CAUSES edge from the drug to the side effect, with source as an edge property. • Drug–Drug interactions (D_D_res.csv): Rows include Drug_1 and Drug_2 IDs with flags for Interaction and Resemble. If Interaction = 1, an INTERACTS_WITH edge is created (bidirectional). If Resemble = 1, a RESEMBLES edge is added. • Drug–Pathway links (D_Pwy_res.csv): Includes Drug ID and Pathway ID. Each row generates an ASSOCIATED_WITH edge from the drug to the pathway. • Disease–Pathway links (Di_Pwy_res.csv): Contains Disease ID and Pathway ID. Each row creates an ASSOCIATED_WITH edge from the disease to the pathway. 2) Data Upload Performance The time required to upload different types of entities and relationships into the Neo4j biomedical knowledge graph, measured in seconds. These measurements reflect both the size and complexity of the data being processed. As shown in Table II, the longest upload time is for Drug- Drug Relationships, which takes approximately 190 seconds due to the large number of edges (over 3 million). Following this, Disease-Disease and Drug-Disease Relationships also require considerable time for loading. On the other hand, individual']","The integration of PharmGKB identifiers into the drug vocabulary significantly enhances the construction and semantic richness of the iBKH (Integrative Biomedical Knowledge Hub) biomedical knowledge graph, particularly in relation to drug-disease and drug-pathway relationships, in the following ways:

1. **Standardized and Unique Identification of Drugs**:  
   During the data import process into the Neo4j graph database, each drug node is created or matched using unique primary identifiers, including PharmGKB IDs alongside other identifiers like DrugBank and KEGG. This use of PharmGKB IDs ensures that each drug entity is uniquely and consistently represented in the graph, preventing duplication and enabling reliable integration of data from multiple sources.

2. **Enhanced Semantic Linking Across Data Sources**:  
   PharmGKB is a well-established pharmacogenomics knowledge resource that links drugs to genetic and clinical information. By incorporating PharmGKB IDs into drug nodes, the iBKH graph can semantically connect drugs to pharmacogenomic data, which enriches the context around drug actions, effects, and interactions. This integration supports more nuanced relationships between drugs and diseases or pathways, reflecting underlying genetic influences on drug efficacy and safety.

3. **Facilitation of Complex Relationship Mapping**:  
   The graph construction process uses these identifiers to accurately match drugs with diseases and pathways when creating relationships such as TREATS, PALLIATES, ASSOCIATED_WITH, and others. PharmGKB IDs help ensure that these relationships are correctly established by providing a reliable key for drug entities, which is crucial when linking drugs to diseases they treat or pathways they influence.

4. **Improved Query Efficiency and Data Integrity**:  
   Using PharmGKB IDs as part of the drug vocabulary supports efficient indexing and querying within Neo4j. This allows for rapid retrieval of drug-related information and their connections to diseases and pathways, which is essential for complex biomedical queries and analyses. The MERGE operation keyed on these IDs prevents duplication of nodes and relationships, maintaining data integrity during incremental updates or multiple data imports.

5. **Support for Precision Medicine and Pharmacogenomics Research**:  
   By integrating PharmGKB identifiers, the iBKH knowledge graph gains the ability to support precision medicine applications that consider genetic factors in drug response. This integration enriches drug-disease and drug-pathway relationships with pharmacogenomic context, enabling advanced analytics such as drug repurposing, individualized treatment strategies, and biomarker prioritization.

In summary, the inclusion of PharmGKB identifiers in the drug vocabulary of the iBKH knowledge graph ensures precise, semantically rich, and interoperable representation of drugs. This facilitates accurate and meaningful connections between drugs, diseases, and biological pathways, thereby enhancing the graph’s utility for biomedical research and clinical decision support.","The PharmGKB identifier is included as a property in the drug nodes created from the drug_vocab.csv file, which contains unique drug IDs along with standard database identifiers such as drugbank_id, kegg_id, pharmgkb_id, umls_cui, mesh_id, iDISK_id, and CID. This integration ensures that each drug node in the iBKH knowledge graph is linked to PharmGKB, a curated biomedical database, enhancing the semantic interoperability and accuracy of the graph. The iBKH knowledge graph uses these drug nodes to establish multiple types of relationships, including drug-disease links (such as TREATS, PALLIATES, ASSOCIATED_WITH) and drug-pathway associations, by matching drug nodes with disease and pathway nodes through their unique IDs. This structured incorporation of PharmGKB identifiers facilitates precise merging of nodes and edges using Cypher's MERGE command, preventing duplication and enabling rich, directional relationships that reflect real biomedical interactions. Consequently, the PharmGKB integration significantly contributes to the comprehensive and semantically rich representation of drug-related biomedical knowledge within the iBKH knowledge graph.",0.43859649122807015,0.9772253817933514,0.7499999999625,1.0
"How do LLaMA and Llama2 models differ in their integration with biomedical knowledge graphs to mitigate hallucinations in large language models, and what impact does this integration have on the performance of retrieval-augmented generation (RAG) systems as demonstrated by meta-analyses?","['Research and Applications\nImproving large language model applications in \nbiomedicine with retrieval-augmented generation: a \nsystematic review, meta-analysis, and clinical \ndevelopment guidelines\nSiru Liu, PhD\x03,1,2, Allison B. McCoy \n, PhD1, Adam Wright \n, PhD1,3 \n1Department of Biomedical Informatics, Vanderbilt University Medical Center, Nashville, TN 37212, United States, 2Department of Computer \nScience, Vanderbilt University, Nashville, TN 37212, United States, 3Department of Medicine, Vanderbilt University Medical Center, \nNashville, TN 37212, United States\n\x03Corresponding author: Siru Liu, PhD, Department of Biomedical Informatics, Vanderbilt University Medical Center, 2525 West End Ave #1475, Nashville, \nTN 37212, United States (siru.liu@vumc.org)\nAbstract\nObjective: The objectives of this study are to synthesize findings from recent research of retrieval-augmented generation (RAG) and large lan\xad\nguage models (LLMs) in biomedicine and provide clinical development guidelines to improve effectiveness.\nMaterials and Methods: We conducted a systematic literature review and a meta-analysis. The report was created in adherence to the Pre\xad\nferred Reporting Items for Systematic Reviews and Meta-Analyses 2020 analysis. Searches were performed in 3 databases (PubMed, Embase, \nPsycINFO) using terms related to “retrieval augmented generation” and “large language model,” for articles published in 2023 and 2024. We \nselected studies that compared baseline LLM performance with RAG performance. We developed a random-effect meta-analysis model, using \nodds ratio as the effect size.\nResults: Among 335 studies, 20 were included in this literature review. The pooled effect size was 1.35, with a 95% confidence interval of \n1.19-1.53, indicating a statistically significant effect (P ¼ .001). We reported clinical tasks, baseline LLMs, retrieval sources and strategies, as \nwell as evaluation methods.\nDiscussion: Building on our literature review, we developed Guidelines for Unified Implementation and Development of Enhanced LLM Appli\xad\ncations with RAG in Clinical Settings to inform clinical applications using RAG.\nConclusion: Overall, RAG implementation showed a 1.35 odds ratio increase in performance compared to baseline LLMs. Future research \nshould focus on (1) system-level enhancement: the combination of RAG and agent, (2) knowledge-level enhancement: deep integration of \nknowledge into LLM, and (3) integration-level enhancement: integrating RAG systems within electronic health records.\nKey words: large language model; retrieval augmented generation; systematic review; meta-analysis. \nIntroduction\nLarge language models (LLMs) have reported remarkable \nperformance in question-answering, summarization, and text \ngeneration.1 Given this, researchers have explored its poten\xad\ntial in biomedical areas.2 For example, several studies \nreported the ability of using LLM to answer patient mes\xad\nsages,3 to analyze alert logic in clinical decision support,4 and \nto make discharge summaries more readable to patients.5\nHowever, several challenges remain.\nLLMs are trained on fixed datasets, which restrict their \nknowledge to information available up to the training cut-off \ndate. For example, GPT-4o’s training data only includes \ninformation up to October 2023, making it unable to \nrespond accurately to findings that emerged afterward. LLM \ntraining datasets are also generally broad and lack the specif\xad\nicity required for biomedical applications. Finally, not all \nsources used to train the LLMs are reliable and trustworthy. \nTo address these limitations, researchers have performed \nfine-tuning and retrieval-augmented generation (RAG) tech\xad\nniques. Fine-tuning can adapt LLMs to specific domains, but \nit is resource-intensive and does not allow for real-time \nupdates. In contrast, RAG maintains the original LLM archi\xad\ntecture while incorporating relevant context directly into \nqueries, offering more flexibility and control. In addition, \nRAG’s unique advantage in biomedical applications lies in its \nability to adapt to dynamic environments by delivering up-to- \ndate information and efficiently integrating external knowl\xad\nedge sources with high interpretability.6\nAnother limitation of using LLMs directly is the risk of hal\xad\nlucination, where the model generates incorrect or fabricated \ninformation.7 To mitigate such issues, researchers have pro\xad\nposed RAG as a solution that integrates up-to-date, relevant \ninformation, enhancing both the accuracy and reliability of \nLLM generated responses.8,9 For example, when ChatGPT \nReceived: November 19, 2024; Revised: December 17, 2024; Editorial Decision: January 2, 2025; Accepted: January 3, 2025 \n© The Author(s) 2025. Published by Oxford University Press on behalf of the American Medical Informatics Association.  \nThis is an Open Access article distributed under the terms of the Creative Commons Attribution-NonCommercial License (https://creativecommons.org/licenses/ \nby-nc/4.0/), which permits non-commercial re-use, distribution, and reproduction in any medium, provided the original work is properly cited. For commercial \nre-use, please contact reprints@oup.com for reprints and translation rights for reprints. All other permissions can be obtained through our RightsLink service via \nthe Permissions link on the article page on our site—for further information please contact journals.permissions@oup.com. \nJournal of the American Medical Informatics Association, 2025, 32(4), 605–615 \nhttps://doi.org/10.1093/jamia/ocaf008 \nAdvance access publication 15 January 2025 \nResearch and Applications \nDownloaded from https://academic.oup.com/jamia/article/32/4/605/7954485 by guest on 11 January 2026'
 'was asked about medications for peripheral artery disease \npatients without increased bleeding risk, it initially omitted \nlow-dose rivaroxaban. After integrating retrieved text from \nthe 2024 American College of Cardiology / American Heart \nAssociation Guideline for the Management of Lower Extrem\xad\nity Peripheral Artery Disease,10 the model correctly recom\xad\nmended rivaroxaban.\nSeveral guidelines exist for evaluating Artificial Intelligence \n(AI) applications and LLMs in healthcare, including \nDECIDE-AI (Developmental and Exploratory Clinical Inves\xad\ntigations of DEcision support systems driven by Artificial \nIntelligence),11 CLAIM (Checklist for Artificial Intelligence in \nMedical Imaging),12\nand CONSORT-AI (Consolidated \nStandards of Reporting Trials-AI).13 In addition, Tam et al. \nintroduced QUEST, a framework specifically for human eval\xad\nuation of LLMs in healthcare.14 However, these guidelines \ndo not cover RAG applications in clinical settings, emphasiz\xad\ning the need for a more specific guideline.\nDespite the promise of RAG in improving LLM performance \nin clinical settings, there is limited understanding of its overall \neffectiveness comparing with the baseline LLM, adoption in \nclinical domains, and optimal strategies for its development in \nbiomedical applications. The aim of this study is to synthesize \nfindings from recent research of RAG and LLM in biomedicine \nand provide clinical development guidelines to improve effec\xad\ntiveness as well as transparency in future research.\nMaterials and methods\nStudy design\nWe conducted a systematic literature review. The report was \ncreated in adherence to the Preferred Reporting Items for Sys\xad\ntematic Reviews and Meta-Analyses 2020 analysis checklist \n(File S1).15 We also performed a meta-analysis following the \nCochrane Handbook guidelines.16\nScreening papers\nWe searched in 3 databases (PubMed, Embase, and Psy\xad\ncINFO), using terms related to “retrieval augmented gener\xad\nation” and “large language model.” PubMed and Embase \nwere selected based on recommendations from the Cochrane \nHandbook, as they are widely recognized for their extensive \ncoverage of biomedical literature.17 PsycINFO was included \nto capture articles from the psychological and behavioral sci\xad\nences. To maintain a high standard of quality and reliability, \nwe focused on peer-reviewed articles and excluded preprints. \nThe specific search terms used for each database are provided \nin File S2. Given that ChatGPT was released on November \n30, 2022, we set the publication filter to search papers pub\xad\nlished in 2023 and 2024. The search was performed on \nDecember 12, 2024. The inclusion criteria were: (1) the study \nmust compare baseline LLM performance with RAG per\xad\nformance and (2) the study must address a biomedical ques\xad\ntion. The exclusion criteria were: (1) literature reviews, \neditorial comments, or viewpoint papers, (2) studies focusing \non LLMs in languages other than English, or (3) studies cen\xad\ntered on a multi-agent system without a focus on RAG. SL \nscreened titles and abstracts, then conducted a full-text \nreview of papers meeting the criteria.\nData extraction\nFor each included study, we extracted the following informa\xad\ntion: author, title, publication year, journal, clinical task, and \nspecialty. Regarding RAG techniques, we gathered details \nabout the baseline LLM, retrieval sources, and strategies used \nin the pre-retrieval, retrieval, and post-retrieval stages. For \nevaluation, we extracted the evaluation method (human, \nautomated, or a combination of both), the number of evalua\xad\ntors, the evaluation dataset, and the evaluation metrics.\nMeta-analysis\nEffect size was defined as a metric quantifying the relation\xad\nship between variables, including both direction and magni\xad\ntude.18 For each included study, we calculated the effect size \nbetween baseline LLM performance and RAG-enhanced \nLLM performance. The outcomes focused on the perform\xad\nance of generation results, such as accuracy and usefulness. \nMetrics related to the retrieval process, cost, or speed were \nnot included as outcomes in the meta-analysis. For continu\xad\nous outcomes, we used Cohen’s d, standardized between- \ngroup mean difference (SMD), calculated as the difference in \nmeans divided by the pooled standard deviation. The stand\xad\nard error (SE) of SMD was calculated using the following for\xad\nmula (1), where n1 and n2 represent the sample sizes of each \ngroup.19 For dichotomous measurements, we calculated the \nlog-odds ratio, obtained by transforming the odds ratio (OR) \nwith the natural logarithm, and the associated SE was calcu\xad\nlated using formula (2), where a, b, c, and d represent the \nnumber of successful and failed events in the baseline LLM \nand RAG-enhanced LLM approaches. For studies reporting \nmultiple outcomes, we used the overall outcome to calculate \neffect size. If no overall outcome was reported, we averaged \nthe effect sizes of all reported outcomes. We excluded out\xad\ncomes with a sample size of less than 30 to avoid small- \nsample bias. \nSESMD ¼\nffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi\nn1 þ n2\nn1n2\nþ\nSMD2\n2 n1 þ n2\nð\nÞ\ns\n(1) \nSElogOR ¼\nffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi\n1\na þ 1\nb þ 1\nc þ 1\nd\nr\n(2) \nWe developed a random-effect meta-analysis model, \nbecause of the variability in RAG architectures and evalua\xad\ntion datasets among the included studies. The random-effect \nmodel was used when individual study effects likely con\xad\ntained additional sources of variance beyond sampling error. \nBetween-study heterogeneity was assessed using Higgins & \nThompson’s I2 statistic, where 25% indicated low heteroge\xad\nneity, 50% moderate, and 75% substantial.20\nWe conducted subgroup analyses to explore performance \nvariations across different factors. First, we analyzed the \ninfluence of the baseline LLM, referring to the foundation \nmodel (eg, GPT-4 or Llama2) that provides the core architec\xad\nture for the system. Second, we examined data retrieval strat\xad\negies, categorizing them as simple or complex. Simple \nstrategies included fixed-length chunking and basic similarity \nsearch, and we performed a subgroup analysis to compare \nthese with complex retrieval strategies. Third, we analyzed \ndifferences based on evaluation methods, distinguishing \nbetween human evaluations, such as Likert scale ratings for \nhelpfulness and accuracy, and automatic evaluation metrics, \nincluding ROUGE-1 and BLEU. Finally, we conducted a sub\xad\ngroup analysis based on the type of task, classifying studies \n606                                                                                                      \nJournal of the American Medical Informatics Association, 2025, Vol. 32, No. 4 \nDownloaded from https://academic.oup.com/jamia/article/32/4/605/7954485 by guest on 11 January 2026'
 'provides an innovative dynamic co-augmentation\nframework for the refinement of large language\nmodels and knowledge graphs. Initially, our ap-\nproach extracts structural insights from the unstruc-\ntured scientific literature, crafting a specialized\nknowledge graph for AD. Subsequently, we employ\na coarse-to-fine sampling technique coupled with\na unique self-aware knowledge retrieval strategy\nto pinpoint relevant information from the knowl-\nedge graph. The extensive evaluation conducted\nin our constructed ADQA benchmark showcases\nthe effectiveness of our method and provides fur-\nther hints into the synergy of LLMs and knowledge\ngraph in the context of AD. In the future, we will\ndo more exploration in adopting and benchmarking\nLLMs in the AD areas.\n7\nLimitations\nIn the development of our AD-KG, our primary\nfocus lies in the exploration of two distinct meth-\nods for extracting relationships between associated\nentities. For entity recognition, we employ a strong\nPubTator annotator directly, without delving into\nthe utilization of LLMs in this context. However,\nwe have observed that LLMs also exhibit promis-\ning entity extraction capabilities in Section 3.2.1.\nWe defer the refinement of methods for extract-\ning entities for KG construction with LLMs to fu-\nture works. Furthermore, a significant contribution\nof our work is the establishment of the ADQA\nbenchmark. Nonetheless, the datasets utilized in\nconstructing ADQA primarily consist of medical\nschool exam questions, potentially exhibiting a do-\nmain gap from the scientific literature informing\nAD-KG. One potential remedy is leveraging Pub-\nmedQA (Jin et al., 2019); however, it is hindered\nby limited data amount. In the future, we will keep\ngathering AD-related QA samples and expanding\nthe size of our ADQA benchmark.\n8\nEthics Statement\nWe have familiarized ourselves with and honour\nthe ethical code set out in the ACL Code of Ethics2.\nThe knowledge graphs constructed in the paper\nare based on published scientific literature from\nPubMed. The ADQA dataset used in the study is\nalso derived from publicly available medical QA\ndatasets that are properly cited. We strive to ensure\nour study upholds ethical principles and not cause\nany kind of safety or privacy concerns. Although\n2https://www.aclweb.org/portal/content/acl-code-ethics\nnot observed in our multiple-choice QA analysis,\nwe recognize the possibility of factual errors and\nhallucinations when using pre-trained LLMs for\nmedical QA tasks in general, and we do not recom-\nmend these models be applied in a practical setting\nat present.\nReferences\nFelix Agbavor and Hualou Liang. 2022. Predicting\ndementia from spontaneous speech using large lan-\nguage models. PLOS Digital Health, 1:1–14.\nRohan Anil, Andrew M Dai, Orhan Firat, Melvin John-\nson, Dmitry Lepikhin, Alexandre Passos, Siamak\nShakeri, Emanuel Taropa, Paige Bailey, Zhifeng\nChen, et al. 2023. Palm 2 technical report. arXiv\npreprint arXiv:2305.10403.\nAparna Balagopalan, Benjamin Eyre, Frank Rudzicz,\nand Jekaterina Novikova. 2020. To BERT or not\nto BERT: Comparing Speech and Language-Based\nApproaches for Alzheimer’s Disease Detection. In\nProc. Interspeech 2020, pages 2167–2171.\nMaciej Besta, Nils Blach, Ales Kubicek, Robert Ger-\nstenberger, Lukas Gianinazzi, Joanna Gajda, Tomasz\nLehmann, Michal Podstawski, Hubert Niewiadomski,\nPiotr Nyczyk, et al. 2023. Graph of thoughts: Solv-\ning elaborate problems with large language models.\narXiv preprint arXiv:2308.09687.\nZhen Bi, Jing Chen, Yinuo Jiang, Feiyu Xiong, Wei Guo,\nHuajun Chen, and Ningyu Zhang. 2024. Codekgc:\nCode language model for generative knowledge\ngraph construction.\nACM Transactions on Asian\nand Low-Resource Language Information Process-\ning, 23(3):1–16.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. Advances in neural information processing\nsystems, 33:1877–1901.\nSalvatore Carta, Alessandro Giuliani, Leonardo Piano,\nAlessandro Sebastian Podda, Livio Pompianu, and\nSandro Gabriele Tiddia. 2023. Iterative zero-shot llm\nprompting for knowledge graph construction. arXiv\npreprint arXiv:2307.01128.\nJiawei Chen, Hongyu Lin, Xianpei Han, and Le Sun.\n2024.\nBenchmarking large language models in\nretrieval-augmented generation. In Proceedings of\nthe AAAI Conference on Artificial Intelligence, vol-\nume 38, pages 17754–17762.\nZeming Chen, Alejandro Hernández Cano, Angelika\nRomanou, Antoine Bonnet, Kyle Matoba, Francesco\nSalvi, Matteo Pagliardini, Simin Fan, Andreas Köpf,\nAmirkeivan Mohtashami, et al. 2023. Meditron-70b:\nScaling medical pretraining for large language mod-\nels. arXiv preprint arXiv:2311.16079.'
 '17. Lefebvre C, Glanville J, Briscoe S, et al. Chapter 4: searching for \nand selecting studies. Cochrane Handbook for Systematic Reviews \nof Interventions Version, Vol. 6. Cochrane, 2024. https://training. \ncochrane.org/handbook/current/chapter-04\n18. Chapter 3 Effect Sizes j Doing Meta-Analysis in R. Accessed Octo\xad\nber 13, 2024. https://bookdown.org/MathiasHarrer/Doing_Meta_ \nAnalysis_in_R/effects.html\n19. Borenstein M, Hedges LV, Higgins JPT, et al. Introduction to \nMeta-Analysis. John Wiley & Sons; 2011.\n20. Higgins JPT, Thompson SG. Quantifying heterogeneity in a meta- \nanalysis. Stat Med. 2002;21:1539-1558. https://doi.org/10.1002/ \nsim.1186\n21. Peters JL, Sutton AJ, Jones DR, et al. Contour-enhanced meta- \nanalysis funnel plots help distinguish publication bias from other \ncauses of asymmetry. J Clin Epidemiol. 2008;61:991-996. https:// \ndoi.org/10.1016/j.jclinepi.2007.11.010\n22. Egger M, Smith GD, Schneider M, et al. Bias in meta-analysis detected \nby a simple, graphical test measures of funnel plot asymmetry. BMJ. \n1997;315:629-634. https://doi.org/10.1136/bmj.315.7109.629\n23. Kreimeyer K, Canzoniero JV, Fatteh M, et al. Using retrieval- \naugmented generation to capture molecularly-driven treatment \nrelationships for precision oncology. Stud Health Technol Inform. \n2024;316:983-987. https://doi.org/10.3233/SHTI240575\n24. Murugan M, Yuan B, Venner E, et al. Empowering personalized \npharmacogenomics with generative AI solutions. J Am Med Inform \nAssoc. 2024;31:1356-1366. https://doi.org/10.1093/jamia/ocae039\n25. Yazaki M, Maki S, Furuya T, et al. Emergency patient triage \nimprovement through a retrieval-augmented generation enhanced \nlarge-scale language model. Prehosp Emerg Care. 2024;1-7. \nhttps://doi.org/10.1080/10903127.2024.2374400\n26. Malik S, Kharel H, Dahiya DS, et al. Assessing ChatGPT4 with \nand without retrieval-augmented generation in anticoagulation \nmanagement for gastrointestinal procedures. Ann Gastroenterol. \n2024;37:514-526. https://doi.org/10.20524/aog.2024.0907\n27. Kresevic S, Giuffr\x12e M, Ajcevic M, et al. Optimization of hepatolog\xad\nical clinical guidelines interpretation by large language models: a \nretrieval augmented generation-based framework. NPJ Digit Med. \n2024;7:102-109. https://doi.org/10.1038/s41746-024-01091-y\n28. Zakka C, Shad R, Chaurasia A, et al. Almanac–retrieval-aug\xad\nmented language models for clinical medicine. NEJM AI. 2024;1 \n(2):10.1056/aioa2300068. https://doi.org/10.1056/aioa2300068\n29. Long C, Subburam D, Lowe K, et al. ChatENT: augmented large \nlanguage model for expert knowledge retrieval in otolaryngology– \nhead and neck surgery. Otolaryngol Head Neck Surg. \n2024;171:1042-1051. https://doi.org/10.1002/ohn.864\n30. Wang D, Liang J, Ye J, et al. Enhancement of the performance of \nlarge language models in diabetes education through retrieval-aug\xad\nmented generation: comparative study. J Med Internet Res. \n2024;26:e58041. https://doi.org/10.2196/58041\n31. Rau S, Rau A, Nattenm€uller J, et al. A retrieval-augmented chatbot \nbased on GPT-4 provides appropriate differential diagnosis in gas\xad\ntrointestinal radiology: a proof of concept study. Eur Radiol Exp. \n2024;8:60. https://doi.org/10.1186/s41747-024-00457-x\n32. Morris JH, Soman K, Akbas RE, et al. The scalable precision \nmedicine open knowledge engine (SPOKE): a massive knowl\xad\nedge \ngraph \nof \nbiomedical \ninformation. \nBioinformatics. \n2023;39(2):btad080. \nhttps://doi.org/10.1093/BIOINFOR\xad\nMATICS/BTAD080\n33. Soman K, Rose PW, Morris JH, et al. Biomedical knowl\xad\nedge graph-optimized prompt generation for large language \nmodels. Commun ACM. 2023;66:7-7. https://doi.org/10.1145/ \n3606337\n34. Jeong M, Sohn J, Sung M, et al. Improving medical reasoning \nthrough retrieval and self-reflection with retrieval-augmented large \nlanguage models. Bioinformatics. 2024;40:i119-i129. https://doi. \norg/10.1093/bioinformatics/btae238\n35. Alkhalaf M, Yu P, Yin M, et al. Applying generative AI with \nretrieval augmented generation to summarize and extract key \nclinical information from electronic health records. J Biomed \nInform. \n2024;156:104662. \nhttps://doi.org/10.1016/j.jbi.2024. \n104662\n36. Tarabanis C, Zahid S, Mamalis M, et al. Performance of publicly \navailable large language models on internal medicine board-style \nquestions. PLOS Digit Health. 2024;3:e0000604. https://doi.org/ \n10.1371/journal.pdig.0000604\n37. Glicksberg BS, Timsina P, Patel D, et al. Evaluating the accuracy of \na state-of-the-art large language model for prediction of admis\xad\nsions from the emergency room. J Am Med Inform Assoc. \n2024;31:1921-1928. https://doi.org/10.1093/jamia/ocae103\n38. Zelin C, Chung WK, Jeanne M, et al. Rare disease diagnosis using \nknowledge guided retrieval augmentation for ChatGPT. J Biomed \nInform. 2024;157:104702. https://doi.org/10.1016/j.jbi.2024.104702\n39. Chen X, Wang L, You MK, et al. Evaluating and enhancing large \nlanguage models’ performance in domain-specific medicine: devel\xad\nopment and usability study with DocOA. J Med Internet Res. \n2024;26:e58158. https://doi.org/10.2196/58158\n40. Du X, Novoa-Laurentiev J, Plasaek JM, et al. Enhancing early \ndetection of cognitive decline in the elderly: a comparative study \nutilizing large language models in clinical notes. medRxiv. \n2024;109:105401. https://doi.org/10.1101/2024.04.03.24305298\n41. Zhang Y, Li Y, Cui L, et al. Siren’s song in the AI Ocean: a survey \non hallucination in large language models. 2023, preprint: not \npeer reviewed. https://arxiv.org/abs/2309.01219\n42. Li X, Ouyang JA. Systematic investigation of knowledge retrieval \nand selection for retrieval augmented generation. 2024, preprint: \nnot peer reviewed. https://arxiv.org/abs/2410.13258\n43. Xiong G, Jin Q, Lu Z, et al. Benchmarking retrieval-augmented \ngeneration for medicine. Findings of the Association for Computa\xad\ntional Linguistics: ACL 2024, 6233-6251. Bangkok, Thailand: \nAssociation for Computational Linguistics. \n44. Hewitt KJ, Wiest IC, Carrero ZI, et al. Large language models as a \ndiagnostic support tool in neuropathology. J Pathol Clin Res. \n2024;10:e70009. https://doi.org/10.1002/2056-4538.70009\n45. Allahverdiyev R, Taha M, Akalin A, et al. ChunkRAG: novel \nLLM-chunk filtering method for RAG systems. October 25, 2024, \npreprint: not peer reviewed. https://arxiv.org/abs/2410.19572\n46. Cai B, Zhang FL, Wang C. Research on chunking algorithms of \ndata de-duplication. Advances in Intelligent Systems and Comput\xad\ning, Vol. 181. 2013:1019-1025. Springer, Berlin, Heidelberg. \nhttps://doi.org/10.1007/978-3-642-31698-2_144\n47. Optimizing RAG with Advanced Chunking Techniques. Accessed \nDecember 15, 2024. https://antematter.io/blogs/optimizing-rag- \nadvanced-chunking-techniques-study\n48. Chen J, Xiao S, Zhang P, et al. M3-Embedding: multi-lingual, \nmulti-functionality, multi-granularity text embeddings through \nself-knowledge distillation. Findings of the Association for Com\xad\nputational Linguistics ACL 2024. 2024:2318-2335. Association \nfor Computational Linguistics. https://doi.org/10.18653/V1/2024. \nFINDINGS-ACL.137\n49. Sawarkar K, Mangal A, Solanki SR. Blended RAG: improving \nRAG (Retriever-Augmented Generation) accuracy with semantic \nsearch and hybrid query-based retrievers. 2024 IEEE 7th Interna\xad\ntional Conference on Multimedia Information Processing and \nRetrieval (MIPR), 155-161. San Jose, CA, USA: IEEE. https://doi. \norg/10.1109/MIPR62202.2024.00031\n50. Edge D, Trinh H, Cheng N, et al. From local to global: a graph \nRAG approach to query-focused summarization. 2024, preprint: \nnot peer reviewed. https://arxiv.org/abs/2404.16130\n51. Wu Z, Hasan A, Wu J, et al. KnowLab_AIMed at MEDIQA- \nCORR 2024: Chain-of-Though (CoT) prompting strategies for \nmedical error detection and correction. Proceedings of the 6th \nClinical Natural Language Processing Workshop. 2024:353-359. \nAssociation for Computational Linguistics. 10.18653/v1/2024. \nclinicalnlp-1.33\n52. Kwon T, Tzu-Iunn Ong K, Kang D, et al. Large language models \nare clinical reasoners: reasoning-aware diagnosis framework with \n614                                                                                                      \nJournal of the American Medical Informatics Association, 2025, Vol. 32, No. 4 \nDownloaded from https://academic.oup.com/jamia/article/32/4/605/7954485 by guest on 11 January 2026'
 'Limitations\nThis study was limited to peer-reviewed publications avail\xad\nable in biomedical databases (eg, PubMed, Embase), exclud\xad\ning \npreprint \narticles \nfrom \nrepositories \nlike \nArXiv. \nAdditionally, only studies in English language were included, \nwhich might have excluded relevant studies in other lan\xad\nguages. We did not include sources such as IEEE Xplore or \nGoogle Scholar, which might have additional relevant stud\xad\nies. However, our focus was on biomedicine, and we priori\xad\ntized databases specifically tailored to biomedical research to \nmaintain the relevance and quality of the included studies. \nFurthermore, we used free-text searches in the databases, \nwhich activated automatic mapping to Medical Subject \nHeadings (MeSH) and Emtree terms, improving retrieval \naccuracy. However, the limitations of automatic term map\xad\nping cannot be ignored, as it may introduce variability if the \nunderlying algorithms change. To address this, we have docu\xad\nmented all identified papers from our search. The title, publi\xad\ncation year, PMID, PUI, and database source for each study \nare provided in File S3.\nConclusion\nWe conducted a systematic literature review of studies \nexploring the use of RAG and LLM in clinical tasks. RAG \nimplementation showed a 1.35 odds ratio increase in per\xad\nformance compared to baseline LLMs. To improve perform\xad\nance and transparency in future studies, we developed \nguidelines for improving clinical RAG applications based on \ncurrent research findings. Future research could focus on \nthese 3 directions: (1) system-level enhancement: the combi\xad\nnation of RAG and agent, (2) knowledge-level enhancements: \ndeep integration of knowledge into LLM, and (3) integration- \nlevel enhancements: integrating RAG systems within EHRs.\nAuthor contributions\nSiru Liu (Conceptualization, Data curation, Formal analysis, \nInvestigation, Methodology, Software, Visualization, Writing \n– original draft), Allison B. McCoy (Conceptualization, Writ\xad\ning – review & editing), Adam Wright (Conceptualization, \nWriting – review & editing).\nSupplementary material\nSupplementary material is available at Journal of the Ameri\xad\ncan Medical Informatics Association online.\nFunding\nThis work was supported by National Institutes of Health \ngrants: R00LM014097-02 and R01LM013995-01.\nConflicts of interest\nThe authors do not have conflicts of interest related to this \nstudy.\nData availability\nThe characteristics and outcomes for each included study \nwere reported in the File S2.\nReferences\n01. Raiaan MAK, Mukta MSH, Fatema K, et al. A review on large lan\xad\nguage models: architectures, applications, taxonomies, open issues \nand challenges. IEEE Access. 2024;12:26839-26874. https://doi. \norg/10.1109/ACCESS.2024.3365742\n02. Thirunavukarasu AJ, Ting DSJ, Elangovan K, et al. Large language \nmodels in medicine. Nat Med. 2023;29:1930-1940. https://doi. \norg/10.1038/s41591-023-02448-8\n03. Ayers JW, Poliak A, Dredze M, et al. Comparing physician and arti\xad\nficial intelligence Chatbot responses to patient questions posted to a \npublic social media forum. JAMA Intern Med. 2023;183:589-596. \nhttps://doi.org/10.1001/jamainternmed.2023.1838\n04. Liu S, Wright AP, Patterson BL, et al. Using AI-generated sugges\xad\ntions from ChatGPT to optimize clinical decision support. J Am \nMed Inform Assoc. 2023;30:1237-1245. https://doi.org/10.1093/ \njamia/ocad072\n05. Zaretsky J, Kim JM, Baskharoun S, et al. Generative artificial intel\xad\nligence to transform inpatient discharge summaries to patient- \nfriendly language and format. JAMA Netw Open. 2024;7: \ne240357. https://doi.org/10.1001/jamanetworkopen.2024.0357\n06. Gao Y, Xiong Y, Gao X, et al. Retrieval-augmented generation for \nlarge language models: a survey. 2023, preprint: not peer reviewed. \nhttps://arxiv.org/abs/2312.10997\n07. Xu Z, Jain S, Kankanhalli M. Hallucination is inevitable: an innate \nlimitation of large language models. January 22, 2024, preprint: \nnot peer reviewed. https://arxiv.org/abs/2401.11817\n08. Shuster K, Poff S, Chen M, et al. Retrieval augmentation reduces \nhallucination in conversation. Findings of the Association for Com\xad\nputational Linguistics, Findings of ACL: EMNLP 2021. Associa\xad\ntion for Computational Linguistics (ACL); 2021:3784-3803.\n09. Ayala  O, Bechard P. Reducing hallucination in structured outputs \nvia Retrieval-Augmented Generation. Proceedings of the 2024 \nConference of the North American Chapter of the Association for \nComputational Linguistics: Human Language Technologies. \nAssociation for Computational Linguistics; 2024;228-38. https:// \ndoi.org/10.18653/v1/2024.naacl-industry.19\n10. Gornik HL, Aronow HD, Goodney PP, et al. 2024 ACC/AHA/ \nAACVPR/APMA/ABC/SCAI/SVM/SVN/SVS/SIR/VESS guideline \nfor the management of lower extremity peripheral artery disease: a \nreport of the American College of Cardiology/American Heart \nAssociation Joint Committee on Clinical Practice Guidelines. Cir\xad\nculation. 2024;149:e1313-e1410. https://doi.org/10.1161/CIR. \n0000000000001251\n11. Vasey B, Nagendran M, Campbell B, et al. Reporting guideline for \nthe early stage clinical evaluation of decision support systems \ndriven by artificial intelligence: DECIDE-AI. BMJ. 2022;377: \ne070904. https://doi.org/10.1136/bmj-2022-070904\n12. Mongan J, Moy L, Kahn CE. Checklist for Artificial Intelligence in \nMedical Imaging (CLAIM): a guide for authors and reviewers. \nRadiol Artif Intell. 2020;2:e200029. https://doi.org/10.1148/ryai. \n2020200029\n13. Martindale APL, Ng B, Ngai V, et al. Concordance of randomised \ncontrolled trials for artificial intelligence interventions with the \nCONSORT-AI \nreporting \nguidelines. \nNat \nCommun. \n2024;15:6376-6311. https://doi.org/10.1038/s41467-024-45355-3\n14. Tam TYC, Sivarajkumar S, Kapoor S, et al. A framework for \nhuman evaluation of large language models in healthcare derived \nfrom literature review. NPJ Digit Med. 2024;7:258. https://doi. \norg/10.1038/s41746-024-01258-7\n15. Shamseer L, Moher D, Clarke M, et al. Preferred reporting items \nfor systematic review and meta-analysis protocols (PRISMA-p) \n2015: elaboration and explanation. BMJ. 2015;350:g7647. \nhttps://doi.org/10.1136/bmj.g7647\n16. Higgins JPT, Thomas J, Chandler J, et al. (editors). Cochrane Hand\xad\nbook for Systematic Reviews of Interventions version 6.5 (updated \nAugust 2024). Cochrane, 2024. Available from www.training. \ncochrane.org/handbook. Date accessed December 11, 2024.\nJournal of the American Medical Informatics Association, 2025, Vol. 32, No. 4                                                                                                      \n613 \nDownloaded from https://academic.oup.com/jamia/article/32/4/605/7954485 by guest on 11 January 2026']","['<1-hop>\n\n2 | GigaScience , 2025, Vol. 14 LLaMA with medical liter atur e. Additionall y, Med-P aLM [ 9 ] shows promising performance on the MedQA exam based on clinical cor por a and human feedback. Meanwhile, aiming at the Chinese medical domain, LLMs such as BenTsao [ 10 ], DoctorGLM [ 11 ], and HuatuoGPT [ 12 ] are developed on the Chinese medical dialogue data. Mor e r ecentl y, Zhongjing [ 13 ] and ChiMed-GPT [ 14 ] adopted full pipeline training from pretraining, SFT, to reinforcement learning with human feedback (RLHF) [ 15 ]. While fine-tuning can reduce hallucinations in LLMs, it brings about considerable train- ing expenses. Additionally, it poses a critical challenge known as catastrophic forgetting. This issue manifests when a model for gets its pr e viousl y learned information as a consequence of parameter modifications during the acquisition of new tasks. This forgetfulness results in a deterioration of performance on prior tasks, consequently constraining the model’s practical applicability [ 16 , 17 ]. In addition to fine-tuning, r esearc hers also enhance the out- put of LLMs through the field of prompt engineering. Prompt en- gineering focuses on the creation and optimization of prompts to impr ov e the effectiv eness of LLMs acr oss v arious a pplications and r esearc h domains [ 18 ]. It can enhance the ca pabilities of LLMs in a wide range of complex tasks, including question answering, sentiment classification, and commonsense reasoning. Chain-of- thought (CoT) prompts [ 19 ] enable complex reasoning capabili- ties by incor por ating intermediate r easoning steps . T he Automatic Pr ompt Engineer (APE) pr oposes an automatic pr ompt gener ation method aimed at enhancing the performance of LLMs [ 20 ]. Prompt engineering offers a straightforw ar d approach to harnessing the potential of LLMs without fine-tuning. On the other hand, knowledge gr a phs (KGs) ar e r epositories of vast quantities of high-quality structured data, offering the po- tential to effectiv el y mitigate the issue of factual hallucinations when integrated with LLMs . Hence , employing KGs for question answering can enhance the precision of the responses and fur- nish a dependable foundation for the factual verification of in- formation produced by LLMs. Knowledge gr a ph question answer- ing (KGQA) has long been a hot r esearc h topic. Befor e the ad- vent of LLMs, certain studies [ 21–23 ] typically begin by r etrie ving a subgr a ph r elated to the question to r educe the searc h space, then perform m ultihop r easoning on this basis . T his r etrie v al- plus-r easoning par adigm has shown its adv anta ges ov er dir ect r easoning acr oss the entir e KG [ 24 , 25 ]. Additionall y, r esearc hers tackle KGQA by parsing the question into a structured query lan- guage (e.g., SPARQL) and using a query engine to obtain accu- rate answers [ 26 , 27 ]. UniKGQA [ 28 ] introduces a unified fine- tuning fr ame work for r etrie v al and r easoning, mor e closel y linking these 2 stages. Ho w ever, traditional KGQA methods usually per- form poorly in accurate semantic understanding and high-quality text generation due to the lack of LLMs for r etrie v al and r eason- ing. Hence, r ecent r esearc h is incr easingl y utilizing external KGs to enhance LLMs in addressing KGQA challenges. For instance, StructGPT [ 29 ] navigates through knowledge graphs by identify- ing pathways from an initial seed entity to the target answer en- tity, while Think-on-Gr a ph (ToG) [ 30 ] intr oduces iter ativ e explo- ration of the knowledge gr a ph, whic h can become inefficient with v ery lar ge KGs. Additionall y, Reasoning on Gr a phs (RoG) [ 31 ] ne- cessitates fine-tuning to accur atel y gener ate and plan the r elation paths. KG-GPT [ 32 ] opts for r etrie ving an entire subgraph from the knowledge gr a ph and then deduces the answer thr ough infer ence. Although these methods hav e ac hie v ed gr atifying r esults in gen- er al ar eas, as shown in Fig. 1 B, when the intermediate entity in the multihop question is unknown, it is impossible to retrieve the a ppr opriate knowledge from the KG. In this article, we intr oduce an innov ativ e fr ame work called knowledge gr a ph–based thought (KGT), whic h integr ates LLMs with KGs through employing LLMs for reasoning on the schema of KGs to mitigate factual hallucinations of LLMs, as shown in Fig. 1 C. Unlike traditional methods, KGT does not dir ectl y r etrie v e factual information based on the question. Instead, it uses LLMs to infer entity information on the schema of the knowledge graph, generating an optimal subgraph based on k e y information di- r ectl y extr acted fr om the question and inferr ed information fr om the sc hema. Subsequentl y, the optimal subgr a ph is used to in- fer the answer to the question through LLMs. KGT r equir es no fine-tuning, offers seamless integration with multiple LLMs, and is plug-and-play, facilitating easy deployment. It demonstrates generalizability, making it adaptable for use with diverse knowl- edge gr a phs . T his fr ame work is tailor ed for wide-r anging a ppli- cations in numerous biomedical challenges, such as (i) enhanc- ing clinical decision-making for physicians and medical organi- zations, (ii) delivering medical advice to patients and health care pro viders , (iii) unco vering crucial biomarkers for early disease de- tection and tailored therapy, and (iv) exploring novel therapeutic applications for existing medications through insights into their mechanisms , side effects , and the biological processes of associ- ated diseases . Furthermore , we utilize the'
 '<2-hop>\n\nMeta-analysis Effect size was defined as a metric quantifying the relation\xad ship between variables, including both direction and magni\xad tude.18 For each included study, we calculated the effect size between baseline LLM performance and RAG-enhanced LLM performance. The outcomes focused on the perform\xad ance of generation results, such as accuracy and usefulness. Metrics related to the retrieval process, cost, or speed were not included as outcomes in the meta-analysis. For continu\xad ous outcomes, we used Cohen’s d, standardized between- group mean difference (SMD), calculated as the difference in means divided by the pooled standard deviation. The stand\xad ard error (SE) of SMD was calculated using the following for\xad mula (1), where n1 and n2 represent the sample sizes of each group.19 For dichotomous measurements, we calculated the log-odds ratio, obtained by transforming the odds ratio (OR) with the natural logarithm, and the associated SE was calcu\xad lated using formula (2), where a, b, c, and d represent the number of successful and failed events in the baseline LLM and RAG-enhanced LLM approaches. For studies reporting multiple outcomes, we used the overall outcome to calculate effect size. If no overall outcome was reported, we averaged the effect sizes of all reported outcomes. We excluded out\xad comes with a sample size of less than 30 to avoid small- sample bias. SESMD ¼ ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi n1 þ n2 n1n2 þ SMD2 2 n1 þ n2 ð Þ s (1) SElogOR ¼ ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi 1 a þ 1 b þ 1 c þ 1 d r (2) We developed a random-effect meta-analysis model, because of the variability in RAG architectures and evalua\xad tion datasets among the included studies. The random-effect model was used when individual study effects likely con\xad tained additional sources of variance beyond sampling error. Between-study heterogeneity was assessed using Higgins & Thompson’s I2 statistic, where 25% indicated low heteroge\xad neity, 50% moderate, and 75% substantial.20 We conducted subgroup analyses to explore performance variations across different factors. First, we analyzed the influence of the baseline LLM, referring to the foundation model (eg, GPT-4 or Llama2) that provides the core architec\xad ture for the system. Second, we examined data retrieval strat\xad egies, categorizing them as simple or complex. Simple strategies included fixed-length chunking and basic similarity search, and we performed a subgroup analysis to compare these with complex retrieval strategies. Third, we analyzed differences based on evaluation methods, distinguishing between human evaluations, such as Likert scale ratings for helpfulness and accuracy, and automatic evaluation metrics, including ROUGE-1 and BLEU. Finally, we conducted a sub\xad group analysis based on the type of task, classifying studies 606 Journal of the American Medical Informatics Association, 2025, Vol. 32, No. 4 Downloaded from https://academic.oup.com/jamia/article/32/4/605/7954485 by guest on 11 January 2026']","Based on the provided context, the differences between LLaMA and Llama2 models in their integration with biomedical knowledge graphs to mitigate hallucinations in large language models (LLMs), and the impact of this integration on retrieval-augmented generation (RAG) system performance as demonstrated by meta-analyses, can be summarized as follows:

### Differences in Integration of LLaMA and Llama2 with Biomedical Knowledge Graphs

- **LLaMA and Llama2 as Baseline Models:**  
  Both LLaMA and Llama2 serve as foundational large language models (LLMs) used as baseline architectures in biomedical RAG systems. The meta-analysis included studies that used these models as the core LLMs for retrieval-augmented generation.

- **Integration Approach:**  
  The integration of biomedical knowledge graphs with these models involves augmenting the LLMs with external, structured biomedical knowledge extracted from scientific literature and databases (e.g., PubMed). This is done to provide up-to-date, domain-specific, and reliable information that the base LLMs alone may lack due to their fixed training data and potential for hallucination.

- **Specific Techniques:**  
  For example, one approach described involves extracting structural insights from unstructured scientific literature to build specialized knowledge graphs (e.g., for Alzheimer’s Disease), then using coarse-to-fine sampling and self-aware knowledge retrieval strategies to identify relevant information from the knowledge graph. While this example is not explicitly tied to LLaMA or Llama2, it illustrates the type of integration methods used with LLMs in biomedical contexts.

- **Use of LLMs in Entity Recognition:**  
  The context notes that in some biomedical knowledge graph construction efforts, entity recognition was performed using tools like PubTator rather than LLMs, although LLMs show promise in this area. This suggests that LLaMA and Llama2 may not yet be fully leveraged for all aspects of knowledge graph construction but are primarily used for retrieval and generation tasks augmented by these graphs.

### Impact on Mitigating Hallucinations

- **Hallucination Mitigation via RAG:**  
  Both LLaMA and Llama2 models, when integrated with biomedical knowledge graphs through RAG, benefit from reduced hallucination. RAG provides relevant, up-to-date, and trustworthy biomedical information that the LLMs incorporate into their responses, thereby reducing the generation of incorrect or fabricated information.

- **Example:**  
  The context provides an example where ChatGPT initially omitted a key medication recommendation but corrected its response after integrating retrieved guideline text, illustrating how retrieval augmentation helps mitigate hallucinations. Although this example is with ChatGPT, the principle applies similarly to LLaMA and Llama2 when augmented with biomedical knowledge graphs.

### Impact on RAG System Performance (Meta-Analysis Findings)

- **Overall Performance Improvement:**  
  The systematic review and meta-analysis of 20 studies comparing baseline LLMs (including LLaMA and Llama2) with RAG-enhanced versions showed a statistically significant improvement in performance. The pooled odds ratio was 1.35 (95% CI: 1.19–1.53, P = .001), indicating that RAG systems outperform baseline LLMs in biomedical tasks.

- **Subgroup Analyses:**  
  The meta-analysis included subgroup analyses based on the baseline LLM used, retrieval strategies, evaluation methods, and task types. These analyses confirmed that integrating knowledge graphs and retrieval mechanisms with LLaMA and Llama2 models consistently enhances accuracy and usefulness in biomedical question answering and related tasks.

- **Guidelines and Future Directions:**  
  The study recommends future research focus on deeper integration of knowledge into LLMs (knowledge-level enhancement), combining RAG with agent systems (system-level enhancement), and embedding RAG within electronic health records (integration-level enhancement) to further improve performance and reduce hallucinations.

### Summary

| Aspect                          | LLaMA Model                          | Llama2 Model                        | Impact of Integration with Biomedical Knowledge Graphs via RAG                  |
|--------------------------------|------------------------------------|-----------------------------------|---------------------------------------------------------------------------------|
| Role in RAG Systems             | Baseline LLM architecture          | Baseline LLM architecture         | Both serve as core LLMs augmented by retrieval of biomedical knowledge          |
| Integration Method              | Augmented with external biomedical knowledge graphs using retrieval strategies | Similar approach with retrieval-augmented biomedical knowledge graphs | Enables incorporation of up-to-date, domain-specific information                 |
| Use in Knowledge Graph Construction | Limited use in entity recognition; external tools like PubTator used | Similar limitations noted; LLMs show promise but not fully utilized yet | Knowledge graphs provide structured, reliable biomedical data to LLMs          |
| Hallucination Mitigation        | Reduced hallucination through retrieval of relevant biomedical facts | Same benefit observed             | RAG integration significantly reduces hallucinations by grounding responses in factual data |
| Performance Improvement (Meta-Analysis) | Shows significant improvement in biomedical tasks when combined with RAG | Likewise shows significant improvement | Meta-analysis shows 1.35 odds ratio increase in performance over baseline LLMs  |
| Future Enhancement Directions  | Deep knowledge integration, system-level and EHR integration recommended | Same recommendations              | Further integration expected to enhance accuracy, reliability, and clinical utility |

---

**In conclusion,** both LLaMA and Llama2 models, when integrated with biomedical knowledge graphs through retrieval-augmented generation, demonstrate improved performance and reduced hallucination in biomedical applications. The meta-analysis confirms that RAG-enhanced LLMs outperform baseline models, supporting the effectiveness of this integration strategy in clinical and biomedical settings.","LLaMA-based models in the biomedical domain have been enhanced through various approaches, including fine-tuning and prompt engineering, to reduce hallucinations and improve performance. The knowledge graph–based thought (KGT) framework integrates LLMs like LLaMA with biomedical knowledge graphs by using LLMs to reason on the schema of knowledge graphs rather than directly retrieving factual information. This method generates an optimal subgraph based on key information extracted from the question and inferred schema data, which is then used by LLMs to infer answers. KGT requires no fine-tuning, supports seamless integration with multiple LLMs including LLaMA, and is plug-and-play, facilitating deployment and generalizability across diverse biomedical knowledge graphs. On the other hand, Llama2 serves as a foundational model in retrieval-augmented generation (RAG) systems, where meta-analyses have quantified the effect size of RAG-enhanced LLM performance compared to baseline LLMs. These analyses, using metrics such as Cohen’s d and log-odds ratios, demonstrate that integrating retrieval mechanisms with Llama2 improves generation accuracy and usefulness. Subgroup analyses further reveal that the choice of baseline LLM, including Llama2, and the complexity of data retrieval strategies influence performance outcomes. Thus, while LLaMA models benefit from knowledge graph integration via frameworks like KGT to mitigate hallucinations and enhance reasoning, Llama2’s role as a base model in RAG systems is supported by meta-analytic evidence showing improved generation performance when retrieval is incorporated.",0.9733333333333334,0.9723940765347233,0.3333333333,0.375
"How does the integration of single-cell RNA sequencing data from the cellxgene Atlas enhance the co-expression networks in STRING version 12.5, and what advantages does this expansion provide for understanding gene expression dynamics across different organisms?","['Nucleic Acids Research , 2025, Vol. 53, Database issue \nD 733 \nFigure 1. Illustration of the new ‘regulatory network’ mode in STRING, where the network edges visually indicate the direction, confidence and sources \nof each regulatory interaction. Clicking on an edge within the network brings up a pop-up window with a detailed o v ervie w of the a v ailable e vidence and \nthe annotated type of regulation. Users can further explore the data behind the interactions by accessing the specific evidence viewers linked within the \npop-up, including the text-mining evidence viewer. This viewer presents excerpts from literature pertaining to the inspected interactions, as well as an \nautomatically generated large language model summary of the presented e x cerpts. \nrylation , Catalysis of Small Molecule Conjugation and Cataly- \nsis of Small Molecule Removal . To enhance the interoperabil- \nity of the annotations, these chosen relation types align with \nGene Ontology ( 32 ). \nTo identify and classify a wide array of interaction types, we \nfine-tuned the RoBERT a-large-PM-M3-V oc language model, \na transformer-based model well suited for biological tasks. \nThis model was trained on the RegulaTome dataset for multi- \nlabel extraction of the directed, typed and signed interactions \nmentioned above, and achieved an average F 1 score of 73.5% \n(with a precision of 75.2% and a recall of 71.8%) on a held- \nout test set, although the exact performance varied across dif- \nferent types of relationships. This level of accuracy is substan- \ntial, considering the complexity of the underlying task. Using \nthis model, we processed over 1.2 billion sentence-level pairs \nextracted from all available PubMed abstracts and PMC Open \nAccess full-text documents, and assigned 3.5% of them ( ∼43 \nmillion, of which ∼18 million in human) with at least one \npositive label indicating directed (72.9%) or signed (33.1%) \nrelationships among the proteins. Furthermore, we made use \nof the existing curated knowledge of regulatory interactions in \nthe database channel, by parsing regulatory information from \nthe SIGNOR, KEGG and Reactome databases. \nTo integrate these interactions into STRING, we followed \na similar score aggregation and benchmarking approach as \nfor physical interactions ( 21 ). Here, we perform benchmark- \ning in five categories for which we can derive gold stan- \ndard datasets of known human regulatory interactions from \nSIGNOR: regulation , upregulation , downregulation , tran- \nscriptional regulation and phosphorylation . The resulting \ncalibration functions are then applied to extracted regula- \ntions of all types to produce the final regulatory confidence \nscores. \nThe new regulatory network has been seamlessly integrated \ninto the existing STRING framework, complementing the \n‘functional’ and ‘physical’ network types. Directional edges \nwithin this network are visually represented by arrows, depict- \ning both bidirectional and unidirectional relationships. The \nnetwork visualization can show varying confidence levels be- \ntween directions (in the ‘confidence’ view) or the specific di- \nrectionality of the sources (in the ‘evidence’ view). The user \ninterface retains its intuitive design, enabling users to access \nthe type of regulatory event and the evidence for the interac- \ntion by clicking on the edge (Figure 1 ). Additionally, all API \nfunctions have been updated to fully support the new net- \nwork type, which can be accessed by specifying the parameter \nnetw or k_type = regulatory in the API call. \nExpanded co-expression networks \nThe co-expression channel in STRING identifies genes with \nsimilar expression profiles across various tissues and condi- \ntions, revealing their involvement in shared biological pro- \ncesses. It compiles data from large-scale gene expression stud- \nies, analyzing transcript and protein levels to detect gene pairs \nwith coordinated expression. This method highlights poten- \ntial functional relationships between genes. Importantly, the \nanalysis is free from study bias, as it directly uses raw omics \ndata instead of relying on manually curated or experimental \ndatasets, providing a more precise and objective view of gene \ninteractions. \nFor the upcoming version 12.5, we are expanding the \nco-expression channel by generating additional networks \nthrough the application of FAVA (Functional Association of \nVariational Autoencoders) ( 25 ) on a wider range of single- \ncell RNA sequencing data. Specifically, we are incorporating \nsingle-cell data from the cellxgene Atlas ( 33 ) to enhance the \nhuman and mouse networks and from the EBI Single Cell Ex- \npression Atlas to extend coverage to more model organisms \n( 34 ). This expansion will provide more detailed, organism- \nspecific co-expression maps, enabling more precise identifi- \ncation of gene co-expression patterns and cross-species com- \nparisons, particularly for development, immune response and \ndisease-related gene expression dynamics. \nImpro v ed enric hment analysis \nGene set enrichment analysis is a critical component of high- \nthroughput biological studies, providing key insights into the \nbiological characteristics of datasets. This analysis is pivotal in \nidentifying essential aspects such as biological processes, path- \nways and phenotypes, which are crucial for understanding the \nbiological implications of experiments. Typically, overrepre- \nsentation analysis is employed, wherein a subset of data ex- \nceeding a specific cutoff (empirical or customary) is compared \nagainst an experimental background. By default, STRING \nDownloaded from https://academic.oup.com/nar/article/53/D1/D730/7903368 by guest on 11 January 2026'
 'D 736 \nNucleic Acids Research , 2025, Vol. 53, Database issue \nsure that clusters are easily identifiable. This functionality ex- \ntends beyond cluster analysis and is applicable to any gene set. \nAs such, it is also available through an API, complementing \nour suite of other API methods. The new API function, named \ngeneset_description , requires only a set of genes as input and \nautomatically performs enrichment analysis. It returns up to \nthree descriptions—primary, secondary and tertiary—based \non relevance and availability, identical to the cluster naming \nin the user interface. These descriptions are filled sequentially, \nwith the ‘primary’ always representing the most relevant term. \nOne application of this API is in the stringApp Cytoscape plu- \ngin, which utilizes it to automatically overlay the assigned de- \nscriptions onto each network cluster after clustering is exe- \ncuted. \nNetwork and protein embeddings \nTo facilitate the use of STRING in machine learning applica- \ntions, users can now directly download precomputed ProtT5 \n( 39 ) sequence and cross-species protein network embeddings \nderived from STRING, for all eukaryotes, and utilize the em- \nbeddings on their own labeled datasets. The embeddings en- \ncode information from two aspects. Sequence embeddings can \ncapture protein domains and shorter sequence motifs, while \nnetwork embeddings can complement these with information \non cellular organization such as protein complexes and path- \nways that may not be evident from sequence data alone. \nWhile protein sequence embeddings are inherently compa- \nrable across species due to the universal nature of amino acid \nsequences, the primary obstacle to using network embeddings \nis ensuring that network embeddings from different species \nare directly comparable. This challenge arises because pro- \ntein networks are independent for each species, and tradi- \ntional network embedding methods are not designed to ad- \ndress cross-species comparability. The lack of comparability \nbetween network embeddings prevents the effective transfer \nof knowledge and findings across different species. To address \nthis, we developed a method to align eukaryotic network em- \nbeddings across species using orthologous relationships based \non FedCoder ( 40 ). This alignment technique creates a uni- \nfied embedding space where proteins from different eukary- \notic species can be directly compared. As a result, it enhances \ncross-species protein predictions, particularly in tasks such as \nsubcellular localization and function prediction. \nIn the protein machine learning field, having precomputed \nsequence embeddings and cross-species compatible network \nembeddings from STRING enables researchers to use these re- \nsources directly, eliminating the need to calculate embeddings \nthemselves. This strategy not only reduces the energy footprint \nresulting from redundant calculations across research groups, \nbut also enhances reproducibility in computational biology re- \nsearch promoting more sustainable and consistent scientific \npractices. \nData availability \nThe STRING database is freely available online at https:// \nstring-db.org/. \nA c kno wledg ements \nThe authors wish to thank Yan P. Yuan (EMBL Heidelberg) \nand Dandan Xue (University of Copenhagen) for IT support. \nThomas Rattei (University of Vienna) is thanked for extensive \ncomputational work toward resolving orthology relations. We \nthank the CSC—IT Center for Science, Finland, for generous \ncomputational resources. \nFunding \nSwiss Institute of Bioinformatics; Novo Nordisk Founda- \ntion [NNF14CC0001, NNF20SA0035590]; European Molec- \nular Biology Laboratory (EMBL Heidelberg); HORIZON \nEUROPE Marie Skłodowska-Curie Actions [101023676 to \nK.N.]; Academy of Finland [332844 to F.M. and S.P.]. Fund- \ning for open access charge: University of Zurich. \nConflict of interest statement \nNone declared. \nReferences \n1. Marsh, J.A. and Teichmann, S.A. (2015) Structure, dynamics, \nassembly, and evolution of protein complexes. Annu. Rev. \nBiochem., 84 , 551–575.\n2. Cesnik, A. , Schaffer, L.V. , Gaur, I. , Jain, M. , Ideker, T. and \nLundberg,E. (2024) Mapping the multiscale proteomic \norganization of cellular and disease phenotypes. Annu. Rev. \nBiomed. Data Sci., 7 , 369–389.\n3. Milacic, M. , Beavers, D. , Conley, P. , Gong, C. , Gillespie, M. , Griss, J. , \nHaw, R. , Jassal, B. , Matthews, L. , May, B. , et al. (2024) The \nReactome Pathway Knowledgebase 2024. Nucleic Acids Res., 52 , \nD672–D678.\n4. Kanehisa, M. , Furumichi, M. , Sato, Y. , Kawashima, M. and \nIshiguro-Watanabe,M. (2023) KEGG for taxonomy-based analysis \nof pathways and genomes. Nucleic Acids Res. , 51 , D587–D592. \n5. Oughtred, R. , Stark, C. , Breitkreutz, B.-J. , Rust, J. , Boucher, L. , \nChang, C. , Kolas, N. , O’Donnell, L. , Leung, G. , McAdam, R. , et al. \n(2019) The BioGRID interaction database: 2019 update. Nucleic \nAcids Res., 47 , D529–D541.\n6. Del Toro, N. , Shrivastava, A. , Ragueneau, E. , Meldal, B. , Combe, C. , \nBarrera, E. , Perfetto, L. , How, K. , Ratan, P. , Shirodkar, G. , et al. \n(2022) The IntAct database: efficient access to fine-grained \nmolecular interaction data. Nucleic Acids Res. , 50 , D648–D653. \n7. Licata, L. , Briganti, L. , Peluso, D. , Perfetto, L. , Iannuccelli, M. , \nGaleota, E. , Sacco, F. , Palma, A. , Nardozza, A.P. , Santonico, E. , et al. \n(2012) MINT, the molecular interaction database: 2012 update. \nNucleic Acids Res., 40 , D857–D861.\n8. Meldal, B.H.M. , Perfetto, L. , Combe, C. , Lubiana, T. , Ferreira \nCavalcante, J.V. , Bye-A-Jee, H. , Waagmeester, A. , Del-Toro, N. , \nShrivastava, A. , Barrera, E. , et al. (2022) Complex Portal 2022: \nnew curation frontiers. Nucleic Acids Res. , 50 , D578–D586. \n9. Lo Surdo, P. , Iannuccelli, M. , Contino, S. , Castagnoli, L. , Licata, L. , \nCesareni, G. and Perfetto, L. (2023) SIGNOR 3.0, the SIGnaling \nnetwork open resource 3.0: 2022 update. Nucleic Acids Res., 51 , \nD631–D637.\n10. Szklarczyk, D. , Kirsch, R. , Koutrouli, M. , Nastou, K. , Mehryary, F. , \nHachilif, R. , Gable, A.L. , Fang, T. , Doncheva, N.T. , Pyysalo, S. , et al. \n(2023) The STRING database in 2023: protein–protein \nassociation networks and functional enrichment analyses for any \nsequenced genome of interest. Nucleic Acids Res. , 51 , D638–D646. \n11. Warde-Farley, D. , Donaldson, S.L. , Comes, O. , Zuberi, K. , \nBadrawi, R. , Chao, P. , Franz, M. , Grouios, C. , Kazi, F. , Lopes, C.T. , \net al. (2010) The GeneMANIA prediction server: biological \nnetwork integration for gene prioritization and predicting gene \nfunction. Nucleic Acids Res. , 38 , W214–W220. \n12. Persson, E. , Castresana-Aguirre, M. , Buzzao, D. , Guala, D. and \nSonnhammer,E.L.L. (2021) FunCoup 5: functional association \nDownloaded from https://academic.oup.com/nar/article/53/D1/D730/7903368 by guest on 11 January 2026'
 'Nucleic Acids Research , 2025, Vol. 53, Database issue \nD 735 \nFigure 2. Enrichment analysis interface from the STRING database website. Lef t: Sc hematic of the enrichment analysis tab highlighting various sections \nof the webpage. Top right: A zoomed-in view of the analysis table with two user-highlighted terms (colored lines). Bottom right: A visualization of the \nenrichment depicting the two terms highlighted in corresponding colors. \nFurthermore, a distinctive feature of STRING’s visualiza- \ntion is the incorporation of similarity groups, which visually \ngroup related terms on the plot, adding an additional dimen- \nsion to the enrichment results. These groups utilize computed \nsimilarities to cluster terms together, highlighting relationships \nbetween the terms and aiding the analysis. The clustering em- \nploys the average linkage method based on the Jaccard index, \nwith a user-specified cutoff applied to form groups that, on \naverage, share the specified similarity. The groups are sorted \nby the maximum signal of their terms, with each term within a \ngroup subsequently sorted by its individual signal. This group- \ning is visually highlighted on the right side of the plot. \nS TRING clust ering and g ene set descriptions \nSTRING offers network clustering options for user-submitted \ngene lists. This feature visually connects nodes that are more \ninterconnected to each other than to other nodes, reflecting \nthe functional modularity of the user’s gene list. By grouping \nproteins that closely interact, clustering aids in the discovery \nof functional modules and facilitates hypothesis generation. \nClustering is based on the connectivity between nodes, incor- \nporating edge weights (combined scores) reflecting the confi- \ndence level of the interactions. Only visible edges—those cor- \nresponding to active channels with confidence higher than the \nuser-specified cutoff—are considered, ensuring that clustering \nis consistent with the network being shown. \nUsers have the choice of two different clustering algorithms. \n‘ K -means clustering’ is implemented via the k -medoids algo- \nrithm [pyclustering package ( 37 )], allowing users to specify \nthe desired number of clusters ( k ). This method forms clusters \nbased on a distance matrix derived from the most probable \npaths between nodes. In case of multiple disconnected compo- \nnents within the network, STRING aims for a balanced distri- \nbution of cluster sizes by iteratively dividing the largest com- \nponents first. In contrast, ‘MCL clustering’, implemented with \nstandalone binaries (release 14-137), automatically generates \nnatural divisions based on the network’s inherent structure. It \nutilizes the Markov cluster algorithm ( 38 ) to simulate stochas- \ntic flow in graphs, identifying groups of highly interconnected \nnodes as clusters based on their combined STRING scores. \nThe user-controlled inflation parameter influences the granu- \nlarity of the clustering. This method is particularly effective \nin capturing the true modular nature of biological networks, \nallowing for intuitive groupings that correspond to biological \nfunctions and relationships. \nResearchers often analyze the biological context of the ob- \ntained clusters to interpret and draw conclusions from net- \nwork data. To aid this process, we have developed a novel \ngene set naming feature that significantly enhances the inter- \npretability of cluster analyses. This feature automatically as- \nsigns the best description for each cluster based on STRING’s \nrobust statistical enrichment analysis, simplifying the often \ncumbersome task of manually interpreting cluster data. It \nranks the enriched terms by their enrichment signal (see \nthe ‘Improved enrichment analysis’ section), choosing the pri- \nmary, secondary and tertiary names among various enrich- \nment categories. This prioritization ensures that the names re- \nflect the most statistically significant and biologically pertinent \nattributes of each cluster, providing a clear, immediate under- \nstanding of its functional characteristics. The enrichment pri- \nmarily draws from categories such as Gene Ontology Biolog- \nical Processes, while incorporating a range of other categories \nfor broader annotations. In case the enrichment analysis yields \nno significant results or if the cluster contains only one gene, \nSTRING assigns canonical gene names as descriptions to en- \nDownloaded from https://academic.oup.com/nar/article/53/D1/D730/7903368 by guest on 11 January 2026'
 'D 734 \nNucleic Acids Research , 2025, Vol. 53, Database issue \nuses the whole genome / proteome background; however, it \nis recommended that users provide a custom background \nlist representing a more realistic universe of genes / proteins \ndetected by their assay ( 35 ). Alternatively, the entire sorted \ndataset can be analyzed using gene set enrichment analysis to \ndetermine whether the distribution of genes at the top, bottom \nor both ends deviates significantly from what would be ex- \npected by chance. STRING supports both of these enrichment \nmethods. The most recent update to STRING introduces vari- \nous improvements, including a new false discovery rate (FDR) \ncorrection, enhanced filtering options and a new graphical vi- \nsualization. \nFDR correction \nIn its analyses, STRING performs a test for each path- \nway (‘term’) within a given pathway collection (‘category’). \nLike many other enrichment tools, STRING employs the \nBenjamini–Hochberg correction ( 36 ) to adjust for multiple \nhypothesis testing. This statistical correction is essential for \nmanaging the FDR effectively. However, the larger the term \ncount and the greater the diversity of the terms, the higher \nthe required correction and the lower the chance of exceed- \ning the alpha level (significance threshold) for a given term. \nOne method used to address this issue involves restricting the \nanalysis to a subset of terms, such as ‘GO slims’ that focus \non broad, high-level terms with a simplified ontology struc- \nture ( 16 ). However, this approach has some disadvantages: it \nmight exclude terms particularly relevant to a tested gene set, \nit may not cover all functions by design and it is mainly ap- \nplicable to hierarchical classification systems such as the Gene \nOntology. \nSTRING utilizes several hierarchical ontologies, such as \nthe Brenda Tissue Ontology, Disease Ontology, Human Phe- \nnotype Ontology and the aforementioned Gene Ontology; \nthey typically contain many more leaf-ward nodes than nodes \ncloser to the root. The parental nodes must contain all the \ngenes included in the child terms, creating an imbalance in \nthe ontology with many more smaller terms and fewer larger \nterms. Removing smaller terms can substantially increase sta- \ntistical power . However , simply eliminating all small terms \ncould negatively impact discoverability, especially for small \nquery gene sets where the smaller terms might be the most rel- \nevant for biological interpretation. The ontologies are usually \nlarge, with > 10 000 terms (and therefore tests) in the Biolog- \nical Process branch of the Gene Ontology tree. Such strong \ncorrections have the effect that for larger query gene sets, or \nfor backgrounds that cover only part of the proteome, it might \nnot be statistically possible to detect enrichments for a subset \nof the terms; these can therefore be removed from considera- \ntion. \nIn its new version, STRING takes a more flexible \napproach—by only testing terms that have a priori any statis- \ntical potential for enrichment. This decision is based on sev- \neral parameters: the size of the term, the size of the query set, \nthe background size and the number of tests conducted. By \ncalculating the smallest and largest term sizes that could the- \noretically be enriched given the FDR correction, STRING can \ndetermine which terms are viable for inclusion in the anal- \nysis. Terms outside these bounds are excluded from testing. \nThis methodical exclusion significantly enhances the statisti- \ncal power of the enrichment analysis, without omitting any \nterms that could be enriched. For smaller query sets, this strat- \negy will maintain all terms, as even the smallest tested sets \n(term size = 2) might still be theoretically enriched. How- \never , for larger , less specific sets or for tests against a cus- \ntom smaller background, this approach markedly improves \nthe statistical power, enabling STRING to perform more fo- \ncused analyses without the drawbacks of increased false posi- \ntive rates or omitting potentially biologically interesting terms. \nAlthough this method leverages the characteristics of ontolo- \ngies, it does not require a hierarchy (parent–child relationships \nbetween the terms) to function and can be applied to any \npathway / term collection with an imbalance of small terms, \nsuch as PMID-derived gene sets and STRING neighborhood \nclusters. \nTerm filtering \nOne key insight from our online user surveys is that the of- \nten overwhelming number of enrichment results can make it \nchallenging to discern the most relevant enriched terms. We \nhave thus enhanced the display of enrichment results, allow- \ning users to filter and sort them based on (i) FDR, (ii) strength, \n(iii) signal, (iv) term size and (v) term similarity. \nThe newest addition to our filtering options are the ‘term \nsimilarity’ and ‘signal’ filters. The ‘term similarity’ filter uses \nthe Jaccard index to measure the similarity between the gene \nsets of terms within the same category. The method proceeds \nby sorting the terms according to their P -values, prioritizing \nthose with the strongest statistical significance. The algorithm \nthen iterates through the list of terms; any term that exhibits \na similarity exceeding a predetermined, user-defined threshold \nrelative to terms already included in the results is subsequently \nexcluded. This approach requires no additional information \nbeyond what is already available in the dataset and can be \nuniformly applied across all categories of terms. The ‘signal’ \nfilter is defined as a weighted harmonic mean between the ra- \ntio of observed to expected gene occurrences in an enriched \nterm and its −log(FDR), respectively. This filter balances the \nimpact of both metrics and provides a more intuitive ordering \nof enriched terms and has been implemented as the default \nsorting criterion. \nThese filters are designed to exclude potentially less relevant \nresults, such as terms that are marginally significant, small or \nshow significant overlap in terms of gene content. This en- \nhancement streamlines the search for relevant observations, \nhelping users focus on the most meaningful results. \nGraphical visualization \nIn response to user feedback requesting more intuitive, graph- \nical visualizations of the detected enrichments, STRING now \nhas adopted an interactive dot plot as a primary display \nmethod for enrichment analysis outcomes. We chose the dot \nplot for its clarity in representing enriched functions across \nthree dimensions: (i) enrichment signal along the X -axis, (ii) \nthe FDR indicated through the color coding of the dots and \n(iii) the term’s protein count in the network represented by the \nsize of each dot. The terms (listed on the Y -axis) are ranked \nby their enrichment signal or, if grouped by similarity, by the \nenrichment signal of their group first (Figure 2 ). This visual- \nization reflects all filter settings chosen by the user to manage \nthe volume of results displayed. These plots can be addition- \nally customized in the interface and the resulting graphs can be \ndownloaded as PNG for immediate use or as SVG for further \nmodifications. \nDownloaded from https://academic.oup.com/nar/article/53/D1/D730/7903368 by guest on 11 January 2026'
 'Nucleic Acids Research , 2025, Vol. 53, Database issue \nD 737 \nnetworks in all domains of life, supporting directed links and \ntissue-specificity. J. Mol. Biol., 433 , 166835.\n13. Kim, C.Y. , Baek, S. , Cha, J. , Yang, S. , Kim, E. , Marcotte, E.M. , Hart, T. \nand Lee,I. (2022) HumanNet v3: an improved database of human \ngene networks for disease research. Nucleic Acids Res., 50 , \nD632–D639.\n14. Huang, J.K. , Carlin, D.E. , Yu, M.K. , Zhang, W. , Kreisberg, J.F. , \nTamayo, P. and Ideker, T. (2018) Systematic evaluation of molecular \nnetworks for discovery of disease genes. Cell Syst. , 6 , 484–495.e5. \n15. Wright, S.N. , Colton, S. , Schaffer, L.V. , Pillich, R.T. , Churas, C. , \nPratt, D. and Ideker, T. (2024) State of the interactomes: an \nevaluation of molecular networks for generating biological \ninsights. bioRxiv doi: \nhttps:// doi.org/ 10.1101/ 2024.04.26.587073 , 29 April 2024, \npreprint: not peer reviewed.\n16. Ontology Consortium, G. , Aleksander, S.A. , Balhoff, J. , Carbon, S. , \nCherry, J.M. , Drabkin, H.J. , Ebert, D. , Feuermann, M. , Gaudet, P. , \nHarris, N.L. , et al. (2023) The Gene Ontology knowledgebase in \n2023. Genetics , 224 , iyad031.\n17. Liberzon, A. , Birger, C. , Thorvaldsdóttir, H. , Ghandi, M. , \nMesirov, J.P. and Tamayo, P. (2015) The Molecular Signatures \nDatabase hallmark gene set collection. Cell Syst. , 1 , 417–425. \n18. Amberger, J.S. , Bocchini, C.A. , Schiettecatte, F. , Scott, A.F. and \nHamosh,A. (2015) OMIM.org: Online Mendelian Inheritance in \nMan (OMIM ®), an online catalog of human genes and genetic \ndisorders. Nucleic Acids Res. , 43 , D789–D798. \n19. Snel, B. , Bork, P. and Huynen, M.A. (2002) The identification of \nfunctional modules from the genomic association of genes. Proc. \nNatl Acad. Sci. U.S.A., 99 , 5890–5895.\n20. Guala, D. , Ogris, C. , Müller, N. and Sonnhammer, E.L.L. (2020) \nGenome-wide functional association networks: background, data \n& state-of-the-art resources. Brief. Bioinform., 21 , 1224.\n21. Szklarczyk, D. , Gable, A.L. , Nastou, K.C. , Lyon, D. , Kirsch, R. , \nPyysalo, S. , Doncheva, N.T. , Legeay, M. , Fang, T. , Bork, P. , et al. \n(2021) The STRING database in 2021: customizable \nprotein–protein networks, and functional characterization of \nuser-uploaded gene / measurement sets. Nucleic Acids Res., 49 , \nD605–D612.\n22. Mering,C.V. (2003) STRING: a database of predicted functional \nassociations between proteins. Nucleic Acids Res. , 31 , 258–261. \n23. Enright, A.J. , Iliopoulos, I. , Kyrpides, N.C. and Ouzounis, C.A. \n(1999) Protein interaction maps for complete genomes based on \ngene fusion events. Nature , 402 , 86–90.\n24. Franceschini, A. , Lin, J. , von Mering, C. and Jensen, L.J. (2016) \nSVD-phy: improved prediction of protein functional associations \nthrough singular value decomposition of phylogenetic profiles. \nBioinformatics , 32 , 1085–1087.\n25. Koutrouli, M. , Nastou, K. , Piera Líndez, P. , Bouwmeester, R. , \nRasmussen, S. , Martens, L. and Jensen, L.J. (2024) FAVA: \nhigh-quality functional association networks inferred from \nscRNA-seq and proteomics data. Bioinformatics , 40 , btae010.\n26. Porras, P. , Barrera, E. , Bridge, A. , Del-Toro, N. , Cesareni, G. , \nDuesbury, M. , Hermjakob, H. , Iannuccelli, M. , Jurisica, I. , \nKotlyar, M. , et al. (2020) Towards a unified open access dataset of \nmolecular interactions. Nat. Commun., 11 , 6144.\n27. Franceschini, A. , Szklarczyk, D. , Frankild, S. , Kuhn, M. , \nSimonovic, M. , Roth, A. , Lin, J. , Minguez, P. , Bork, P. , von Mering, C. , \net al. (2013) STRING v9.1: protein–protein interaction networks, \nwith increased coverage and integration. Nucleic Acids Res., 41 , \nD808–D815.\n28. Yu, H. , Luscombe, N.M. , Lu, H.X. , Zhu, X. , Xia, Y. , Han, J .-D.J ., \nBertin, N. , Chung, S. , V idal, M. and Gerstein, M. (2004) Annotation \ntransfer between genomes: protein–protein interologs and \nprotein–DNA regulogs. Genome Res. , 14 , 1107–1118. \n29. Szklarczyk, D. , Gable, A.L. , Lyon, D. , Junge, A. , Wyder, S. , \nHuerta-Cepas, J. , Simonovic, M. , Doncheva, N.T. , Morris, J.H. , \nBork, P. , et al. (2019) STRING v11: protein–protein association \nnetworks with increased coverage, supporting functional discovery \nin genome-wide experimental datasets. Nucleic Acids Res., 47 , \nD607–D613.\n30. Doncheva, N.T. , Morris, J.H. , Gorodkin, J. and Jensen, L.J. (2019) \nCytoscape StringApp: network analysis and visualization of \nproteomics data. J. Proteome Res., 18 , 623–632.\n31. Jensen, L.J. , Kuhn, M. , Stark, M. , Chaffron, S. , Creevey, C. , Muller, J. , \nDoerks, T. , Julien, P. , Roth, A. , Simonovic, M. , et al. (2009) STRING \n8—a global view on proteins and their functional interactions in \n630 organisms. Nucleic Acids Res. , 37 , D412–D416. \n32. Nastou, K. , Mehryary, F. , Ohta, T. , Luoma, J. , Pyysalo, S. and \nJensen,L.J. (2024) RegulaTome: a corpus of typed, directed, and \nsigned relations between biomedical entities in the scientific \nliterature. Database , 2024 , baae095.\n33. Megill, C. , Martin, B. , Weaver, C. , Bell, S. , Prins, L. , Badajoz, S. , \nMcCandless, B. , Pisco, A.O. , Kinsella, M. , Griffin, F. , et al. (2021) \ncellxgene: a performant, scalable exploration platform for high \ndimensional sparse matrices. bioRxiv doi: \nhttps:// doi.org/ 10.1101/ 2021.04.05.438318 , 6 April 2021, \npreprint: not peer reviewed.\n34. Moreno, P. , Fexova, S. , George, N. , Manning, J.R. , Miao, Z. , \nMohammed, S. , Muñoz-Pomer, A. , Fullgrabe, A. , Bi, Y. , Bush, N. , \net al. (2022) Expression Atlas update: gene and protein expression \nin multiple species. Nucleic Acids Res. , 50 , D129–D140. \n35. T immons, J.A. , Szkop, K.J. and Gallagher, I.J. (2015) Multiple \nsources of bias confound functional enrichment analysis of global \n-omics data. Genome Biol. , 16 , 186. \n36. Benjamini, Y. and Hochberg, Y. (1995) Controlling the false \ndiscovery rate: a practical and powerful approach to multiple \ntesting. J. R. Stat. Soc. Ser. B Stat. Methodol., 57 , 289–300.\n37. Novikov,A. (2019) PyClustering: data mining library. J. Open \nSource Softw., 4 , 1230.\n38. Van Dongen,S. (2008) Graph clustering via a discrete uncoupling \nprocess. SIAM J. Matrix Anal. Appl., 30 , 121–141.\n39. Elnaggar, A. , Heinzinger, M. , Dallago, C. , Rehawi, G. , Wang, Y. , \nJones, L. , Gibbs, T. , Feher, T. , Angerer, C. , Steinegger, M. , et al. \n(2022) ProtTrans: toward understanding the language of life \nthrough self-supervised learning. IEEE Trans. Pattern Anal. Mach. \nIntell., 44 , 7112–7127.\n40. Baumgartner, M. , Dell’Aglio, D. , Paulheim, H. and Bernstein, A. \n(2023) Towards the Web of Embeddings: integrating multiple \nknowledge graph embedding spaces with FedCoder. J. Web \nSemant., 75 , 100741.\nReceived: September 15, 2024. Revised: October 18, 2024. Editorial Decision: October 21, 2024. Accepted: October 29, 2024 \n© The Author(s) 2024. Published by Oxford University Press on behalf of Nucleic Acids Research. \nThis is an Open Access article distributed under the terms of the Creative Commons Attribution License (https: // creativecommons.org / licenses / by / 4.0 / ), which permits unrestricted reuse, \ndistribution, and reproduction in any medium, provided the original work is properly cited. \nDownloaded from https://academic.oup.com/nar/article/53/D1/D730/7903368 by guest on 11 January 2026']","['<1-hop>\n\nNucleic Acids Research , 2025, Vol. 53, Database issue D 737 networks in all domains of life, supporting directed links and tissue-specificity. J. Mol. Biol., 433 , 166835. 13. Kim, C.Y. , Baek, S. , Cha, J. , Yang, S. , Kim, E. , Marcotte, E.M. , Hart, T. and Lee,I. (2022) HumanNet v3: an improved database of human gene networks for disease research. Nucleic Acids Res., 50 , D632–D639. 14. Huang, J.K. , Carlin, D.E. , Yu, M.K. , Zhang, W. , Kreisberg, J.F. , Tamayo, P. and Ideker, T. (2018) Systematic evaluation of molecular networks for discovery of disease genes. Cell Syst. , 6 , 484–495.e5. 15. Wright, S.N. , Colton, S. , Schaffer, L.V. , Pillich, R.T. , Churas, C. , Pratt, D. and Ideker, T. (2024) State of the interactomes: an evaluation of molecular networks for generating biological insights. bioRxiv doi: https:// doi.org/ 10.1101/ 2024.04.26.587073 , 29 April 2024, preprint: not peer reviewed. 16. Ontology Consortium, G. , Aleksander, S.A. , Balhoff, J. , Carbon, S. , Cherry, J.M. , Drabkin, H.J. , Ebert, D. , Feuermann, M. , Gaudet, P. , Harris, N.L. , et al. (2023) The Gene Ontology knowledgebase in 2023. Genetics , 224 , iyad031. 17. Liberzon, A. , Birger, C. , Thorvaldsdóttir, H. , Ghandi, M. , Mesirov, J.P. and Tamayo, P. (2015) The Molecular Signatures Database hallmark gene set collection. Cell Syst. , 1 , 417–425. 18. Amberger, J.S. , Bocchini, C.A. , Schiettecatte, F. , Scott, A.F. and Hamosh,A. (2015) OMIM.org: Online Mendelian Inheritance in Man (OMIM ®), an online catalog of human genes and genetic disorders. Nucleic Acids Res. , 43 , D789–D798. 19. Snel, B. , Bork, P. and Huynen, M.A. (2002) The identification of functional modules from the genomic association of genes. Proc. Natl Acad. Sci. U.S.A., 99 , 5890–5895. 20. Guala, D. , Ogris, C. , Müller, N. and Sonnhammer, E.L.L. (2020) Genome-wide functional association networks: background, data & state-of-the-art resources. Brief. Bioinform., 21 , 1224. 21. Szklarczyk, D. , Gable, A.L. , Nastou, K.C. , Lyon, D. , Kirsch, R. , Pyysalo, S. , Doncheva, N.T. , Legeay, M. , Fang, T. , Bork, P. , et al. (2021) The STRING database in 2021: customizable protein–protein networks, and functional characterization of user-uploaded gene / measurement sets. Nucleic Acids Res., 49 , D605–D612. 22. Mering,C.V. (2003) STRING: a database of predicted functional associations between proteins. Nucleic Acids Res. , 31 , 258–261. 23. Enright, A.J. , Iliopoulos, I. , Kyrpides, N.C. and Ouzounis, C.A. (1999) Protein interaction maps for complete genomes based on gene fusion events. Nature , 402 , 86–90. 24. Franceschini, A. , Lin, J. , von Mering, C. and Jensen, L.J. (2016) SVD-phy: improved prediction of protein functional associations through singular value decomposition of phylogenetic profiles. Bioinformatics , 32 , 1085–1087. 25. Koutrouli, M. , Nastou, K. , Piera Líndez, P. , Bouwmeester, R. , Rasmussen, S. , Martens, L. and Jensen, L.J. (2024) FAVA: high-quality functional association networks inferred from scRNA-seq and proteomics data. Bioinformatics , 40 , btae010. 26. Porras, P. , Barrera, E. , Bridge, A. , Del-Toro, N. , Cesareni, G. , Duesbury, M. , Hermjakob, H. , Iannuccelli, M. , Jurisica, I. , Kotlyar, M. , et al. (2020) Towards a unified open access dataset of molecular interactions. Nat. Commun., 11 , 6144. 27. Franceschini, A. , Szklarczyk, D. , Frankild, S. , Kuhn, M. , Simonovic, M. , Roth, A. , Lin, J. , Minguez, P. , Bork, P. , von Mering, C. , et al. (2013) STRING v9.1: protein–protein interaction networks, with increased coverage and integration. Nucleic Acids Res., 41 , D808–D815. 28. Yu, H. , Luscombe, N.M. , Lu, H.X. , Zhu, X. , Xia, Y. , Han, J .-D.J ., Bertin, N. , Chung, S. , V idal, M. and Gerstein, M. (2004) Annotation transfer between genomes: protein–protein interologs and protein–DNA regulogs. Genome Res. , 14 , 1107–1118. 29. Szklarczyk, D. , Gable, A.L. , Lyon, D. , Junge, A. , Wyder, S. , Huerta-Cepas, J. , Simonovic, M. , Doncheva, N.T. , Morris, J.H. , Bork, P. , et al. (2019) STRING v11: protein–protein association networks with increased coverage, supporting functional discovery in genome-wide experimental datasets. Nucleic Acids Res., 47 , D607–D613. 30. Doncheva, N.T. , Morris, J.H. , Gorodkin, J. and Jensen, L.J. (2019) Cytoscape StringApp: network analysis and visualization of proteomics data. J. Proteome Res., 18 , 623–632. 31. Jensen, L.J. , Kuhn, M. , Stark, M. , Chaffron, S. , Creevey, C. , Muller, J. , Doerks, T. , Julien, P. , Roth, A. , Simonovic, M. , et al. (2009) STRING 8—a global view on proteins and their functional interactions in 630 organisms. Nucleic Acids Res. , 37 , D412–D416. 32. Nastou, K. , Mehryary, F. , Ohta, T. , Luoma, J. , Pyysalo, S. and Jensen,L.J. (2024) RegulaTome: a corpus of typed, directed, and signed relations between biomedical entities in the scientific literature. Database , 2024 , baae095. 33. Megill, C. , Martin, B. , Weaver, C. , Bell, S. , Prins, L. , Badajoz, S. , McCandless, B. , Pisco, A.O. , Kinsella, M. , Griffin, F. , et al. (2021) cellxgene: a performant, scalable exploration platform for high dimensional sparse matrices. bioRxiv doi: https:// doi.org/ 10.1101/ 2021.04.05.438318 , 6 April 2021, preprint: not peer reviewed. 34. Moreno, P. , Fexova, S. , George, N. , Manning, J.R. , Miao, Z. , Mohammed, S. , Muñoz-Pomer, A. , Fullgrabe, A. , Bi, Y. , Bush, N. , et al. (2022) Expression Atlas update: gene and protein expression in multiple species. Nucleic Acids Res. , 50 , D129–D140. 35. T immons, J.A. , Szkop, K.J. and Gallagher, I.J. (2015) Multiple sources of bias confound functional enrichment analysis of global -omics data. Genome Biol. , 16 , 186. 36. Benjamini, Y. and Hochberg, Y. (1995) Controlling the false discovery rate: a practical and powerful approach to multiple testing. J. R. Stat. Soc. Ser. B Stat. Methodol., 57 ,'
 '<2-hop>\n\nExpanded co-expression networks The co-expression channel in STRING identifies genes with similar expression profiles across various tissues and condi- tions, revealing their involvement in shared biological pro- cesses. It compiles data from large-scale gene expression stud- ies, analyzing transcript and protein levels to detect gene pairs with coordinated expression. This method highlights poten- tial functional relationships between genes. Importantly, the analysis is free from study bias, as it directly uses raw omics data instead of relying on manually curated or experimental datasets, providing a more precise and objective view of gene interactions. For the upcoming version 12.5, we are expanding the co-expression channel by generating additional networks through the application of FAVA (Functional Association of Variational Autoencoders) ( 25 ) on a wider range of single- cell RNA sequencing data. Specifically, we are incorporating single-cell data from the cellxgene Atlas ( 33 ) to enhance the human and mouse networks and from the EBI Single Cell Ex- pression Atlas to extend coverage to more model organisms ( 34 ). This expansion will provide more detailed, organism- specific co-expression maps, enabling more precise identifi- cation of gene co-expression patterns and cross-species com- parisons, particularly for development, immune response and disease-related gene expression dynamics. Impro v ed enric hment analysis Gene set enrichment analysis is a critical component of high- throughput biological studies, providing key insights into the biological characteristics of datasets. This analysis is pivotal in identifying essential aspects such as biological processes, path- ways and phenotypes, which are crucial for understanding the biological implications of experiments. Typically, overrepre- sentation analysis is employed, wherein a subset of data ex- ceeding a specific cutoff (empirical or customary) is compared against an experimental background. By default, STRING Downloaded from https://academic.oup.com/nar/article/53/D1/D730/7903368 by guest on 11 January 2026']","The integration of single-cell RNA sequencing (scRNA-seq) data from the cellxgene Atlas into STRING version 12.5 enhances the co-expression networks by providing additional, high-resolution gene expression data specifically from single cells. This integration is achieved through the application of FAVA (Functional Association of Variational Autoencoders) on a broader range of scRNA-seq datasets, including those from the cellxgene Atlas for human and mouse, and from the EBI Single Cell Expression Atlas for other model organisms.

**Enhancement of Co-expression Networks:**

- **Increased Data Diversity and Resolution:** Incorporating single-cell data allows STRING to capture gene expression profiles at the resolution of individual cells rather than bulk tissue samples. This finer granularity improves the detection of gene pairs with coordinated expression patterns across various cell types and states.
  
- **Organism-Specific Networks:** The use of cellxgene Atlas data enhances the human and mouse co-expression networks, while data from the EBI Single Cell Expression Atlas extends coverage to additional model organisms. This results in more detailed and organism-specific co-expression maps.

- **Improved Functional Associations:** By leveraging scRNA-seq data, the co-expression channel can more precisely identify genes involved in shared biological processes, especially those relevant to development, immune response, and disease-related gene expression dynamics.

**Advantages for Understanding Gene Expression Dynamics Across Organisms:**

- **More Precise Identification of Co-expression Patterns:** Single-cell data enables the detection of subtle and cell-type-specific gene expression relationships that bulk data might obscure, leading to a better understanding of gene regulation and interaction networks.

- **Cross-Species Comparisons:** The expansion to include multiple organisms facilitates comparative analyses of gene expression dynamics, helping to identify conserved and divergent functional relationships across species.

- **Enhanced Biological Insights:** The detailed co-expression maps support the study of complex biological processes such as development and immune responses at a cellular level, improving the ability to link gene expression patterns to phenotypic outcomes and disease mechanisms.

In summary, integrating single-cell RNA sequencing data from the cellxgene Atlas into STRING 12.5 significantly enriches the co-expression networks by adding high-resolution, organism-specific expression data. This expansion provides researchers with more precise and comprehensive tools to explore gene expression dynamics, functional associations, and regulatory mechanisms across different organisms.","The integration of single-cell RNA sequencing data from the cellxgene Atlas into STRING version 12.5 enhances the co-expression networks by generating additional networks through the application of FAVA (Functional Association of Variational Autoencoders) on a wider range of single-cell data. This incorporation specifically improves the human and mouse networks and, together with data from the EBI Single Cell Expression Atlas, extends coverage to more model organisms. This expansion provides more detailed, organism-specific co-expression maps, enabling more precise identification of gene co-expression patterns and facilitating cross-species comparisons. These improvements are particularly valuable for studying development, immune response, and disease-related gene expression dynamics, offering a more precise and objective view of gene interactions by directly using raw omics data free from study bias.",0.8387096774193549,0.9871647920435752,0.9999999999,1.0
"How do the improvements introduced in Vol. 53 of Nucleic Acids Research enhance gene set enrichment analysis, and how does the RAG system described in Vol. 32 of JAMIA apply retrieval strategies in clinical decision-making?","['Nucleic Acids Research , 2025, Vol. 53, Database issue \nD 735 \nFigure 2. Enrichment analysis interface from the STRING database website. Lef t: Sc hematic of the enrichment analysis tab highlighting various sections \nof the webpage. Top right: A zoomed-in view of the analysis table with two user-highlighted terms (colored lines). Bottom right: A visualization of the \nenrichment depicting the two terms highlighted in corresponding colors. \nFurthermore, a distinctive feature of STRING’s visualiza- \ntion is the incorporation of similarity groups, which visually \ngroup related terms on the plot, adding an additional dimen- \nsion to the enrichment results. These groups utilize computed \nsimilarities to cluster terms together, highlighting relationships \nbetween the terms and aiding the analysis. The clustering em- \nploys the average linkage method based on the Jaccard index, \nwith a user-specified cutoff applied to form groups that, on \naverage, share the specified similarity. The groups are sorted \nby the maximum signal of their terms, with each term within a \ngroup subsequently sorted by its individual signal. This group- \ning is visually highlighted on the right side of the plot. \nS TRING clust ering and g ene set descriptions \nSTRING offers network clustering options for user-submitted \ngene lists. This feature visually connects nodes that are more \ninterconnected to each other than to other nodes, reflecting \nthe functional modularity of the user’s gene list. By grouping \nproteins that closely interact, clustering aids in the discovery \nof functional modules and facilitates hypothesis generation. \nClustering is based on the connectivity between nodes, incor- \nporating edge weights (combined scores) reflecting the confi- \ndence level of the interactions. Only visible edges—those cor- \nresponding to active channels with confidence higher than the \nuser-specified cutoff—are considered, ensuring that clustering \nis consistent with the network being shown. \nUsers have the choice of two different clustering algorithms. \n‘ K -means clustering’ is implemented via the k -medoids algo- \nrithm [pyclustering package ( 37 )], allowing users to specify \nthe desired number of clusters ( k ). This method forms clusters \nbased on a distance matrix derived from the most probable \npaths between nodes. In case of multiple disconnected compo- \nnents within the network, STRING aims for a balanced distri- \nbution of cluster sizes by iteratively dividing the largest com- \nponents first. In contrast, ‘MCL clustering’, implemented with \nstandalone binaries (release 14-137), automatically generates \nnatural divisions based on the network’s inherent structure. It \nutilizes the Markov cluster algorithm ( 38 ) to simulate stochas- \ntic flow in graphs, identifying groups of highly interconnected \nnodes as clusters based on their combined STRING scores. \nThe user-controlled inflation parameter influences the granu- \nlarity of the clustering. This method is particularly effective \nin capturing the true modular nature of biological networks, \nallowing for intuitive groupings that correspond to biological \nfunctions and relationships. \nResearchers often analyze the biological context of the ob- \ntained clusters to interpret and draw conclusions from net- \nwork data. To aid this process, we have developed a novel \ngene set naming feature that significantly enhances the inter- \npretability of cluster analyses. This feature automatically as- \nsigns the best description for each cluster based on STRING’s \nrobust statistical enrichment analysis, simplifying the often \ncumbersome task of manually interpreting cluster data. It \nranks the enriched terms by their enrichment signal (see \nthe ‘Improved enrichment analysis’ section), choosing the pri- \nmary, secondary and tertiary names among various enrich- \nment categories. This prioritization ensures that the names re- \nflect the most statistically significant and biologically pertinent \nattributes of each cluster, providing a clear, immediate under- \nstanding of its functional characteristics. The enrichment pri- \nmarily draws from categories such as Gene Ontology Biolog- \nical Processes, while incorporating a range of other categories \nfor broader annotations. In case the enrichment analysis yields \nno significant results or if the cluster contains only one gene, \nSTRING assigns canonical gene names as descriptions to en- \nDownloaded from https://academic.oup.com/nar/article/53/D1/D730/7903368 by guest on 11 January 2026'
 'Clinical applications of RAG\nRAG techniques have been applied across a broad range of \nmedical specialties, as shown in Table 1. These applications \ninclude clinical decision-making and medical question- \nanswering. In clinical decision making, RAG has supported \npersonalized treatment,23,24 emergency triage,25 and disease \nmanagement.26,27 For medical question-answering, RAG’s \ncapability has been explored to address complex treatment \nguidelines questions,28 as well as queries focused on specific \nareas, such as head and neck surgery-related questions,29 and \npatient questions regarding diabetes.30 In the subgroup anal\xad\nysis, 13 studies focused on clinical decision-making (OR \n1.46, 95% CI [1.16, 1.71]) and 7 studies focused on medical \nquestion-answering (OR 1.32, 95% CI [1.08, 1.63]), with no \nstatistically significant difference observed between these 2 \ngroups.\nBaseline LLMs\nThe baseline LLMs varied across studies, with GPT-4 being \nthe most common, used in 14 studies, (OR: 1.58, 95% CI: \n1.21-2.04). GPT-3.5, used in 6 studies, showed an OR of \n1.43 (95% CI: 1.06-1.93). Llama2 was applied in 5 studies \n(OR: 1.25, 95% CI: 1.08-1.44).\nRetrieval sources\nRetrieval sources were categorized as pre-stored documents \nand real-time online browsing. Regarding pre-stored docu\xad\nments, 6 studies used clinical guidelines, such as the Emer\xad\ngency Severity Index (ESI) Ver.3 Field Triage.25 Five studies \nused academic articles from sources like PubMed abstracts or \nfull texts, or document sets such as the Radiographics Top 10 \nReading List on Gastrointestinal Imaging.31 Three studies \nused specialized knowledge bases, including ChatENT, \nOncoKB, and RareDis Corpus, while one study employed a \ngeneral biomedical knowledge graph (Scalable Precision \nMedicine Open Knowledge Engine [SPOKE]). SPOKE \nTable 1. Distribution of RAG applications by medical specialty and task \ntype.\nMedical Specialty\nFrequency\nInternal medicine\n4\nGeneral medicine\n3\nOncology\n3\nEmergency medicine\n2\nGastroenterology\n2\nOtolaryngology\n1\nHepatology\n1\nRare diseases\n1\nOrthopedics\n1\nNeurology\n1\nOphthalmology\n1\nTask\nFrequency (%)\nClinical decision-making\n13 (65%)\nMedical question-answering\n7 (35%)\nFigure 2. A forest plot showing the odds ratio (OR) of total impacts of the RAG-enhanced system compared with the baseline LLM system in clinical \ntasks. The left panel lists individual studies (e.g., Zakka et al., Murugan et al., Long et al.) along with their corresponding ORs and 95% confidence \nintervals (CIs). The right panel visualizes these ORs, with square markers proportional to study weights and horizontal lines representing CIs. The overall \npooled OR is 1.35 (95% CI: 1.19–1.53), represented by a diamond at the bottom.\n608                                                                                                      \nJournal of the American Medical Informatics Association, 2025, Vol. 32, No. 4 \nDownloaded from https://academic.oup.com/jamia/article/32/4/605/7954485 by guest on 11 January 2026'
 ""integrates over 40 publicly available biomedical knowledge \nsources across separate domains, such as genes, proteins, \ndrugs, compounds, and diseases, along with their known \nrelationships.32 Two studies used textbooks, such as Harri\xad\nson's Principles of Internal Medicine, while 3 others utilized \nelectronic health record (EHR) data. Additionally, Zakka \net al. added over 500 markdown files from MDCalc to \nimprove clinical calculation capabilities in LLM.28 Two stud\xad\nies employed real-time online browsing to search academic \nsites, such as PubMed and UpToDate. The amount of \nretrieval resources varied across studies, ranging from a small \ndataset specific to 6 osteoarthritis guidelines to a large dataset \nof EHR data from 7 hospitals.\nRetrieval strategies\nIdentified retrieval strategies were grouped based on the RAG \nstages: pre-retrieval, retrieval, and post-retrieval. Figure 3 \npresents an example of how RAG is applied and lists identi\xad\nfied strategies within each stage.\nIn the pre-retrieval stage, 50% of studies (n ¼ 10) reported \nstrategies, such as query rewriting, document preprocessing, \nand assessing the necessity of retrieval. Zakka et al. simplified \nqueries by rephrasing text into search terms that are better \nsuited for website browsing,28 while Wang et al. focused on \ntechniques such as correcting errors, expanding abbrevia\xad\ntions, and matching synonyms in user queries.30 Soman et al. \nextracted disease entities in queries and retrieved correspond\xad\ning nodes from a knowledge graph.33 Document preprocess\xad\ning involved removing non-textual elements from PMC \npapers (eg, figures, references, and author disclosures),30\nextracted tables from PDFs using pdfplumber, structured the \ncontent with pydantic for seamless integration.25 In addition \nto query modification and document preprocessing, Jeong \net al. fine-tuned a model to determine whether retrieval was \nnecessary for a given query.34\nDuring the data retrieval stage, 85% of studies (n ¼ 17) \nreported strategies regarding indexing, aligning queries with \ndocuments, and ranking retrieval chunks. Chunking methods \nranged from fixed-size chunks35 to recursive splits.36 Embed\xad\nding models such as Text-embedding-ada-002,24,28–30,36,37\nMiniLM, and PubMedBERT33 were commonly used to con\xad\nvert sentences into vectors. Cosine similarity was the primary \nmetric for measuring query-document alignment. Two stud\xad\nies adopted Maximal Marginal Relevance for search and \nhighlighted its improved performance over similarity-based \nmethods.24,35 A domain-specific retriever, MedCPT, was \nused in one study.34 Another study used the multi-vector \nretriever that leveraged summarized document sections to \nidentify the original content for final answer generation.25\nThe retrieval cutoff parameters varied widely, with probabil\xad\nity thresholds up to 0.83 and the number of retrieved chunks \nranging from 3 to 90.28,36,38 Vector databases like FAISS and \nChroma were frequently reported, and LangChain was \nwidely used for document processing and retrieval.23,25,35,38\nIn the subgroup analysis, 12 studies used simple data retrieval \nstrategies (OR 1.30, 95% CI [1.16, 1.45]), while 5 studies \nused complex data retrieval strategies (OR 1.30, 95% CI \n[1.07, 1.24]), with no statistically significant difference \nobserved between the 2 approaches.\nIn the post-retrieval stage, 65% of studies (n ¼ 13) imple\xad\nmented specific strategies to refine outputs. Murugan et al. \ntailored prompts by providing clear context, defining roles \n(eg, distinguishing between healthcare providers and patients \nto deliver appropriately detailed information), and incorpo\xad\nrating relevant citations from retrieval sources such as the \nClinical \nPharmacogenetics \nImplementation \nConsortium \nguidelines and Food and Drug Administration (FDA) label\xad\ning.24 Soman et al. utilized prompt engineering to integrate \naccurate knowledge sources and statistical evidence, such as \nP-values and z-scores, from the SPOKE knowledge graph \ninto their outputs.33 Wang et al. outlined a detailed process \nin the post-retrieval stage using prompt engineering, which \ninvolved decomposing retrieved text into individual claims, \nverifying each claim with external knowledge sources, \nFigure 3. An example of using RAG in clinical applications, with identified strategies in 3 stages: pre-retrieval, retrieval, and post-retrieval. The user query \nseeks recommended medications for symptomatic peripheral artery disease without increased bleeding risk. The system retrieves evidence from clinical \nguidelines, processes it through embeddings and a vector database, and outputs a response, including rivaroxaban with low-dose aspirin, as recommended \nby the retrieved guideline information. In this example, GPT-4 suggested a dose of 75 mg Aspirin, but the common low-dose Aspirin is 81 mg.\nJournal of the American Medical Informatics Association, 2025, Vol. 32, No. 4                                                                                                      \n609 \nDownloaded from https://academic.oup.com/jamia/article/32/4/605/7954485 by guest on 11 January 2026""
 'D 734 \nNucleic Acids Research , 2025, Vol. 53, Database issue \nuses the whole genome / proteome background; however, it \nis recommended that users provide a custom background \nlist representing a more realistic universe of genes / proteins \ndetected by their assay ( 35 ). Alternatively, the entire sorted \ndataset can be analyzed using gene set enrichment analysis to \ndetermine whether the distribution of genes at the top, bottom \nor both ends deviates significantly from what would be ex- \npected by chance. STRING supports both of these enrichment \nmethods. The most recent update to STRING introduces vari- \nous improvements, including a new false discovery rate (FDR) \ncorrection, enhanced filtering options and a new graphical vi- \nsualization. \nFDR correction \nIn its analyses, STRING performs a test for each path- \nway (‘term’) within a given pathway collection (‘category’). \nLike many other enrichment tools, STRING employs the \nBenjamini–Hochberg correction ( 36 ) to adjust for multiple \nhypothesis testing. This statistical correction is essential for \nmanaging the FDR effectively. However, the larger the term \ncount and the greater the diversity of the terms, the higher \nthe required correction and the lower the chance of exceed- \ning the alpha level (significance threshold) for a given term. \nOne method used to address this issue involves restricting the \nanalysis to a subset of terms, such as ‘GO slims’ that focus \non broad, high-level terms with a simplified ontology struc- \nture ( 16 ). However, this approach has some disadvantages: it \nmight exclude terms particularly relevant to a tested gene set, \nit may not cover all functions by design and it is mainly ap- \nplicable to hierarchical classification systems such as the Gene \nOntology. \nSTRING utilizes several hierarchical ontologies, such as \nthe Brenda Tissue Ontology, Disease Ontology, Human Phe- \nnotype Ontology and the aforementioned Gene Ontology; \nthey typically contain many more leaf-ward nodes than nodes \ncloser to the root. The parental nodes must contain all the \ngenes included in the child terms, creating an imbalance in \nthe ontology with many more smaller terms and fewer larger \nterms. Removing smaller terms can substantially increase sta- \ntistical power . However , simply eliminating all small terms \ncould negatively impact discoverability, especially for small \nquery gene sets where the smaller terms might be the most rel- \nevant for biological interpretation. The ontologies are usually \nlarge, with > 10 000 terms (and therefore tests) in the Biolog- \nical Process branch of the Gene Ontology tree. Such strong \ncorrections have the effect that for larger query gene sets, or \nfor backgrounds that cover only part of the proteome, it might \nnot be statistically possible to detect enrichments for a subset \nof the terms; these can therefore be removed from considera- \ntion. \nIn its new version, STRING takes a more flexible \napproach—by only testing terms that have a priori any statis- \ntical potential for enrichment. This decision is based on sev- \neral parameters: the size of the term, the size of the query set, \nthe background size and the number of tests conducted. By \ncalculating the smallest and largest term sizes that could the- \noretically be enriched given the FDR correction, STRING can \ndetermine which terms are viable for inclusion in the anal- \nysis. Terms outside these bounds are excluded from testing. \nThis methodical exclusion significantly enhances the statisti- \ncal power of the enrichment analysis, without omitting any \nterms that could be enriched. For smaller query sets, this strat- \negy will maintain all terms, as even the smallest tested sets \n(term size = 2) might still be theoretically enriched. How- \never , for larger , less specific sets or for tests against a cus- \ntom smaller background, this approach markedly improves \nthe statistical power, enabling STRING to perform more fo- \ncused analyses without the drawbacks of increased false posi- \ntive rates or omitting potentially biologically interesting terms. \nAlthough this method leverages the characteristics of ontolo- \ngies, it does not require a hierarchy (parent–child relationships \nbetween the terms) to function and can be applied to any \npathway / term collection with an imbalance of small terms, \nsuch as PMID-derived gene sets and STRING neighborhood \nclusters. \nTerm filtering \nOne key insight from our online user surveys is that the of- \nten overwhelming number of enrichment results can make it \nchallenging to discern the most relevant enriched terms. We \nhave thus enhanced the display of enrichment results, allow- \ning users to filter and sort them based on (i) FDR, (ii) strength, \n(iii) signal, (iv) term size and (v) term similarity. \nThe newest addition to our filtering options are the ‘term \nsimilarity’ and ‘signal’ filters. The ‘term similarity’ filter uses \nthe Jaccard index to measure the similarity between the gene \nsets of terms within the same category. The method proceeds \nby sorting the terms according to their P -values, prioritizing \nthose with the strongest statistical significance. The algorithm \nthen iterates through the list of terms; any term that exhibits \na similarity exceeding a predetermined, user-defined threshold \nrelative to terms already included in the results is subsequently \nexcluded. This approach requires no additional information \nbeyond what is already available in the dataset and can be \nuniformly applied across all categories of terms. The ‘signal’ \nfilter is defined as a weighted harmonic mean between the ra- \ntio of observed to expected gene occurrences in an enriched \nterm and its −log(FDR), respectively. This filter balances the \nimpact of both metrics and provides a more intuitive ordering \nof enriched terms and has been implemented as the default \nsorting criterion. \nThese filters are designed to exclude potentially less relevant \nresults, such as terms that are marginally significant, small or \nshow significant overlap in terms of gene content. This en- \nhancement streamlines the search for relevant observations, \nhelping users focus on the most meaningful results. \nGraphical visualization \nIn response to user feedback requesting more intuitive, graph- \nical visualizations of the detected enrichments, STRING now \nhas adopted an interactive dot plot as a primary display \nmethod for enrichment analysis outcomes. We chose the dot \nplot for its clarity in representing enriched functions across \nthree dimensions: (i) enrichment signal along the X -axis, (ii) \nthe FDR indicated through the color coding of the dots and \n(iii) the term’s protein count in the network represented by the \nsize of each dot. The terms (listed on the Y -axis) are ranked \nby their enrichment signal or, if grouped by similarity, by the \nenrichment signal of their group first (Figure 2 ). This visual- \nization reflects all filter settings chosen by the user to manage \nthe volume of results displayed. These plots can be addition- \nally customized in the interface and the resulting graphs can be \ndownloaded as PNG for immediate use or as SVG for further \nmodifications. \nDownloaded from https://academic.oup.com/nar/article/53/D1/D730/7903368 by guest on 11 January 2026'
 'was asked about medications for peripheral artery disease \npatients without increased bleeding risk, it initially omitted \nlow-dose rivaroxaban. After integrating retrieved text from \nthe 2024 American College of Cardiology / American Heart \nAssociation Guideline for the Management of Lower Extrem\xad\nity Peripheral Artery Disease,10 the model correctly recom\xad\nmended rivaroxaban.\nSeveral guidelines exist for evaluating Artificial Intelligence \n(AI) applications and LLMs in healthcare, including \nDECIDE-AI (Developmental and Exploratory Clinical Inves\xad\ntigations of DEcision support systems driven by Artificial \nIntelligence),11 CLAIM (Checklist for Artificial Intelligence in \nMedical Imaging),12\nand CONSORT-AI (Consolidated \nStandards of Reporting Trials-AI).13 In addition, Tam et al. \nintroduced QUEST, a framework specifically for human eval\xad\nuation of LLMs in healthcare.14 However, these guidelines \ndo not cover RAG applications in clinical settings, emphasiz\xad\ning the need for a more specific guideline.\nDespite the promise of RAG in improving LLM performance \nin clinical settings, there is limited understanding of its overall \neffectiveness comparing with the baseline LLM, adoption in \nclinical domains, and optimal strategies for its development in \nbiomedical applications. The aim of this study is to synthesize \nfindings from recent research of RAG and LLM in biomedicine \nand provide clinical development guidelines to improve effec\xad\ntiveness as well as transparency in future research.\nMaterials and methods\nStudy design\nWe conducted a systematic literature review. The report was \ncreated in adherence to the Preferred Reporting Items for Sys\xad\ntematic Reviews and Meta-Analyses 2020 analysis checklist \n(File S1).15 We also performed a meta-analysis following the \nCochrane Handbook guidelines.16\nScreening papers\nWe searched in 3 databases (PubMed, Embase, and Psy\xad\ncINFO), using terms related to “retrieval augmented gener\xad\nation” and “large language model.” PubMed and Embase \nwere selected based on recommendations from the Cochrane \nHandbook, as they are widely recognized for their extensive \ncoverage of biomedical literature.17 PsycINFO was included \nto capture articles from the psychological and behavioral sci\xad\nences. To maintain a high standard of quality and reliability, \nwe focused on peer-reviewed articles and excluded preprints. \nThe specific search terms used for each database are provided \nin File S2. Given that ChatGPT was released on November \n30, 2022, we set the publication filter to search papers pub\xad\nlished in 2023 and 2024. The search was performed on \nDecember 12, 2024. The inclusion criteria were: (1) the study \nmust compare baseline LLM performance with RAG per\xad\nformance and (2) the study must address a biomedical ques\xad\ntion. The exclusion criteria were: (1) literature reviews, \neditorial comments, or viewpoint papers, (2) studies focusing \non LLMs in languages other than English, or (3) studies cen\xad\ntered on a multi-agent system without a focus on RAG. SL \nscreened titles and abstracts, then conducted a full-text \nreview of papers meeting the criteria.\nData extraction\nFor each included study, we extracted the following informa\xad\ntion: author, title, publication year, journal, clinical task, and \nspecialty. Regarding RAG techniques, we gathered details \nabout the baseline LLM, retrieval sources, and strategies used \nin the pre-retrieval, retrieval, and post-retrieval stages. For \nevaluation, we extracted the evaluation method (human, \nautomated, or a combination of both), the number of evalua\xad\ntors, the evaluation dataset, and the evaluation metrics.\nMeta-analysis\nEffect size was defined as a metric quantifying the relation\xad\nship between variables, including both direction and magni\xad\ntude.18 For each included study, we calculated the effect size \nbetween baseline LLM performance and RAG-enhanced \nLLM performance. The outcomes focused on the perform\xad\nance of generation results, such as accuracy and usefulness. \nMetrics related to the retrieval process, cost, or speed were \nnot included as outcomes in the meta-analysis. For continu\xad\nous outcomes, we used Cohen’s d, standardized between- \ngroup mean difference (SMD), calculated as the difference in \nmeans divided by the pooled standard deviation. The stand\xad\nard error (SE) of SMD was calculated using the following for\xad\nmula (1), where n1 and n2 represent the sample sizes of each \ngroup.19 For dichotomous measurements, we calculated the \nlog-odds ratio, obtained by transforming the odds ratio (OR) \nwith the natural logarithm, and the associated SE was calcu\xad\nlated using formula (2), where a, b, c, and d represent the \nnumber of successful and failed events in the baseline LLM \nand RAG-enhanced LLM approaches. For studies reporting \nmultiple outcomes, we used the overall outcome to calculate \neffect size. If no overall outcome was reported, we averaged \nthe effect sizes of all reported outcomes. We excluded out\xad\ncomes with a sample size of less than 30 to avoid small- \nsample bias. \nSESMD ¼\nffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi\nn1 þ n2\nn1n2\nþ\nSMD2\n2 n1 þ n2\nð\nÞ\ns\n(1) \nSElogOR ¼\nffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi\n1\na þ 1\nb þ 1\nc þ 1\nd\nr\n(2) \nWe developed a random-effect meta-analysis model, \nbecause of the variability in RAG architectures and evalua\xad\ntion datasets among the included studies. The random-effect \nmodel was used when individual study effects likely con\xad\ntained additional sources of variance beyond sampling error. \nBetween-study heterogeneity was assessed using Higgins & \nThompson’s I2 statistic, where 25% indicated low heteroge\xad\nneity, 50% moderate, and 75% substantial.20\nWe conducted subgroup analyses to explore performance \nvariations across different factors. First, we analyzed the \ninfluence of the baseline LLM, referring to the foundation \nmodel (eg, GPT-4 or Llama2) that provides the core architec\xad\nture for the system. Second, we examined data retrieval strat\xad\negies, categorizing them as simple or complex. Simple \nstrategies included fixed-length chunking and basic similarity \nsearch, and we performed a subgroup analysis to compare \nthese with complex retrieval strategies. Third, we analyzed \ndifferences based on evaluation methods, distinguishing \nbetween human evaluations, such as Likert scale ratings for \nhelpfulness and accuracy, and automatic evaluation metrics, \nincluding ROUGE-1 and BLEU. Finally, we conducted a sub\xad\ngroup analysis based on the type of task, classifying studies \n606                                                                                                      \nJournal of the American Medical Informatics Association, 2025, Vol. 32, No. 4 \nDownloaded from https://academic.oup.com/jamia/article/32/4/605/7954485 by guest on 11 January 2026']","['<1-hop>\n\nD 734 Nucleic Acids Research , 2025, Vol. 53, Database issue uses the whole genome / proteome background; however, it is recommended that users provide a custom background list representing a more realistic universe of genes / proteins detected by their assay ( 35 ). Alternatively, the entire sorted dataset can be analyzed using gene set enrichment analysis to determine whether the distribution of genes at the top, bottom or both ends deviates significantly from what would be ex- pected by chance. STRING supports both of these enrichment methods. The most recent update to STRING introduces vari- ous improvements, including a new false discovery rate (FDR) correction, enhanced filtering options and a new graphical vi- sualization.'
 '<2-hop>\n\nFigure 3. An example of using RAG in clinical applications, with identified strategies in 3 stages: pre-retrieval, retrieval, and post-retrieval. The user query seeks recommended medications for symptomatic peripheral artery disease without increased bleeding risk. The system retrieves evidence from clinical guidelines, processes it through embeddings and a vector database, and outputs a response, including rivaroxaban with low-dose aspirin, as recommended by the retrieved guideline information. In this example, GPT-4 suggested a dose of 75 mg Aspirin, but the common low-dose Aspirin is 81 mg. Journal of the American Medical Informatics Association, 2025, Vol. 32, No. 4 609 Downloaded from https://academic.oup.com/jamia/article/32/4/605/7954485 by guest on 11 January 2026']","The improvements introduced in Vol. 53 of *Nucleic Acids Research* enhance gene set enrichment analysis primarily through several key advancements in the STRING database's enrichment analysis interface and methodology:

1. **False Discovery Rate (FDR) Correction Enhancement**:  
   STRING now employs a more flexible FDR correction approach that only tests terms with a priori statistical potential for enrichment. This is based on parameters such as term size, query set size, background size, and the number of tests. By excluding terms unlikely to be enriched, this method significantly increases the statistical power of enrichment analysis without omitting biologically relevant terms. This approach is applicable even to non-hierarchical pathway/term collections and helps avoid the overly conservative corrections that reduce sensitivity in large ontologies.

2. **Advanced Term Filtering Options**:  
   New filtering options allow users to sort and filter enrichment results by FDR, strength, signal, term size, and term similarity. Notably, the ‘term similarity’ filter uses the Jaccard index to exclude terms with high overlap in gene content, reducing redundancy. The ‘signal’ filter balances observed-to-expected gene ratios and statistical significance (−log(FDR)) to prioritize biologically meaningful terms. These filters help users focus on the most relevant and interpretable enrichment results.

3. **Graphical Visualization Improvements**:  
   STRING introduced an interactive dot plot visualization that represents enriched terms across three dimensions: enrichment signal (X-axis), FDR (color coding), and protein count (dot size). Terms are ranked by enrichment signal or by grouped similarity, providing a clear, customizable, and intuitive graphical summary of enrichment outcomes. This visualization aids users in quickly grasping the functional context of gene sets.

4. **Incorporation of Similarity Groups in Visualization**:  
   The enrichment results include similarity groups that cluster related terms based on computed similarities (using average linkage clustering with the Jaccard index). This grouping highlights relationships between terms and adds an additional dimension to the analysis, facilitating better interpretation of functional modules.

5. **Network Clustering and Automated Gene Set Naming**:  
   STRING offers clustering of user-submitted gene lists using algorithms like k-medoids and Markov Cluster Algorithm (MCL), which identify functionally modular groups of proteins. A novel gene set naming feature automatically assigns descriptive names to clusters based on the most significant enriched terms, improving interpretability and hypothesis generation.

---

Regarding the RAG (Retrieval-Augmented Generation) system described in Vol. 32 of *JAMIA*, its application of retrieval strategies in clinical decision-making involves a structured, multi-stage approach to enhance large language model (LLM) performance in biomedical tasks:

1. **Pre-Retrieval Stage**:  
   Strategies include query rewriting (e.g., simplifying or rephrasing queries for better retrieval), document preprocessing (e.g., removing non-textual elements, structuring content), and assessing whether retrieval is necessary for a given query. These steps improve the quality and relevance of the information retrieved.

2. **Retrieval Stage**:  
   This stage focuses on indexing, aligning queries with documents, and ranking retrieved chunks. Techniques include chunking documents into manageable pieces, embedding sentences into vector representations using models like Text-embedding-ada-002 or PubMedBERT, and measuring similarity with cosine similarity. Some studies use advanced methods like Maximal Marginal Relevance to improve retrieval diversity and relevance. Vector databases (e.g., FAISS, Chroma) and frameworks like LangChain facilitate efficient retrieval.

3. **Post-Retrieval Stage**:  
   After retrieval, strategies refine the output by prompt engineering, integrating relevant citations, verifying claims with external knowledge sources, and tailoring responses to the clinical context (e.g., distinguishing between provider and patient audiences). This ensures that the generated answers are accurate, contextually appropriate, and supported by evidence.

4. **Clinical Applications and Effectiveness**:  
   RAG-enhanced systems have been applied across various medical specialties to support personalized treatment, emergency triage, disease management, and medical question-answering. Meta-analyses show that RAG systems improve clinical task performance compared to baseline LLMs, with odds ratios indicating significant positive impact.

5. **Use of Diverse Retrieval Sources**:  
   Retrieval sources include clinical guidelines, academic articles, specialized knowledge bases, biomedical knowledge graphs, textbooks, and electronic health records. Some systems also employ real-time online browsing to access up-to-date information.

In summary, the RAG system applies a comprehensive retrieval pipeline—preparing queries and documents, retrieving relevant biomedical information using advanced embedding and ranking methods, and refining outputs through prompt engineering and evidence integration—to enhance clinical decision-making support by LLMs.","Vol. 53 of Nucleic Acids Research introduces improvements to STRING, including a new false discovery rate (FDR) correction, enhanced filtering options, and a new graphical visualization, which collectively enhance gene set enrichment analysis by allowing users to analyze either a whole genome/proteome background or a custom background list more realistically representing detected genes or proteins. Meanwhile, Vol. 32 of the Journal of the American Medical Informatics Association describes the use of a Retrieval-Augmented Generation (RAG) system in clinical applications, employing three stages—pre-retrieval, retrieval, and post-retrieval—to answer queries such as recommending medications for symptomatic peripheral artery disease without increased bleeding risk. This system retrieves evidence from clinical guidelines, processes it through embeddings and a vector database, and outputs responses like rivaroxaban with low-dose aspirin, demonstrating how retrieval strategies support accurate clinical decision-making.",1.0,0.9921461484826906,0.0,1.0
