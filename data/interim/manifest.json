{
  "id": "ragas_pipeline_f1eb144c-3c06-4354-bf0c-e0f752faedfe",
  "generated_at": "2026-03-01T06:59:32.140375Z",
  "run": {
    "random_seed": 42
  },
  "env": {
    "python": "3.13.2",
    "os": "Linux 6.6.87.2-microsoft-standard-WSL2",
    "langchain": "0.3.27",
    "ragas": "0.2.10",
    "datasets": "4.6.1",
    "pyarrow": "23.0.1",
    "huggingface_hub": "1.5.0"
  },
  "params": {
    "OPENAI_MODEL_NAME": "gpt-4.1-mini",
    "EMBEDDING_MODEL_NAME": "text-embedding-3-small",
    "TESTSET_SIZE": 10,
    "MAX_DOCS": null
  },
  "paths": {
    "sources": {
      "jsonl": "/home/donbr/open-biosciences/biosciences-research/data/interim/sources.docs.jsonl",
      "parquet": "/home/donbr/open-biosciences/biosciences-research/data/interim/sources.docs.parquet",
      "hfds": "/home/donbr/open-biosciences/biosciences-research/data/interim/sources.hfds"
    },
    "golden_testset": {
      "jsonl": "/home/donbr/open-biosciences/biosciences-research/data/interim/golden_testset.jsonl",
      "parquet": "/home/donbr/open-biosciences/biosciences-research/data/interim/golden_testset.parquet",
      "hfds": "/home/donbr/open-biosciences/biosciences-research/data/interim/golden_testset.hfds"
    }
  },
  "fingerprints": {
    "sources": {
      "jsonl_sha256": "78d5a84e23a949f88f259c8ef8a82179fa85b85218f1af2de467a4b3a466e51a",
      "parquet_sha256": "02d2eac9de94a99251698c4337cd92bf3d5fbc54720108f331fb73662d0d43f8"
    },
    "golden_testset": {
      "jsonl_sha256": "cbd520971dc809cef39b51189b4908c1d719ee8a90eb0d71f708751368e69277",
      "parquet_sha256": "d43ed25d709a60dec66c4c21a9a75baad98ba6f1602c281f540c3c8549c29ef9"
    }
  },
  "quick_schema": {
    "sources_jsonl": {
      "columns": [
        "metadata",
        "metadata.author",
        "metadata.creationDate",
        "metadata.creationdate",
        "metadata.creator",
        "metadata.file_path",
        "metadata.format",
        "metadata.keywords",
        "metadata.modDate",
        "metadata.moddate",
        "metadata.page",
        "metadata.producer",
        "metadata.source",
        "metadata.subject",
        "metadata.title",
        "metadata.total_pages",
        "metadata.trapped",
        "page_content"
      ],
      "sample": [
        {
          "page_content": "Vol. 06, No. 02, pp. 342 –357 (2025) \nISSN: 2708-0757 \n \nJOURNAL OF APPLIED SCIENCE AND TECHNOLOGY TRENDS \n \nwww.jastt.org  \n \n                                                          342 \ndoi: 10.38094/jastt62404  \nA Hybrid LLM–Knowledge Graph Framework for Accurate \nBiomedical Question Answering \n \nHavraz Y. Omar1,²*, Abdulhakeem O. Mohammed³  \n \n¹Department of Information Technology, Technical College of Duhok, Duhok Polytechnic University, Duhok, Kurdistan Region, \nIraq. havraz.omar@dpu.edu.krd  \n² Department of Information Technology, Technical College of Informatics – Akre, Akre University for Applied Sciences, Akre, \nKurdistan Region, Iraq. \n³ Department of Computer Science, College of Science, University of Zakho, Zakho, Kurdistan Region, Iraq. \na.mohammed@uoz.edu.krd \n \n*Correspondence: havraz.omar@dpu.edu.krd \n \nAbstract \nBiomedical question answering requires accurate and interpretable systems; however, existing approaches often face challenges such as \nlanguage model hallucinations and limited reasoning when relying solely on standalone knowledge graphs. To address these limitations, \nthis study proposes a hybrid framework that integrates the LLaMA-3B language model with a Neo4j-based drug–disease–symptom \nknowledge graph. The system translates natural language questions into executable Cypher queries, operates on an iBKH-derived graph \ncomprising over 65,000 entities and 3 million relationships, and returns answers with supporting evidence through a transparent interface. \nExperiments conducted on 60 biomedical questions across three levels of difficulty demonstrate the robustness of the approach: 96% \nexact match for simple queries, 95% for medium queries, and 86.7% for complex queries. Overall, the system achieves Precision@5 of \n96.1%, Recall@5 of 89.0%, F1@5 of 91.0%, Hits@k of 96.1%, and an MRR of 94.4%, while maintaining an average response time of \nonly 6.07 seconds. These results indicate that the system retrieves nearly all relevant answers, ranks them correctly, and delivers them \nwith latency low enough for interactive use. Moreover, unlike cloud-based APIs such as ChatGPT, which require internet connectivity \nand external data transmission, the proposed framework operates fully offline, ensuring privacy, reproducibility, and compliance with \nbiomedical data governance. Overall, this pipeline provides an accurate, efficient, and privacy-preserving solution for biomedical question \nanswering, making it a practical alternative to cloud-dependent approaches in sensitive healthcare contexts. \n \nKeywords: Knowledge Graph, LLM, Question Answering, Neo4j, Biomedical Informatics, Healthcare AI, LLaMA 3. \n \nReceived: August 14th, 2025 / Revised: October 10th, 2025 / Accepted: October 16th, 2025 / Online: October 20th, 2025 \n \nI. INTRODUCTION \nAnswering questions in the biomedical field is a difficult task \ndue to the complexity of medical knowledge and the need for \nprecision. In recent years, large language models  (LLMs) like \nLLaMA, GPT-4 have made progress in understanding and \ngenerating human-like responses to medical questions  [1,  2]. \nThese models can process large amounts of information and \nrespond in natural language, which makes them helpful in \nhealthcare settings [3]. However, they often struggle to provide \naccurate answers when dealing with specialized biomedical \ncontent [4, 5]. \nOne major issue with LLMs is a problem called hallucination, \nwhere the model generates information that sounds right but is \nactually incorrect or unsupported [6]. In medical applications, \nthis can be dangerous, as healthcare professionals rely on precise \nand trustworthy information  [7]. Therefore, researchers are \nexploring ways to combine LLMs with structured sources of \nknowledge to improve their reliability [8]. \nLLM-only systems in biomedicine still hallucinate and are \nhard to verify, limiting safe use [9, 10]. Biomedical knowledge \ngraphs (BKGs) such as iBKH and SPOKE curate multi-source \nfacts and enable multi-hop reasoning, yet they neither interpret \nfree text nor generate answers [11, 12]. Recent hybrids (KG-\naware RAG) improve grounding but often lack explicit path-level \njustifications and robust end-to-end answer evaluation [13, 14]. \nRecent studies have increasingly integrated Knowledge \nGraphs (KGs) with Large Language Models (LLMs) to improve \nfactual accuracy, reasoning, and reduce hallucinations. Notable \nexamples include DR.KNOWS, which combines UMLS-based \nKGs with LLMs for better diagnostic reasoning [15], KnowNet",
          "metadata": {
            "producer": "Microsoft® Word for Microsoft 365",
            "creator": "Microsoft® Word for Microsoft 365",
            "creationdate": "2025-10-20T16:37:36+03:00",
            "source": "/home/donbr/open-biosciences/biosciences-research/data/raw/JASTTytm_A+Hybrid+LLM–Knowledge+Graph_.pdf",
            "file_path": "/home/donbr/open-biosciences/biosciences-research/data/raw/JASTTytm_A+Hybrid+LLM–Knowledge+Graph_.pdf",
            "total_pages": 16,
            "format": "PDF 1.7",
            "title": "A Hybrid LLM–Knowledge Graph Framework for Accurate Biomedical Question Answering",
            "author": "",
            "subject": "",
            "keywords": "",
            "moddate": "2025-10-20T16:37:36+03:00",
            "trapped": "",
            "modDate": "D:20251020163736+03'00'",
            "creationDate": "D:20251020163736+03'00'",
            "page": 0
          }
        },
        {
          "page_content": "Omar & Mohammed / Journal of Applied Science and Technology Trends Vol. 06, No. 02, pp. 342 –357 (2025) \n \n343 \nfor visualizing and validating LLM outputs [16], and MedKA for \nKG-enhanced question answering [17]. \nTo address these challenges, several recent works have \nexplored the integration of large language models with \nbiomedical knowledge graphs (KGs). A biomedical KG is a \nstructured network that connects entities such as diseases, drugs, \nand symptoms using defined relationships [18, 19]. These graphs \nstore verified medical knowledge from trusted databases, \nallowing for more accurate and explainable responses [12]. KGs \nare especially useful in multi-step reasoning tasks, where finding \nan answer requires connecting different pieces of information \n[20]. These entities and relationships can be visually represented \nin a biomedical knowledge graph, as shown in Fig. 1, where \nnodes represent medical concepts such as drugs, diseases, \nsymptoms, and pathways, and edges denote their semantic \nrelationships. \n \nFig. 1. Overview of Biomedical Knowledge Graph Entities and Relationships \n \nOne example of a widely used biomedical KG is SPOKE, \nwhich includes millions of nodes and relationships from over 40 \nbiomedical databases [12]. Integrating an LLM with a KG allows \nthe strengths of both technologies to work together: the LLM \nprovides language understanding, and the KG provides \nstructured, factual knowledge [21, 22]. A common method is \nretrieval-augmented generation (RAG), where the LLM retrieves \ninformation from the KG and uses it to generate more accurate \nresponses [13, 23]. In more advanced setups, the LLM can even \ngenerate queries like Cypher to fetch specific data from the graph \n[24, 25]. Neo4j is a popular graph database that supports fast and \nflexible storage and querying of knowledge graphs using Cypher \n[21]. It is well-suited for biomedical applications because it \nallows easy exploration of complex medical relationships. Recent \nwork has shown that combining Neo4j with LLMs can lead to \nbetter accuracy, fewer hallucinations, and more explainable \nresults [24, 26]. \nDespite improvements, building a reliable hybrid system that \ncombines an LLM with a biomedical KG remains a technical \nchallenge. Some approaches require complex pipelines or large \ntraining datasets, while others rely on fine-tuning specific to a \nnarrow set of questions [27, 28]. There is still a need for systems \nthat are both accurate and easy to scale, especially in domains like \nmedicine where the cost of errors is high [22]. \nRecent advances in KG-augmented LLMs have improved \nperformance, yet biomedical QA continues to face three practical \ngaps: \n1. Traceability: LLM-only or text-retrieval-only pipelines \nrarely provide graph-grounded justifications; users lack the \nability to inspect the exact nodes and edges that support an \nanswer. \n2. Evaluation: Prior work often judges quality via surface-\nform checks (e.g., matching a Cypher template), which fails to \ncapture end-to-end answer correctness or ranking quality \nacross different difficulty levels. \n3. Deployment: Many solutions assume cloud resources or \ndomain-specific fine-tuning, yet biomedical contexts typically \ndemand a local, privacy-preserving system with low latency \nand reproducible behavior. \nTimestamp-aware execution and periodic KG refresh help \navoid deprecated or contraindicated links, making the tool better \nsuited for safety-critical clinical contexts (e.g., drug–drug \ninteractions). \nTo address these limitations, Our work introduces a locally \ndeployable pipeline that translates biomedical questions into \nexecutable Cypher queries over a Neo4j knowledge graph. The \nsystem returns answers with supporting nodes and edges, and is",
          "metadata": {
            "producer": "Microsoft® Word for Microsoft 365",
            "creator": "Microsoft® Word for Microsoft 365",
            "creationdate": "2025-10-20T16:37:36+03:00",
            "source": "/home/donbr/open-biosciences/biosciences-research/data/raw/JASTTytm_A+Hybrid+LLM–Knowledge+Graph_.pdf",
            "file_path": "/home/donbr/open-biosciences/biosciences-research/data/raw/JASTTytm_A+Hybrid+LLM–Knowledge+Graph_.pdf",
            "total_pages": 16,
            "format": "PDF 1.7",
            "title": "A Hybrid LLM–Knowledge Graph Framework for Accurate Biomedical Question Answering",
            "author": "",
            "subject": "",
            "keywords": "",
            "moddate": "2025-10-20T16:37:36+03:00",
            "trapped": "",
            "modDate": "D:20251020163736+03'00'",
            "creationDate": "D:20251020163736+03'00'",
            "page": 1
          }
        },
        {
          "page_content": "Omar & Mohammed / Journal of Applied Science and Technology Trends Vol. 06, No. 02, pp. 342 –357 (2025) \n \n344 \nevaluated using Exact Match, Precision, Recall, F1, Hits@k, \nMRR, and latency across simple, medium, and complex question \nsets. Unlike prior template-based methods, our approach enables \ntraceable, outcome-level validation. In summary, the main \ncontributions of this work are as follows: \n• Hybrid LLM to Cypher QA: A system that translates \nnatural language questions into accurate, executable \nCypher over a Neo4j drug, disease, and symptom KG. \n• Prompt-driven query generation: Schema,  entity \naware prompting that reliably maps diverse biomedical \nquestions to graph queries. \n• Evidence transparency: Along with each answer, we \nsurface the generated Cypher and the supporting \njustification subgraph (nodes, edges) plus a brief natural \nlanguage rationale. \n• Answer-level evaluation: End-to-end assessment using \nExact Match, F1, Precision/Recall, Hits@k, MRR and \nlatency across simple, medium and complex tiers. \n• Local, reproducible deployment: On-prem LLaMA 3 \ninference (no cloud dependency) suitable for biomedical \nsettings requiring low latency and strong data control. \n \nThe remainder of this paper is organized as follows: Section \n2 reviews related work on biomedical knowledge graphs and \nLLM-based QA systems. Section 3 provides background on \nknowledge graphs, large language models, and question \nanswering frameworks. Section 4 details the proposed \nmethodology, \nincluding \nsystem \narchitecture, \ndataset \nconstruction, and query translation. Section 5 presents the \nexperimental results through both quantitative metrics and \nqualitative case studies. Section 6 discusses the findings, analyzes \nlimitations, and compares performance against baseline models. \nFinally, Section 7 concludes this paper and outlines directions for \nfuture work. \nII. RELATED WORK \nRecently, studies have concentrated on the integration of \nclinical and medical knowledge graphs (LLM) to improve the \nanswer to medical questions. Researchers have derived several \nbiomedical KGs using Neo4j and incorporated the application of \nLLMs like LLaMA and GPT to convert natural language \nquestions into graph queries. Improvements in answer \ncorrectness, reduction of hallucination errors, one-to-many \nrelationships, and support for complex reasoning were the \nobjectives of these efforts. Some frameworks also adopted \nretrieval methods to ground responses in secure data. \nSu et al.[11] developed an integrative Biomedical Knowledge \nHub (iBKH), a huge biomedical knowledge graph that comprises \n18 of the very best data sources. The deployment of the iBKH in \nNeo4j allows for a user-friendly web portal to allow fast and \ninteractive knowledge retrieval. The system implemented \nadvanced graph learning techniques to enable the discovery of \nbiomedical knowledge, illustrated by an example of repurposing \nin silico drugs for Alzheimer’s disease. iBKH achieved \npromising predictive performance for known drugs and proposed \npossible new drug candidates. \nRajabi and Kafaie[19] proposed a disease knowledge graph \nusing a cross-referential disease database comprising diseases, \nsymptoms, and drugs interconnected with relationships. They \ntransferred the data into Neo4j to create a graph of 9,400 nodes \nand 45,000 relationships representing the semantic links between \nmedical concepts. Applying Cypher queries enabled answering \ncomplex medical questions regarding identifying drugs that may \ncause certain diseases; it was demonstrated that the graph inferred \nnew relationships not explicitly existing in the original data. The \nconclusion was that disease knowledge graphs sped up clinical \ndiscovery and contributed to understanding complex medical \nrelationships. \nHou et al.[3] assessed and contrasted ChatGPT (both GPT-\n3.5 and GPT-4) and the biomedical knowledge graphs (BKGs) \nconcerning their ability to answer biomedical questions, generate \nnew knowledge, and reason. Their datasets were focused on \ndietary supplements and drugs, while evaluation criteria entailed \naccuracy, novelty, and reasoning ability. The results indicate that \nwhile GPT-4 surpassed GPT-3.5 and BKGs in knowledge \nprovision, it proved inconsistent with regard to citations and \nreasoning. Compared to them, BKGs scored higher in accuracy \nand reliability, especially in discovering novel links as well as \nwithin structured reasoning. \nSoman et al.[13] presented a novel framework called KG-\nRAG that integrates a large biomedical knowledge graph \n(SPOKE) with LLaMA 2, GPT-3.5, and GPT-4 (LLMs) to \nproduce accurate biomedical text. They optimized the retrieval of \nrelevant graph context to cut over 50% tokens without losing \naccuracy. It aided LLMs in performing better on biomedical \nquestion answering with very high accuracy boosts, especially in \nthe case of LLaMA 2. They compared KG-RAG to other retrieval \nmethods and indicated its comparatively more robust and \nefficient results. The framework produced reliable evidence-\nbased answers grounded in biomedical knowledge. \nLuo et al.[23] created ChatKBQA, a new framework with a \nquestion-and-answer approach over knowledge bases that first \ngenerates logical forms with the help of fine-tuned LLMs and \nthen retrieves the relevant entities and relations. This generate-\nthen-retrieve approach is supposed to handle a couple of issues \nwith the earlier methods concerning tedious retrieval and error \npropagation. They fine-tuned open-source LLMs like LLaMA 2 \nto change natural-language questions into logical forms with high \naccuracy. The retrieval phase uses unsupervised phrase-level \nsemantic matching in a way that enhances the alignment of \nentities and relations. Experiments on benchmark datasets \nindicate ChatKBQA to be superior to its predecessors, with the \nhighest accuracy to date. \nPusch and Conrad[6] conducted work under a hybrid \napproach conflating LLMs and biomedical Knowledge Graphs \n(KGs) to suppress hallucination errors in question-answering. \nThey proposed query-checking algorithms for validating, \ncorrecting, and executing the KG Cypher queries that LLMs \ngenerated, thereby attaining accurate and understandable \nanswers. The system used retrieval-augmented generation (RAG) \nto ground answers within KG data. The methodology was \nvalidated on a biomedical KG called PrimeKG using 50 \nbenchmark questions, assessing models like GPT-4 Turbo and \nLLaMA 3. Commercially available GPT-4 Turbo obtained",
          "metadata": {
            "producer": "Microsoft® Word for Microsoft 365",
            "creator": "Microsoft® Word for Microsoft 365",
            "creationdate": "2025-10-20T16:37:36+03:00",
            "source": "/home/donbr/open-biosciences/biosciences-research/data/raw/JASTTytm_A+Hybrid+LLM–Knowledge+Graph_.pdf",
            "file_path": "/home/donbr/open-biosciences/biosciences-research/data/raw/JASTTytm_A+Hybrid+LLM–Knowledge+Graph_.pdf",
            "total_pages": 16,
            "format": "PDF 1.7",
            "title": "A Hybrid LLM–Knowledge Graph Framework for Accurate Biomedical Question Answering",
            "author": "",
            "subject": "",
            "keywords": "",
            "moddate": "2025-10-20T16:37:36+03:00",
            "trapped": "",
            "modDate": "D:20251020163736+03'00'",
            "creationDate": "D:20251020163736+03'00'",
            "page": 2
          }
        },
        {
          "page_content": "Omar & Mohammed / Journal of Applied Science and Technology Trends Vol. 06, No. 02, pp. 342 –357 (2025) \n \n345 \nrecord-high accuracy, while open-source models achieved \nimpressive gains through prompt optimization. \nFeng et al.[22] developed the Knowledge Graph-based \nThought (KGT) framework that integrated LLMs with a pan-\ncancer knowledge graph for biomedical question answering. \nKGT was designed to reason on the knowledge graph schema and \nidentify optimal subgraphs to use for directing accurate answer \ngeneration, all without fine-tuning the LLMs. The framework is \nbenchmarked against a new dataset (PcQA) designed specifically \nfor pan-cancer KGQA tasks and has outperformed all existing \nstate-of-the-art approaches by a rather large margin. KGT’s \npracticality in biomedical issues was highlighted through case \nstudies for drug repositioning, drug resistance, and biomarker \ndiscovery. Their approach exhibited robust adaptability among \nvarious LLMs. \nRezaei et al.[26] developed AMG-RAG, a dynamic \nframework that utilizes autonomous LLM agents with medical \nsearch tools in the continuous construction and real-time updating \nof Medical Knowledge Graphs (MKGs). Their system \nincorporated confidence scoring and multi-hop reasoning to \nimprove accuracy and interpretability in medical question \nanswering. AMG-RAG outperformed size models on both very \nhard MEDQA benchmarks and more accessible MedMCQA \nones, proving that it could conduct efficient reasoning based on \ncurrent structured medical knowledge. They also used Neo4j to \nmanage the knowledge graphs while adding external searches to \nensure the latest data. \nTiwari et al.[24] presented Auto-Cypher, a recent automated \npipeline for producing high-quality synthetic data for training \nLLMs by mapping natural language to Cypher queries for graph \ndatabases like Neo4j. The pipeline deployed the novelty of LLM-\nas-database-filler to synthesize Neo4j databases for the execution \nof generated queries to ensure their correctness. A sizable dataset \ncalled SynthCypher was created, spanning multiple domains and \ncomplex queries, leading to a 40% improvement in LLM \nperformance on Cypher generation. The datasets were used to \nfine-tune open-source models such as LLaMA, Mistral, and \nQwen, and the SPIDER benchmark was adapted for evaluation \npurposes. \nMohammed et al.[29] proposed a hybridized GraphRAG \nframework combining Neo4j-based UMLS knowledge graphs \nwith a vector store for medical textbooks to create an improved \nU.S.M.L.E.-style clinical question-answering approach. The \nproject integrated symbolic reasoning from knowledge graphs \nwith semantic retrieval performed on text embeddings to enhance \nrelevance and accuracy via adaptive re-ranking and query \nexpansion. The system had the answers produced by GPT-4o-\nMini, with different prompting strategies encouraging evidence-\nbased and traceable responses grounded in verified medical \nknowledge. Experiments showed that the hybrid approach \nimproved factual accuracy and citation fidelity as compared to \nthe L.L.M.-only approach, enhancing transparency and \nreliability. It is shown that binding both structured and \nunstructured medical knowledge sources could aid in \nameliorating hallucinations and hence improve clinical \ntrustworthiness in AI-driven medical QA. \nYang et al.[30] articulated sepsis knowledge graph was \ncrafted by combining multicenter clinical data from over 10,000 \npatients with the help of GPT-4 for entity recognition and \nrelationship extraction. Real-world data were collected from \nthree hospitals and integrated with clinical guidelines and \ndatabases from the public domain. The knowledge graph \ncontained 1,894 nodes and 2,021 relationships pertaining to \ndiseases, symptoms, biomarkers, treatments, and complications. \nGPT outperformed other models in every resolution on sepsis-\nspecific datasets to obtain high F1-score results. The constructed \ngraph highlighted complex interactions in sepsis for assisting \nclinical decision-making and was implemented on Neo4j. \nGuan et al.[20] proposed a novel method for constructing a \nlocal knowledge graph from retrieved biomedical documents by \nextracting propositional claims. They carried out layer wise \nsummarization on this graph to capture multi-document \nrelationships and provide comprehensive contextual information \nto a language model for question-answering purposes. The \nmethod resolved issues in multi-document biomedical QA, such \nas noise cancellation and efficient context usage. They then tested \ntheir method on several benchmarks for biomedical question \nanswering, achieving performance at least comparable to, if not \nbetter than, existing retrieval-augmented generation (RAG) \nbaselines. The study established enhanced reasoning and answer \naccuracy of the model achieved through structured graph \nsummarization.   \nPrevious studies have improved biomedical QA using KGs \nand LLMs, but important gaps remain. Most systems lack \ntransparent, graph-based justifications, rely on limited evaluation \nmethods, or depend on cloud resources that reduce privacy and \nreproducibility. Our framework addresses these gaps by \nproviding visible Cypher queries with evidence subgraphs, \napplying comprehensive performance metrics across difficulty \nlevels, and ensuring fully local, privacy-preserving deployment. \nTable I summarizes key previous studies on biomedical \nknowledge graphs and question answering, outlining their \nmethods, datasets, and main limitations. \nIII. PRELIMINARIES \nThis section outlines the fundamental concepts required to \nunderstand the proposed system. It introduces biomedical \nknowledge graphs, explains how Neo4j stores data in graph \nform, and describes the use of Cypher for querying. It also \nprovides a brief overview of large language models (LLMs) and \ntheir role in interpreting natural language. \n \n \nA. Biomedical Knowledge Graphs \nBiomedical Knowledge Graphs (BKGs) provide a structured \nrepresentation of complex biomedical information by modeling \ndiverse medical entities, such as diseases, drugs, symptoms, and \nbiological pathways, as interconnected nodes within a graph \nstructure. The edges in these graphs represent the semantic \nrelationships between these entities, including ’treats’, ’causes’, \n’interacts with’ and many others, as illustrated in Fig 1. This \nform of representation enables the integration of heterogeneous \nbiomedical data from a wide range of sources, including",
          "metadata": {
            "producer": "Microsoft® Word for Microsoft 365",
            "creator": "Microsoft® Word for Microsoft 365",
            "creationdate": "2025-10-20T16:37:36+03:00",
            "source": "/home/donbr/open-biosciences/biosciences-research/data/raw/JASTTytm_A+Hybrid+LLM–Knowledge+Graph_.pdf",
            "file_path": "/home/donbr/open-biosciences/biosciences-research/data/raw/JASTTytm_A+Hybrid+LLM–Knowledge+Graph_.pdf",
            "total_pages": 16,
            "format": "PDF 1.7",
            "title": "A Hybrid LLM–Knowledge Graph Framework for Accurate Biomedical Question Answering",
            "author": "",
            "subject": "",
            "keywords": "",
            "moddate": "2025-10-20T16:37:36+03:00",
            "trapped": "",
            "modDate": "D:20251020163736+03'00'",
            "creationDate": "D:20251020163736+03'00'",
            "page": 3
          }
        },
        {
          "page_content": "Omar & Mohammed / Journal of Applied Science and Technology Trends Vol. 06, No. 02, pp. 342 –357 (2025) \n \n346 \nscientific literature, clinical records, genomic databases, and \nexperimental findings [19, 31]. \nSuch integration creates a comprehensive biomedical \nknowledge base that supports advanced analytics and discovery. \nFor example, biomedical knowledge graphs can reveal previously \nunknown relationships (e.g., between drugs and diseases) and \nhelp prioritize potential biomarkers for complex conditions. The \nIntegrative Biomedical Knowledge Hub (iBKH) is one such \nlarge-scale graph that consolidates diverse biomedical resources \ninto a unified hub, enabling discovery at scale [11]. Beyond \niBKH, large biomedical knowledge graphs such as SPOKE \nfurther illustrate how graph integration accelerates research and \nsupports precision-medicine use cases [12]. Overall, these graphs \nserve as foundational resources for data-driven and personalized \nmedicine. These knowledge graphs serve as foundational \nresources for precision medicine, where treatment can be tailored \nto the individual’s biological profile, improving outcomes and \nminimizing side effects [19, 31]. \n \nTABLE I.  SUMMARY OF RELATED RESEARCH ON BIOMEDICAL KGS AND QUESTION ANSWERING \nRef. \nYear \nData/Graph \nMethod \nBaselines \nKey Metric \nLimitation \n[11] \n2023 \niBKH (18 biomedical \nsources, Neo4j) \nIntegrative KG + Graph \nlearning; drug repurposing \ncase \nKnown drugs, \nAlzheimer’s \nstudy \nPredictive performance \n(drug repurposing) \nLimited to Alzheimer’s case \nstudy; scalability and updates not \ndetailed \n[19] \n2023 \nDisease KG (9,400 \nnodes, 45,000 relations \nin Neo4j) \nCypher queries for disease–\ndrug–symptom reasoning \nCross-referential \ndisease DB \nNew relation inference; \ncomplex query \nanswering \nLimited to single domain; lacks \nlarge-scale evaluation \n[3] \n2023 \nBKGs vs. GPT-3.5/4 \nComparative QA study: \nLLMs vs. KGs \nGPT-3.5, GPT-4, \nKG reasoning \nAccuracy, Novelty, \nReasoning \nGPT-4 inconsistent in \nreasoning/citations; KG less fluent \nbut more reliable \n[13] \n2024 \nSPOKE KG + \nLLaMA2, GPT-3.5, \nGPT-4 \nKG-optimized retrieval for \nLLMs (RAG) \nOther retrieval \nmethods \nAccuracy, token \nreduction >50% \nFocus on retrieval optimization, \nnot KG construction \n[23] \n2024 \nBenchmark KB datasets \nGenerate-then-retrieve \n(LLM → logical form → \nKB retrieval) \nPrior KBQA \nmethods \nAccuracy (highest to \ndate) \nRisk of error in logical form \ngeneration \n[6] \n2024 \nPrimeKG \nLLM + KG hybrid, Cypher \nquery validation, RAG \nGPT-4 Turbo, \nLLaMA 3 \nAccuracy, \nExplainability \nDependent on KG coverage; \ncomputationally intensive \n[22] \n2025 \nPan-cancer KG (PcQA \ndataset) \nKG-enhanced reasoning \n(subgraph selection) \nSOTA KGQA \nmethods \nOutperformed SOTA on \nPcQA \nLimited to pan-cancer focus; no \nfine-tuning explored \n[26] \n2025 \nDynamic Medical KG + \nNeo4j \nLLM agents + multi-hop \nreasoning \nMEDQA, \nMedMCQA \nbaselines \nAccuracy, \nInterpretability \nHigh system complexity; requires \ncontinuous updating \n[24] \n2025 \nSynthCypher dataset \n(Neo4j + synthetic \nqueries) \nLLM-supervised Cypher \ngeneration and verification \nSPIDER \nbenchmark \nCypher accuracy 40% \nSynthetic dataset may not capture \nall real-world cases \n[29] \n2025 \nUMLS KG + Neo4j \nHybrid GraphRAG \nLLM-only QA \nAccuracy, Citation \nfidelity \nMore complex pipeline; relies on \nexternal vector store \n[30] \n2025 \nClinical data (10k \npatients, 1,894 nodes, \nNeo4j) \nKG construction using \nGPT-4 for entity/relation \nextraction \nOther KG \nconstruction \nmethods \nHigh F1-scores \nFocus only on sepsis; limited \ngeneralization \n[20] \n2025 \nLocal KG from \nbiomedical documents \nMulti-level summarization \nover KG for QA \nRAG baselines \nQA accuracy, reasoning \nTested mainly on document QA; \nscalability not proven \n \nB. Neo4j Graph Database \nTo manage the complexity and large size of biomedical \nknowledge graphs, specialized graph databases are needed. \nNeo4j is one of the most popular graph databases designed to \nstore and query data structured as nodes (entities) and \nrelationships (edges), both of which can have descriptive \nproperties [32, 33]. It uses the property graph model, which \nmakes it easy to represent complex, connected biomedical data \nsuch as drug-gene interactions or disease pathways. Neo4j’s \nCypher query language is especially advantageous because it \nallows users to write expressive and efficient queries to explore \nmulti-step connections in the data [34]. \nNeo4j works well for biomedical data because it can quickly \nrun complicated queries over highly interconnected datasets. This \nis important in biology and medicine, where relationships \nbetween entities like proteins, diseases, and drugs are often \ncomplex and layered. Studies have shown that Neo4j handles \nlarge biomedical graphs efficiently, making it a favorite among \nresearchers and industry users alike [33, 35, 36]. Its indexing and \ncaching mechanisms also help speed up query processing and \ndata retrieval [37]. \nMoreover, \nNeo4j \nintegrates \nsmoothly \nwith \nmany \nprogramming languages and analytics tools, which makes it \neasier to build interactive biomedical applications and clinical \ndecision support systems that can turn complex graph data into \nuseful insights [38, 39]. \nC. Large Language Models (LLMs) in Biomedical Question \nAnswering \nLarge Language Models (LLMs) are powerful AI systems \ntrained on vast amounts of text data. They learn the structure and \npatterns of language, enabling them to understand questions, \ngenerate responses, summarize information, and perform other \ncomplex language tasks. Well-known models such as LLaMA \nand GPT-3 have greatly advanced the field of natural language \nprocessing by showing strong performance across many tasks \n[40, 41].",
          "metadata": {
            "producer": "Microsoft® Word for Microsoft 365",
            "creator": "Microsoft® Word for Microsoft 365",
            "creationdate": "2025-10-20T16:37:36+03:00",
            "source": "/home/donbr/open-biosciences/biosciences-research/data/raw/JASTTytm_A+Hybrid+LLM–Knowledge+Graph_.pdf",
            "file_path": "/home/donbr/open-biosciences/biosciences-research/data/raw/JASTTytm_A+Hybrid+LLM–Knowledge+Graph_.pdf",
            "total_pages": 16,
            "format": "PDF 1.7",
            "title": "A Hybrid LLM–Knowledge Graph Framework for Accurate Biomedical Question Answering",
            "author": "",
            "subject": "",
            "keywords": "",
            "moddate": "2025-10-20T16:37:36+03:00",
            "trapped": "",
            "modDate": "D:20251020163736+03'00'",
            "creationDate": "D:20251020163736+03'00'",
            "page": 4
          }
        }
      ]
    },
    "golden_jsonl": {
      "columns": [
        "reference",
        "reference_contexts",
        "synthesizer_name",
        "user_input"
      ],
      "sample": [
        {
          "user_input": "Can you explane what AMG-RAG is and how it improves medical question answering?",
          "reference_contexts": [
            "Omar & Mohammed / Journal of Applied Science and Technology Trends Vol. 06, No. 02, pp. 342 –357 (2025) 345 record-high accuracy, while open-source models achieved impressive gains through prompt optimization. Feng et al.[22] developed the Knowledge Graph-based Thought (KGT) framework that integrated LLMs with a pan- cancer knowledge graph for biomedical question answering. KGT was designed to reason on the knowledge graph schema and identify optimal subgraphs to use for directing accurate answer generation, all without fine-tuning the LLMs. The framework is benchmarked against a new dataset (PcQA) designed specifically for pan-cancer KGQA tasks and has outperformed all existing state-of-the-art approaches by a rather large margin. KGT’s practicality in biomedical issues was highlighted through case studies for drug repositioning, drug resistance, and biomarker discovery. Their approach exhibited robust adaptability among various LLMs. Rezaei et al.[26] developed AMG-RAG, a dynamic framework that utilizes autonomous LLM agents with medical search tools in the continuous construction and real-time updating of Medical Knowledge Graphs (MKGs). Their system incorporated confidence scoring and multi-hop reasoning to improve accuracy and interpretability in medical question answering. AMG-RAG outperformed size models on both very hard MEDQA benchmarks and more accessible MedMCQA ones, proving that it could conduct efficient reasoning based on current structured medical knowledge. They also used Neo4j to manage the knowledge graphs while adding external searches to ensure the latest data. Tiwari et al.[24] presented Auto-Cypher, a recent automated pipeline for producing high-quality synthetic data for training LLMs by mapping natural language to Cypher queries for graph databases like Neo4j. The pipeline deployed the novelty of LLM- as-database-filler to synthesize Neo4j databases for the execution of generated queries to ensure their correctness. A sizable dataset called SynthCypher was created, spanning multiple domains and complex queries, leading to a 40% improvement in LLM performance on Cypher generation. The datasets were used to fine-tune open-source models such as LLaMA, Mistral, and Qwen, and the SPIDER benchmark was adapted for evaluation purposes. Mohammed et al.[29] proposed a hybridized GraphRAG framework combining Neo4j-based UMLS knowledge graphs with a vector store for medical textbooks to create an improved U.S.M.L.E.-style clinical question-answering approach. The project integrated symbolic reasoning from knowledge graphs with semantic retrieval performed on text embeddings to enhance relevance and accuracy via adaptive re-ranking and query expansion. The system had the answers produced by GPT-4o- Mini, with different prompting strategies encouraging evidence- based and traceable responses grounded in verified medical knowledge. Experiments showed that the hybrid approach improved factual accuracy and citation fidelity as compared to the L.L.M.-only approach, enhancing transparency and reliability. It is shown that binding both structured and unstructured medical knowledge sources could aid in ameliorating hallucinations and hence improve clinical trustworthiness in AI-driven medical QA. Yang et al.[30] articulated sepsis knowledge graph was crafted by combining multicenter clinical data from over 10,000 patients with the help of GPT-4 for entity recognition and relationship extraction. Real-world data were collected from three hospitals and integrated with clinical guidelines and databases from the public domain. The knowledge graph contained 1,894 nodes and 2,021 relationships pertaining to diseases, symptoms, biomarkers, treatments, and complications. GPT outperformed other models in every resolution on sepsis- specific datasets to obtain high F1-score results. The constructed graph highlighted complex interactions in sepsis for assisting clinical decision-making and was implemented on Neo4j. Guan et al.[20] proposed a novel method for constructing a local knowledge graph from retrieved biomedical documents by extracting propositional claims. They carried out layer wise summarization on this graph to capture multi-document relationships and provide comprehensive contextual information to a language model for question-answering purposes. The method resolved issues in multi-document biomedical QA, such as noise cancellation and efficient context usage. They then tested their method on several benchmarks for biomedical question answering, achieving performance at least comparable to, if not better than, existing retrieval-augmented generation (RAG) baselines. The study established enhanced reasoning and answer accuracy of the model achieved through structured graph summarization. Previous studies have improved biomedical QA using KGs and LLMs, but important gaps remain. Most systems lack transparent, graph-based justifications, rely on limited evaluation methods, or depend on cloud resources that reduce privacy and reproducibility. Our framework addresses these gaps by providing visible Cypher queries with evidence subgraphs, applying comprehensive performance metrics across difficulty levels, and ensuring fully local, privacy-preserving deployment. Table I summarizes key previous studies on biomedical knowledge graphs and question answering, outlining their methods, datasets, and main limitations."
          ],
          "reference": "AMG-RAG is a dynamic framework developed by Rezaei et al. that utilizes autonomous large language model (LLM) agents combined with medical search tools to continuously construct and update Medical Knowledge Graphs (MKGs) in real time. The system incorporates confidence scoring and multi-hop reasoning to enhance accuracy and interpretability in medical question answering. AMG-RAG outperformed larger models on challenging MEDQA benchmarks as well as more accessible MedMCQA tasks, demonstrating its ability to efficiently reason based on current structured medical knowledge. It uses Neo4j to manage the knowledge graphs and integrates external searches to ensure the inclusion of the latest data.",
          "synthesizer_name": "single_hop_specifc_query_synthesizer"
        },
        {
          "user_input": "What LLMs do in biomedical system for understand natural language?",
          "reference_contexts": [
            "III. PRELIMINARIES This section outlines the fundamental concepts required to understand the proposed system. It introduces biomedical knowledge graphs, explains how Neo4j stores data in graph form, and describes the use of Cypher for querying. It also provides a brief overview of large language models (LLMs) and their role in interpreting natural language. A. Biomedical Knowledge Graphs Biomedical Knowledge Graphs (BKGs) provide a structured representation of complex biomedical information by modeling diverse medical entities, such as diseases, drugs, symptoms, and biological pathways, as interconnected nodes within a graph structure. The edges in these graphs represent the semantic relationships between these entities, including ’treats’, ’causes’, ’interacts with’ and many others, as illustrated in Fig 1. This form of representation enables the integration of heterogeneous biomedical data from a wide range of sources, including"
          ],
          "reference": "Large language models (LLMs) play a role in interpreting natural language within the proposed biomedical system.",
          "synthesizer_name": "single_hop_specifc_query_synthesizer"
        },
        {
          "user_input": "how LLaMA 3 help in answer from biomedical data?",
          "reference_contexts": [
            "Omar & Mohammed / Journal of Applied Science and Technology Trends Vol. 06, No. 02, pp. 342 –357 (2025) 348 drugs, diseases, symptoms) and execution returns structured data (tuples) relevant to the question. Step 5. Answer Synthesis: The structured tuples flow to Answer Synthesis, which aggregates and formats them into a concise raw answer. This raw answer is sent back to LLaMA 3 to optionally refine the phrasing while preserving the retrieved facts. Step 6. Result Presentation: LLaMA 3 produces the final answer, which the interface displays together with the executed Cypher query and an optional preview of the returned rows, improving transparency and trust. The pipeline couples LLM-based language understanding (LLaMA 3) with a schema-grounded Neo4j knowledge graph. The Cypher Query Gen refines the query formulation, Query Execution retrieves evidence and Answer Synthesis converts structured results into readable outputs that produce answers that are accurate, interpretable, and easy to audit directly from the displayed query and evidence."
          ],
          "reference": "LLaMA 3 is used to refine the phrasing of the raw answer while preserving the retrieved facts, and it produces the final answer that is displayed along with the executed Cypher query and an optional preview of the returned rows, improving transparency and trust in the biomedical question answering pipeline.",
          "synthesizer_name": "single_hop_specifc_query_synthesizer"
        },
        {
          "user_input": "Can you explane in detale what the Biomedicl Knowldge Hub is and how it is constucted, including the types of entities and relationships it contains?",
          "reference_contexts": [
            "B. Dataset and Knowledge Graph Construction 1) Dataset The integrated Biomedical Knowledge Hub (iBKH), a large biomedical knowledge base, forms the first level of the system and integrates information from various curated high-quality biomedical databases. This implies that the data set includes various types of entities, such as diseases, symptoms, drugs, biological pathways, etc. This study used the representative subset of the iBKH dataset, which contained 65828 biomedical entities. These entities are semantically interconnected through a total of 3004166 relationships, thus creating a rich knowledge graph. The iBKH dataset was originally introduced in [11], and it is freely available at (https://github.com/wcm-wanglab/iBKH). This dataset is the core semantic foundation upon which this study is built. The knowledge graph is populated from multiple tabular sources (CSV files), each listing entities or relationships. The main input files and their contents are as follows: • Disease vocabulary(disease_vocab.csv): Contains columns such as primary (a unique disease ID), name, do_id (Disease Ontology ID), kegg_id, and umls_cui (UMLS Concept ID). Each row represents a disease node with external identifiers. • Drug vocabulary (drug_vocab.csv): Includes primary (unique drug ID), name, drugbank_id, kegg_id, pharmgkb_id, umls_cui, mesh_id, iDISK_id and CID (PubChem ID). Each row defines a drug node with standard database identifiers. • Symptom vocabulary (symptom_vocab.csv): Contains primary (unique symptom ID), name, mesh_id, umls_cui and iDISK_id. Each row defines a symptom node. • Side effect vocabulary (side_effect_vocab.csv): Includes primary (unique side-effect ID) and name. Each row represents a side-effect node (with UMLS ID when available). • Pathway vocabulary (pathway_vocab.csv): Contains primary (unique pathway ID), name, reactome_id, go_id, and kegg_id. Each row defines a biological pathway node. Relationship files (each row typically contains two entity IDs and one or more boolean flags or codes) include: • Disease–Symptom links (Di_Sy_res.csv): Rows include Disease and Symptom IDs, a presence flag (1 or 0) and a data source. If Present = 1, a HAS_SYMPTOM edge is created from the disease to the symptom, with properties for presence and source. • Disease–Disease links (di_di_res.csv): Rows include Disease_1 and Disease_2 IDs with binary flags for is_a and Resemble. If is_a = 1, an (IS_A) edge is created (Disease_1 → Disease_2); if Resemble = 1, a RESEMBLES edge is created. The source field is used for provenance. • Drug–Disease links (D_Di_res.csv): Includes Drug and Disease IDs with several binary flags. If a flag equals 1, a corresponding edge is created: o TREATS (Treats = 1) o PALLIATES (Palliates = 1) o ASSOCIATED_WITH (Associate = 1) o ALLEVIATES_REDUCES (alleviates = 1) o TREATMENT_THERAPY (treatment/therapy = 1) o INHIBITS_CELL_GROWTH (inhibits cell growth = 1) o HAS_BIOMARKER (biomarkers = 1) o PREVENTS_SUPPRESSES (prevents/suppresses = 1) o ROLE_IN_PATHOGENESIS (role in disease pathogenesis = 1) • Drug–SideEffect links (D_SE_res.csv): Contains Drug and SideEffect IDs with a Source column. Each row creates a CAUSES edge from the drug to the side effect, with source as an edge property. • Drug–Drug interactions (D_D_res.csv): Rows include Drug_1 and Drug_2 IDs with flags for Interaction and Resemble. If Interaction = 1, an INTERACTS_WITH edge is created (bidirectional). If Resemble = 1, a RESEMBLES edge is added. • Drug–Pathway links (D_Pwy_res.csv): Includes Drug ID and Pathway ID. Each row generates an ASSOCIATED_WITH edge from the drug to the pathway. • Disease–Pathway links (Di_Pwy_res.csv): Contains Disease ID and Pathway ID. Each row creates an ASSOCIATED_WITH edge from the disease to the pathway. 2) Data Upload Performance The time required to upload different types of entities and relationships into the Neo4j biomedical knowledge graph, measured in seconds. These measurements reflect both the size and complexity of the data being processed. As shown in Table II, the longest upload time is for Drug- Drug Relationships, which takes approximately 190 seconds due to the large number of edges (over 3 million). Following this, Disease-Disease and Drug-Disease Relationships also require considerable time for loading. On the other hand, individual"
          ],
          "reference": "The integrated Biomedical Knowledge Hub (iBKH) is a large biomedical knowledge base that forms the first level of the system by integrating information from various curated high-quality biomedical databases. It includes various types of entities such as diseases, symptoms, drugs, biological pathways, and more. The representative subset used in this study contains 65,828 biomedical entities that are semantically interconnected through a total of 3,004,166 relationships, creating a rich knowledge graph. The knowledge graph is populated from multiple tabular sources (CSV files), each listing entities or relationships. Entity files include disease vocabulary, drug vocabulary, symptom vocabulary, side effect vocabulary, and pathway vocabulary, each containing unique IDs and standard database identifiers. Relationship files describe links such as Disease–Symptom, Disease–Disease, Drug–Disease, Drug–SideEffect, Drug–Drug interactions, Drug–Pathway, and Disease–Pathway, with specific edge types like HAS_SYMPTOM, IS_A, TREATS, CAUSES, INTERACTS_WITH, and ASSOCIATED_WITH. These relationships are defined by binary flags or presence indicators and include provenance information where applicable.",
          "synthesizer_name": "single_hop_specifc_query_synthesizer"
        },
        {
          "user_input": "How do biomedical knowledge graphs utilize the Neo4j graph database and Cypher queries to improve biomedical question answering accuracy and transparency?",
          "reference_contexts": [
            "<1-hop>\n\nOmar & Mohammed / Journal of Applied Science and Technology Trends Vol. 06, No. 02, pp. 342 –357 (2025) 345 record-high accuracy, while open-source models achieved impressive gains through prompt optimization. Feng et al.[22] developed the Knowledge Graph-based Thought (KGT) framework that integrated LLMs with a pan- cancer knowledge graph for biomedical question answering. KGT was designed to reason on the knowledge graph schema and identify optimal subgraphs to use for directing accurate answer generation, all without fine-tuning the LLMs. The framework is benchmarked against a new dataset (PcQA) designed specifically for pan-cancer KGQA tasks and has outperformed all existing state-of-the-art approaches by a rather large margin. KGT’s practicality in biomedical issues was highlighted through case studies for drug repositioning, drug resistance, and biomarker discovery. Their approach exhibited robust adaptability among various LLMs. Rezaei et al.[26] developed AMG-RAG, a dynamic framework that utilizes autonomous LLM agents with medical search tools in the continuous construction and real-time updating of Medical Knowledge Graphs (MKGs). Their system incorporated confidence scoring and multi-hop reasoning to improve accuracy and interpretability in medical question answering. AMG-RAG outperformed size models on both very hard MEDQA benchmarks and more accessible MedMCQA ones, proving that it could conduct efficient reasoning based on current structured medical knowledge. They also used Neo4j to manage the knowledge graphs while adding external searches to ensure the latest data. Tiwari et al.[24] presented Auto-Cypher, a recent automated pipeline for producing high-quality synthetic data for training LLMs by mapping natural language to Cypher queries for graph databases like Neo4j. The pipeline deployed the novelty of LLM- as-database-filler to synthesize Neo4j databases for the execution of generated queries to ensure their correctness. A sizable dataset called SynthCypher was created, spanning multiple domains and complex queries, leading to a 40% improvement in LLM performance on Cypher generation. The datasets were used to fine-tune open-source models such as LLaMA, Mistral, and Qwen, and the SPIDER benchmark was adapted for evaluation purposes. Mohammed et al.[29] proposed a hybridized GraphRAG framework combining Neo4j-based UMLS knowledge graphs with a vector store for medical textbooks to create an improved U.S.M.L.E.-style clinical question-answering approach. The project integrated symbolic reasoning from knowledge graphs with semantic retrieval performed on text embeddings to enhance relevance and accuracy via adaptive re-ranking and query expansion. The system had the answers produced by GPT-4o- Mini, with different prompting strategies encouraging evidence- based and traceable responses grounded in verified medical knowledge. Experiments showed that the hybrid approach improved factual accuracy and citation fidelity as compared to the L.L.M.-only approach, enhancing transparency and reliability. It is shown that binding both structured and unstructured medical knowledge sources could aid in ameliorating hallucinations and hence improve clinical trustworthiness in AI-driven medical QA. Yang et al.[30] articulated sepsis knowledge graph was crafted by combining multicenter clinical data from over 10,000 patients with the help of GPT-4 for entity recognition and relationship extraction. Real-world data were collected from three hospitals and integrated with clinical guidelines and databases from the public domain. The knowledge graph contained 1,894 nodes and 2,021 relationships pertaining to diseases, symptoms, biomarkers, treatments, and complications. GPT outperformed other models in every resolution on sepsis- specific datasets to obtain high F1-score results. The constructed graph highlighted complex interactions in sepsis for assisting clinical decision-making and was implemented on Neo4j. Guan et al.[20] proposed a novel method for constructing a local knowledge graph from retrieved biomedical documents by extracting propositional claims. They carried out layer wise summarization on this graph to capture multi-document relationships and provide comprehensive contextual information to a language model for question-answering purposes. The method resolved issues in multi-document biomedical QA, such as noise cancellation and efficient context usage. They then tested their method on several benchmarks for biomedical question answering, achieving performance at least comparable to, if not better than, existing retrieval-augmented generation (RAG) baselines. The study established enhanced reasoning and answer accuracy of the model achieved through structured graph summarization. Previous studies have improved biomedical QA using KGs and LLMs, but important gaps remain. Most systems lack transparent, graph-based justifications, rely on limited evaluation methods, or depend on cloud resources that reduce privacy and reproducibility. Our framework addresses these gaps by providing visible Cypher queries with evidence subgraphs, applying comprehensive performance metrics across difficulty levels, and ensuring fully local, privacy-preserving deployment. Table I summarizes key previous studies on biomedical knowledge graphs and question answering, outlining their methods, datasets, and main limitations.",
            "<2-hop>\n\nIII. PRELIMINARIES This section outlines the fundamental concepts required to understand the proposed system. It introduces biomedical knowledge graphs, explains how Neo4j stores data in graph form, and describes the use of Cypher for querying. It also provides a brief overview of large language models (LLMs) and their role in interpreting natural language. A. Biomedical Knowledge Graphs Biomedical Knowledge Graphs (BKGs) provide a structured representation of complex biomedical information by modeling diverse medical entities, such as diseases, drugs, symptoms, and biological pathways, as interconnected nodes within a graph structure. The edges in these graphs represent the semantic relationships between these entities, including ’treats’, ’causes’, ’interacts with’ and many others, as illustrated in Fig 1. This form of representation enables the integration of heterogeneous biomedical data from a wide range of sources, including"
          ],
          "reference": "Biomedical knowledge graphs (BKGs) provide a structured representation of complex biomedical information by modeling diverse medical entities such as diseases, drugs, symptoms, and biological pathways as interconnected nodes, with edges representing semantic relationships like 'treats', 'causes', and 'interacts with'. Neo4j stores this data in graph form, enabling efficient management and querying of these relationships. Cypher, Neo4j's query language, is used to retrieve and manipulate data within these graphs. Frameworks such as AMG-RAG utilize Neo4j to manage continuously updated medical knowledge graphs, incorporating confidence scoring and multi-hop reasoning to enhance accuracy and interpretability in medical question answering. Additionally, Auto-Cypher automates the generation of Cypher queries to produce high-quality synthetic data for training large language models, improving their performance in graph query generation. Hybrid approaches like GraphRAG combine Neo4j-based knowledge graphs with vector stores for semantic retrieval, enhancing factual accuracy, citation fidelity, and transparency in clinical question answering. Overall, the integration of biomedical knowledge graphs with Neo4j and Cypher queries enables accurate, transparent, and efficient biomedical question answering by combining structured graph reasoning with advanced language model capabilities.",
          "synthesizer_name": "multi_hop_abstract_query_synthesizer"
        }
      ]
    }
  },
  "artifacts": {
    "sources": {
      "jsonl": {
        "path": "/home/donbr/open-biosciences/biosciences-research/data/interim/sources.docs.jsonl",
        "bytes": 684426
      },
      "parquet": {
        "path": "/home/donbr/open-biosciences/biosciences-research/data/interim/sources.docs.parquet",
        "bytes": 315160
      },
      "hfds": {
        "path": "/home/donbr/open-biosciences/biosciences-research/data/interim/sources.hfds"
      }
    },
    "golden_testset": {
      "jsonl": {
        "path": "/home/donbr/open-biosciences/biosciences-research/data/interim/golden_testset.jsonl",
        "bytes": 80963
      },
      "parquet": {
        "path": "/home/donbr/open-biosciences/biosciences-research/data/interim/golden_testset.parquet",
        "bytes": 46347
      },
      "hfds": {
        "path": "/home/donbr/open-biosciences/biosciences-research/data/interim/golden_testset.hfds"
      }
    }
  }
}