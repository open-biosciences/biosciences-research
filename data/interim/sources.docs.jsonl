{"page_content": "Vol. 06, No. 02, pp. 342 –357 (2025) \nISSN: 2708-0757 \n \nJOURNAL OF APPLIED SCIENCE AND TECHNOLOGY TRENDS \n \nwww.jastt.org  \n \n                                                          342 \ndoi: 10.38094/jastt62404  \nA Hybrid LLM–Knowledge Graph Framework for Accurate \nBiomedical Question Answering \n \nHavraz Y. Omar1,²*, Abdulhakeem O. Mohammed³  \n \n¹Department of Information Technology, Technical College of Duhok, Duhok Polytechnic University, Duhok, Kurdistan Region, \nIraq. havraz.omar@dpu.edu.krd  \n² Department of Information Technology, Technical College of Informatics – Akre, Akre University for Applied Sciences, Akre, \nKurdistan Region, Iraq. \n³ Department of Computer Science, College of Science, University of Zakho, Zakho, Kurdistan Region, Iraq. \na.mohammed@uoz.edu.krd \n \n*Correspondence: havraz.omar@dpu.edu.krd \n \nAbstract \nBiomedical question answering requires accurate and interpretable systems; however, existing approaches often face challenges such as \nlanguage model hallucinations and limited reasoning when relying solely on standalone knowledge graphs. To address these limitations, \nthis study proposes a hybrid framework that integrates the LLaMA-3B language model with a Neo4j-based drug–disease–symptom \nknowledge graph. The system translates natural language questions into executable Cypher queries, operates on an iBKH-derived graph \ncomprising over 65,000 entities and 3 million relationships, and returns answers with supporting evidence through a transparent interface. \nExperiments conducted on 60 biomedical questions across three levels of difficulty demonstrate the robustness of the approach: 96% \nexact match for simple queries, 95% for medium queries, and 86.7% for complex queries. Overall, the system achieves Precision@5 of \n96.1%, Recall@5 of 89.0%, F1@5 of 91.0%, Hits@k of 96.1%, and an MRR of 94.4%, while maintaining an average response time of \nonly 6.07 seconds. These results indicate that the system retrieves nearly all relevant answers, ranks them correctly, and delivers them \nwith latency low enough for interactive use. Moreover, unlike cloud-based APIs such as ChatGPT, which require internet connectivity \nand external data transmission, the proposed framework operates fully offline, ensuring privacy, reproducibility, and compliance with \nbiomedical data governance. Overall, this pipeline provides an accurate, efficient, and privacy-preserving solution for biomedical question \nanswering, making it a practical alternative to cloud-dependent approaches in sensitive healthcare contexts. \n \nKeywords: Knowledge Graph, LLM, Question Answering, Neo4j, Biomedical Informatics, Healthcare AI, LLaMA 3. \n \nReceived: August 14th, 2025 / Revised: October 10th, 2025 / Accepted: October 16th, 2025 / Online: October 20th, 2025 \n \nI. INTRODUCTION \nAnswering questions in the biomedical field is a difficult task \ndue to the complexity of medical knowledge and the need for \nprecision. In recent years, large language models  (LLMs) like \nLLaMA, GPT-4 have made progress in understanding and \ngenerating human-like responses to medical questions  [1,  2]. \nThese models can process large amounts of information and \nrespond in natural language, which makes them helpful in \nhealthcare settings [3]. However, they often struggle to provide \naccurate answers when dealing with specialized biomedical \ncontent [4, 5]. \nOne major issue with LLMs is a problem called hallucination, \nwhere the model generates information that sounds right but is \nactually incorrect or unsupported [6]. In medical applications, \nthis can be dangerous, as healthcare professionals rely on precise \nand trustworthy information  [7]. Therefore, researchers are \nexploring ways to combine LLMs with structured sources of \nknowledge to improve their reliability [8]. \nLLM-only systems in biomedicine still hallucinate and are \nhard to verify, limiting safe use [9, 10]. Biomedical knowledge \ngraphs (BKGs) such as iBKH and SPOKE curate multi-source \nfacts and enable multi-hop reasoning, yet they neither interpret \nfree text nor generate answers [11, 12]. Recent hybrids (KG-\naware RAG) improve grounding but often lack explicit path-level \njustifications and robust end-to-end answer evaluation [13, 14]. \nRecent studies have increasingly integrated Knowledge \nGraphs (KGs) with Large Language Models (LLMs) to improve \nfactual accuracy, reasoning, and reduce hallucinations. Notable \nexamples include DR.KNOWS, which combines UMLS-based \nKGs with LLMs for better diagnostic reasoning [15], KnowNet", "metadata": {"producer": "Microsoft® Word for Microsoft 365", "creator": "Microsoft® Word for Microsoft 365", "creationdate": "2025-10-20T16:37:36+03:00", "source": "/home/donbr/open-biosciences/biosciences-research/data/raw/JASTTytm_A+Hybrid+LLM–Knowledge+Graph_.pdf", "file_path": "/home/donbr/open-biosciences/biosciences-research/data/raw/JASTTytm_A+Hybrid+LLM–Knowledge+Graph_.pdf", "total_pages": 16, "format": "PDF 1.7", "title": "A Hybrid LLM–Knowledge Graph Framework for Accurate Biomedical Question Answering", "author": "", "subject": "", "keywords": "", "moddate": "2025-10-20T16:37:36+03:00", "trapped": "", "modDate": "D:20251020163736+03'00'", "creationDate": "D:20251020163736+03'00'", "page": 0}}
{"page_content": "Omar & Mohammed / Journal of Applied Science and Technology Trends Vol. 06, No. 02, pp. 342 –357 (2025) \n \n343 \nfor visualizing and validating LLM outputs [16], and MedKA for \nKG-enhanced question answering [17]. \nTo address these challenges, several recent works have \nexplored the integration of large language models with \nbiomedical knowledge graphs (KGs). A biomedical KG is a \nstructured network that connects entities such as diseases, drugs, \nand symptoms using defined relationships [18, 19]. These graphs \nstore verified medical knowledge from trusted databases, \nallowing for more accurate and explainable responses [12]. KGs \nare especially useful in multi-step reasoning tasks, where finding \nan answer requires connecting different pieces of information \n[20]. These entities and relationships can be visually represented \nin a biomedical knowledge graph, as shown in Fig. 1, where \nnodes represent medical concepts such as drugs, diseases, \nsymptoms, and pathways, and edges denote their semantic \nrelationships. \n \nFig. 1. Overview of Biomedical Knowledge Graph Entities and Relationships \n \nOne example of a widely used biomedical KG is SPOKE, \nwhich includes millions of nodes and relationships from over 40 \nbiomedical databases [12]. Integrating an LLM with a KG allows \nthe strengths of both technologies to work together: the LLM \nprovides language understanding, and the KG provides \nstructured, factual knowledge [21, 22]. A common method is \nretrieval-augmented generation (RAG), where the LLM retrieves \ninformation from the KG and uses it to generate more accurate \nresponses [13, 23]. In more advanced setups, the LLM can even \ngenerate queries like Cypher to fetch specific data from the graph \n[24, 25]. Neo4j is a popular graph database that supports fast and \nflexible storage and querying of knowledge graphs using Cypher \n[21]. It is well-suited for biomedical applications because it \nallows easy exploration of complex medical relationships. Recent \nwork has shown that combining Neo4j with LLMs can lead to \nbetter accuracy, fewer hallucinations, and more explainable \nresults [24, 26]. \nDespite improvements, building a reliable hybrid system that \ncombines an LLM with a biomedical KG remains a technical \nchallenge. Some approaches require complex pipelines or large \ntraining datasets, while others rely on fine-tuning specific to a \nnarrow set of questions [27, 28]. There is still a need for systems \nthat are both accurate and easy to scale, especially in domains like \nmedicine where the cost of errors is high [22]. \nRecent advances in KG-augmented LLMs have improved \nperformance, yet biomedical QA continues to face three practical \ngaps: \n1. Traceability: LLM-only or text-retrieval-only pipelines \nrarely provide graph-grounded justifications; users lack the \nability to inspect the exact nodes and edges that support an \nanswer. \n2. Evaluation: Prior work often judges quality via surface-\nform checks (e.g., matching a Cypher template), which fails to \ncapture end-to-end answer correctness or ranking quality \nacross different difficulty levels. \n3. Deployment: Many solutions assume cloud resources or \ndomain-specific fine-tuning, yet biomedical contexts typically \ndemand a local, privacy-preserving system with low latency \nand reproducible behavior. \nTimestamp-aware execution and periodic KG refresh help \navoid deprecated or contraindicated links, making the tool better \nsuited for safety-critical clinical contexts (e.g., drug–drug \ninteractions). \nTo address these limitations, Our work introduces a locally \ndeployable pipeline that translates biomedical questions into \nexecutable Cypher queries over a Neo4j knowledge graph. The \nsystem returns answers with supporting nodes and edges, and is", "metadata": {"producer": "Microsoft® Word for Microsoft 365", "creator": "Microsoft® Word for Microsoft 365", "creationdate": "2025-10-20T16:37:36+03:00", "source": "/home/donbr/open-biosciences/biosciences-research/data/raw/JASTTytm_A+Hybrid+LLM–Knowledge+Graph_.pdf", "file_path": "/home/donbr/open-biosciences/biosciences-research/data/raw/JASTTytm_A+Hybrid+LLM–Knowledge+Graph_.pdf", "total_pages": 16, "format": "PDF 1.7", "title": "A Hybrid LLM–Knowledge Graph Framework for Accurate Biomedical Question Answering", "author": "", "subject": "", "keywords": "", "moddate": "2025-10-20T16:37:36+03:00", "trapped": "", "modDate": "D:20251020163736+03'00'", "creationDate": "D:20251020163736+03'00'", "page": 1}}
{"page_content": "Omar & Mohammed / Journal of Applied Science and Technology Trends Vol. 06, No. 02, pp. 342 –357 (2025) \n \n344 \nevaluated using Exact Match, Precision, Recall, F1, Hits@k, \nMRR, and latency across simple, medium, and complex question \nsets. Unlike prior template-based methods, our approach enables \ntraceable, outcome-level validation. In summary, the main \ncontributions of this work are as follows: \n• Hybrid LLM to Cypher QA: A system that translates \nnatural language questions into accurate, executable \nCypher over a Neo4j drug, disease, and symptom KG. \n• Prompt-driven query generation: Schema,  entity \naware prompting that reliably maps diverse biomedical \nquestions to graph queries. \n• Evidence transparency: Along with each answer, we \nsurface the generated Cypher and the supporting \njustification subgraph (nodes, edges) plus a brief natural \nlanguage rationale. \n• Answer-level evaluation: End-to-end assessment using \nExact Match, F1, Precision/Recall, Hits@k, MRR and \nlatency across simple, medium and complex tiers. \n• Local, reproducible deployment: On-prem LLaMA 3 \ninference (no cloud dependency) suitable for biomedical \nsettings requiring low latency and strong data control. \n \nThe remainder of this paper is organized as follows: Section \n2 reviews related work on biomedical knowledge graphs and \nLLM-based QA systems. Section 3 provides background on \nknowledge graphs, large language models, and question \nanswering frameworks. Section 4 details the proposed \nmethodology, \nincluding \nsystem \narchitecture, \ndataset \nconstruction, and query translation. Section 5 presents the \nexperimental results through both quantitative metrics and \nqualitative case studies. Section 6 discusses the findings, analyzes \nlimitations, and compares performance against baseline models. \nFinally, Section 7 concludes this paper and outlines directions for \nfuture work. \nII. RELATED WORK \nRecently, studies have concentrated on the integration of \nclinical and medical knowledge graphs (LLM) to improve the \nanswer to medical questions. Researchers have derived several \nbiomedical KGs using Neo4j and incorporated the application of \nLLMs like LLaMA and GPT to convert natural language \nquestions into graph queries. Improvements in answer \ncorrectness, reduction of hallucination errors, one-to-many \nrelationships, and support for complex reasoning were the \nobjectives of these efforts. Some frameworks also adopted \nretrieval methods to ground responses in secure data. \nSu et al.[11] developed an integrative Biomedical Knowledge \nHub (iBKH), a huge biomedical knowledge graph that comprises \n18 of the very best data sources. The deployment of the iBKH in \nNeo4j allows for a user-friendly web portal to allow fast and \ninteractive knowledge retrieval. The system implemented \nadvanced graph learning techniques to enable the discovery of \nbiomedical knowledge, illustrated by an example of repurposing \nin silico drugs for Alzheimer’s disease. iBKH achieved \npromising predictive performance for known drugs and proposed \npossible new drug candidates. \nRajabi and Kafaie[19] proposed a disease knowledge graph \nusing a cross-referential disease database comprising diseases, \nsymptoms, and drugs interconnected with relationships. They \ntransferred the data into Neo4j to create a graph of 9,400 nodes \nand 45,000 relationships representing the semantic links between \nmedical concepts. Applying Cypher queries enabled answering \ncomplex medical questions regarding identifying drugs that may \ncause certain diseases; it was demonstrated that the graph inferred \nnew relationships not explicitly existing in the original data. The \nconclusion was that disease knowledge graphs sped up clinical \ndiscovery and contributed to understanding complex medical \nrelationships. \nHou et al.[3] assessed and contrasted ChatGPT (both GPT-\n3.5 and GPT-4) and the biomedical knowledge graphs (BKGs) \nconcerning their ability to answer biomedical questions, generate \nnew knowledge, and reason. Their datasets were focused on \ndietary supplements and drugs, while evaluation criteria entailed \naccuracy, novelty, and reasoning ability. The results indicate that \nwhile GPT-4 surpassed GPT-3.5 and BKGs in knowledge \nprovision, it proved inconsistent with regard to citations and \nreasoning. Compared to them, BKGs scored higher in accuracy \nand reliability, especially in discovering novel links as well as \nwithin structured reasoning. \nSoman et al.[13] presented a novel framework called KG-\nRAG that integrates a large biomedical knowledge graph \n(SPOKE) with LLaMA 2, GPT-3.5, and GPT-4 (LLMs) to \nproduce accurate biomedical text. They optimized the retrieval of \nrelevant graph context to cut over 50% tokens without losing \naccuracy. It aided LLMs in performing better on biomedical \nquestion answering with very high accuracy boosts, especially in \nthe case of LLaMA 2. They compared KG-RAG to other retrieval \nmethods and indicated its comparatively more robust and \nefficient results. The framework produced reliable evidence-\nbased answers grounded in biomedical knowledge. \nLuo et al.[23] created ChatKBQA, a new framework with a \nquestion-and-answer approach over knowledge bases that first \ngenerates logical forms with the help of fine-tuned LLMs and \nthen retrieves the relevant entities and relations. This generate-\nthen-retrieve approach is supposed to handle a couple of issues \nwith the earlier methods concerning tedious retrieval and error \npropagation. They fine-tuned open-source LLMs like LLaMA 2 \nto change natural-language questions into logical forms with high \naccuracy. The retrieval phase uses unsupervised phrase-level \nsemantic matching in a way that enhances the alignment of \nentities and relations. Experiments on benchmark datasets \nindicate ChatKBQA to be superior to its predecessors, with the \nhighest accuracy to date. \nPusch and Conrad[6] conducted work under a hybrid \napproach conflating LLMs and biomedical Knowledge Graphs \n(KGs) to suppress hallucination errors in question-answering. \nThey proposed query-checking algorithms for validating, \ncorrecting, and executing the KG Cypher queries that LLMs \ngenerated, thereby attaining accurate and understandable \nanswers. The system used retrieval-augmented generation (RAG) \nto ground answers within KG data. The methodology was \nvalidated on a biomedical KG called PrimeKG using 50 \nbenchmark questions, assessing models like GPT-4 Turbo and \nLLaMA 3. Commercially available GPT-4 Turbo obtained", "metadata": {"producer": "Microsoft® Word for Microsoft 365", "creator": "Microsoft® Word for Microsoft 365", "creationdate": "2025-10-20T16:37:36+03:00", "source": "/home/donbr/open-biosciences/biosciences-research/data/raw/JASTTytm_A+Hybrid+LLM–Knowledge+Graph_.pdf", "file_path": "/home/donbr/open-biosciences/biosciences-research/data/raw/JASTTytm_A+Hybrid+LLM–Knowledge+Graph_.pdf", "total_pages": 16, "format": "PDF 1.7", "title": "A Hybrid LLM–Knowledge Graph Framework for Accurate Biomedical Question Answering", "author": "", "subject": "", "keywords": "", "moddate": "2025-10-20T16:37:36+03:00", "trapped": "", "modDate": "D:20251020163736+03'00'", "creationDate": "D:20251020163736+03'00'", "page": 2}}
{"page_content": "Omar & Mohammed / Journal of Applied Science and Technology Trends Vol. 06, No. 02, pp. 342 –357 (2025) \n \n345 \nrecord-high accuracy, while open-source models achieved \nimpressive gains through prompt optimization. \nFeng et al.[22] developed the Knowledge Graph-based \nThought (KGT) framework that integrated LLMs with a pan-\ncancer knowledge graph for biomedical question answering. \nKGT was designed to reason on the knowledge graph schema and \nidentify optimal subgraphs to use for directing accurate answer \ngeneration, all without fine-tuning the LLMs. The framework is \nbenchmarked against a new dataset (PcQA) designed specifically \nfor pan-cancer KGQA tasks and has outperformed all existing \nstate-of-the-art approaches by a rather large margin. KGT’s \npracticality in biomedical issues was highlighted through case \nstudies for drug repositioning, drug resistance, and biomarker \ndiscovery. Their approach exhibited robust adaptability among \nvarious LLMs. \nRezaei et al.[26] developed AMG-RAG, a dynamic \nframework that utilizes autonomous LLM agents with medical \nsearch tools in the continuous construction and real-time updating \nof Medical Knowledge Graphs (MKGs). Their system \nincorporated confidence scoring and multi-hop reasoning to \nimprove accuracy and interpretability in medical question \nanswering. AMG-RAG outperformed size models on both very \nhard MEDQA benchmarks and more accessible MedMCQA \nones, proving that it could conduct efficient reasoning based on \ncurrent structured medical knowledge. They also used Neo4j to \nmanage the knowledge graphs while adding external searches to \nensure the latest data. \nTiwari et al.[24] presented Auto-Cypher, a recent automated \npipeline for producing high-quality synthetic data for training \nLLMs by mapping natural language to Cypher queries for graph \ndatabases like Neo4j. The pipeline deployed the novelty of LLM-\nas-database-filler to synthesize Neo4j databases for the execution \nof generated queries to ensure their correctness. A sizable dataset \ncalled SynthCypher was created, spanning multiple domains and \ncomplex queries, leading to a 40% improvement in LLM \nperformance on Cypher generation. The datasets were used to \nfine-tune open-source models such as LLaMA, Mistral, and \nQwen, and the SPIDER benchmark was adapted for evaluation \npurposes. \nMohammed et al.[29] proposed a hybridized GraphRAG \nframework combining Neo4j-based UMLS knowledge graphs \nwith a vector store for medical textbooks to create an improved \nU.S.M.L.E.-style clinical question-answering approach. The \nproject integrated symbolic reasoning from knowledge graphs \nwith semantic retrieval performed on text embeddings to enhance \nrelevance and accuracy via adaptive re-ranking and query \nexpansion. The system had the answers produced by GPT-4o-\nMini, with different prompting strategies encouraging evidence-\nbased and traceable responses grounded in verified medical \nknowledge. Experiments showed that the hybrid approach \nimproved factual accuracy and citation fidelity as compared to \nthe L.L.M.-only approach, enhancing transparency and \nreliability. It is shown that binding both structured and \nunstructured medical knowledge sources could aid in \nameliorating hallucinations and hence improve clinical \ntrustworthiness in AI-driven medical QA. \nYang et al.[30] articulated sepsis knowledge graph was \ncrafted by combining multicenter clinical data from over 10,000 \npatients with the help of GPT-4 for entity recognition and \nrelationship extraction. Real-world data were collected from \nthree hospitals and integrated with clinical guidelines and \ndatabases from the public domain. The knowledge graph \ncontained 1,894 nodes and 2,021 relationships pertaining to \ndiseases, symptoms, biomarkers, treatments, and complications. \nGPT outperformed other models in every resolution on sepsis-\nspecific datasets to obtain high F1-score results. The constructed \ngraph highlighted complex interactions in sepsis for assisting \nclinical decision-making and was implemented on Neo4j. \nGuan et al.[20] proposed a novel method for constructing a \nlocal knowledge graph from retrieved biomedical documents by \nextracting propositional claims. They carried out layer wise \nsummarization on this graph to capture multi-document \nrelationships and provide comprehensive contextual information \nto a language model for question-answering purposes. The \nmethod resolved issues in multi-document biomedical QA, such \nas noise cancellation and efficient context usage. They then tested \ntheir method on several benchmarks for biomedical question \nanswering, achieving performance at least comparable to, if not \nbetter than, existing retrieval-augmented generation (RAG) \nbaselines. The study established enhanced reasoning and answer \naccuracy of the model achieved through structured graph \nsummarization.   \nPrevious studies have improved biomedical QA using KGs \nand LLMs, but important gaps remain. Most systems lack \ntransparent, graph-based justifications, rely on limited evaluation \nmethods, or depend on cloud resources that reduce privacy and \nreproducibility. Our framework addresses these gaps by \nproviding visible Cypher queries with evidence subgraphs, \napplying comprehensive performance metrics across difficulty \nlevels, and ensuring fully local, privacy-preserving deployment. \nTable I summarizes key previous studies on biomedical \nknowledge graphs and question answering, outlining their \nmethods, datasets, and main limitations. \nIII. PRELIMINARIES \nThis section outlines the fundamental concepts required to \nunderstand the proposed system. It introduces biomedical \nknowledge graphs, explains how Neo4j stores data in graph \nform, and describes the use of Cypher for querying. It also \nprovides a brief overview of large language models (LLMs) and \ntheir role in interpreting natural language. \n \n \nA. Biomedical Knowledge Graphs \nBiomedical Knowledge Graphs (BKGs) provide a structured \nrepresentation of complex biomedical information by modeling \ndiverse medical entities, such as diseases, drugs, symptoms, and \nbiological pathways, as interconnected nodes within a graph \nstructure. The edges in these graphs represent the semantic \nrelationships between these entities, including ’treats’, ’causes’, \n’interacts with’ and many others, as illustrated in Fig 1. This \nform of representation enables the integration of heterogeneous \nbiomedical data from a wide range of sources, including", "metadata": {"producer": "Microsoft® Word for Microsoft 365", "creator": "Microsoft® Word for Microsoft 365", "creationdate": "2025-10-20T16:37:36+03:00", "source": "/home/donbr/open-biosciences/biosciences-research/data/raw/JASTTytm_A+Hybrid+LLM–Knowledge+Graph_.pdf", "file_path": "/home/donbr/open-biosciences/biosciences-research/data/raw/JASTTytm_A+Hybrid+LLM–Knowledge+Graph_.pdf", "total_pages": 16, "format": "PDF 1.7", "title": "A Hybrid LLM–Knowledge Graph Framework for Accurate Biomedical Question Answering", "author": "", "subject": "", "keywords": "", "moddate": "2025-10-20T16:37:36+03:00", "trapped": "", "modDate": "D:20251020163736+03'00'", "creationDate": "D:20251020163736+03'00'", "page": 3}}
{"page_content": "Omar & Mohammed / Journal of Applied Science and Technology Trends Vol. 06, No. 02, pp. 342 –357 (2025) \n \n346 \nscientific literature, clinical records, genomic databases, and \nexperimental findings [19, 31]. \nSuch integration creates a comprehensive biomedical \nknowledge base that supports advanced analytics and discovery. \nFor example, biomedical knowledge graphs can reveal previously \nunknown relationships (e.g., between drugs and diseases) and \nhelp prioritize potential biomarkers for complex conditions. The \nIntegrative Biomedical Knowledge Hub (iBKH) is one such \nlarge-scale graph that consolidates diverse biomedical resources \ninto a unified hub, enabling discovery at scale [11]. Beyond \niBKH, large biomedical knowledge graphs such as SPOKE \nfurther illustrate how graph integration accelerates research and \nsupports precision-medicine use cases [12]. Overall, these graphs \nserve as foundational resources for data-driven and personalized \nmedicine. These knowledge graphs serve as foundational \nresources for precision medicine, where treatment can be tailored \nto the individual’s biological profile, improving outcomes and \nminimizing side effects [19, 31]. \n \nTABLE I.  SUMMARY OF RELATED RESEARCH ON BIOMEDICAL KGS AND QUESTION ANSWERING \nRef. \nYear \nData/Graph \nMethod \nBaselines \nKey Metric \nLimitation \n[11] \n2023 \niBKH (18 biomedical \nsources, Neo4j) \nIntegrative KG + Graph \nlearning; drug repurposing \ncase \nKnown drugs, \nAlzheimer’s \nstudy \nPredictive performance \n(drug repurposing) \nLimited to Alzheimer’s case \nstudy; scalability and updates not \ndetailed \n[19] \n2023 \nDisease KG (9,400 \nnodes, 45,000 relations \nin Neo4j) \nCypher queries for disease–\ndrug–symptom reasoning \nCross-referential \ndisease DB \nNew relation inference; \ncomplex query \nanswering \nLimited to single domain; lacks \nlarge-scale evaluation \n[3] \n2023 \nBKGs vs. GPT-3.5/4 \nComparative QA study: \nLLMs vs. KGs \nGPT-3.5, GPT-4, \nKG reasoning \nAccuracy, Novelty, \nReasoning \nGPT-4 inconsistent in \nreasoning/citations; KG less fluent \nbut more reliable \n[13] \n2024 \nSPOKE KG + \nLLaMA2, GPT-3.5, \nGPT-4 \nKG-optimized retrieval for \nLLMs (RAG) \nOther retrieval \nmethods \nAccuracy, token \nreduction >50% \nFocus on retrieval optimization, \nnot KG construction \n[23] \n2024 \nBenchmark KB datasets \nGenerate-then-retrieve \n(LLM → logical form → \nKB retrieval) \nPrior KBQA \nmethods \nAccuracy (highest to \ndate) \nRisk of error in logical form \ngeneration \n[6] \n2024 \nPrimeKG \nLLM + KG hybrid, Cypher \nquery validation, RAG \nGPT-4 Turbo, \nLLaMA 3 \nAccuracy, \nExplainability \nDependent on KG coverage; \ncomputationally intensive \n[22] \n2025 \nPan-cancer KG (PcQA \ndataset) \nKG-enhanced reasoning \n(subgraph selection) \nSOTA KGQA \nmethods \nOutperformed SOTA on \nPcQA \nLimited to pan-cancer focus; no \nfine-tuning explored \n[26] \n2025 \nDynamic Medical KG + \nNeo4j \nLLM agents + multi-hop \nreasoning \nMEDQA, \nMedMCQA \nbaselines \nAccuracy, \nInterpretability \nHigh system complexity; requires \ncontinuous updating \n[24] \n2025 \nSynthCypher dataset \n(Neo4j + synthetic \nqueries) \nLLM-supervised Cypher \ngeneration and verification \nSPIDER \nbenchmark \nCypher accuracy 40% \nSynthetic dataset may not capture \nall real-world cases \n[29] \n2025 \nUMLS KG + Neo4j \nHybrid GraphRAG \nLLM-only QA \nAccuracy, Citation \nfidelity \nMore complex pipeline; relies on \nexternal vector store \n[30] \n2025 \nClinical data (10k \npatients, 1,894 nodes, \nNeo4j) \nKG construction using \nGPT-4 for entity/relation \nextraction \nOther KG \nconstruction \nmethods \nHigh F1-scores \nFocus only on sepsis; limited \ngeneralization \n[20] \n2025 \nLocal KG from \nbiomedical documents \nMulti-level summarization \nover KG for QA \nRAG baselines \nQA accuracy, reasoning \nTested mainly on document QA; \nscalability not proven \n \nB. Neo4j Graph Database \nTo manage the complexity and large size of biomedical \nknowledge graphs, specialized graph databases are needed. \nNeo4j is one of the most popular graph databases designed to \nstore and query data structured as nodes (entities) and \nrelationships (edges), both of which can have descriptive \nproperties [32, 33]. It uses the property graph model, which \nmakes it easy to represent complex, connected biomedical data \nsuch as drug-gene interactions or disease pathways. Neo4j’s \nCypher query language is especially advantageous because it \nallows users to write expressive and efficient queries to explore \nmulti-step connections in the data [34]. \nNeo4j works well for biomedical data because it can quickly \nrun complicated queries over highly interconnected datasets. This \nis important in biology and medicine, where relationships \nbetween entities like proteins, diseases, and drugs are often \ncomplex and layered. Studies have shown that Neo4j handles \nlarge biomedical graphs efficiently, making it a favorite among \nresearchers and industry users alike [33, 35, 36]. Its indexing and \ncaching mechanisms also help speed up query processing and \ndata retrieval [37]. \nMoreover, \nNeo4j \nintegrates \nsmoothly \nwith \nmany \nprogramming languages and analytics tools, which makes it \neasier to build interactive biomedical applications and clinical \ndecision support systems that can turn complex graph data into \nuseful insights [38, 39]. \nC. Large Language Models (LLMs) in Biomedical Question \nAnswering \nLarge Language Models (LLMs) are powerful AI systems \ntrained on vast amounts of text data. They learn the structure and \npatterns of language, enabling them to understand questions, \ngenerate responses, summarize information, and perform other \ncomplex language tasks. Well-known models such as LLaMA \nand GPT-3 have greatly advanced the field of natural language \nprocessing by showing strong performance across many tasks \n[40, 41].", "metadata": {"producer": "Microsoft® Word for Microsoft 365", "creator": "Microsoft® Word for Microsoft 365", "creationdate": "2025-10-20T16:37:36+03:00", "source": "/home/donbr/open-biosciences/biosciences-research/data/raw/JASTTytm_A+Hybrid+LLM–Knowledge+Graph_.pdf", "file_path": "/home/donbr/open-biosciences/biosciences-research/data/raw/JASTTytm_A+Hybrid+LLM–Knowledge+Graph_.pdf", "total_pages": 16, "format": "PDF 1.7", "title": "A Hybrid LLM–Knowledge Graph Framework for Accurate Biomedical Question Answering", "author": "", "subject": "", "keywords": "", "moddate": "2025-10-20T16:37:36+03:00", "trapped": "", "modDate": "D:20251020163736+03'00'", "creationDate": "D:20251020163736+03'00'", "page": 4}}
{"page_content": "Omar & Mohammed / Journal of Applied Science and Technology Trends Vol. 06, No. 02, pp. 342 –357 (2025) \n \n347 \nIn biomedical research and clinical settings, LLMs help \ntranslate natural language questions from doctors, researchers, or \npatients into precise, structured queries that can be executed on \nbiomedical knowledge graphs and databases. This makes it easier \nto retrieve detailed biomedical information like drug interactions, \ngene-disease associations, and symptom descriptions [42, 43]. \nDespite their power, LLMs can sometimes generate incorrect \nor fabricated responses, a phenomenon known as hallucination, \nwhich poses risks in sensitive biomedical contexts. These \nhallucinations occur because the models generate plausible \nanswers based on patterns learned from data rather than verified \nknowledge. To mitigate this, researchers integrate LLMs with \nbiomedical knowledge graphs to ground answers in factual data, \nsignificantly improving accuracy and reducing misinformation \n[4]. Further improvements come from fine-tuning LLMs on \nbiomedical corpora and carefully engineering prompts, which \nenhance their reliability and relevance in medical question \nanswering. \nAdditionally, combining LLMs with knowledge graphs and \nreasoning techniques is an active area of research that promises \nto increase the interpretability and trustworthiness of AI systems \nin biomedicine. These advances are critical for developing tools \nthat assist clinical decision-making and accelerate biomedical \ndiscoveries [43, 44]. \nIV. METHODS AND MATERIALS \nThis section describes the methodology used to build a \nbiomedical question-answer system. The proposed method \nconsists of three main stages; First, a biomedical knowledge \ngraph is constructed in the data ingestion phase, using structured \ndata sources (e.g., diseases, drugs, symptoms). Second, a \nlanguage model (LLaMA 3) interprets the user’s question \nwritten in English in the user interaction phase and converts it \ninto a Cypher query. Third, a graphical user interface allows \nusers to type questions and view the results interactively. \nA. System Architecture \nThe proposed framework is organized as a step-by-step \npipeline that integrates a local large language model (LLM) with \na biomedical knowledge graph stored in Neo4j. The overall \nworkflow is illustrated in Fig. 2. Each module performs a \nspecific function, and together they ensure that the system \ndelivers accurate, reliable, and explainable answers. \n \n \nFig. 2. Workflow of the LLaMA 3 and Neo4j-Based QA System \n \nStep 1. User Input (Flask Web Interface): A user submits \na biomedical question in natural language through a Flask-based \nweb interface. The same interface will later display the answer, \nthe executed query, and a compact preview of the retrieved rows. \nStep 2. LLM Processing and Initial Cypher: The text query \nis forwarded to a local LLaMA 3 module, which interprets the \nintent and drafts an initial Cypher pattern suitable for querying \nthe graph. \nStep 3. Cypher Query Generation: The Cypher Query Gen \nblock receives the initial pattern from LLaMA 3, canonicalizes \nand completes it (projection, DISTINCT, filters), and returns a \nfinalized Cypher query to the model. \nStep 4. Query execution on Neo4j: LLaMA 3 passes the \nfinalized query to the Query execution component (inside the \n’Answer modules’ box), which runs it against the Neo4j \nKnowledge Graph. KG supplies knowledge graph data (e.g.,", "metadata": {"producer": "Microsoft® Word for Microsoft 365", "creator": "Microsoft® Word for Microsoft 365", "creationdate": "2025-10-20T16:37:36+03:00", "source": "/home/donbr/open-biosciences/biosciences-research/data/raw/JASTTytm_A+Hybrid+LLM–Knowledge+Graph_.pdf", "file_path": "/home/donbr/open-biosciences/biosciences-research/data/raw/JASTTytm_A+Hybrid+LLM–Knowledge+Graph_.pdf", "total_pages": 16, "format": "PDF 1.7", "title": "A Hybrid LLM–Knowledge Graph Framework for Accurate Biomedical Question Answering", "author": "", "subject": "", "keywords": "", "moddate": "2025-10-20T16:37:36+03:00", "trapped": "", "modDate": "D:20251020163736+03'00'", "creationDate": "D:20251020163736+03'00'", "page": 5}}
{"page_content": "Omar & Mohammed / Journal of Applied Science and Technology Trends Vol. 06, No. 02, pp. 342 –357 (2025) \n \n348 \ndrugs, diseases, symptoms) and execution returns structured data \n(tuples) relevant to the question. \nStep 5. Answer Synthesis: The structured tuples flow to \nAnswer Synthesis, which aggregates and formats them into a \nconcise raw answer. This raw answer is sent back to LLaMA 3 \nto optionally refine the phrasing while preserving the retrieved \nfacts. \nStep 6. Result Presentation: LLaMA 3 produces the final \nanswer, which the interface displays together with the executed \nCypher query and an optional preview of the returned rows, \nimproving transparency and trust. \nThe pipeline couples LLM-based language understanding \n(LLaMA 3) with a schema-grounded Neo4j knowledge graph. \nThe Cypher Query Gen refines the query formulation, Query \nExecution retrieves evidence and Answer Synthesis converts \nstructured results into readable outputs that produce answers that \nare accurate, interpretable, and easy to audit directly from the \ndisplayed query and evidence. \nB. Dataset and Knowledge Graph Construction \n1) \nDataset \nThe integrated Biomedical Knowledge Hub (iBKH), a large \nbiomedical knowledge base, forms the first level of the system \nand integrates information from various curated high-quality \nbiomedical databases. This implies that the data set includes \nvarious types of entities, such as diseases, symptoms, drugs, \nbiological pathways, etc. This study used the representative \nsubset of the iBKH dataset, which contained 65828 biomedical \nentities. These entities are semantically interconnected through a \ntotal of 3004166 relationships, thus creating a rich knowledge \ngraph. The iBKH dataset was originally introduced in [11], and it \nis freely available at (https://github.com/wcm-wanglab/iBKH). \nThis dataset is the core semantic foundation upon which this \nstudy is built. The knowledge graph is populated from multiple \ntabular sources (CSV files), each listing entities or relationships. \nThe main input files and their contents are as follows: \n• Disease \nvocabulary(disease_vocab.csv): \nContains \ncolumns such as primary (a unique disease ID), name, \ndo_id (Disease Ontology ID), kegg_id, and umls_cui \n(UMLS Concept ID). Each row represents a disease node \nwith external identifiers. \n• Drug vocabulary (drug_vocab.csv): Includes primary \n(unique drug ID), name, drugbank_id, kegg_id, \npharmgkb_id, umls_cui, mesh_id, iDISK_id and CID \n(PubChem ID). Each row defines a drug node with \nstandard database identifiers. \n• Symptom vocabulary (symptom_vocab.csv): Contains \nprimary (unique symptom ID), name, mesh_id, umls_cui \nand iDISK_id. Each row defines a symptom node. \n• Side \neffect \nvocabulary \n(side_effect_vocab.csv): \nIncludes primary (unique side-effect ID) and name. Each \nrow represents a side-effect node (with UMLS ID when \navailable). \n• Pathway vocabulary (pathway_vocab.csv): Contains \nprimary (unique pathway ID), name, reactome_id, go_id, \nand kegg_id. Each row defines a biological pathway \nnode. \nRelationship files (each row typically contains two entity IDs \nand one or more boolean flags or codes) include: \n• Disease–Symptom links (Di_Sy_res.csv): Rows include \nDisease and Symptom IDs, a presence flag (1 or 0) and a \ndata source. If Present = 1, a HAS_SYMPTOM edge is \ncreated from the disease to the symptom, with properties \nfor presence and source. \n• Disease–Disease links (di_di_res.csv): Rows include \nDisease_1 and Disease_2 IDs with binary flags for is_a \nand Resemble. If is_a = 1, an (IS_A) edge is created \n(Disease_1 → Disease_2); if Resemble = 1, a \nRESEMBLES edge is created. The source field is used \nfor provenance. \n• Drug–Disease links (D_Di_res.csv): Includes Drug and \nDisease IDs with several binary flags. If a flag equals 1, \na corresponding edge is created: \no \nTREATS (Treats = 1) \no \nPALLIATES (Palliates = 1) \no \nASSOCIATED_WITH (Associate = 1) \no \nALLEVIATES_REDUCES (alleviates = 1) \no \nTREATMENT_THERAPY (treatment/therapy = \n1) \no \nINHIBITS_CELL_GROWTH \n(inhibits \ncell \ngrowth = 1) \no \nHAS_BIOMARKER (biomarkers = 1) \no \nPREVENTS_SUPPRESSES \n(prevents/suppresses = 1) \no \nROLE_IN_PATHOGENESIS (role in disease \npathogenesis = 1) \n• Drug–SideEffect links (D_SE_res.csv): Contains Drug \nand SideEffect IDs with a Source column. Each row \ncreates a CAUSES edge from the drug to the side effect, \nwith source as an edge property. \n• Drug–Drug interactions (D_D_res.csv): Rows include \nDrug_1 and Drug_2 IDs with flags for Interaction and \nResemble. If Interaction = 1, an INTERACTS_WITH \nedge is created (bidirectional). If Resemble = 1, a \nRESEMBLES edge is added. \n• Drug–Pathway links (D_Pwy_res.csv): Includes Drug \nID and Pathway ID. Each row generates an \nASSOCIATED_WITH edge from the drug to the \npathway. \n• Disease–Pathway links (Di_Pwy_res.csv): Contains \nDisease ID and Pathway ID. Each row creates an \nASSOCIATED_WITH edge from the disease to the \npathway. \n2) \nData Upload Performance \nThe time required to upload different types of entities and \nrelationships into the Neo4j biomedical knowledge graph, \nmeasured in seconds. These measurements reflect both the size \nand complexity of the data being processed. \nAs shown in Table II, the longest upload time is for Drug-\nDrug Relationships, which takes approximately 190 seconds due \nto the large number of edges (over 3 million). Following this, \nDisease-Disease and Drug-Disease Relationships also require \nconsiderable time for loading. On the other hand, individual", "metadata": {"producer": "Microsoft® Word for Microsoft 365", "creator": "Microsoft® Word for Microsoft 365", "creationdate": "2025-10-20T16:37:36+03:00", "source": "/home/donbr/open-biosciences/biosciences-research/data/raw/JASTTytm_A+Hybrid+LLM–Knowledge+Graph_.pdf", "file_path": "/home/donbr/open-biosciences/biosciences-research/data/raw/JASTTytm_A+Hybrid+LLM–Knowledge+Graph_.pdf", "total_pages": 16, "format": "PDF 1.7", "title": "A Hybrid LLM–Knowledge Graph Framework for Accurate Biomedical Question Answering", "author": "", "subject": "", "keywords": "", "moddate": "2025-10-20T16:37:36+03:00", "trapped": "", "modDate": "D:20251020163736+03'00'", "creationDate": "D:20251020163736+03'00'", "page": 6}}
{"page_content": "Omar & Mohammed / Journal of Applied Science and Technology Trends Vol. 06, No. 02, pp. 342 –357 (2025) \n \n349 \nentities such as Diseases and Drugs are uploaded much faster, \ngenerally under 2 seconds. \nTABLE II.  DATA UPLOAD TIMES FOR DIFFERENT ENTITY AND RELATIONSHIP \nTYPES IN NEO4J \nEntity / Relationship Type \nUpload Time (seconds) \nDisease \n0.81 \nDrugs \n1.08 \nSymptoms \n0.06 \nSide Effects \n0.14 \nPathways \n0.08 \nDisease-Disease Relationships \n30.97 \nDrug-Disease Relationships \n30.28 \nDrug-SideEffect Relationships \n5.24 \nDrug-Drug Relationships \n190.09 \nDrug-Pathway Relationships \n0.14 \nDisease-Pathway Relationships \n0.06 \nDisease-Symptom Relationships \n0.12 \n \nFig. 3, presents a vertical bar chart that visually compares \nthese upload times across the different entity and relationship \ntypes. The chart clearly shows the significant difference in upload \nduration between nodes and edges, emphasizing the higher cost \nof ingesting complex relationships in the graph. \n3) \nExperimental Environment \nIn this study, the proposed biomedical question answering \nsystem was evaluated using a locally hosted environment. All \nexperiments were conducted on a Windows 11 Pro (64-bit) \nsystem equipped with an Intel Core i5-10500H processor running \nat 2.50 GHz (12 logical CPUs), 24 GB of RAM, and an NVIDIA \nGeForce GTX 1650 GPU with Max-Q Design. The Neo4j graph \ndatabase (v4.4.5) was managed through Neo4j Desktop (v1.6.2), \nand the LLaMA 3B language model was executed locally using \noptimized configurations suitable for this hardware setup. \n \nEach Cypher query generated by the system was executed \nmultiple times to calculate an average response time, ensuring \nconsistency across varying levels of question difficulty. The \nknowledge graph was constructed using the iBKH dataset, and \ndata loading and system performance were carefully monitored \nto maintain stability during testing. This experimental setup \nprovides a reliable and reproducible environment for \nbenchmarking the effectiveness and responsiveness of the hybrid \nQA system. \n \nFig. 3. Upload times for various biomedical entities and relationships in Neo4j. \n \nWe run LLaMA 3 locally (not via cloud APIs) to satisfy \nbiomedical privacy/governance (no data leaves the host) and to \nmaximize reproducibility (fixed GGUF checkpoint, pinned \nllama.cpp commit, controlled seeds settings, constant hardware). \nLocal execution yields predictable cost availability and stable \nlatency (no network jitter) and lets us enforce executable Cypher \ngrounding with per edge provenance and timestamp aware \nexecution. \n4) \nKnowledge Graph(KG) Construction \nThe Neo4j graph database was used as the backend to store \nand query the KG, and it is a graph database designed for highly \nconnected data. Before loading data, uniqueness constraints were \ncreated on the primary property for each node label (Disease, \nDrug, Symptom, SideE ffect, Pathway). This enforces that each \nprimary ID appears only once, preventing duplicate entities. For \nefficient lookups in queries, a search index was created based on \nthe name property of each node label. As noted in the Neo4j", "metadata": {"producer": "Microsoft® Word for Microsoft 365", "creator": "Microsoft® Word for Microsoft 365", "creationdate": "2025-10-20T16:37:36+03:00", "source": "/home/donbr/open-biosciences/biosciences-research/data/raw/JASTTytm_A+Hybrid+LLM–Knowledge+Graph_.pdf", "file_path": "/home/donbr/open-biosciences/biosciences-research/data/raw/JASTTytm_A+Hybrid+LLM–Knowledge+Graph_.pdf", "total_pages": 16, "format": "PDF 1.7", "title": "A Hybrid LLM–Knowledge Graph Framework for Accurate Biomedical Question Answering", "author": "", "subject": "", "keywords": "", "moddate": "2025-10-20T16:37:36+03:00", "trapped": "", "modDate": "D:20251020163736+03'00'", "creationDate": "D:20251020163736+03'00'", "page": 7}}
{"page_content": "Omar & Mohammed / Journal of Applied Science and Technology Trends Vol. 06, No. 02, pp. 342 –357 (2025) \n \n350 \ndocumentation, indexes “enable quicker and more efficient \npattern matching” by allowing the query planner to rapidly locate \nnodes by label and property. \nWith the schema in place, data was imported using Cypher’s \nLOAD CSV commands. For each vocabulary file, rows with \nnonnull primary IDs were processed: the code used MERGE to \ncreate (or match existing) nodes with the given label and primary \nproperty, and then SET the remaining attributes from the CSV \ncolumns. For example, in importing disease_vocab.csv, each row \nproduced a node (:Disease primary: <id>) with properties name, \ndo_id, kegg_id, and umls_cui set from the row (empty strings \nwere turned into null). Similarly, drug_vocab.csv rows produced \ndrug nodes with properties for DrugBank, KEGG, PharmGKB \nIDs, etc. This approach follows best practice: using MERGE on \nthe unique ID ensures that nodes are not duplicated during \nmultiple passes. \nAfter all nodes were created, the relationship CSVs were \nloaded. Each row in those files was matched to the corresponding \nsource and target nodes by their primary IDs, and conditional \nlogic was used to create edges. For example, the disease-\nsymptom file (Di_Sy_res.csv) was processed by matching a \ndisease node and a symptom node for each row, then executing \nMERGE (d)-[r:HAS_SYMPTOM]-(s) if the present column is \nnonzero; the edge was given a present property and a source \nproperty from the CSV. The disease-disease file (di_di_res.csv) \nwas processed by matching disease nodes d1 and d2:If is_a = 1, \na (:Disease)-[:IS_A]->(:Disease) edge was merged; if Resemble \n= 1, a (:Disease)-[:RESEMBLES]->(:Disease) edge was merged. \nDrug-disease relationships were handled similarly: the script \nmatched a Drug node and a Disease node for each row of \nD_Di_res.csv, then for each flag that was 1, it merged the \ncorresponding edge label (such as TREATS, PALLIATES, \nASSOCIATED_WITH, etc.) from the drug to the disease. Drug–\nside-effect rows produced (:Drug)-[:CAUSES]->(:SideEffect) \nedges with the source noted, and drug–drug rows produced either \nINTERACTS_WITH or RESEMBLES edges between matched \ndrug pairs. Finally, the drug-pathway and disease-pathway files \neach produced:ASSOCIATED_WITH edges linking drugs or \ndiseases to pathway nodes. \nIn this graph model, most relationships are directional (for \nexample, a drug TREATS a disease; a disease does not TREAT \na drug). This follows common practice in biomedical KGs. The \nsame relations (such as RESEMBLES or INTERACTS_WITH) \nare inherently symmetric, but were stored as two directed edges \nor one undirected edge depending on implementation. All \nrelationship creation steps used Cypher’s MERGE so that \nrepeated loads or out-of-order imports did not create duplicate \nedges. \nThis study used a static KG snapshot for reproducibility, but \nthe system also supports incremental updates through the Neo4j \nMERGE and batch import functions. A summary of the different \ntypes of nodes and relationships is provided, together with their \ncounts, descriptions, and examples in Table III.\n \nTABLE III.  DETAILED SUMMARY OF KNOWLEDGE GRAPH COMPONENTS IN IBKH SUBSET \nComponent Type \nEntity/Relationship \nCount \nDescription \nExamples \n \nDisease \n19,236 \nMedical conditions and disorders \nDiabetes, Hypertension, Cancer \n \nDrug \n37,997 \nPharmaceutical compounds \nAspirin, Insulin, Amoxicillin \nNodes \nSymptom \n1,361 \nClinical signs and symptoms \nHeadache, Fever, Nausea \n \nSideEffect \n4,251 \nNegative outcomes of drugs \nNausea, Drowsiness \n \nPathway \n2,983 \nBiological processes and pathways \nApoptosis, Glycolysis \n \nASSOCIATED_WITH \n101,534 \nGeneral associations (disease–\npathway, drug–pathway, etc.) \nDiabetes ASSOCIATED_WITH Pathway \n \nCAUSES \n145,321 \nDrug-SideEffect relationships \nAspirin CAUSES Gastric_Bleeding \n \nHAS_SYMPTOM \n3,357 \nDisease-symptom links \nCOVID-19 HAS_SYMPTOM Fever \n \nHAS_BIOMARKER \n412 \nBiomarkers linked to disease \nPSA HAS_BIOMARKER Prostate_Cancer \nRelationships \nINHIBITS_CELL_GROWTH \n1,508 \nDrugs inhibiting cell growth \nChemo INHIBITS_CELL_GROWTH Tumor \n \nINTERACTS_WITH \n2,682,142 \nDrug-drug interactions \nAspirin INTERACTS_WITH Warfarin \n \nIS_A \n10,529 \nSubtype hierarchy \nFlu IS_A Viral_Infection \n \nPALLIATES \n388 \nDrug palliates disease \nMorphine PALLIATES Cancer \n \nPREVENTS_SUPPRESSES \n859 \nPreventive links \nVaccine PREVENTS_SUPPRESSES Measles \n \nRESEMBLES \n7,000 \nSimilarity relationships \nDrugA RESEMBLES DrugB \n \nTREATMENT_THERAPY \n44,852 \nTherapy relationships \nRadiotherapy TREATMENT_THERAPY Tumor \n \nTREATS \n5,491 \nDrug-disease links \nInsulin TREATS Diabetes \n \nALLEVIATES_REDUCES \n∼180,000 \nSymptom relief \nParacetamol ALLEVIATES_REDUCES Fever \nTotal \nNodes \n65,828 \nTotal biomedical entities \n— \n \nRelationships \n3,004,166 \nTotal knowledge graph links \n— \nC. Natural Language to Cypher Query Translation \nA key feature of the system is its ability to accept questions \nwritten in plain English and automatically generate the \ncorresponding Cypher queries. This is accomplished using \nMeta’s LLaMA 3 large language model, which runs entirely on \na local machine through the open-source llama.cpp framework. \nRunning the model locally ensures low-latency execution and \nkeeps sensitive queries within the user’s environment. \nTo generate a Cypher query, LLaMA 3 is prompted with \nexamples of natural language questions along with their correct \nCypher translations. The prompt also includes instructions on \nhow to navigate the structure of the graph schema. When a user \nenters a question (e.g., ’What are the symptoms of Alzheimer’s \ndisease?’), the system inserts it into the prompt and asks LLaMA \n3 to produce a corresponding query. For example, the model may \nreturn: \nMATCH (d:Disease)-[:HAS_SYMPTOM]->(s:Symptom) \nWHERE toLower(d.name) CONTAINS \"alzheimer\" \nRETURN s.name", "metadata": {"producer": "Microsoft® Word for Microsoft 365", "creator": "Microsoft® Word for Microsoft 365", "creationdate": "2025-10-20T16:37:36+03:00", "source": "/home/donbr/open-biosciences/biosciences-research/data/raw/JASTTytm_A+Hybrid+LLM–Knowledge+Graph_.pdf", "file_path": "/home/donbr/open-biosciences/biosciences-research/data/raw/JASTTytm_A+Hybrid+LLM–Knowledge+Graph_.pdf", "total_pages": 16, "format": "PDF 1.7", "title": "A Hybrid LLM–Knowledge Graph Framework for Accurate Biomedical Question Answering", "author": "", "subject": "", "keywords": "", "moddate": "2025-10-20T16:37:36+03:00", "trapped": "", "modDate": "D:20251020163736+03'00'", "creationDate": "D:20251020163736+03'00'", "page": 8}}
{"page_content": "Omar & Mohammed / Journal of Applied Science and Technology Trends Vol. 06, No. 02, pp. 342 –357 (2025) \n \n351 \nThis query searches for a disease node whose name contains \n’alzheimer’ and follows HAS_SYMPTOM edges to list related \nsymptom names. The system then executes this cypher to retrieve \nanswers. The prompts (such as few-shot examples and schema \nhints) were carefully designed to help LLaMA 3 generate correct \nCypher queries. The model learns how to use the graph’s labels \nand relationships properly. For example, if a user asks, ’Which \ndrugs treat diabetes?’, LLaMA might respond with a suitable \nCypher query: \n \nMATCH (d:Drug)-[:TREATS]->(di:Disease) \nWHERE toLower(di.name) CONTAINS \"diabetes\" \nRETURN d.name  \n \nThis queries for drug nodes that have a TREATS edge to a \ndiabetes disease node. By leveraging LLaMA 3 in this way, our \nsystem can flexibly handle many phrasing variations without \nmanual mapping rules. \nD. Model Configuration & Decoding \nWe run a local LLaMA 3.2-3B model in GGUF format (llama-\n3.2-3b-instruct-q4_k_m.gguf) via llama.cpp, as shown in Table \nIV. \nTABLE IV.  MODEL RUNTIME AND DECODING SETTINGS \nRuntime settings \nDecoding settings \nn_ctx = 1024 \ntemperature = 0.2 \nn_threads = 12 \ntop_p = 0.95 \nn_gpu_layers = 33 \ntop_k = 40 \nn_batch = 512 \nrepeat_penalty = 1.1 \n \nmax_tokens = 80 \n \nseed = 42 \n \nE. Graph Subset and Versioning \nWe use an iBKH derived subgraph (≈65.8k nodes; ≈3.0M \nedges) spanning DRUG, DISEASE, SYMPTOM, PATHWAY. IDs are \nnormalized to CURIEs and duplicates collapsed across \nUMLS/DrugBank/DisGeNET/SIDER/KEGG. Each edge stores \nprovenance/licensing metadata (source, source_version, license, \nretrieved_at, \nevidence_pmids/urls) \nand, \nwhen \navailable, \ntemporal fields (valid_from, valid_to). We report coverage as \nthe \npercentage \nof \nevaluated \nquestions \nwhose \ngold \nentities/relations are present. \nF. Query Execution and Reliability \nAfter a Cypher query is generated, it is executed on the Neo4j \ndatabase through the official Neo4j Python driver, which \nmanages the secure connection and returns the results. Instead of \nrestricting the output with a fixed LIMIT (e.g., LIMIT 5), the \nsystem retrieves candidate results and evaluates them using \nstandardized retrieval metrics such as Hits@1, Hits@5, and \nHits@10. This approach ensures that the system remains \nresponsive while providing a fair assessment of ranking quality \nacross different cutoff levels, rather than depending solely on a \nfixed number of returned items. Neo4j’s indexing on key node \nproperties, such as name and primary identifiers, also helps speed \nup lookups as the knowledge graph grows. In cases where the \nlanguage model generates an incomplete or incorrect query, such \nas referencing nodes or relationships that do not exist, the system \ncatches the error and either retries with a simpler prompt or \ninforms the user. Together, these steps make sure that queries run \nquickly, return valid results, and keep the overall experience \nsmooth and reliable for biomedical question-answering. \nG. User Interface for Query Transparency \nThe system includes a lightweight, cross-platform graphical \nuser interface (GUI) implemented as a web application using the \nFlask framework in Python, with HTML and Bootstrap for \ninteractive visualization. The interface is designed to make the \nquestion-answering process transparent and accessible to users \nwithout technical expertise. It consists of three main panels: \n1. Input Panel: Where the user can enter a biomedical \nquestion in natural language. \n2. Query Panel: Which displays the Cypher query generated \nby the language model, allowing users to verify how their \nquestion was interpreted. \n3. Results Panel: Which presents the retrieved answers in a \nclear, readable format, accompanied by a brief natural \nlanguage explanation generated by the system. \nBy showing both the query and the answer, the GUI promotes \nuser trust and enables validation of the system’s reasoning \nprocess. The interface is lightweight enough to run smoothly on \nstandard desktop machines without additional dependencies, \nmaking it practical for local deployments in clinical or research \nsettings. Fig. 4 illustrates the overall layout. \nV. EXPERIMENTAL RESULTS \nIn this section, the proposed system is evaluated based on its \nability to translate natural-language biomedical questions into \nexecutable Cypher queries over the iBKH knowledge graph. The \nassessment is conducted end-to-end and focuses on three main \naspects: (i) the accuracy of query generation, measured by the \nproportion of correctly produced Cypher queries; (ii) system \nefficiency, quantified through total response time from question \nsubmission to final answer delivery, covering both query \ngeneration and execution; and (iii) the quality of retrieved \nanswers, evaluated using standardized information retrieval \nmetrics including Precision@k, Recall@k, F1@k, Hits@k, and \nMean Reciprocal Rank (MRR). For clarity, all these metrics are \nformally defined in the following subsections, with their \ncorresponding equations, and will be used consistently \nthroughout the results section. Together, these dimensions \nprovide a comprehensive view of both correctness and efficiency \nacross simple, medium, and complex biomedical queries. \nA. Description of the Experimental Data \nTo evaluate the proposed system, this work used a carefully \nselected set of biomedical questions designed to test how well the \nsystem understands natural language and converts it into Cypher \nqueries for a biomedical knowledge graph. \nTo evaluate the system, a benchmark of 60 biomedical \nquestions was constructed by the authors, guided by the schema \nand relations in iBKH. The questions were grouped into simple, \nmedium, and complex levels to assess performance across \ndifferent reasoning requirements. Gold-standard answers were \nmanually prepared for each question to enable quantitative \nevaluation using Exact Match, Precision@k, Recall@k, F1@k, \nHits@k, and MRR. The complete set of 60 questions is available \nat this link https://drive.google.com/drive/my-drive.", "metadata": {"producer": "Microsoft® Word for Microsoft 365", "creator": "Microsoft® Word for Microsoft 365", "creationdate": "2025-10-20T16:37:36+03:00", "source": "/home/donbr/open-biosciences/biosciences-research/data/raw/JASTTytm_A+Hybrid+LLM–Knowledge+Graph_.pdf", "file_path": "/home/donbr/open-biosciences/biosciences-research/data/raw/JASTTytm_A+Hybrid+LLM–Knowledge+Graph_.pdf", "total_pages": 16, "format": "PDF 1.7", "title": "A Hybrid LLM–Knowledge Graph Framework for Accurate Biomedical Question Answering", "author": "", "subject": "", "keywords": "", "moddate": "2025-10-20T16:37:36+03:00", "trapped": "", "modDate": "D:20251020163736+03'00'", "creationDate": "D:20251020163736+03'00'", "page": 9}}
{"page_content": "Omar & Mohammed / Journal of Applied Science and Technology Trends Vol. 06, No. 02, pp. 342 –357 (2025) \n \n352 \nThe dataset contains 60 questions divided into three difficulty \nlevels based on how complex the language is and how deep the \nbiomedical reasoning needs to be: \n• Level 1: 25 simple questions focusing mostly on easy-to-\nunderstand facts, such as symptoms of certain diseases or \ndrugs used to treat specific conditions. \n• Level 2: 20 medium-level questions that involve more \ndetailed relationships, such as drug interactions and \nSideEffect. \n• Level 3: 15 hard questions requiring multi-step reasoning \nacross multiple biomedical concepts or biological \npathways, similar to the complexity found in real clinical \ncases. \nThe set of evaluation questions was designed to span a wide \nrange of common biomedical topics and to reflect clinically \nrelevant query types reported in prior literature. Each question is \npaired with its corresponding gold standard cypher query and \ncategorized by difficulty level, as summarized in Table V, where \nthree illustrative examples are shown. The questions were derived \nfrom publicly available biomedical QA benchmarks and adapted \nfrom established knowledge bases (e.g., iBKH schema relations), \nensuring both coverage and diversity across diseases, drugs, \nsymptoms, and pathways.\nTABLE V.  QUESTION DIFFICULTY LEVELS AND SAMPLE CYPHER QUERIES \nLevel \nDefinition \nExample \nCypher Query \n1 \nSingle-hop question using \none relationship \nWhat are the symptoms of \nAlzheimer? \nMATCH (d:Disease)-[:HAS_SYMPTOM]->(s:Symptom) \nWHERE toLower(d.name) CONTAINS 'alzheimer' \nRETURN s.name AS symptom \n \n2 \nQuestions requiring one \nrelationship \nWhat are the side effects of \ndrugs used to treat asthma? \nWITH toLower('asthma') AS disease_name \nMATCH (d:Disease)<-[:TREATS]-(dr:Drug) \nWHERE toLower(d.name) CONTAINS disease_name \nMATCH (dr:Drug)-[:CAUSES]->(se:SideEffect) \nRETURN DISTINCT se.name AS side_effect, dr.name AS drug \n \n3 \nQuestions requiring two or \nmore relationships \nWhat cholesterol medications \ncause side effects, and what are \nsome of those effects? \nWITH toLower('cholesterol') AS disease_name \nMATCH (d:Disease)<-[:TREATS]-(dr:Drug) \nWHERE toLower(d.name) CONTAINS disease_name \nMATCH (dr:Drug)-[:CAUSES]->(se:SideEffect) \nRETURN DISTINCT dr.name AS drug, se.name AS side_effect \n \n \n \nFig. 4. Graphical User Interface of the Biomedical Knowledge Explorer System \nB. Quantitative Evaluation \nThe performance of the proposed system is evaluated on the \niBKH knowledge graph using Exact Match, Precision@k, \nRecall@k, F1@k, Hits@k, MRR, and total response time across \nsimple, medium, and complex biomedical queries. These metrics \nwere measured for each difficulty level (simple, medium, and \ncomplex) to see how performance changes as questions become \nmore challenging.\n \nTABLE VI.  QUERY GENERATION EXACT MATCH BY DIFFICULTY LEVEL", "metadata": {"producer": "Microsoft® Word for Microsoft 365", "creator": "Microsoft® Word for Microsoft 365", "creationdate": "2025-10-20T16:37:36+03:00", "source": "/home/donbr/open-biosciences/biosciences-research/data/raw/JASTTytm_A+Hybrid+LLM–Knowledge+Graph_.pdf", "file_path": "/home/donbr/open-biosciences/biosciences-research/data/raw/JASTTytm_A+Hybrid+LLM–Knowledge+Graph_.pdf", "total_pages": 16, "format": "PDF 1.7", "title": "A Hybrid LLM–Knowledge Graph Framework for Accurate Biomedical Question Answering", "author": "", "subject": "", "keywords": "", "moddate": "2025-10-20T16:37:36+03:00", "trapped": "", "modDate": "D:20251020163736+03'00'", "creationDate": "D:20251020163736+03'00'", "page": 10}}
{"page_content": "Omar & Mohammed / Journal of Applied Science and Technology Trends Vol. 06, No. 02, pp. 342 –357 (2025) \n \n353 \nDifficulty Level \nTotal Questions \nCorrect Queries \nCypher Exact Match (EM) (%) \nSimple \n25 \n24 \n96% \nMedium \n20 \n19 \n95% \nComplex \n15 \n13 \n86.7% \navg \n \n \n92.6% \n \nTo better understand the quality of the responses returned by \nthe system, this work examined real examples. One of the test \nquestions was \"What drugs are used to treat breast cancer?\" As \nshown in Fig. 4, the system was able to understand the question, \ngenerate the correct Cypher query using the TREATS \nrelationship, and retrieve the right information from the \nbiomedical knowledge graph. It returned a list of five drugs, \nincluding Leuprolide, Goserelin, and Trastuzumab, which are \nknown treatments for breast cancer. This result shows that the \nsystem is able to connect the question to the right part of the graph \nand provide accurate and helpful answers, even for medical \nquestions that involve specific treatments. \n1) \nExact Match of Query Generation \nAs shown in Table VI, how often the system generated the \ncorrect query for each difficulty level. Here, accuracy is defined \nas the percentage of questions for which the system’s generated \nCypher query matched the expected query. It is calculated using \nthe Eq. (1)     \n \nExact Match (%) = Correct Queries\nTotal Questions × 100 \n(1) \n \n \nThese findings highlight the purpose of this experiment to test \nwhether the framework can reliably map natural language to \nCypher across varying levels of complexity. The graceful drop \nfrom 96% on simple to 86.7% on complex queries indicates that \nthe system is robust for straightforward questions but still \nchallenged by multi-hop reasoning. This points to clear \nopportunities for improvement, such as synonym expansion, \nconstrained decoding, or enhanced error handling for multi-step \nqueries. \n2) \nLatency \nTable VII reports the average latency per difficulty level and \ndecomposes it into query generation and Neo4j execution. The \ntotal response time is computed as in Eq. (2). Execution time is \neffectively constant across all levels ( ≈0.04–0.05  s), so \nvariation in total latency is dominated by query generation. As \ndifficulty increases, the mean total time rises from 5.12  s \n(simple) to 5.75 s (medium) and 7.35 s (complex). Dispersion \n(Std. Dev.) grows with task complexity 0.72 s (simple), 0.32 s \n(medium), 2.09 s (complex) reflecting more variable planning \nand reasoning needed to assemble correct Cypher for harder \nquestions. Pooled over all questions, the overall mean is 6.07 s \nwith an overall SD of 1.38 s, keeping latencies within single-digit \nseconds and practical for interactive, real-world use. \n \n𝑇total = 𝑇gen + 𝑇exec   \n(2)     \nStandard Deviation (SD). Unless otherwise stated, SD is the \nunbiased sample standard deviation computed over per-question \ntotal times within each difficulty group 𝐺 with 𝑛𝐺 questions: \n \n𝑇‾𝐺= 1\n𝑛𝐺\n∑𝑇total\n(𝑖)\n𝑛𝐺\n𝑖=1\n,\n𝑠𝐺= √\n1\n 𝑛𝐺−1  ∑\n𝑛𝐺\n𝑖=1\n(𝑇total\n(𝑖) −𝑇‾𝐺)\n2 .\n \n \n(3)   \n   \n    (4)    \n \nOverall values are computed by pooling all questions across \nlevels, with 𝑁= ∑𝑛𝐺\n𝐺\n: \n \n𝑇‾overall = 1\n𝑁∑𝑇total\n(𝑖)\n𝑁\n𝑖=1\n,\n𝑠overall = √\n1\n 𝑁−1  ∑\n𝑁\n𝑖=1\n(𝑇total\n(𝑖) −𝑇‾overall)\n2 .\n \n \n(5)   \n   \n       \n(6) \n \n \nTABLE VII.  AVERAGE LATENCY (S) BY DIFFICULTY LEVEL (SD = STANDARD \nDEVIATION) \nDifficulty \nLevel \nQuery \nGeneration (s) \nQuery \nExecution (s) \nTotal \nTime (s) \nStd. \nDev. (s) \nSimple \n5.09 \n0.03 \n5.12 \n0.72 \nMedium \n5.69 \n0.01 \n5.75 \n0.32 \nComplex \n6.94 \n0.4 \n7.35 \n2.09 \nOverall \n5.9 \n0.15 \n6.07 \n1.38 \n \n \n \n \n \n3) \nAnswer-Level Evaluation Metrics \nAs shown in Table VIII, the quality of the returned items is \nevaluated using five standardized metrics: Precision@k, \nRecall@k, F1@k, Hits@k, and MRR. Precision@k measures the \nproportion of correct answers among the top-𝑘 retrieved items \nEq. (7), while Recall@k quantifies the fraction of gold-standard \nanswers covered within the top-𝑘 results Eq. (8). F1@k combines \nboth aspects through the harmonic mean Eq. (9). Hits@k reports \nwhether at least one correct answer appears in the top-𝑘 Eq. \n(10, 11), and MRR captures how early the first correct answer is \nretrieved in the ranking Eq. (12). Together, these metrics provide \na comprehensive view of both the correctness and completeness \nof retrieval, as well as the ranking quality across simple, medium \nand complex queries. \n \nPrecision@𝑘= |{ relevant ∩retrieved@𝑘 }|\n𝑘\n \n(7)   \n   \nPrecision@k measures the fraction of the top-𝑘 retrieved \nitems that are correct (i.e., appear in the gold set). For example,", "metadata": {"producer": "Microsoft® Word for Microsoft 365", "creator": "Microsoft® Word for Microsoft 365", "creationdate": "2025-10-20T16:37:36+03:00", "source": "/home/donbr/open-biosciences/biosciences-research/data/raw/JASTTytm_A+Hybrid+LLM–Knowledge+Graph_.pdf", "file_path": "/home/donbr/open-biosciences/biosciences-research/data/raw/JASTTytm_A+Hybrid+LLM–Knowledge+Graph_.pdf", "total_pages": 16, "format": "PDF 1.7", "title": "A Hybrid LLM–Knowledge Graph Framework for Accurate Biomedical Question Answering", "author": "", "subject": "", "keywords": "", "moddate": "2025-10-20T16:37:36+03:00", "trapped": "", "modDate": "D:20251020163736+03'00'", "creationDate": "D:20251020163736+03'00'", "page": 11}}
{"page_content": "Omar & Mohammed / Journal of Applied Science and Technology Trends Vol. 06, No. 02, pp. 342 –357 (2025) \n \n354 \nP@1 refers to the accuracy of the very first retrieved item, P@5 \nevaluates correctness within the top three results, and P@10 \nconsiders the top ten. Higher values indicate that relevant items \ntend to appear early in the ranked list. \nTABLE VIII.  EVALUATION METRICS ACROSS LEVELS \n \nSimple \nMedium \nComplex \nOverall \nP@1 \n100 \n95 \n86.67 \n93.8 \nP@5 \n100 \n95 \n93.33 \n96.11 \nP@10 \n100 \n95 \n88 \n94.33 \nR@1 \n88 \n90 \n86.67 \n88.22 \nR@5 \n90.65 \n89.67 \n86.67 \n88.9 \nR@10 \n89.60 \n89 \n83.33 \n87.31 \nF1@1 \n88 \n90 \n86.67 \n88.22 \nF1@5 \n93.20 \n91.25 \n88.67 \n91.03 \nF1@10 \n93.28 \n91.39 \n85.24 \n89.97 \nHits@k \n100 \n95 \n93 \n96.1 \nMRR \n100 \n95 \n88 \n94.4 \n \nRecall@𝑘=\n|{ relevant∩retrieved@𝑘 }|\n|{ relevant }|\n  \n(8)   \nRecall@k measures the proportion of all relevant items (in the \ngold set) that are successfully retrieved within the top- 𝑘 \npositions. For instance, R@5 indicates how many of the expected \nanswers are covered by the top three results. This metric is \nparticularly important when the gold answer set is larger than 𝑘. \n \nF1@𝑘= 2 ⋅Precision@𝑘⋅Recall@𝑘\nPrecision@𝑘+ Recall@𝑘 \n(9)   \nF1@k is the harmonic mean of Precision@k and Recall@k. \nIt balances the trade-off between retrieving only relevant items \n(precision) and covering as many relevant items as possible \n(recall). For example, F1@10 reflects the combined quality of the \nsystem when retrieving the top ten results. \n \nHits@𝑘= {1\nif { relevant ∩retrieved@𝑘 } ≠⌀\n0\notherwise\n \n(10)   \n   \nHits@k is a binary measure at the query level: it equals 1 if at \nleast one correct answer is present among the top-𝑘 retrieved \nitems, and 0 otherwise. For example, Hits@5 reports the \npercentage of queries where the system was able to “hit” at least \none correct answer in the top five results. \n \nHits@𝑘= 1\n|𝑄| ∑𝟏\n𝑞∈𝑄\n({ relevant𝑞\n∩retrieved𝑞@𝑘 } ≠⌀) \n \n         (11)   \nWhen averaged across a set of queries 𝑄, Hits@k gives the \nproportion of queries for which at least one relevant item is \nreturned in the top-𝑘 results. This measure is less sensitive to \nranking quality but emphasizes coverage across queries. \n \nMRR = 1\n|𝑄| ∑\n1\nrank𝑞\n𝑞∈𝑄\n \n(12)   \nMean Reciprocal Rank (MRR) averages the reciprocal of the \nrank of the first correct answer for each query 𝑞∈𝑄. For \nexample, if the first correct answer appears in the top position, \nthe reciprocal rank is 1/1 = 1.0; if it appears at rank 5, the \ncontribution is 1/5 = 0.2. MRR therefore rewards systems that \nretrieve correct answers as early as possible. \nAccording to the results in Table VIII, this work reports \nstandardized metrics, including Precision, Recall, F1, Hits@k, \nand MRR at different cutoffs (𝑘= 1,5,10). Precision@k captures \nthe proportion of retrieved items among the top-𝑘 that are correct, \nwhile Recall@k measures the coverage of gold-standard answers \nwithin the same cutoff. F1@k balances both aspects. Hits@k \nreflects whether at least one correct answer appears in the top-𝑘, \nand MRR evaluates how early the first correct answer is retrieved. \nFor simple questions ( 𝑁= 25 ), the performance is \nconsistently near perfect, with P @ 10 = 100%, R @ 10 (89. 6%) \nand F1 @ 10(93. 3%), along with Hits @ 10 = 100% and MRR = \n100%. For medium questions (𝑁= 20), P @ 10 = 95%, R @ 10 \n(89%) and F1 @ 10 (91. 4%), with a strong ranking quality \nreflected in Hits@10 = 95% and MRR = 95%. For complex \nqueries (𝑁= 15), the performance remains robust but slightly \nlower, with P @ 10 (93. 3%), R @ 10 (88%) and F1 @ 10 (90. \n2%), alongside Hits @ 10 (93. 3%) and MRR 93. 3%. \nIn general, at all levels, the system achieves P @ 10 (96. 1%), \nR @ 10 (88. 9%) and F1 @ 10 (91. 6%), with Hits @ 10 (96. 1%) \nand MRR 96. 1%. These results indicate that the system not only \nretrieves nearly all expected answers but also ranks them highly, \nensuring both completeness and correctness. This level of \nreliability is particularly valuable in biomedical applications \nwhere precision and trustworthiness are critical. \nThe main purpose of this experiment was to assess not only \nwhether queries execute, but whether the returned results are both \ncorrect and complete. The precision consistently above 95% \nconfirms that almost all the items retrieved are clinically valid, \nwhile Hits@5 near 95% shows that the system usually returns \nclose to the expected five answers per question. Together, these \nmetrics demonstrate that knowledge graph grounding effectively \nminimizes hallucinations and ensures trustworthy biomedical \noutput. \nWe evaluated performance differences across difficulty levels \nusing a two-sample t-test, which revealed statistically significant \ndifferences. Error analysis indicates that failures in complex \nqueries are mainly due to missing relation hops, whereas errors \nin medium-level queries are mostly caused by syntax \nmismatches. These findings highlight the challenges associated \nwith query complexity and provide insights for targeted \nimprovements. \nC. Qualitative Evaluation \nIn addition to quantitative metrics, the system’s outputs were \nevaluated for contextual accuracy and alignment with the \nstructure of the knowledge graph. Two plain English questions \nwere selected and for each, the generated Cypher query, the \nNeo4j output, and the visual graph representation were reviewed \nto verify that the answers reflected valid relationships in the \ngraph.", "metadata": {"producer": "Microsoft® Word for Microsoft 365", "creator": "Microsoft® Word for Microsoft 365", "creationdate": "2025-10-20T16:37:36+03:00", "source": "/home/donbr/open-biosciences/biosciences-research/data/raw/JASTTytm_A+Hybrid+LLM–Knowledge+Graph_.pdf", "file_path": "/home/donbr/open-biosciences/biosciences-research/data/raw/JASTTytm_A+Hybrid+LLM–Knowledge+Graph_.pdf", "total_pages": 16, "format": "PDF 1.7", "title": "A Hybrid LLM–Knowledge Graph Framework for Accurate Biomedical Question Answering", "author": "", "subject": "", "keywords": "", "moddate": "2025-10-20T16:37:36+03:00", "trapped": "", "modDate": "D:20251020163736+03'00'", "creationDate": "D:20251020163736+03'00'", "page": 12}}
{"page_content": "Omar & Mohammed / Journal of Applied Science and Technology Trends Vol. 06, No. 02, pp. 342 –357 (2025) \n \n355 \nFor the question ’What are the symptoms of brain cancer?’ \nThe system generated a Cypher query that correctly followed the \nHAS_SYMPTOM relationship from disease nodes to symptoms \nnodes, filtering by the specified disease name. The results \nretrieved included terms such as aphasia (Broca, Wernicke), \nanomia, agraphia, agnosia, amnesia (including retrograde \namnesia), olfaction disorders, and apnea symptoms consistent \nwith established neurological manifestations of brain tumors. In \nNeo4j, the data formed a clear center-and-spoke pattern, with \nbrain cancer at the center and its associated nodes of symptoms \nradiating outward, as shown in Fig. 5. \nCypher: \nWITH toLower(\"brain cancer\") AS disease_name \nMATCH (d:Disease)-[:HAS_SYMPTOM]->(s:Symptom) \nWHERE toLower(d.name) CONTAINS disease_name \nRETURN DISTINCT s,d \nLIMIT 10; \n \n \nFig. 5. Graphical user interface displaying Cypher query and results for breast \ncancer treatment \n \n \nA second query, What are the side effects of insulin?, \nproduced a Cypher statement starting from the drug node for \ninsulin and traversing the CAUSES relationship to the connected \nSideEffect nodes. The resulting list included dizziness, diarrhea, \ncough, back pain, weakness, rash/dermatitis, and hypersensitivity \nside effects well documented in insulin safety profiles. In the \nvisual representation in Fig. 6, insulin appeared centrally \nconnected to these side effect nodes, further strengthening the \ncorrectness of the relationship mapping of the system. \n \nCypher: \nWITH toLower(\"insulin\") AS drug_name  \nMATCH (d:Drug)-[:CAUSES]->(se:SideEffect)  \nWHERE toLower(d.name) CONTAINS drug_name  \nRETURN se,d \nLIMIT 20; \n \nFig. 6. Knowledge Graph Visualization of Insulin and Its Related Side Effects \n \n \nThese examples demonstrate the system’s ability to interpret \nplain English biomedical questions, generate correct Cypher \nqueries, and return results that are clinically plausible, easy to \ninterpret, and directly traceable to specific graph relationships. \nThis supports both the accuracy and the transparency of the \nproposed approach. \nTo illustrate how biomedical questions can be translated into \ngraph queries, consider the natural language question: ’What are \nthe side effects of drugs that treat epilepsy?’. The following \nCypher query demonstrates how this question is mapped in the \nknowledge graph, where diseases are connected to drugs through \nthe TREATS relation and drugs are connected to side effects \nthrough the CAUSES relation. \n \nCypher: \nMATCH (d:Disease)<-[:TREATS]-(dr:Drug)  \nWHERE toLower(d.name) CONTAINS \"epilepsy\" \nMATCH (dr:Drug)-[:CAUSES]->(se:SideEffect) \nRETURN DISTINCT se,d,dr \nlimit 10", "metadata": {"producer": "Microsoft® Word for Microsoft 365", "creator": "Microsoft® Word for Microsoft 365", "creationdate": "2025-10-20T16:37:36+03:00", "source": "/home/donbr/open-biosciences/biosciences-research/data/raw/JASTTytm_A+Hybrid+LLM–Knowledge+Graph_.pdf", "file_path": "/home/donbr/open-biosciences/biosciences-research/data/raw/JASTTytm_A+Hybrid+LLM–Knowledge+Graph_.pdf", "total_pages": 16, "format": "PDF 1.7", "title": "A Hybrid LLM–Knowledge Graph Framework for Accurate Biomedical Question Answering", "author": "", "subject": "", "keywords": "", "moddate": "2025-10-20T16:37:36+03:00", "trapped": "", "modDate": "D:20251020163736+03'00'", "creationDate": "D:20251020163736+03'00'", "page": 13}}
{"page_content": "Omar & Mohammed / Journal of Applied Science and Technology Trends Vol. 06, No. 02, pp. 342 –357 (2025) \n \n356 \n \nFig. 7. Side effects of drugs that treat epilepsy \n \nExecuting this query returns drugs associated with epilepsy \nand their corresponding side effects. For example, the query \nidentifies Pregabalin as a treatment for epilepsy and retrieves \nmultiple side effects such as anxiety, arrhythmia, gastrointestinal \npain, and others. The visualization of the graph in Fig. 7, \nhighlights this pattern, where the drug node is linked to epilepsy \nvia TREATS and to several side effect nodes via CAUSES, \nproviding an interpretable biomedical knowledge structure. \nVI. DISCUSSION \nUnder \na \nunified \nevaluation \nprotocol \nreporting \nP@k/R@k/F1@k, Hits@k, and MRR at 𝑘∈{1,5,10} , the \npipeline exhibits consistent end-to-end behavior across the three \ndifficulty tiers. The 𝑘-ablation shows strong early precision (high \nP@1 and elevated MRR), while recall increases with larger 𝑘, \nindicating that correct answers typically surface near the top yet \npersist deeper in the ranked slate. At the query-generation level, \nexact-match (EM) is high for simple questions (e.g., ∼96%) and \nlower for complex, multi-hop questions (e.g., ∼86.7%), which \naligns with increased compositionality. End-to-end latency \n(mean total ≈6.07 s from question to final answer) remains within \ninteractive bounds on our local setup. Practically, a fully offline \ndeployment avoids internet connectivity, API keys, and external \ndata \ntransfer, \nstrengthening \nprivacy, \ncompliance, \nand \nreproducibility in biomedical contexts. \nSeveral challenges qualify these findings. First, the \ncomparability challenge: to our knowledge, no prior study \nevaluates iBKH using the same metric suite, making cross-paper \nnumeric comparisons not “apples to apples.” We therefore \ninterpret the results as controlled evidence about this pipeline \nunder a single, consistent protocol rather than as a cross-study \nleaderboard. Beyond comparability, performance may vary with \nother biomedical knowledge graphs; the current iBKH snapshot \nis static, limiting real-time updates; and scaling to larger or \ndynamically refreshed graphs can introduce latency and \nconsistency trade-offs. \nError analysis shows that residual failures concentrate in \ncomplex, multi-hop queries where missing relation hops or brittle \nname-based matching (synonyms, abbreviations, homonyms) \nlead to partial answers. These observations motivate concept-\nlevel normalization via biomedical identifiers (e.g., UMLS, \nSNOMED, RxNorm) with synonym expansion, as well as \nschema-constrained query generation and path-guided decoding \nto better satisfy multi-hop constraints. Finally, correctness is \nassessed primarily at the answer level (Hits@k, MRR, \nprecision/recall) and does not yet include full semantic-\nequivalence checks across alternative Cypher queries, which may \noverlook cases where different queries yield the same correct \nresults. Complementing template EM with result-set equivalence \nchecks, expanding the metric suite (e.g., nDCG/MAP) with per-\nquestion 95% bootstrap confidence intervals, and supporting \nincremental graph updates with distributed storage are promising \nsteps to enhance robustness, scalability, and external validity. \nVII. CONCLUSION \nThis study introduced a hybrid biomedical question \nanswering framework that couples the LLaMA-3B language \nmodel with a Neo4j-based iBKH knowledge graph to enable the \nautomatic generation of executable Cypher queries and to deliver \ntransparent, evidence-grounded answers through justification \nsubgraphs. Evaluation in simple, medium, and complex queries \ndemonstrated consistently high performance, with strong \nprecision, recall, F1, Hits@k, and MRR values, while \nmaintaining low latency suitable for interactive biomedical \napplications. Beyond quantitative performance, the proposed \nsystem provides a reproducible and privacy-preserving solution \nby operating fully offline, a property of particular importance in \nsensitive clinical and research contexts. However, certain \nlimitations remain. The reliance on a static iBKH snapshot \nconstrains coverage and adaptability, recall is lower for complex \nmulti-hop reasoning, and the absence of canonical entity \nnormalization (e.g., UMLS, SNOMED, RxNorm) may reduce \nsemantic robustness. Future research will therefore focus on \nintegrating standardized biomedical entity normalization, \nenabling dynamic and incremental knowledge graph updates, and \nleveraging domain-adapted or fine-tuned biomedical LLMs. \nThese directions are expected to further strengthen the \nrobustness, scalability, and applicability of the framework in real-\nworld biomedical and clinical environments. \n \nREFERENCES \n[1] S. Bubeck, V. Chandrasekaran, R. Eldan, J. Gehrke, E. Horvitz, E. Kamar, \nP. Lee, Y. T. Lee, Y. Li, S. Lundberg, et al., “Sparks of artificial general \nintelligence: \nEarly experiments with \ngpt-4,” arXiv preprint \narXiv:2303.12712, 2023. \n[2] J. Achiam, S. Adler, S. Agarwal, L. Ahmad, I. Akkaya, F. L. Aleman, D. \nAlmeida, J. Altenschmidt, S. Altman, S. Anadkat, et al., “Gpt-4 \ntechnical report,” arXiv preprint arXiv:2303.08774, 2023. \n[3] Y. Hou, J. Yeung, H. Xu, C. Su, F. Wang, and R. Zhang, “From answers to \ninsights: unveiling the strengths and limitations of chatgpt and \nbiomedical knowledge graphs,” Research Square, pp. rs–3, 2023.", "metadata": {"producer": "Microsoft® Word for Microsoft 365", "creator": "Microsoft® Word for Microsoft 365", "creationdate": "2025-10-20T16:37:36+03:00", "source": "/home/donbr/open-biosciences/biosciences-research/data/raw/JASTTytm_A+Hybrid+LLM–Knowledge+Graph_.pdf", "file_path": "/home/donbr/open-biosciences/biosciences-research/data/raw/JASTTytm_A+Hybrid+LLM–Knowledge+Graph_.pdf", "total_pages": 16, "format": "PDF 1.7", "title": "A Hybrid LLM–Knowledge Graph Framework for Accurate Biomedical Question Answering", "author": "", "subject": "", "keywords": "", "moddate": "2025-10-20T16:37:36+03:00", "trapped": "", "modDate": "D:20251020163736+03'00'", "creationDate": "D:20251020163736+03'00'", "page": 14}}
{"page_content": "Omar & Mohammed / Journal of Applied Science and Technology Trends Vol. 06, No. 02, pp. 342 –357 (2025) \n \n357 \n[4] Z. Ji, N. Lee, R. Frieske, T. Yu, D. Su, Y. Xu, E. Ishii, Y. J. Bang, A. Madotto, \nand P. Fung, “Survey of hallucination in natural language generation,” \nACM Computing Surveys, vol. 55, no. 12, pp. 1–38, 2023. \n[5] C. Malaviya, S. Lee, S. Chen, E. Sieber, M. Yatskar, and D. Roth, “Expertqa: \nexpert-curated questions and attributed answers,” arXiv preprint \narXiv:2309.07852, 2023. \n[6] L. Pusch and T. O. Conrad, “Combining llms and knowledge graphs to reduce \nhallucinations \nin \nquestion \nanswering,” \narXiv \npreprint \narXiv:2409.04181, 2024. \n[7] H. Nori, N. King, S. M. McKinney, D. Carignan, and E. Horvitz, \n“Capabilities of gpt-4 on medical challenge problems,” arXiv preprint \narXiv:2303.13375, 2023. \n[8] T. Sekar, Kushal, S. Shankar, S. Mohammed, and J. Fiaidhi, “Investigations \non using evidence-based graphrag pipeline using llm tailored for usmle \nstyle questions,” medRxiv, pp. 2025–05, 2025. \n[9] S. Farquhar, J. Kossen, L. Kuhn, and Y. Gal, “Detecting hallucinations in \nlarge language models using semantic entropy,” Nature, vol. 630, no. \n8017, pp. 625–630, 2024. \n[10] E. Asgari, N. Montaña-Brown, M. Dubois, S. Khalil, J. Balloch, J. A. \nYeung, and D. Pimenta, “A framework to assess clinical safety and \nhallucination rates of llms for medical text summarisation,” npj Digital \nMedicine, vol. 8, no. 1, p. 274, 2025. \n[11] C. Su, Y. Hou, M. Zhou, S. Rajendran, J. R. Maasch, Z. Abedi, H. Zhang, \nZ. Bai, A. Cuturrufo, W. Guo, et al., “Biomedical discovery through the \nintegrative biomedical knowledge hub (ibkh),” Iscience, vol. 26, no. 4, \n2023. \n[12] J. H. Morris, K. Soman, R. E. Akbas, X. Zhou, B. Smith, E. C. Meng, C. C. \nHuang, G. Cerono, G. Schenk, A. Rizk-Jackson, et al., “The scalable \nprecision medicine open knowledge engine (spoke): a massive \nknowledge graph of biomedical information,” Bioinformatics, vol. 39, \nno. 2, p. btad080, 2023. \n[13] K. Soman, P. W. Rose, J. H. Morris, R. E. Akbas, B. Smith, B. Peetoom, C. \nVillouta-Reyes, G. Cerono, Y. Shi, A. Rizk-Jackson, et al., “Biomedical \nknowledge graph-optimized prompt generation for large language \nmodels,” Bioinformatics, vol. 40, no. 9, p. btae560, 2024. \n[14] F. Frau, P. Loustalot, M. Törnqvist, N. Temam, J. Cupe, M. Montmerle, and \nF. Augé, “Connecting electronic health records to a biomedical \nknowledge graph to link clinical phenotypes and molecular endotypes \nin atopic dermatitis,” Scientific Reports, vol. 15, no. 1, p. 3082, 2025. \n[15] Y. Gao, R. Li, E. Croxford, J. Caskey, B. W. Patterson, M. Churpek, T. \nMiller, D. Dligach, and M. Afshar, “Leveraging medical knowledge \ngraphs into large language models for diagnosis prediction: design and \napplication study,” Jmir AI, vol. 4, p. e58670, 2025. \n[16] Y. Yan, Y. Hou, Y. Xiao, R. Zhang, and Q. Wang, “Knownet: guided health \ninformation seeking from llms via knowledge graph integration,” IEEE \nTransactions on Visualization and Computer Graphics, 2024. \n[17] Y. Deng, S. Zhao, Y. Miao, J. Zhu, and J. Li, “Medka: a knowledge graph-\naugmented approach to improve factuality in medical large language \nmodels,” Journal of Biomedical Informatics, p. 104871, 2025. \n[18] L. Ehrlinger and W. Wöß, “Towards a definition of knowledge graphs,” \nSEMANTiCS (Posters, Demos, SuCCESS), vol. 48, no. 1–4, p. 2, 2016. \n[19] E. Rajabi and S. Kafaie, “Building a disease knowledge graph,” in Caring \nis Sharing – Exploiting the Value in Data for Health and Innovation, \npp. 701–705, IOS Press, 2023. \n[20] L. Guan, Y. Huang, and J. Liu, “Biomedical question answering via multi-\nlevel summarization on a local knowledge graph,” arXiv preprint \narXiv:2504.01309, 2025. \n[21] D. Steinigen, R. Teucher, T. H. Ruland, M. Rudat, N. Flores-Herr, P. \nFischer, N. Milosevic, C. Schymura, and A. Ziletti, “Fact finder – \nenhancing domain expertise of large language models by incorporating \nknowledge graphs,” arXiv preprint arXiv:2408.03010, 2024. \n[22] Y. Feng, L. Zhou, C. Ma, Y. Zheng, R. He, and Y. Li, “Knowledge graph–\nbased thought: a knowledge graph–enhanced llm framework for pan-\ncancer question answering,” GigaScience, vol. 14, p. giae082, 2025. \n[23] H. Luo, Z. Tang, S. Peng, Y. Guo, W. Zhang, C. Ma, G. Dong, M. Song, \nW. Lin, Y. Zhu, et al., “Chatkbqa: a generate-then-retrieve framework \nfor knowledge base question answering with fine-tuned large language \nmodels,” arXiv preprint arXiv:2310.08975, 2023. \n[24] A. Tiwari, S. K. R. Malay, V. Yadav, M. Hashemi, and S. T. Madhusudhan, \n“Auto-cypher: improving llms on cypher generation via llm-supervised \ngeneration-verification framework,” in Proceedings of the 2025 \nConference of the Nations of the Americas Chapter of the Association \nfor Computational Linguistics: Human Language Technologies \n(Volume 2: Short Papers), pp. 623–640, 2025. \n[25] R. Wang, Z. Zhang, L. Rossetto, F. Ruosch, and A. Bernstein, “Nlqxform: \na language model-based question to sparql transformer,” arXiv preprint \narXiv:2311.07588, 2023. \n[26] M. R. Rezaei, R. S. Fard, J. L. Parker, R. G. Krishnan, and M. Lankarany, \n“Agentic medical knowledge graphs enhance medical question \nanswering: bridging the gap between llms and evolving medical \nknowledge,” arXiv preprint arXiv:2502.13010, 2025. \n[27] Z. Dong, B. Peng, Y. Wang, J. Fu, X. Wang, Y. Shan, and X. Zhou, “Effiqa: \nefficient question-answering with strategic multi-model collaboration \non knowledge graphs,” arXiv preprint arXiv:2406.01238, 2024. \n[28] Y. Duan, Q. Zhou, Y. Li, C. Qin, Z. Wang, H. Kan, and J. Hu, “Research \non a traditional chinese medicine case-based question-answering \nsystem integrating large language models and knowledge graphs,” \nFrontiers in Medicine, vol. 11, p. 1512329, 2025. \n[29] S. Mohammed, J. Fiaidhi, T. Sekar, K. Kushal, and S. Shankar, \n“Investigations on using evidence-based graphrag pipeline using llm \ntailored for answering usmle medical exam questions,” medRxiv, pp. \n2025–05, 2025. \n[30] H. Yang, J. Li, C. Zhang, A. P. Sierra, and B. Shen, “Large language model–\ndriven knowledge graph construction in sepsis care using multicenter \nclinical databases: development and usability study,” Journal of \nMedical Internet Research, vol. 27, p. e65537, 2025. \n[31] K.-L. Hsieh, G. Plascencia-Villa, K.-H. Lin, G. Perry, X. Jiang, and Y. Kim, \n“Synthesize heterogeneous biological knowledge via representation \nlearning for alzheimer’s disease drug repurposing,” Iscience, vol. 26, \nno. 1, 2023. \n[32] R. Angles and C. Gutierrez, “Survey of graph database models,” ACM \nComputing Surveys (CSUR), vol. 40, no. 1, pp. 1–39, 2008. \n[33] B. Chicho and A. O. Mohammed, “An empirical comparison of neo4j and \ntigergraph databases for network centrality,” Science Journal of \nUniversity of Zakho, vol. 11, no. 2, pp. 190–201, 2023. \n[34] I. Robinson, J. Webber, and E. Eifrem, Graph Databases: New \nOpportunities for Connected Data, O’Reilly Media, 2015. \n[35] A. Lysenko, I. A. Roznovăţ, M. Saqi, A. Mazein, C. J. Rawlings, and C. \nAuffray, “Representing and querying disease networks using graph \ndatabases,” BioData Mining, vol. 9, no. 1, p. 23, 2016. \n[36] M. Šestak, M. Heričko, T. W. Družovec, and M. Turkanović, “Applying k-\nvertex cardinality constraints on a neo4j graph database,” Future \nGeneration Computer Systems, vol. 115, pp. 459–474, 2021. \n[37] M. Desai, R. G. Mehta, and D. P. Rana, “An empirical analysis to identify \nthe effect of indexing on influence detection using graph databases,” \nInternational Journal of Innovative Technology and Exploring \nEngineering, vol. 8, no. 9S, pp. 414–421, 2019. \n[38] S. Beis, S. Papadopoulos, and Y. Kompatsiaris, “Benchmarking graph \ndatabases on the problem of community detection,” in New Trends in \nDatabase and Information Systems II, pp. 3–14, Springer, 2015. \n[39] R. Wang, Z. Yang, W. Zhang, and X. Lin, “An empirical study on recent \ngraph database systems,” in International Conference on Knowledge \nScience, Engineering and Management, pp. 328–340, Springer, 2020. \n[40] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “Bert: pre-training of \ndeep bidirectional transformers for language understanding,” in \nProceedings of the 2019 Conference of the North American Chapter of \nthe Association for Computational Linguistics: Human Language \nTechnologies, vol. 1, pp. 4171–4186, 2019. \n[41] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. \nNeelakantan, P. Shyam, G. Sastry, A. Askell, et al., “Language models \nare few-shot learners,” Advances in Neural Information Processing \nSystems, vol. 33, pp. 1877–1901, 2020. \n[42] J. Lee, W. Yoon, S. Kim, D. Kim, S. Kim, C. H. So, and J. Kang, “Biobert: \na pre-trained biomedical language representation model for biomedical \ntext mining,” Bioinformatics, vol. 36, no. 4, pp. 1234–1240, 2020. \n[43] E. Alsentzer, J. R. Murphy, W. Boag, W.-H. Weng, D. Jin, T. Naumann, \nand M. McDermott, “Publicly available clinical bert embeddings,” \narXiv preprint arXiv:1904.03323, 2019. \n[44] Z. He, S. Sunkara, X. Zang, Y. Xu, L. Liu, N. Wichers, G. Schubiner, R. \nLee, and J. Chen, “Actionbert: leveraging user actions for semantic \nunderstanding of user interfaces,” in Proceedings of the AAAI \nConference on Artificial Intelligence, vol. 35, pp. 5931–5938, 2021.", "metadata": {"producer": "Microsoft® Word for Microsoft 365", "creator": "Microsoft® Word for Microsoft 365", "creationdate": "2025-10-20T16:37:36+03:00", "source": "/home/donbr/open-biosciences/biosciences-research/data/raw/JASTTytm_A+Hybrid+LLM–Knowledge+Graph_.pdf", "file_path": "/home/donbr/open-biosciences/biosciences-research/data/raw/JASTTytm_A+Hybrid+LLM–Knowledge+Graph_.pdf", "total_pages": 16, "format": "PDF 1.7", "title": "A Hybrid LLM–Knowledge Graph Framework for Accurate Biomedical Question Answering", "author": "", "subject": "", "keywords": "", "moddate": "2025-10-20T16:37:36+03:00", "trapped": "", "modDate": "D:20251020163736+03'00'", "creationDate": "D:20251020163736+03'00'", "page": 15}}
{"page_content": "DALK: Dynamic Co-Augmentation of LLMs and KG to answer\nAlzheimer’s Disease Questions with Scientific Literature\nDawei Li1*, Shu Yang2∗, Zhen Tan1, Jae Young Baik2, Sunkwon Yun3, Joseph Lee2, Aaron Chacko2,\nBojian Hou2, Duy Duong-Tran2,4, Ying Ding5, Huan Liu1†, Li Shen2†, Tianlong Chen3†\n1School of Computing, and Augmented Intelligence, Arizona State University\n2Department of Biostatistics, Epidemiology and Informatics, University of Pennsylvania Perelman School of Medicine\n3Department of Computer Science, The University of North Carolina at Chapel Hill\n4Department of Mathematics, United States Naval Academy\n5School of Information, The University of Texas at Austin, Austin\nAbstract\nRecent advancements in large language mod-\nels (LLMs) have achieved promising perfor-\nmances across various applications. Nonethe-\nless, the ongoing challenge of integrating\nlong-tail knowledge continues to impede the\nseamless adoption of LLMs in specialized do-\nmains. In this work, we introduce DALK, a.k.a.\nDynamic Co-Augmentation of LLMs and KG,\nto address this limitation and demonstrate its\nability on studying Alzheimer’s Disease (AD),\na specialized sub-field in biomedicine and a\nglobal health priority. With a synergized frame-\nwork of LLM and KG mutually enhancing each\nother, we first leverage LLM to construct an\nevolving AD-specific knowledge graph (KG)\nsourced from AD-related scientific literature,\nand then we utilize a coarse-to-fine sampling\nmethod with a novel self-aware knowledge re-\ntrieval approach to select appropriate knowl-\nedge from the KG to augment LLM infer-\nence capabilities. The experimental results,\nconducted on our constructed AD question\nanswering (ADQA) benchmark, underscore\nthe efficacy of DALK. Additionally, we per-\nform a series of detailed analyses that can of-\nfer valuable insights and guidelines for the\nemerging topic of mutually enhancing KG and\nLLM. We will release the code and data at\nhttps://github.com/David-Li0406/DALK.\n1\nIntroduction\nAlzheimer’s Disease (AD) is a neurodegenerative\ndisorder characterized by progressive declines in\ncognitive and functional status over a span of\ndecades (Report, 2023). However, current AD ther-\napy developments are facing critical challenges\ndue to the lack of knowledge and understanding of\nthe underlying etiological mechanisms of the dis-\nease. Although scientific literature and dedicated\nbiomedical databases could supply rich sources of\n* Equal Constributions\n† Corresponding authors\nAD knowledge, manual review of relevant informa-\ntion is impossible due to the large volume.\nAs large language models (LLMs) (Brown et al.,\n2020; Zhang et al., 2022; Anil et al., 2023; Touvron\net al., 2023) with chain-of-thought (CoT)-based\nprompting (Wei et al., 2022; Wang et al., 2022;\nTong et al., 2023; Yao et al., 2023; Besta et al.,\n2023) demonstrate strong language capabilities\nacross various tasks, there have been attempts to\nleverage LLMs-based systems in general biomedi-\ncal and AD-related applications (Mao et al., 2023;\nLi et al., 2023c; Yan et al., 2024; Feng et al., 2023).\nHowever, while the LLMs have shown promising\nperformances in many general tasks, recent studies\nrevealed LLMs’ limitations in long-tail (Kandpal\net al., 2023) and domain-specific (Li et al., 2023b,\n2024) knowledge, thereby significantly impeding\ntheir adaptations in vertical fields such as AD. To\ndeal with this issue, the most common strategies are\nretrieval augmented generation (RAG) and domain-\nspecific LLMs training.\nNevertheless, directly applying these strategies\nin the context like AD would still suffer from sev-\neral issues.\nFirst, Data Quality: As in many\nbiomedical fields, scientific literature composes\nthe largest publicly available corpus source in AD.\nYet, the dense and information-overloaded nature\nof scientific literature, when combined with auto-\nmatic retrieval methods, can lead to the retrieval\nof irrelevant and noisy information. Previous re-\nsearch has shown that noisy and irrelevant corpora\ncan significantly undermine the performance of\nLLMs (Yu et al., 2023; Chen et al., 2024; Wu et al.,\n2024). Second, Efficiency & Scale Issues: Being\nan critical field of research, the knowledge of AD\nis rapidly evolving with scientific advancements at\na remarkable pace and scale. However, retraining\na domain-specific LLM or updating certain knowl-\nedge in it demands substantial computational re-\nsources (Hu et al., 2021; Ovadia et al., 2023; Zhang\net al., 2024). This efficiency issue would also limit\narXiv:2405.04819v1  [cs.CL]  8 May 2024", "metadata": {"producer": "pdfTeX-1.40.25", "creator": "LaTeX with hyperref", "creationdate": "2024-05-09T00:18:37+00:00", "source": "/home/donbr/open-biosciences/biosciences-research/data/raw/2405.04819v1.pdf", "file_path": "/home/donbr/open-biosciences/biosciences-research/data/raw/2405.04819v1.pdf", "total_pages": 18, "format": "PDF 1.5", "title": "", "author": "", "subject": "", "keywords": "", "moddate": "2024-05-09T00:18:37+00:00", "trapped": "", "modDate": "D:20240509001837Z", "creationDate": "D:20240509001837Z", "page": 0}}
{"page_content": "the sizes of domain-specific LLMs, consequently\naffecting their performances.\nTo tackle these limitations, here we propose\na Dynamic Co-Augmentation of LLMs and KG\n(DALK) framework that facilitates mutual benefits\nbetween LLMs and knowledge graphs (KG) for\nthe AD domain. Initially, our framework addresses\nthe data quality challenge by extracting more struc-\ntural and accurate knowledge from unstructured\nand dense scientific literature and constructing a\ndomain-specific knowledge graph tailored to AD.\nWe employ two widely utilized knowledge graph\nconstruction methods, namely pair-wise construc-\ntion (Carta et al., 2023; Wadhwa et al., 2023) and\ngenerative construction (Han et al., 2023; Bi et al.,\n2024), to comprehensively assess their impact on\nknowledge graph quality. Then, we adopt a coarse-\nto-fine sampling method with a novel self-aware\nknowledge retrieval approach to select appropri-\nate knowledge from the knowledge graph and thus\nfurther address the data quality problem. Notably,\nthe tuning-free nature of our framework signifi-\ncantly enhances efficiency and facilitates its ap-\nplication in large-scale and API-based language\nmodels (OpenAI, 2022). In the evaluation sec-\ntion, we derive an Alzheimer’s Disease question\nanswering (ADQA) benchmark from existing gen-\neral medical QA datasets with millions of sam-\nples filtered by a curated keyword list and self-\nsampling of LLMs. Our extensive experiment on\nADQA demonstrates the effectiveness of our frame-\nwork in domain-specific applications compared\nwith general biomedical LLMs and retrieval aug-\nmented models. Further evaluation and analysis\nprovide valuable insights into constructing high-\nquality knowledge graphs and sampling accurate\nknowledge from them.\nIn summary, our contribution in this work can\nbe summarized as follows:\n• We identify the constraints of the current meth-\nods for LLMs in domain-specific areas like\nAD and introduce DALK, a co-augmentation\nframework of the LLM and KG to address\nthese issues.\n• We build AD-specific KG and QA benchmark.\nThrough extensive comparisons with other\nmethods, we showcase the effectiveness of\nDALK.\n• We delve into a comprehensive analysis of\nour proposed method and provide valuable\ninsights and guidance on how to construct a\nhigh-quality KG and sample accurate knowl-\nedge from it.\n2\nRelated Work\nThe\ninterplay\nbetween\nLLMs\nand\nKGs\nKGs (Miller, 1995; Speer et al., 2017; Vrandeˇci´c\nand Krötzsch, 2014) serve as structured represen-\ntations of factual knowledge, typically expressed\nas (head, relation, tail) triples. Their structured,\nfactual, and interpretable nature renders them ex-\ncellent complements to parametric language mod-\nels (Pan et al., 2024). Recently, with the rise of\nlarge language models (LLMs), numerous studies\nhave delved into exploring the synergy between\nLLMs and KGs for various purposes (Pan et al.,\n2024; Tan et al., 2024). There are a lot of efforts in\nconducting knowledge graph construction (Carta\net al., 2023; Wadhwa et al., 2023; Han et al., 2023;\nBi et al., 2024; Datta et al., 2024), completion (Wei\net al., 2023; Zhang et al., 2023b; Li et al., 2024)\nwith the aid of LLMs. Conversely, other works aim\nto enhance LLMs by integrating knowledge sam-\npled from KGs during both training (Tang et al.,\n2023; Luo et al., 2024; Dernbach et al., 2024;\nRangel et al., 2024) and inference (Kim et al., 2023;\nWen et al., 2023; Jiang et al., 2023; Sun et al.,\n2023a) times. Our work distinguishes itself by\nproposing a co-augmentation framework for LLMs\nand KGs, facilitating their mutual enhancement,\nand applying it to the domain of AD.\nLLMs and KGs for AD research\nLLMs and\nKGs have both been applied to Alzheimer’s Dis-\nease research in previous studies. Pre-trained lan-\nguage models are utilized to work on AD de-\ntection and many other related tasks based on\nspeech recordings and transcripts (Balagopalan\net al., 2020; Agbavor and Liang, 2022), electronic\nhealth records (EHRs) (Mao et al., 2023; Li et al.,\n2023c; Yan et al., 2024), and tabular data (Feng\net al., 2023).\nKGs have been widely used in\nbiomedical research, yet only a few are specifically\nfor AD research (Romano et al., 2023; Pu et al.,\n2023; Hsieh et al., 2023; Nian et al., 2022; Daluwa-\ntumulle et al., 2023). These KGs were generally\nconstructed from a variety of information derived\nfrom heterogeneous biomedical databases (e.g. for\ngenes, drugs, pathways, etc.) or scientific literature\nrelated to AD. Despite the aforementioned efforts\nfor LLMs and KGs in AD research, no prior study\nhas explored using LLM to augment AD-KG, or", "metadata": {"producer": "pdfTeX-1.40.25", "creator": "LaTeX with hyperref", "creationdate": "2024-05-09T00:18:37+00:00", "source": "/home/donbr/open-biosciences/biosciences-research/data/raw/2405.04819v1.pdf", "file_path": "/home/donbr/open-biosciences/biosciences-research/data/raw/2405.04819v1.pdf", "total_pages": 18, "format": "PDF 1.5", "title": "", "author": "", "subject": "", "keywords": "", "moddate": "2024-05-09T00:18:37+00:00", "trapped": "", "modDate": "D:20240509001837Z", "creationDate": "D:20240509001837Z", "page": 1}}
{"page_content": "vice versa, let alone the potential for mutual en-\nhancement between the two as we propose here.\n3\nOur Methodology\nThis section elaborates on our dynamic co-\naugmentation framework of LLMs and KG. Sec-\ntion 3.1 presents the details of augmenting an AD-\nspecific evolving KG with LLMs and literature\ncorpus in a time-slicing fashion (i.e. year by year).\nFollowing it, Section 3.2 describes the process of\nsampling appropriate knowledge from the evolving\nKG to enhance LLMs’ reasoning. Figure 1 illus-\ntrates an overall pipeline of our method DALK.\nKGpair\nKGgen\n#Corpus\n9,764\n9,764\n#Nodes\n13,509\n20,545\n#Relations\n3,952\n3,651\n#Triples\n171,431\n53,585\nTable 1: Detailed statistics about our augmented knowl-\nedge graph.\n3.1\nLLMs for KG\nCorpus Collection\nTo create an AD-specific\nknowledge graph, we follow (Pu et al., 2023) and\nuse the AD corpus collected by a domain expert\nProfessor Colin Masters at the University of Mel-\nbourne who discovered amyloid proteins being the\npotential cause of AD (Masters et al., 1985). The\ncorpus is based on his extensive bibliography of\nrepresentative AD-related papers and consists of\nmore than 16K PMID (PubMed ID)-indexed arti-\ncles from 1977 to 2021. For our study, we focus on\nthe papers since 2011 which reflect the most recent\nknowledge in the field and get 9,764 articles.\nEntity Recognition\nIn order to identify knowl-\nedge at the proper granularity level for AD, we\nextract relevant entities from the corpus by utiliz-\ning the PubTator Central (PTC) (Wei et al., 2013)\ndeveloped and continuously maintained by NCBI.\nPTC is a widely-used tool to provide state-of-the-\nart annotations of biomedical concepts for PubMed\nabstracts and full-text articles, and it supports six\nbioconcept types including genes, diseases, chem-\nicals, mutations, species and cell lines. We apply\nPTC to the abstracts of all our AD papers and ob-\ntain the relevant named entities which will serve as\nnodes in the knowledge graph.\nRelation Extraction\nTo build an accurate and\nhigh-quality knowledge graph on AD, we aim to as-\nsign a specific relation type between the two related\nentities. Through a comprehensive survey of rela-\ntion extraction methods for knowledge graph con-\nstruction, we categorize current approaches with\nLLMs into two main groups: (a). Pair-wised Rela-\ntion Extraction (Carta et al., 2023; Wadhwa et al.,\n2023) aims to prompt the LLMs to describe the\nrelationship between any two entities in a segment\nof text. (b). Generative Relation Extraction (Han\net al., 2023; Bi et al., 2024; Datta et al., 2024),\nwhere LLMs directly output all related entity pairs\nand their corresponding relationships. As shown\nin Figure 2, we incorporate both of these rela-\ntion extraction methods into our knowledge graph\naugmentation process to provide a comprehensive\ncomparison between them. We denote the result-\ning knowledge graphs from these approaches as\nKGpair and KGgen respectively.\nTable 1 presents the detailed statistics about our\naugmented knowledge graph, including the num-\nber of corpora we used, and the number of nodes,\nrelations and triples in KGpair and KGgen.\n3.2\nKG for LLMs\nIn this section, we begin by outlining our process\nfor sampling coarse-grained augmented knowledge\nfrom our evolving knowledge graph (Section 3.2.1).\nSubsequently, we delve into detail regarding our\nself-aware knowledge retrieval method, which aims\nto filter out noise and retrieve the most pertinent\nknowledge to provide to the LLM (Section 3.2.2).\n3.2.1\nCoarse-grained Knowledge Sample\nGiven a question query Q, we first construct a\nprompt and ask LLMs to extract all the domain-\nspecific entities E = {e1, e2, ...} from it.\nAf-\nterward, we adhere to the methodology proposed\nby Wen et al. (2023) and execute a similarity-based\nentity linking process to connect all entities within\nE to the entity structure in our knowledge graph\nG. Specifically, we employ a semantic similarity\nmodel (Reimers and Gurevych, 2019) to encode all\nentities in G and E into dense embeddings, denoted\nas HG and HE, respectively. Subsequently, utiliz-\ning cosine similarity, we establish links between\neach entity in E and its nearest neighbor entity in\nG. This procedure yields an initial entity set EG\nfor the subsequent knowledge sampling step.\nTo build an evidence sub-graph to boost\nLLMs’ reasoning process, we follow the previous", "metadata": {"producer": "pdfTeX-1.40.25", "creator": "LaTeX with hyperref", "creationdate": "2024-05-09T00:18:37+00:00", "source": "/home/donbr/open-biosciences/biosciences-research/data/raw/2405.04819v1.pdf", "file_path": "/home/donbr/open-biosciences/biosciences-research/data/raw/2405.04819v1.pdf", "total_pages": 18, "format": "PDF 1.5", "title": "", "author": "", "subject": "", "keywords": "", "moddate": "2024-05-09T00:18:37+00:00", "trapped": "", "modDate": "D:20240509001837Z", "creationDate": "D:20240509001837Z", "page": 2}}
{"page_content": "… Successful application of \nAD preventative approaches \nmay hinge on an accurate and \ncomprehensive view of \ncomorbidities, including \ncardiovascular disease, \ndiabetes, and head trauma.\nLiterature Corpus\nLLMs for KG\nKG for LLMs\nHead\nRelation\nTail\nDiabetes\nAD\nHead trauma\n…\n…\n…\nrisk factor\nAD\nrisk factor\nExtracted Triplets\nEvolving KG\nEvolving KG\nAD\nCognition\nlevel\nDown’s \nsyndrome\nHead\ntrauma\nDiabetes\nSelected Knowledge\nLLMs\nLLMs\nExtract\nProcess\nAugment\nQ: Following are \npredispositions to \nAlzheimer's disease \nexcept ___.\nA). Down’s syndrome\nB). Head trauma\nC). Smoking\nD). Low education group\nAnswer\nRerank\nSample\nAD\nCognition\nlevel\nDown’s \nsyndrome\nHead\ntrauma\nDiabetes\nSubgraph Sampling\nGenetic\nfactors\nLifestyle\nInput\nQ: Following are \npredispositions to \nAlzheimer's disease \nexcept ___.\nA). Down’s syndrome\nB). Head trauma\nC). Smoking\nD). Low education group\nQuestion\nInput\nFigure 1: The overview pipeline of DALK. We first extract structural knowledge from unstructured corpora and\nconstruct a domain-specific knowledge graph tailored to AD (Section 3.1). Then, we utilize a coarse-to-fine sampling\nmethod with a novel self-aware knowledge retrieval approach to select appropriate knowledge from the knowledge\ngraph (Section 3.2).\n… Successful application of AD preventative approaches… , \nincluding cardiovascular disease, diabetes, and head trauma.\n… Successful application of AD preventative approaches… , \nincluding cardiovascular disease, diabetes, and head trauma.\nStep1: Entity Recongnition\nStep2: Relation Extraction\nPair-wised Relation Extraction\nGenerative Relation Extraction\n…\nReading the following abstract: [Abstract]\nQuestion: \nWhat is the relationship between AD \nand cardiovascular disease?\nQuestion: \nWhat is the relationship between AD \nand diabetes?\nReading the following abstract: [Abstract]\nHere are all the biomedicine-related \nentities: [Entities]\nQuestion: \nPlease extract all the valid relationship \nbetween the provided entities.\nFigure 2: The detailed process of AD-specific KG con-\nstruction.\nstudy (Wen et al., 2023) and consider the following\ntwo kinds of explorations in our AD-KG:\nPath-based Exploration\nentails the extraction of\na sub-graph from G to encompass all entities within\nEG. The process unfolds as follows: (a) Begin\nby selecting one node from e0\nQ as the initial node,\ndenoted as e1, and place the remaining nodes into a\ncandidate node set, Ecand. Explore at most k hops\nfrom e1 to identify the subsequent node, e2, where\ne1 ∈Ecand. If e2 is successfully reached within\nk hops, update the start node to e2 and remove e2\nfrom Ecand. In the event e2 cannot be found within\nk hops, concatenate the segment paths acquired\nthus far and store them in Gpath\nQ\n. Subsequently,\nchoose another node e′\n1 from Vcand as the new start\nnode, and eliminate both the original start node e1\nand the current node e2 from Ecand. (b) Verify if\nEcand is empty. If not, repeat step (a) to identify\nthe next segment of the path. If Ecand is empty,\ncombine all segments to construct a set of sub-\ngraphs and place them into Gpath\nQ\n.\nNeighbor-based Exploration\nendeavors to aug-\nment the evidence relevant to the query within GQ.\nThis process consists of two steps: (a) Initially,\nexpand each node e within EG by 1-hop to incor-\nporate their neighbors e′, thus appending triples\n(e, r, e′) to Gnei\nQ . (b) Then assess whether each e′\nexhibits semantic relevance to the query. If affir-\nmative, further expand the 1-hop neighbors of e′,\nconsequently adding triples (enei, r′, e′) to Gnei\nQ .\nAfter obtaining the two sub-graphs Gpath\nQ\nand\nGnei\nQ , we perform post-processing to further prune\nredundant information in sub-graphs and prompt\nLLMs to describe the structure of each sub-graph.\n3.2.2\nSelf-aware Knowledge Retrieval\nIn our initial experiment, we noticed the coarse-\ngrained knowledge sampled with the above-\nmentioned approaches still contained redundant\nand irrelevant information. This issue of noise is a\ncommon challenge encountered in automatically-\nconstructed knowledge graphs (Fang et al., 2021;\nZhang et al., 2020; Li et al., 2022; Bi et al., 2024).", "metadata": {"producer": "pdfTeX-1.40.25", "creator": "LaTeX with hyperref", "creationdate": "2024-05-09T00:18:37+00:00", "source": "/home/donbr/open-biosciences/biosciences-research/data/raw/2405.04819v1.pdf", "file_path": "/home/donbr/open-biosciences/biosciences-research/data/raw/2405.04819v1.pdf", "total_pages": 18, "format": "PDF 1.5", "title": "", "author": "", "subject": "", "keywords": "", "moddate": "2024-05-09T00:18:37+00:00", "trapped": "", "modDate": "D:20240509001837Z", "creationDate": "D:20240509001837Z", "page": 3}}
{"page_content": "Moreover, many recent works (Yu et al., 2023; Li\net al., 2023d; Chen et al., 2024; Wu et al., 2024)\nhave demonstrated LLMs can indeed be influenced\nby such noisy information. To address this chal-\nlenge, we borrow insights from the recent self-\npowered LLMs (Wang et al., 2022; Pan et al., 2023;\nLi et al., 2023a; Yuan et al., 2024; Tong et al.,\n2024) and propose a self-aware knowledge retrieval\nmethod to leverage LLMs’ ranking capability (Sun\net al., 2023b; Ma et al., 2023) to filter out noisy\ninformation.\nIn particular, we directly prompt the LLM to\nrerank the sampled knowledge and only retrieve\ntop k triples to provide for itself in the final-round\ninference. Given the question Q and either the path-\nbased or neighbor-based sub-graph GQ, we create\nprompt pself by filling the pre-defined template:\npself = Templateself(Q, GQ, k).\n(1)\nThen, we use pself as the input to prompt the LLM\nto obtain the self-retrieved knowledge:\nGself\nQ\n= LLM(pself),\n(2)\nFinally, we provide the question Q and fine-grained\nknowledge Gself\nQ\nto the LLM for reasoning and get\nthe predicted answer a in two steps:\npinference = Templateinference(Q, Gself\nQ\n),\n(3)\na = LLM(pinference).\n(4)\nWe provide detailed examples in Appendix A and B\nto demonstrate the input and output in our DALK.\n4\nMain Experiment\n4.1\nADQA Benchmark\nFor performance evaluation, we consider four\nwidely-used medical QA datasets spanning diverse\nbiomedical topics (Jin et al., 2021; Pal et al., 2022;\nHendrycks et al., 2021; Peñas et al., 2013) and de-\nrive an AD-specific QA dataset from them. The\nfour medical QA datasets are all multiple-choice\nbased and include: 1) MedQA (Jin et al., 2021)\nconsisting of US Medical Licensing Examina-\ntion (USMLE)-style questions, 2) MedMCQA (Pal\net al., 2022) containing medical school entrance\nexam questions from India, 3) MMLU (Hendrycks\net al., 2021) consisting of diverse biomedical\nand clinical questions from various sources, 4)\nQA4MRE (Peñas et al., 2013) containing a sub-\nset of questions for AD derived from PubMed\nand Medline. In order to extract from the med-\nical QA datasets a subset of samples related to\nAD for our evaluation, we referred to NIH’s Com-\nmon Alzheimer’s and Related Dementias Research\nOntology (CADRO) 1. Jointly developed by the\nNational Institute on Aging and the Alzheimer’s\nAssociation, CADRO is a three-tiered classifica-\ntion system with eight main categories and a dozen\nsub-categories for AD and related dementia, and it\ncontains common terminologies or keywords used\nin the field. We derived from the CADRO a list of\nAD-related keywords most relevant to the medical\nQA datasets: <Aging, Alzheimer, Amyloid beta,\nAPOE, Dementia, Lipoprotein, Microglia>. Then,\nwe searched against each medical QA dataset for\nmatches with these keywords to find putative QA\nsamples, then further asked GPT-3.5-turbo to judge\nfor each putative sample whether the question is\nindeed related to AD or not. Finally, we filtered\nout a subset of such samples that are considered\nhighly relevant to AD to conduct our evaluation\n(number of samples in each dataset is shown in\nTable 2). More details about ADQA can be found\nin Appendix C.\n4.2\nExperiment Settings\nWe apply our framework with OpenAI GPT-3.5-\nturbo models (OpenAI, 2022). We also include the\nfollowing baseline methods for comparison:\nBiomedical LLMs\nBoth ChatDoctor (Yunxiang\net al., 2023) and Med-Alpaca (Shu et al., 2023)\nare fine-tuned versions of LLaMA (Touvron et al.)\non biomedical corpora.\nCompared with them,\nMeditron (Chen et al., 2023) is built on LLaMA-\n2 (Touvron et al., 2023) and extends its pretrain-\ning on a comprehensively curated medical cor-\npus. BiomedGPT (Zhang et al., 2023a) is also\nbased on LLaMA-2 and pioneer as the first open-\nsource and generalist visual language AI for di-\nverse biomedical tasks. Biomistral (Labrak et al.,\n2024) is an open-source LLM crafted specifically\nfor the biomedical domain, optimized for effi-\nciency through quantization and model merging\ntechniques.\nRetrieval-Augmented LLMs\nFurthermore, we\nalso compare our method with several represen-\ntative retrieval-augmented LLMs in the biomed-\nical domain. Almanac (Zakka et al., 2024) is a\nnovel approach utilizing OpenAI’s GPT model in-\n1https://iadrp.nia.nih.gov/about/cadro", "metadata": {"producer": "pdfTeX-1.40.25", "creator": "LaTeX with hyperref", "creationdate": "2024-05-09T00:18:37+00:00", "source": "/home/donbr/open-biosciences/biosciences-research/data/raw/2405.04819v1.pdf", "file_path": "/home/donbr/open-biosciences/biosciences-research/data/raw/2405.04819v1.pdf", "total_pages": 18, "format": "PDF 1.5", "title": "", "author": "", "subject": "", "keywords": "", "moddate": "2024-05-09T00:18:37+00:00", "trapped": "", "modDate": "D:20240509001837Z", "creationDate": "D:20240509001837Z", "page": 4}}
{"page_content": "MedQA\nMedMCQA\nMMLU\nQA4MRE\nAVG\nBiomedical LLMs\nChatDoctor (Yunxiang et al., 2023)\n25.7\n36.4\n46.9\n51.4\n40.1\nMed-Alpaca (Shu et al., 2023)\n41.4\n42.8\n44.9\n57.1\n46.5\nBiomedGPT (Zhang et al., 2023a)\n38.8\n41.9\n48.9\n42.6\n43.1\nMeditron (Chen et al., 2023)\n27.6\n31.4\n36.7\n25.7\n30.4\nBiomistral (Labrak et al., 2024)\n44.7\n49.5\n53.1\n68.6\n54.0\nRetrieval-augmented LLMs\nGPT-3.5-turbo w/ Ada (OpenAI, 2024)\n57.2\n65.7\n83.7\n62.9\n67.4\nAlmanac (Zakka et al., 2024)\n48.0\n69.5\n71.4\n60.0\n62.2\nClinfo.ai (Lozano et al., 2023)\n54.3\n77.0\n81.3\n67.7\n70.1\nClinfo.ai w/o PubMed API\n49.3\n68.6\n79.6\n74.3\n67.9\nGPT-3.5-turbo\n50.0\n71.9\n83.6\n62.9\n67.1\nDALK\n57.9\n75.2\n85.4\n71.4\n72.6\nTable 2: Experiment results on our constructed ADQA benchmark. The sample size is indicated after each dataset\nname. The best results of each metric are in bold and the second-best results are underlined. The “AVG” column\nrepresents the average accuracy score on the four sub-dataset.\ntegrated with a Qdrant vector database to hold ex-\nternal sources of knowledge retrieved from local\ncorpus, web search, and calculators, designed to\nanswer open-domain clinical questions. Like Al-\nmanac, Lozano et al. (2023) introduced Clinfo.ai,\nwhich is an open-source, end-to-end retrieval-\naugmented LLM (GPT) to answer medical queries\nusing scientific literature summarizations derived\nfrom PubMed search engine. We adopt both Al-\nmanac and Clinfo.ai with the same prompt as ours\nto answer multiple-choice questions to suit the\nADQA benchmark. Lastly, we implement a sim-\nple retrieval-augmented GPT baseline with CoT\nprompting similar to our proposed DALK. All the\nGPT models used are set to GPT-3.5-turbo as de-\ntailed in the next paragraph, to be consistent.\nImplementation Details\nWe use the knowledge\ngraph constructed with the generative approach\n(KGgen) in our main experiment and conduct an\nablation study on the knowledge graph with RE\nmethod (KGpair) in Section 4.5. We use GPT-3.5-\nturbo with the version “gpt-3.5-turbo-0301” and\nset the sampling temperature to 0.7. We utilize 7B\nversions of all the biomedical LLMs baselines. For\nRAG methods, we split each document with a max\nlength of 128 and retrieve the top 3 most relevant\ndocuments as the support evidence for LLMs to\ndo inference. We set the parameter k in our self-\naware knowledge retrieval to 5 and conduct further\nanalysis on it in Section 5.2.\n4.3\nMain Result\nTable 2 shows the experimental results on our\nADQA benchmark. We note that upon applying our\ndynamic co-augmentation framework, DALK’s per-\nformance surpasses that of other biomedical LLMs\nand RAG methods overall. It consistently achieves\neither the best or the second-best accuracy score\nacross all sub-datasets and attains the highest AVG\nscore. Furthermore, the substantial improvement\nover vanilla GPT-3.5-turbo underscores the efficacy\nof our approach in domain-specific ADQA.\nFurthermore, we observe that the performance\nof biomedical-specific LLMs generally lags be-\nhind that of GPT-3.5-turbo. We attribute this dis-\ncrepancy to the smaller size of these biomedical\nLLMs. While they may perform adequately in gen-\neral medical contexts, they fall short in the AD\nscenario, which demands more domain-specific\nknowledge. In the case of GPT-3.5-turbo combined\nwith various RAG methods, it is evident that most\nRAG methods enhance the models’ performance.\nAmong them, GPT-3.5-turbo with Clinfo.ai yields\nthe most significant improvement, boosting the ac-\ncuracy score from 67.1 to 70.1 compared to vanilla\nGPT-3.5-turbo. However, it is important to note\nthat the original Clinfo.ai necessitates access to\nthe PubMed API, constituting an external resource.\nWhen we disable this access and solely utilize the\nsame corpora as in DALK within the Clinfo.ai re-\ntrieval system, the improvement it brings becomes\nmarginal and incomparable to our method. Due to\nthe space limitation, we put more RAG results with\ndifferent hyper-parameters in Appendix D.\n4.4\nAblation Study on Self-aware Knowledge\nRetrieval\nIn this section, we evaluate the efficacy of our\nproposed self-aware knowledge retrieval method\nthrough an ablation study.\nAs depicted in Ta-\nble 3, we observe that while the dynamic co-\naugmentation framework without the self-aware", "metadata": {"producer": "pdfTeX-1.40.25", "creator": "LaTeX with hyperref", "creationdate": "2024-05-09T00:18:37+00:00", "source": "/home/donbr/open-biosciences/biosciences-research/data/raw/2405.04819v1.pdf", "file_path": "/home/donbr/open-biosciences/biosciences-research/data/raw/2405.04819v1.pdf", "total_pages": 18, "format": "PDF 1.5", "title": "", "author": "", "subject": "", "keywords": "", "moddate": "2024-05-09T00:18:37+00:00", "trapped": "", "modDate": "D:20240509001837Z", "creationDate": "D:20240509001837Z", "page": 5}}
{"page_content": "MedQA\nMedMCQA\nMMLU\nQA4MRE\nAVG\nAVG Length\n107.4\n23.8\n342.9\n17.6\n122.9\nGPT-3.5-turbo\n50.0\n71.9\n83.6\n62.9\n67.1\nDALK\n57.9\n75.2\n85.4\n71.4\n72.6\nDALK\nw/o self-aware\nknowledge retrieval\n56.5\n71.0\n77.6\n77.1\n70.6\nTable 3: Ablation study results with and without our\nproposed self-aware knowledge retrieval.\nknowledge retrieval module still enhances the\nmodel’s performance, the overall improvement is\nless pronounced. Furthermore, we observe that the\nefficacy of self-aware knowledge retrieval corre-\nlates with the length of queries within a given con-\ntext. For instance, a notable enhancement in per-\nformance is evident within the MMLU sub-dataset\nupon the implementation of self-aware knowledge\nretrieval. We attribute this to the fact that questions\nin the MMLU dataset typically contain longer con-\ntexts compared to other medical QA datasets inte-\ngrated into ADQA. Consequently, irrelevant knowl-\nedge sourced from the context may exacerbate the\nissue of information noise thus underscoring the ne-\ncessity for self-aware retrieval. Conversely, within\nQA4MRE, characterized by shorter query lengths,\nthe application of self-aware knowledge retrieval\ncan even lead to a decline in performance.\n4.5\nAblation Study on KG Construction\nAVG\n#Triples\nGPT-3.5-turbo\n67.1\n-\nDALK\nw/ Generative KG\n72.6\n53,585\nDALK\nw/ RE KG\n66.3\n171,431\nTable 4: Ablation study results with generative construc-\ntion and RE construction.\nTable 4 illustrates the results of the ablation study\nconducted using generatively constructed KG and\nRE-constructed KG. Surprisingly, despite the RE\nmethod yielding a KG with a larger scale and more\ntriples, knowledge sampled from it has unexpect-\nedly resulted in a non-trivial drop in performance\nwithin ADQA. After a manual examination of the\ntwo constructed knowledge graphs, we find LLMs\nwith the RE construction method have a strong in-\nclination to wrongly assign a relationship to two\nunrelated entities, which has been exposed by the\nprevious studies (Wan et al., 2023). In contrast, the\ngenerative construction approach exclusively out-\nputs triples that LLMs confidently endorse, yield-\ning a smaller yet more precise knowledge graph.\nThis trade-off between coverage and accuracy un-\nderscores the critical importance of denoising in\nthe construction of KGs by LLMs.\n5\nFurther Analysis\n5.1\nCo-augmentation Analysis\nFigure 3: The size of the knowledge graph (triplet num-\nber) and the KG-augmented GPT-3.5-turbo’s perfor-\nmance (accuracy) over time.\nTo comprehensively understand how the perfor-\nmance of LLMs evolves in response to increasing\nKG sizes, we undertake a detailed co-augmentation\nanalysis. Illustrated in Figure 3, our experiments\naim to discern the changing performance trends of\nLLMs as the knowledge triples accumulate annu-\nally. Our findings reveal that our framework effec-\ntively fosters the co-evolution of LLMs and KG,\nwith the performance of KG-augmented LLMs ex-\nhibiting a generally upward trajectory as the KG\nexpands. Notably, when we remove the self-aware\nknowledge retrieval module, this upward trend be-\ncomes less significant. This further implies the\nimportance of sampling and selecting appropriate\nknowledge for LLMs when the KG’s size increases.\n5.2\nHyper-parameter Analysis\nIn this section, we do a hyper-parameter analy-\nsis on the retrieval number k of our self-aware re-\ntrieval module. We select a group of value for k\n([1,3,5,10,20,30]) and present the experiment re-\nsults in Figure 4. We show the accuracy score on\nMedQA, MedMCQA, QA4MRE and AVG with\ndifferent k. We find when k is small, an incre-\nment to it can lead to a performance enhancement.\nAfter the best performance shows up, continually\nincreasing the value of k will cause a smooth de-\ncrease in the model accuracy score. This result\nindicates the knowledge ranked in the top positions", "metadata": {"producer": "pdfTeX-1.40.25", "creator": "LaTeX with hyperref", "creationdate": "2024-05-09T00:18:37+00:00", "source": "/home/donbr/open-biosciences/biosciences-research/data/raw/2405.04819v1.pdf", "file_path": "/home/donbr/open-biosciences/biosciences-research/data/raw/2405.04819v1.pdf", "total_pages": 18, "format": "PDF 1.5", "title": "", "author": "", "subject": "", "keywords": "", "moddate": "2024-05-09T00:18:37+00:00", "trapped": "", "modDate": "D:20240509001837Z", "creationDate": "D:20240509001837Z", "page": 6}}
{"page_content": "Path-based Sub-graph\nAnswer\nBaseline\n-\nC %\nDALK\n-w/o self-aware\nknowledge retrieval\nneurofibrillary tangles->FORM BY->microtubule-associated protein tau...\n...\nentorhinal cortex->is a part of->brain->ASSOCIATES->mouse with\nAlzheimer’s disease->brain region->temporal lobe\nC %\nDALK\nReranked Triples1: entorhinal cortex ->is a part of ->brain\nReranked Triples2: entorhinal cortex ->associates ->mouse with Alzheimer’s disease\nReranked Triples3: temporal lobe ->affected by ->Alzheimer’s disease\nD !\nTable 5: A case to show the effectiveness of DALK. The question is: “The area of the brain resistant to Neurofibrillary\ntangles of Alzheimer’s disease is: A. Visual association areas B. Entorhinal coex C. Temporal lobe D.Lateral\ngeniculate body”\nFigure 4: Different self-aware knowledge retrieval set-\ntings in MedQA, MedMCQA, QA4MRE and the aver-\nage score (AVG).\nis more helpful while the knowledge ranked behind\nis something not very useful, thus successfully val-\nidating the capability of LLMs to do a fine-grained\nknowledge reranking.\nMoreover, we find the best k value is correlated\nwith the length of queries in each single dataset. For\nexample, the best performance in MedQA (average\nquery length is 107.4) shows up when k = 10 while\nthe best performance in MedMCQA and QA4MRE\nshows up when k = 5 and 3 respectively. This is\nconsistent with our findings in Section 4.4 that a\nlonger query corresponds to a larger and noisier\nsub-knowledge graph.\n5.3\nSensitivity Analysis on ADQA Benchmark\nIn this section, we conduct a sensitivity analysis\nfor our constructed ADQA by conducting a leave-\none-out evaluation on AD-related keywords. We\ndo it by removing the samples with each keyword\nin our keyword list and calculating the AVG score\nof the remaining samples. As the result shown\nin Table 6, we find not all of the keywords are\nincorporated in our ADQA benchmark. Notably,\nBenchmark\nDALK\nDALK\nw/o self-aware\nknowledge retrieval\nADQA\n72.6\n70.6\nw/o “Alzheimer”\n72.1\n70.4\nw/o “Dementia”\n72.4\n71.3\nw/o “APOE”\n73.2\n71.2\nw/o “Amyloid beta”\n73.5\n70.7\nw/o “Aging”\n72.9\n71.4\nw/o “Lipoprotein”\n73.1\n71.0\nw/o “Microglia”\n72.8\n70.9\nTable 6: Sensitivity analysis for ADQA benchmark with\na leave-one-out evaluation on AD-related keywords.\nthe keywords “CSF Biomarkers”, “Neurogenesis”,\n“PET Amyloid”, “PET Tau”, “Tau Phosphorylation”\nlack corresponding samples in ADQA. We believe\none critical work in the future for benchmarking\nAD-related knowledge is to collect QA samples\nto cover these missing keywords. Moreover, ana-\nlyzing the performance variation upon removing\nsamples linked to each keyword offers insight into\ndetermining the relevance of the keyword to AD.\n5.4\nCase Study\nWe put an example in Table 5 to showcase the effi-\ncacy of DALK. We notice while the path-based sub-\ngraph contains the relevant knowledge to exclude\noption C, it still involves other irrelevant informa-\ntion and finally fails to prompt the LLMs to pro-\nduce the correct answer. In contrast, our self-aware\nknowledge retrieval method successfully chooses\nthe top 3 most relevant triples for the given problem\nand results in the correct answer D.\n6\nConclusion\nIn this research, we begin by analyzing the main\nlimitations of adopting the existing LLMs-based\nmethods in AD-specific areas. To address these\nissues, we propose a novel approach in the merging\nof large language models and knowledge graphs\nin the context of Alzheimer’s Disease. Our team", "metadata": {"producer": "pdfTeX-1.40.25", "creator": "LaTeX with hyperref", "creationdate": "2024-05-09T00:18:37+00:00", "source": "/home/donbr/open-biosciences/biosciences-research/data/raw/2405.04819v1.pdf", "file_path": "/home/donbr/open-biosciences/biosciences-research/data/raw/2405.04819v1.pdf", "total_pages": 18, "format": "PDF 1.5", "title": "", "author": "", "subject": "", "keywords": "", "moddate": "2024-05-09T00:18:37+00:00", "trapped": "", "modDate": "D:20240509001837Z", "creationDate": "D:20240509001837Z", "page": 7}}
{"page_content": "provides an innovative dynamic co-augmentation\nframework for the refinement of large language\nmodels and knowledge graphs. Initially, our ap-\nproach extracts structural insights from the unstruc-\ntured scientific literature, crafting a specialized\nknowledge graph for AD. Subsequently, we employ\na coarse-to-fine sampling technique coupled with\na unique self-aware knowledge retrieval strategy\nto pinpoint relevant information from the knowl-\nedge graph. The extensive evaluation conducted\nin our constructed ADQA benchmark showcases\nthe effectiveness of our method and provides fur-\nther hints into the synergy of LLMs and knowledge\ngraph in the context of AD. In the future, we will\ndo more exploration in adopting and benchmarking\nLLMs in the AD areas.\n7\nLimitations\nIn the development of our AD-KG, our primary\nfocus lies in the exploration of two distinct meth-\nods for extracting relationships between associated\nentities. For entity recognition, we employ a strong\nPubTator annotator directly, without delving into\nthe utilization of LLMs in this context. However,\nwe have observed that LLMs also exhibit promis-\ning entity extraction capabilities in Section 3.2.1.\nWe defer the refinement of methods for extract-\ning entities for KG construction with LLMs to fu-\nture works. Furthermore, a significant contribution\nof our work is the establishment of the ADQA\nbenchmark. Nonetheless, the datasets utilized in\nconstructing ADQA primarily consist of medical\nschool exam questions, potentially exhibiting a do-\nmain gap from the scientific literature informing\nAD-KG. One potential remedy is leveraging Pub-\nmedQA (Jin et al., 2019); however, it is hindered\nby limited data amount. In the future, we will keep\ngathering AD-related QA samples and expanding\nthe size of our ADQA benchmark.\n8\nEthics Statement\nWe have familiarized ourselves with and honour\nthe ethical code set out in the ACL Code of Ethics2.\nThe knowledge graphs constructed in the paper\nare based on published scientific literature from\nPubMed. The ADQA dataset used in the study is\nalso derived from publicly available medical QA\ndatasets that are properly cited. We strive to ensure\nour study upholds ethical principles and not cause\nany kind of safety or privacy concerns. Although\n2https://www.aclweb.org/portal/content/acl-code-ethics\nnot observed in our multiple-choice QA analysis,\nwe recognize the possibility of factual errors and\nhallucinations when using pre-trained LLMs for\nmedical QA tasks in general, and we do not recom-\nmend these models be applied in a practical setting\nat present.\nReferences\nFelix Agbavor and Hualou Liang. 2022. Predicting\ndementia from spontaneous speech using large lan-\nguage models. PLOS Digital Health, 1:1–14.\nRohan Anil, Andrew M Dai, Orhan Firat, Melvin John-\nson, Dmitry Lepikhin, Alexandre Passos, Siamak\nShakeri, Emanuel Taropa, Paige Bailey, Zhifeng\nChen, et al. 2023. Palm 2 technical report. arXiv\npreprint arXiv:2305.10403.\nAparna Balagopalan, Benjamin Eyre, Frank Rudzicz,\nand Jekaterina Novikova. 2020. To BERT or not\nto BERT: Comparing Speech and Language-Based\nApproaches for Alzheimer’s Disease Detection. In\nProc. Interspeech 2020, pages 2167–2171.\nMaciej Besta, Nils Blach, Ales Kubicek, Robert Ger-\nstenberger, Lukas Gianinazzi, Joanna Gajda, Tomasz\nLehmann, Michal Podstawski, Hubert Niewiadomski,\nPiotr Nyczyk, et al. 2023. Graph of thoughts: Solv-\ning elaborate problems with large language models.\narXiv preprint arXiv:2308.09687.\nZhen Bi, Jing Chen, Yinuo Jiang, Feiyu Xiong, Wei Guo,\nHuajun Chen, and Ningyu Zhang. 2024. Codekgc:\nCode language model for generative knowledge\ngraph construction.\nACM Transactions on Asian\nand Low-Resource Language Information Process-\ning, 23(3):1–16.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. Advances in neural information processing\nsystems, 33:1877–1901.\nSalvatore Carta, Alessandro Giuliani, Leonardo Piano,\nAlessandro Sebastian Podda, Livio Pompianu, and\nSandro Gabriele Tiddia. 2023. Iterative zero-shot llm\nprompting for knowledge graph construction. arXiv\npreprint arXiv:2307.01128.\nJiawei Chen, Hongyu Lin, Xianpei Han, and Le Sun.\n2024.\nBenchmarking large language models in\nretrieval-augmented generation. In Proceedings of\nthe AAAI Conference on Artificial Intelligence, vol-\nume 38, pages 17754–17762.\nZeming Chen, Alejandro Hernández Cano, Angelika\nRomanou, Antoine Bonnet, Kyle Matoba, Francesco\nSalvi, Matteo Pagliardini, Simin Fan, Andreas Köpf,\nAmirkeivan Mohtashami, et al. 2023. Meditron-70b:\nScaling medical pretraining for large language mod-\nels. arXiv preprint arXiv:2311.16079.", "metadata": {"producer": "pdfTeX-1.40.25", "creator": "LaTeX with hyperref", "creationdate": "2024-05-09T00:18:37+00:00", "source": "/home/donbr/open-biosciences/biosciences-research/data/raw/2405.04819v1.pdf", "file_path": "/home/donbr/open-biosciences/biosciences-research/data/raw/2405.04819v1.pdf", "total_pages": 18, "format": "PDF 1.5", "title": "", "author": "", "subject": "", "keywords": "", "moddate": "2024-05-09T00:18:37+00:00", "trapped": "", "modDate": "D:20240509001837Z", "creationDate": "D:20240509001837Z", "page": 8}}
{"page_content": "Geesa Daluwatumulle, Rupika Wijesinghe, and Ruvan\nWeerasinghe. 2023. In silico drug repurposing us-\ning knowledge graph embeddings for alzheimer’s\ndisease. In Proceedings of the 9th International Con-\nference on Bioinformatics Research and Applications,\nICBRA ’22, page 61–66, New York, NY, USA. As-\nsociation for Computing Machinery.\nPreetha Datta, Fedor Vitiugin, Anastasiia Chizhikova,\nand Nitin Sawhney. 2024. Construction of hyper-\nrelational knowledge graphs using pre-trained large\nlanguage models. arXiv preprint arXiv:2403.11786.\nStefan Dernbach, Khushbu Agarwal, Alejandro Zu-\nniga, Michael Henry, and Sutanay Choudhury. 2024.\nGlam: Fine-tuning large language models for domain\nknowledge graph alignment via neighborhood par-\ntitioning and generative subgraph encoding. arXiv\npreprint arXiv:2402.06764.\nTianqing Fang,\nHongming Zhang,\nWeiqi Wang,\nYangqiu Song, and Bin He. 2021. Discos: bridg-\ning the gap between discourse knowledge and com-\nmonsense knowledge. In Proceedings of the Web\nConference 2021, pages 2648–2659.\nYingjie Feng, Xiaoyin Xu, Yueting Zhuang, and Min\nZhang. 2023.\nLarge language models improve\nalzheimer’s disease diagnosis using multi-modality\ndata. In 2023 IEEE International Conference on\nMedical Artificial Intelligence (MedAI), pages 61–66.\nIEEE.\nJiuzhou Han, Nigel Collier, Wray Buntine, and Ehsan\nShareghi. 2023. Pive: Prompting with iterative verifi-\ncation improving graph-based generative capability\nof llms. arXiv preprint arXiv:2305.12392.\nDan Hendrycks, Collin Burns, Steven Basart, Andy Zou,\nMantas Mazeika, Dawn Song, and Jacob Steinhardt.\n2021. Measuring massive multitask language under-\nstanding. In International Conference on Learning\nRepresentations.\nDaniel Scott Himmelstein, Antoine Lizee, Christine\nHessler, Leo Brueggeman, Sabrina L Chen, Dexter\nHadley, Ari Green, Pouya Khankhanian, and Ser-\ngio E Baranzini. 2017. Systematic integration of\nbiomedical knowledge prioritizes drugs for repurpos-\ning. Elife, 6:e26726.\nKang-Lin Hsieh, German Plascencia-Villa, Ko-Hong\nLin, George Perry, Xiaoqian Jiang, and Yejin Kim.\n2023. Synthesize heterogeneous biological knowl-\nedge via representation learning for alzheimer’s dis-\nease drug repurposing. iScience, 26(1):105678.\nEdward J Hu, Phillip Wallis, Zeyuan Allen-Zhu,\nYuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen,\net al. 2021. Lora: Low-rank adaptation of large lan-\nguage models. In International Conference on Learn-\ning Representations.\nJinhao Jiang, Kun Zhou, Zican Dong, Keming Ye,\nWayne Xin Zhao, and Ji-Rong Wen. 2023. Structgpt:\nA general framework for large language model to\nreason over structured data. In Proceedings of the\n2023 Conference on Empirical Methods in Natural\nLanguage Processing, pages 9237–9251.\nDi Jin, Eileen Pan, Nassim Oufattole, Wei-Hung Weng,\nHanyi Fang, and Peter Szolovits. 2021. What disease\ndoes this patient have? a large-scale open domain\nquestion answering dataset from medical exams. Ap-\nplied Sciences, 11(14):6421.\nQiao Jin, Bhuwan Dhingra, Zhengping Liu, William W\nCohen, and Xinghua Lu. 2019. Pubmedqa: A dataset\nfor biomedical research question answering. arXiv\npreprint arXiv:1909.06146.\nNikhil Kandpal, Haikang Deng, Adam Roberts, Eric\nWallace, and Colin Raffel. 2023. Large language\nmodels struggle to learn long-tail knowledge. In In-\nternational Conference on Machine Learning, pages\n15696–15707. PMLR.\nJiho Kim, Yeonsu Kwon, Yohan Jo, and Edward Choi.\n2023. Kg-gpt: A general framework for reasoning\non knowledge graphs using large language models.\nIn Findings of the Association for Computational\nLinguistics: EMNLP 2023, pages 9410–9421.\nYanis Labrak, Adrien Bazoge, Emmanuel Morin, Pierre-\nAntoine Gourraud, Mickael Rouvier, and Richard\nDufour. 2024.\nBiomistral: A collection of open-\nsource pretrained large language models for medical\ndomains. arXiv preprint arXiv:2402.10373.\nDawei Li, Yanran Li, Jiayi Zhang, Ke Li, Chen Wei,\nJianwei Cui, and Bin Wang. 2022. C3kg: A chi-\nnese commonsense conversation knowledge graph.\nIn Findings of the Association for Computational\nLinguistics: ACL 2022, pages 1369–1383.\nDawei Li, Yaxuan Li, Dheeraj Mekala, Shuyao\nLi, Xueqi Wang, William Hogan, Jingbo Shang,\net al. 2023a.\nDail:\nData augmentation for in-\ncontext learning via self-paraphrase. arXiv preprint\narXiv:2311.03319.\nDawei Li, Zhen Tan, Tianlong Chen, and Huan Liu.\n2024. Contextualization distillation from large lan-\nguage model for knowledge graph completion. arXiv\npreprint arXiv:2402.01729.\nDawei Li, Hengyuan Zhang, Yanran Li, and Shiping\nYang. 2023b. Multi-level contrastive learning for\nscript-based character understanding. In Proceedings\nof the 2023 Conference on Empirical Methods in\nNatural Language Processing, pages 5995–6013.\nRumeng Li, Xun Wang, and Hong Yu. 2023c. Two\ndirections for clinical data generation with large lan-\nguage models: Data-to-label and label-to-data. In\nProceedings of the 2023 Conference on Empirical\nMethods in Natural Language Processing, pages\n7129–7143.\nYucheng Li, Bo Dong, Frank Guerin, and Chenghua Lin.\n2023d. Compressing context to enhance inference\nefficiency of large language models. In Proceedings", "metadata": {"producer": "pdfTeX-1.40.25", "creator": "LaTeX with hyperref", "creationdate": "2024-05-09T00:18:37+00:00", "source": "/home/donbr/open-biosciences/biosciences-research/data/raw/2405.04819v1.pdf", "file_path": "/home/donbr/open-biosciences/biosciences-research/data/raw/2405.04819v1.pdf", "total_pages": 18, "format": "PDF 1.5", "title": "", "author": "", "subject": "", "keywords": "", "moddate": "2024-05-09T00:18:37+00:00", "trapped": "", "modDate": "D:20240509001837Z", "creationDate": "D:20240509001837Z", "page": 9}}
{"page_content": "of the 2023 Conference on Empirical Methods in\nNatural Language Processing, pages 6342–6353.\nAlejandro Lozano, Scott L Fleming, Chia-Chun Chiang,\nand Nigam Shah. 2023. Clinfo. ai: An open-source\nretrieval-augmented large language model system for\nanswering medical questions using scientific litera-\nture. In PACIFIC SYMPOSIUM ON BIOCOMPUT-\nING 2024, pages 8–23. World Scientific.\nXindi Luo, Zequn Sun, Jing Zhao, Zhe Zhao, and Wei\nHu. 2024. Knowla: Enhancing parameter-efficient\nfinetuning with knowledgeable adaptation. arXiv\npreprint arXiv:2403.14950.\nYubo Ma, Yixin Cao, Yong Hong, and Aixin Sun. 2023.\nLarge language model is not a good few-shot informa-\ntion extractor, but a good reranker for hard samples!\nIn Findings of the Association for Computational\nLinguistics: EMNLP 2023, pages 10572–10601.\nChengsheng Mao, Jie Xu, Luke Rasmussen, Yikuan\nLi, Prakash Adekkanattu, Jennifer Pacheco, Borna\nBonakdarpour, Robert Vassar, Li Shen, Guoqian\nJiang, Fei Wang, Jyotishman Pathak, and Yuan Luo.\n2023. Ad-bert: Using pre-trained language model to\npredict the progression from mild cognitive impair-\nment to alzheimer’s disease. Journal of Biomedical\nInformatics, 144:104442.\nC L Masters, G Simms, N A Weinman, G Multhaup,\nB L McDonald, and K Beyreuther. 1985. Amyloid\nplaque core protein in alzheimer disease and down\nsyndrome. Proceedings of the National Academy of\nSciences, 82(12):4245–4249.\nGeorge A Miller. 1995. Wordnet: a lexical database for\nenglish. Communications of the ACM, 38(11):39–41.\nYi Nian, Xinyue Hu, Rui Zhang, Jingna Feng, Jingcheng\nDu, Fang Li, Yong Chen, and Cui Tao. 2022. Mining\non alzheimer’s diseases related knowledge graph to\nidentity potential ad-related semantic triples for drug\nrepurposing. BMC Bioinformatics, 23.\nOpenAI. 2022. Introducing chatgpt.\nOpenAI. 2024. New embedding models and api up-\ndates.\nOded Ovadia, Menachem Brief, Moshik Mishaeli, and\nOren Elisha. 2023. Fine-tuning or retrieval? com-\nparing knowledge injection in llms. arXiv preprint\narXiv:2312.05934.\nAnkit Pal, Logesh Kumar Umapathi, and Malaikannan\nSankarasubbu. 2022. Medmcqa: A large-scale multi-\nsubject multi-choice dataset for medical domain ques-\ntion answering. In Proceedings of the Conference\non Health, Inference, and Learning, volume 174 of\nProceedings of Machine Learning Research, pages\n248–260. PMLR.\nLiangming Pan, Michael Saxon, Wenda Xu, Deepak\nNathani, Xinyi Wang, and William Yang Wang. 2023.\nAutomatically correcting large language models: Sur-\nveying the landscape of diverse self-correction strate-\ngies. arXiv preprint arXiv:2308.03188.\nShirui Pan, Linhao Luo, Yufei Wang, Chen Chen, Ji-\napu Wang, and Xindong Wu. 2024. Unifying large\nlanguage models and knowledge graphs: A roadmap.\nIEEE Transactions on Knowledge and Data Engi-\nneering.\nAnselmo Peñas, Eduard H. Hovy, Pamela Forner, Ál-\nvaro Rodrigo, Richard F. E. Sutcliffe, and Roser\nMorante. 2013. Qa4mre 2011-2013: Overview of\nquestion answering for machine reading evaluation.\nIn International Conference of the Cross-Language\nEvaluation Forum for European Languages, pages\n303–320. Springer.\nYiyuan Pu, Daniel Beck, and Karin Verspoor. 2023.\nGraph embedding-based link prediction for literature-\nbased discovery in alzheimer’s disease. Journal of\nBiomedical Informatics, 145:104464.\nJulio C Rangel, Tarcisio Mendes de Farias, Ana Claudia\nSima, and Norio Kobayashi. 2024. Sparql generation:\nan analysis on fine-tuning openllama for question\nanswering over a life science knowledge graph. arXiv\npreprint arXiv:2402.04627.\nNils Reimers and Iryna Gurevych. 2019. Sentence-bert:\nSentence embeddings using siamese bert-networks.\nIn Proceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the 9th\nInternational Joint Conference on Natural Language\nProcessing (EMNLP-IJCNLP), pages 3982–3992.\nAlzheimer’s\nAssociation\nReport.\n2023.\n2023\nalzheimer’s disease facts and figures. Alzheimer’s\n& Dementia, 19(4):1598–1695.\nJoseph Romano, Van Truong, Rachit Kumar, Mythr-\neye Venkatesan, Britney Graham, Yun Hao, Nick\nMatsumoto, Xi Li, Zhiping Wang, Marylyn Ritchie,\nLi Shen, and Jason Moore. 2023. The alzheimer’s\nknowledge base – a knowledge graph for therapeutic\ndiscovery in alzheimer’s disease research (preprint).\nJournal of Medical Internet Research.\nChang Shu, Baian Chen, Fangyu Liu, Zihao Fu, Ehsan\nShareghi, and Nigel Collier. 2023.\nVisual med-\nalpaca: A parameter-efficient biomedical llm with\nvisual capabilities.\nRobyn Speer, Joshua Chin, and Catherine Havasi. 2017.\nConceptnet 5.5: An open multilingual graph of gen-\neral knowledge. In Proceedings of the AAAI confer-\nence on artificial intelligence, volume 31.\nJiashuo Sun, Chengjin Xu, Lumingyuan Tang, Saizhuo\nWang, Chen Lin, Yeyun Gong, Heung-Yeung Shum,\nand Jian Guo. 2023a. Think-on-graph: Deep and\nresponsible reasoning of large language model with\nknowledge graph. arXiv preprint arXiv:2307.07697.\nWeiwei Sun, Lingyong Yan, Xinyu Ma, Shuaiqiang\nWang, Pengjie Ren, Zhumin Chen, Dawei Yin, and\nZhaochun Ren. 2023b. Is chatgpt good at search?\ninvestigating large language models as re-ranking\nagents. In Proceedings of the 2023 Conference on\nEmpirical Methods in Natural Language Processing,\npages 14918–14937.", "metadata": {"producer": "pdfTeX-1.40.25", "creator": "LaTeX with hyperref", "creationdate": "2024-05-09T00:18:37+00:00", "source": "/home/donbr/open-biosciences/biosciences-research/data/raw/2405.04819v1.pdf", "file_path": "/home/donbr/open-biosciences/biosciences-research/data/raw/2405.04819v1.pdf", "total_pages": 18, "format": "PDF 1.5", "title": "", "author": "", "subject": "", "keywords": "", "moddate": "2024-05-09T00:18:37+00:00", "trapped": "", "modDate": "D:20240509001837Z", "creationDate": "D:20240509001837Z", "page": 10}}
{"page_content": "Zhen Tan, Alimohammad Beigi, Song Wang, Ruocheng\nGuo, Amrita Bhattacharjee, Bohan Jiang, Mansooreh\nKarami, Jundong Li, Lu Cheng, and Huan Liu. 2024.\nLarge language models for data annotation: A survey.\narXiv preprint arXiv:2402.13446.\nJiabin Tang, Yuhao Yang, Wei Wei, Lei Shi, Lixin Su,\nSuqi Cheng, Dawei Yin, and Chao Huang. 2023.\nGraphgpt: Graph instruction tuning for large lan-\nguage models. arXiv preprint arXiv:2310.13023.\nYongqi Tong, Dawei Li, Sizhe Wang, Yujia Wang, Fei\nTeng, and Jingbo Shang. 2024. Can llms learn from\nprevious mistakes? investigating llms’ errors to boost\nfor reasoning. arXiv preprint arXiv:2403.20046.\nYongqi Tong, Yifan Wang, Dawei Li, Sizhe Wang,\nZi Lin, Simeng Han, and Jingbo Shang. 2023. Elimi-\nnating reasoning via inferring with planning: A new\nframework to guide llms’ non-linear thinking. arXiv\npreprint arXiv:2310.12342.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timothee Lacroix,\nBaptiste Rozière, Naman Goyal, Eric Hambro, Faisal\nAzhar, et al. Llama: Open and efficient foundation\nlanguage models.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Al-\nbert, Amjad Almahairi, Yasmine Babaei, Nikolay\nBashlykov, Soumya Batra, Prajjwal Bhargava, Shruti\nBhosale, et al. 2023.\nLlama 2:\nOpen founda-\ntion and fine-tuned chat models.\narXiv preprint\narXiv:2307.09288.\nDenny Vrandeˇci´c and Markus Krötzsch. 2014. Wiki-\ndata: a free collaborative knowledgebase. Communi-\ncations of the ACM, 57(10):78–85.\nSomin Wadhwa, Silvio Amir, and Byron C Wallace.\n2023. Revisiting relation extraction in the era of\nlarge language models. In Proceedings of the 61st\nAnnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pages 15566–\n15589.\nZhen Wan, Fei Cheng, Zhuoyuan Mao, Qianying Liu,\nHaiyue Song, Jiwei Li, and Sadao Kurohashi. 2023.\nGpt-re: In-context learning for relation extraction\nusing large language models. In Proceedings of the\n2023 Conference on Empirical Methods in Natural\nLanguage Processing, pages 3534–3547.\nXuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V Le,\nEd H Chi, Sharan Narang, Aakanksha Chowdhery,\nand Denny Zhou. 2022. Self-consistency improves\nchain of thought reasoning in language models. In\nThe Eleventh International Conference on Learning\nRepresentations.\nChih-Hsuan Wei, Hung-Yu Kao, and Zhiyong Lu. 2013.\nPubtator: a web-based text mining tool for assisting\nbiocuration. Nucleic acids research, 41(W1):W518–\nW522.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou,\net al. 2022. Chain-of-thought prompting elicits rea-\nsoning in large language models. Advances in Neural\nInformation Processing Systems, 35:24824–24837.\nYanbin Wei, Qiushi Huang, Yu Zhang, and James Kwok.\n2023. Kicgpt: Large language model with knowl-\nedge in context for knowledge graph completion. In\nFindings of the Association for Computational Lin-\nguistics: EMNLP 2023, pages 8667–8683.\nYilin Wen, Zifeng Wang, and Jimeng Sun. 2023.\nMindmap: Knowledge graph prompting sparks graph\nof thoughts in large language models. arXiv preprint\narXiv:2308.09729.\nSiye Wu, Jian Xie, Jiangjie Chen, Tinghui Zhu, Kai\nZhang, and Yanghua Xiao. 2024. How easily do\nirrelevant inputs skew the responses of large language\nmodels? arXiv preprint arXiv:2404.03302.\nChao Yan, Monika Grabowska, Alyson Dickson, Bing-\nshan Li, Zhexing Wen, Dan Roden, C. Stein, Pe-\nter Embí, Josh Peterson, Qiping Feng, Bradley Ma-\nlin, and Wei-Qi Wei. 2024.\nLeveraging genera-\ntive ai to prioritize drug repurposing candidates for\nalzheimer’s disease with real-world clinical valida-\ntion. npj Digital Medicine, 7.\nShunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran,\nThomas L Griffiths,\nYuan Cao,\nand Karthik\nNarasimhan. 2023.\nTree of thoughts: Deliberate\nproblem solving with large language models. arXiv\npreprint arXiv:2305.10601.\nWenhao Yu, Hongming Zhang, Xiaoman Pan, Kaixin\nMa, Hongwei Wang, and Dong Yu. 2023. Chain-of-\nnote: Enhancing robustness in retrieval-augmented\nlanguage models. arXiv preprint arXiv:2311.09210.\nWeizhe Yuan, Richard Yuanzhe Pang, Kyunghyun Cho,\nSainbayar Sukhbaatar, Jing Xu, and Jason Weston.\n2024.\nSelf-rewarding language models.\narXiv\npreprint arXiv:2401.10020.\nLi Yunxiang, Li Zihan, Zhang Kai, Dan Ruilong, and\nZhang You. 2023. Chatdoctor: A medical chat model\nfine-tuned on llama model using medical domain\nknowledge. arXiv preprint arXiv:2303.14070.\nCyril Zakka, Rohan Shad, Akash Chaurasia, Alex R\nDalal, Jennifer L Kim, Michael Moor, Robyn Fong,\nCurran Phillips, Kevin Alexander, Euan Ashley,\net al. 2024.\nAlmanac—retrieval-augmented lan-\nguage models for clinical medicine.\nNEJM AI,\n1(2):AIoa2300068.\nHengyuan Zhang, Yanru Wu, Dawei Li, Zacc Yang, Rui\nZhao, Yong Jiang, and Fei Tan. 2024. Balancing spe-\nciality and versatility: a coarse to fine framework for\nsupervised fine-tuning large language model. arXiv\npreprint arXiv:2404.10306.", "metadata": {"producer": "pdfTeX-1.40.25", "creator": "LaTeX with hyperref", "creationdate": "2024-05-09T00:18:37+00:00", "source": "/home/donbr/open-biosciences/biosciences-research/data/raw/2405.04819v1.pdf", "file_path": "/home/donbr/open-biosciences/biosciences-research/data/raw/2405.04819v1.pdf", "total_pages": 18, "format": "PDF 1.5", "title": "", "author": "", "subject": "", "keywords": "", "moddate": "2024-05-09T00:18:37+00:00", "trapped": "", "modDate": "D:20240509001837Z", "creationDate": "D:20240509001837Z", "page": 11}}
{"page_content": "Hongming Zhang, Xin Liu, Haojie Pan, Yangqiu Song,\nand Cane Wing-Ki Leung. 2020. Aser: A large-scale\neventuality knowledge graph. In Proceedings of the\nweb conference 2020, pages 201–211.\nKai Zhang, Jun Yu, Zhiling Yan, Yixin Liu, Eashan Ad-\nhikarla, Sunyang Fu, Xun Chen, Chen Chen, Yuyin\nZhou, Xiang Li, et al. 2023a. Biomedgpt: A uni-\nfied and generalist biomedical generative pre-trained\ntransformer for vision, language, and multimodal\ntasks. arXiv preprint arXiv:2305.17100.\nSusan Zhang, Stephen Roller, Naman Goyal, Mikel\nArtetxe, Moya Chen, Shuohui Chen, Christopher De-\nwan, Mona Diab, Xian Li, Xi Victoria Lin, et al. 2022.\nOpt: Open pre-trained transformer language models.\narXiv preprint arXiv:2205.01068.\nYichi Zhang, Zhuo Chen, Wen Zhang, and Huajun Chen.\n2023b. Making large language models perform bet-\nter in knowledge graph completion. arXiv preprint\narXiv:2310.06671.", "metadata": {"producer": "pdfTeX-1.40.25", "creator": "LaTeX with hyperref", "creationdate": "2024-05-09T00:18:37+00:00", "source": "/home/donbr/open-biosciences/biosciences-research/data/raw/2405.04819v1.pdf", "file_path": "/home/donbr/open-biosciences/biosciences-research/data/raw/2405.04819v1.pdf", "total_pages": 18, "format": "PDF 1.5", "title": "", "author": "", "subject": "", "keywords": "", "moddate": "2024-05-09T00:18:37+00:00", "trapped": "", "modDate": "D:20240509001837Z", "creationDate": "D:20240509001837Z", "page": 12}}
{"page_content": "A\nDetails of LLMs for KG\nTable 7 and 8 present examples of our two KG construction methods respectively. For both methods, we\nadopt a select-or-generate prompt to instruct the LLM whether to choose a relation from hetionet (Him-\nmelstein et al., 2017), a well-built general medical KG, or generate a new one to describe the relationship\nbetween two entities. In the RE construction method, we also conduct a type matching (Table 9) for each\nentity from type name of PubTator to that of Hetionet and ask the LLM to choose from the relation set\nthat corresponds to the two entities’ types (Table 10).\nInput\nRead the following abstract, extract the relationships between each entity.You can\nchoose the relation from: (covaries, interacts, regulates, resembles, downregulates,\nupregulates, associates, binds, treats, palliates), or generate a new predicate to\ndescribe the relationship between the two entities. Output all the extract triples in\nthe format of \"head | relation | tail\". For example: \"Alzheimer’s disease | associates\n| memory deficits\" Abstract: Thiamine pyrophosphate (TPP) and the activities of\nthiamine-dependent enzymes are reduced in Alzheimer’s disease (AD) patients.\nIn this study, we analyzed the relationship between thiamine deficiency (TD) and\namyloid precursor protein (APP) processing in both cellular and animal models of\nTD. In SH-SY5Y neuroblastoma cells overexpressing APP, TD promoted maturation\nof beta-site APP cleaving enzyme 1 (BACE1) and increased beta-secretase activity\nwhich resulted in elevated levels of beta-amyloid (Abeta) as well as beta-secretase\ncleaved C-terminal fragment (beta-CTF). An inhibitor of beta-secretase efficiently\nreduced TD-induced up-regulation of Abeta and beta-CTF. Importantly, thiamine\nsupplementation reversed the TD-induced alterations. Furthermore, TD treatment\ncaused a significant accumulation of reactive oxygen species (ROS); antioxidants\nsuppressed ROS production and maturation of BACE1, as well as TD-induced\nAbeta accumulation. On the other hand, exogenous Abeta(1-40) enhanced TD-\ninduced production of ROS. A study on mice indicated that TD also caused Abeta\naccumulation in the brain, which was reversed by thiamine supplementation. Taken\ntogether, our study suggests that TD could enhance Abeta generation by promoting\nbeta-secretase activity, and the accumulation of Abeta subsequently exacerbated\nTD-induced oxidative stress.Entity: Thiamine (thiamine), deficiency increases beta-\nsecretase, TPP (Thiamine pyrophosphate), Alzheimer’s disease (AD), patients,\namyloid precursor protein (beta-amyloid (Abeta)), neuroblastoma, BACE1 (beta-site\nAPP cleaving enzyme 1), Abeta, ROS (reactive oxygen species), Abeta accumulation,\nmice. Output:\nOutput\nThiamine deficiency | downregulates | TPP, Thiamine deficiency | regulates |\nAlzheimer’s disease, Alzheimer’s disease | involves | beta-secretase, Amyloid precur-\nsor protein | produces | beta-amyloid, Amyloid precursor protein | in | neuroblastoma,\nBACE1 | is | beta-secretase, BACE1 | regulates | beta-amyloid, Abeta | is | beta-\namyloid, Abeta | in | mice, Abeta | causes | reactive oxygen species, Abeta | regulates\n| ROS\nTable 7: An example to extract knowledge triples using generative construction method.", "metadata": {"producer": "pdfTeX-1.40.25", "creator": "LaTeX with hyperref", "creationdate": "2024-05-09T00:18:37+00:00", "source": "/home/donbr/open-biosciences/biosciences-research/data/raw/2405.04819v1.pdf", "file_path": "/home/donbr/open-biosciences/biosciences-research/data/raw/2405.04819v1.pdf", "total_pages": 18, "format": "PDF 1.5", "title": "", "author": "", "subject": "", "keywords": "", "moddate": "2024-05-09T00:18:37+00:00", "trapped": "", "modDate": "D:20240509001837Z", "creationDate": "D:20240509001837Z", "page": 13}}
{"page_content": "Input\nRead the following abstract, answer the following question. Abstract: Thiamine\npyrophosphate (TPP) and the activities of thiamine-dependent enzymes are reduced\nin Alzheimer’s disease (AD) patients. In this study, we analyzed the relationship\nbetween thiamine deficiency (TD) and amyloid precursor protein (APP) process-\ning in both cellular and animal models of TD. In SH-SY5Y neuroblastoma cells\noverexpressing APP, TD promoted maturation of beta-site APP cleaving enzyme 1\n(BACE1) and increased beta-secretase activity which resulted in elevated levels of\nbeta-amyloid (Abeta) as well as beta-secretase cleaved C-terminal fragment (beta-\nCTF). An inhibitor of beta-secretase efficiently reduced TD-induced up-regulation\nof Abeta and beta-CTF. Importantly, thiamine supplementation reversed the TD-\ninduced alterations. Furthermore, TD treatment caused a significant accumulation\nof reactive oxygen species (ROS); antioxidants suppressed ROS production and\nmaturation of BACE1, as well as TD-induced Abeta accumulation. On the other\nhand, exogenous Abeta(1-40) enhanced TD-induced production of ROS. A study\non mice indicated that TD also caused Abeta accumulation in the brain, which\nwas reversed by thiamine supplementation. Taken together, our study suggests that\nTD could enhance Abeta generation by promoting beta-secretase activity, and the\naccumulation of Abeta subsequently exacerbated TD-induced oxidative stress.Entity:\nThiamine (thiamine), deficiency increases beta-secretase, TPP (Thiamine pyrophos-\nphate), Alzheimer’s disease (AD), patients, amyloid precursor protein (beta-amyloid\n(Abeta)), neuroblastoma, BACE1 (beta-site APP cleaving enzyme 1), Abeta, ROS\n(reactive oxygen species), Abeta accumulation, mice. Question: predict the relation-\nship between Disease entity \"sclerosis\" and Disease entity \"multiple sclerosis\", first\nchoose from the following options: A. resembles B. no-relation C. others, please\nspecify by generating a short predicate in 5 words. Answer: Let’s think step by step:\nOutput\n1. Sclerosis is a disease of the central nervous system. 2. Multiple sclerosis is\nthe most common form of sclerosis. 3. So the relationship between sclerosis and\nmultiple sclerosis should be \"sclerosis is a kind of multiple sclerosis\". So the answer\nis: A. resembles. So the answer is: A. resembles\nTable 8: An example to extract the relationship between “sclerosis” and “multiple sclerosis” using RE construction\nmethod.\nPubTator Type\nHetionet Type\nGene\ngenes\nChemical\ncompounds\nDisease\ndiseases\nTable 9: Entity type match from PubTator to Hetionet.\nType-Type\nRelations\ngenes-genes\n[\"covaries\", \"interacts\", \"regulates\"]\ndisease-disease\n[\"resembles\"]\ncompounds-compounds\n[\"resembles\"]\ngenes-diseases\n[\"downregulates\",\"associates\",\"upregulates\"]\ngenes-compounds\n[\"binds\", \"upregulates\", \"downregulates\"]\ncompounds-diseases\n[\"treats\", \"palliates\"]\nTable 10: Type-type to relation match in Hetionet.", "metadata": {"producer": "pdfTeX-1.40.25", "creator": "LaTeX with hyperref", "creationdate": "2024-05-09T00:18:37+00:00", "source": "/home/donbr/open-biosciences/biosciences-research/data/raw/2405.04819v1.pdf", "file_path": "/home/donbr/open-biosciences/biosciences-research/data/raw/2405.04819v1.pdf", "total_pages": 18, "format": "PDF 1.5", "title": "", "author": "", "subject": "", "keywords": "", "moddate": "2024-05-09T00:18:37+00:00", "trapped": "", "modDate": "D:20240509001837Z", "creationDate": "D:20240509001837Z", "page": 14}}
{"page_content": "B\nDetails of KG for LLMs\nIn this section, we provide detailed input and output for adopting KG to augment LLMs, including\npath-based and neighbor-based sub-graph sampling results (Table 11), self-aware knowledge retrieval\n(Table 12), describing sub-graphs with LLMs (Table 13) and inference with sampled knowledge (Table 14).\nThe question we showcase here is “The area of the brain resistant to Neurofibrillary tangles of Alzheimer’s\ndisease is: A. Visual association areas B. Entorhinal coex C. Temporal lobe D.Lateral geniculate body”,\nwhich same as the one we use in Section 5.4.\nPath-based Sub-graph\nneurofibrillary tangles->FORM BY->microtubule-associated protein tau->BINDS->\n(18)F-THK-5117->ADMINISTERED TO->rats->has->Alzheimer’s disease ->\nAlzheimer’s disease ->affects->human->has->AD->DISEASE OF->Brain\nentorhinal cortex->is a part of->brain->ASSOCIATES->\nmouse with Alzheimer’s disease->brain region->temporal lobe\nNeighbor-based Sub-graph\nneurofibrillary tangles->FORM BY->microtubule-associated protein tau\nAlzheimer’s disease ->causes->neuronal death\nAlzheimer’s disease ->associates->cognitive decline\nAlzheimer’s disease ->affects->human\nAlzheimer’s disease ->has subtype->neurodegenerative diseases\nTable 11: An example of path-based and neighbor-based sub-graph for the question.\nInput\nThere is a question and some knowledge graph. The knowledge graphs follow entity->relationship\n->entity list format.\nGraph:\nneurofibrillary tangles->FORM BY->microtubule-associated protein tau->BINDS->\n(18)F-THK-5117->ADMINISTERED TO->rats->has->Alzheimer’s disease ->Alzheimer’s disease ->\naffects->human->has->AD->DISEASE OF->Brain\nentorhinal cortex->is a part of->brain->ASSOCIATES->\nmouse with Alzheimer’s disease->brain region->temporal lobe}\nQuestion:\nThe area of the brain resistant to Neurofibrillary tangles of Alzheimer’s disease is:\nA. Visual association areas\nB. Entorhinal coex\nC. Temporal lobe\nD.Lateral geniculate body\nPlease rerank the knowledge graph and output at most 5 important and relevant triples for\nsolving the given question. Output the reranked knowledge in the following format:\nReranked Triple1: xxx ——>xxx\nReranked Triple2: xxx ——>xxx\nReranked Triple3: xxx ——>xxx\nReranked Triple4: xxx ——>xxx\nReranked Triple5: xxx ——>xxx\nAnswer:\nOutput\nReranked Triples1: entorhinal cortex ->is a part of ->brain\nReranked Triples2: entorhinal cortex ->associates ->mouse with Alzheimer’s disease\nReranked Triples3: temporal lobe ->affected by ->Alzheimer’s disease\nTable 12: An example of our self-aware knowledge retrieval in path-based sub-graph.", "metadata": {"producer": "pdfTeX-1.40.25", "creator": "LaTeX with hyperref", "creationdate": "2024-05-09T00:18:37+00:00", "source": "/home/donbr/open-biosciences/biosciences-research/data/raw/2405.04819v1.pdf", "file_path": "/home/donbr/open-biosciences/biosciences-research/data/raw/2405.04819v1.pdf", "total_pages": 18, "format": "PDF 1.5", "title": "", "author": "", "subject": "", "keywords": "", "moddate": "2024-05-09T00:18:37+00:00", "trapped": "", "modDate": "D:20240509001837Z", "creationDate": "D:20240509001837Z", "page": 15}}
{"page_content": "Input\nThere are some knowledge graph paths. They follow entity->relationship->entity format.\nReranked Triples1: entorhinal cortex ->is a part of ->brain\nReranked Triples2: entorhinal cortex ->associates ->mouse with Alzheimer’s disease\nReranked Triples3: temporal lobe ->affected by ->Alzheimer’s disease\nUse the knowledge graph information. Try to convert them to natural language, respectively.\nUse single quotation marks for entity name and relation name.\nAnd name them as Path-based Evidence 1, Path-based Evidence 2,...\nOutput:\nOutput\nPath-based Evidence 1: ’Entorhinal cortex’ is a part of ’brain’.\nPath-based Evidence 2: ’Entorhinal cortex’ associates ’mouse’ with ’Alzheimer’s disease’.\nPath-based Evidence 3: ’Temporal lobe’ is affected by ’Alzheimer’s disease’.\nTable 13: An example to describe the self-retrieved path-based sub-graph with the LLM.\nInput\nQuestion: The area of the brain resistant to Neurofibrillary tangles of Alzheimer’s disease is:\nA.Visual association areas\nB.Entorhinal coex\nC.Temporal lobe\nD.Lateral geniculate body\nYou have some medical knowledge information in the following:\n###Path-based Evidence 1: ’Entorhinal cortex’ is a part of ’brain’.\nPath-based Evidence 2: ’Entorhinal cortex’ associates ’mouse’ with ’Alzheimer’s disease’.\nPath-based Evidence 3: ’Temporal lobe’ is affected by ’Alzheimer’s disease’.\n###Neighbor-based Evidence 1: ’neurofibrillary tangles’ are ’FORMED BY’ ’\nmicrotubule-associated protein tau’.\nNeighbor-based Evidence 2: ’Alzheimer’s disease’ ’CAUSES’ ’neuronal death’.\nNeighbor-based Evidence 3: ’Alzheimer’s disease’ ’ASSOCIATES’ with ’cognitive decline’.\nNeighbor-based Evidence 4: ’Alzheimer’s disease’ ’AFFECTS’ ’human’.\nNeighbor-based Evidence 5: ’Alzheimer’s disease’ ’HAS SUBTYPE’ ’neurodegenerative diseases’.\"\nAnswer: Let’s think step by step:\nOutput\nIn Alzheimer’s disease, neurofibrillary tangles are composed of\nabnormal aggregations of tau protein within neurons, leading to\nneuronal dysfunction and eventual cell death. The lateral geniculate\nbody is a relay center in the thalamus that processes visual\ninformation from the retina to the visual cortex. Since the lateral\ngeniculate body is not directly involved in memory and cognitive\nfunction, it is less likely to be affected by neurofibrillary tangles in\nAlzheimer’s disease compared to other brain regions such as the\nentorhinal cortex and temporal lobe which play crucial roles in memory\nand cognitive processing. Therefore, the area of the brain resistant to\nneurofibrillary tangles of Alzheimer’s disease is the lateral geniculate\nbody (option D)\nTable 14: An example of inference with selected knowledge\nC\nADQA Benchmark\nTable 15 shows the detailed prompt of our self-sample method to construct ADQA benchmark. Table 16\ngives the statistics of our ADQA benchmark.\nInput\nJudge whether the question below is related to Alzheimer’s Disease. Please answer yes or no.\nQuestion: Treatable causes of dementia are ___.\na).AD b).Hypothyroidism c).Multi-infarct dementia d).SDH e).Hydrocephalus\nIs the question related to Alzheimer’s Disease? Answer:\nOutput\nYes\nTable 15: An example from MedMCQA to self-sample AD-related QA sample with LLMs.", "metadata": {"producer": "pdfTeX-1.40.25", "creator": "LaTeX with hyperref", "creationdate": "2024-05-09T00:18:37+00:00", "source": "/home/donbr/open-biosciences/biosciences-research/data/raw/2405.04819v1.pdf", "file_path": "/home/donbr/open-biosciences/biosciences-research/data/raw/2405.04819v1.pdf", "total_pages": 18, "format": "PDF 1.5", "title": "", "author": "", "subject": "", "keywords": "", "moddate": "2024-05-09T00:18:37+00:00", "trapped": "", "modDate": "D:20240509001837Z", "creationDate": "D:20240509001837Z", "page": 16}}
{"page_content": "Dataset\nMedQA\nMedMCQA\nMMLU\nQA4MRE\nTotal\nNumber\n152\n210\n49\n35\n446\nTable 16: Statistics of our ADQA benchmark.\nD\nFurther Experiment for RAG\nMedQA\nMedMCQA\nNMMLU\nQA4MRE\nAVG\nAlmanac w/ 256 chunk size\n50.0\n69.0\n67.3\n62.9\n62.3\nAlmanac w/ top 10 docuemnt\n48.7\n68.6\n65.3\n62.9\n61.4\nAlmanac w/ CoT\n50.0\n65.7\n77.6\n65.7\n64.7\nClinfo.ai w/ 256 chunk size\n48.6\n66.7\n81.6\n65.7\n65.7\nClinfo.ai w/ top 5 docuemnt\n43.4\n68.1\n77.6\n68.6\n64.4\nClinfo.ai w/ CoT\n48.7\n68.6\n79.6\n68.6\n65.0\nTable 17: Further experiment in RAG methods with different hyper-parameter settings.", "metadata": {"producer": "pdfTeX-1.40.25", "creator": "LaTeX with hyperref", "creationdate": "2024-05-09T00:18:37+00:00", "source": "/home/donbr/open-biosciences/biosciences-research/data/raw/2405.04819v1.pdf", "file_path": "/home/donbr/open-biosciences/biosciences-research/data/raw/2405.04819v1.pdf", "total_pages": 18, "format": "PDF 1.5", "title": "", "author": "", "subject": "", "keywords": "", "moddate": "2024-05-09T00:18:37+00:00", "trapped": "", "modDate": "D:20240509001837Z", "creationDate": "D:20240509001837Z", "page": 17}}
{"page_content": "Databases and ontologies\nBioThings Explorer: a query engine for a federated\nknowledge graph of biomedical APIs\nJackson Callaghan\n1,†, Colleen H. Xu\n1,†, Jiwen Xin1,†, Marco Alvarado Cano1,\nAnders Riutta\n2, Eric Zhou1, Rohan Juneja1, Yao Yao1, Madhumita Narayan1,\nKristina Hanspers2, Ayushi Agrawal\n2, Alexander R. Pico2, Chunlei Wu\n1,*, Andrew I. Su\n1,*\n1Department of Integrative Structural and Computational Biology, The Scripps Research Institute, La Jolla, CA 92037, United States\n2Data Science and Biotechnology, Gladstone Institutes, University of California, San Francisco, CA 94158, United States\n*Corresponding authors. Department of Integrative Structural and Computational Biology, The Scripps Research Institute, La Jolla, CA 92037, United States.\nE-mails: cwu@scripps.edu (C.W.) and asu@scripps.edu (A.I.S.)\n†These authors contributed equally to this work.\nAssociate Editor: Peter N. Robinson\nAbstract\nSummary: Knowledge graphs are an increasingly common data structure for representing biomedical information. These knowledge graphs can\neasily represent heterogeneous types of information, and many algorithms and tools exist for querying and analyzing graphs. Biomedical knowl-\nedge graphs have been used in a variety of applications, including drug repurposing, identiﬁcation of drug targets, prediction of drug side effects,\nand clinical decision support. Typically, knowledge graphs are constructed by centralization and integration of data from multiple disparate sour-\nces. Here, we describe BioThings Explorer, an application that can query a virtual, federated knowledge graph derived from the aggregated infor-\nmation in a network of biomedical web services. BioThings Explorer leverages semantically precise annotations of the inputs and outputs for\neach resource, and automates the chaining of web service calls to execute multi-step graph queries. Because there is no large, centralized\nknowledge graph to maintain, BioThings Explorer is distributed as a lightweight application that dynamically retrieves information at query time.\nAvailability and implementation: More information can be found at https://explorer.biothings.io and code is available at https://github.com/bio\nthings/biothings_explorer.\n1 Introduction\nWhile downloadable files are the most common way to share\nbiomedical data, application programming interfaces (APIs)\nare another popular and powerful mechanism for data dis-\nsemination. Accessing data through APIs has many comple-\nmentary advantages relative to downloading local copies of\ndata. APIs typically allow users to query for specific subsets\nof the data that are of interest. API queries are often highly\nindexed, leading to efficient data retrieval. Finally, API access\nallows for easy incorporation of the most up-to-date data into\nother computational applications or workflows.\nWhile APIs offer many advantages in terms of data acces-\nsibility, these advantages do not immediately translate into\nefficient data integration. APIs generally follow some com-\nmon architectures and protocols [e.g. representational state\ntransfer (REST), output in JavaScript Object Notation\n(JSON)], but alignment at this technical level does not guar-\nantee either syntactic or semantic interoperability. For ex-\nample, APIs can use different identifiers for the same gene,\ndifferent data structures to represent gene attributes, and\ndifferent terms to describe the relationships between bio-\nmedical entities.\nThere have been some efforts to define and enforce seman-\ntic and syntactic standards to achieve data interoperability.\nExamples of this approach include the Beacon API from the\nGA4GH consortium (Rambla et al. 2022) and the DAS speci-\nfication for sharing annotations of genomic features (Dowell\net al. 2001). These efforts rely on the active participation of\nAPI developers in adopting a community standard for their\nAPI endpoints.\nHere, we explore the use of semantically precise API anno-\ntations as a complementary approach to achieving API inter-\noperability. We divided this work into two components. First,\nwe created an extension of the widely used OpenAPI annota-\ntion standard (http://spec.openapis.org/oas/v3.1.0) to seman-\ntically describe the APIs’ inputs and outputs, and a registry to\norganize these API annotations. Second, we created an appli-\ncation called BioThings Explorer to consume the API meta-\ndata and to execute multi-hop graph queries that span\nmultiple APIs. Together, this architecture allows users to\nquery a large, federated knowledge graph based on an inter-\nconnected network of biomedical APIs. This federated design\noffers a unique approach for creating knowledge graphs that\nis complementary to the more common strategy of centraliza-\ntion and local data integration.\nReceived: 18 April 2023; Revised: 18 August 2023; Editorial Decision: 11 September 2023; Accepted: 12 September 2023\nV\nC The Author(s) 2023. Published by Oxford University Press.\nThis is an Open Access article distributed under the terms of the Creative Commons Attribution License (https://creativecommons.org/licenses/by/4.0/), which\npermits unrestricted reuse, distribution, and reproduction in any medium, provided the original work is properly cited.\nBioinformatics, 2023, 39(9), btad570\nhttps://doi.org/10.1093/bioinformatics/btad570\nAdvance Access Publication Date: 14 September 2023\nApplications Note\nDownloaded from https://academic.oup.com/bioinformatics/article/39/9/btad570/7273783 by guest on 11 January 2026", "metadata": {"producer": "Adobe LiveCycle PDF Generator; modified using iTextSharp.LGPLv2.Core 3.7.4.0", "creator": "Arbortext Advanced Print Publisher 9.0.114/W", "creationdate": "2023-09-27T15:50:28+05:30", "source": "/home/donbr/open-biosciences/biosciences-research/data/raw/btad570.pdf", "file_path": "/home/donbr/open-biosciences/biosciences-research/data/raw/btad570.pdf", "total_pages": 4, "format": "PDF 1.3", "title": "OP-CBIO230618 1..4", "author": "", "subject": "", "keywords": "", "moddate": "2026-01-11T01:10:54+00:00", "trapped": "", "modDate": "D:20260111011054+00'00'", "creationDate": "D:20230927155028+05'30'", "page": 0}}
{"page_content": "2 A registry of semantically annotated APIs\nThe first step in creating a network of interoperable APIs is to\nannotate each API in a semantically precise way. We built this\nAPI annotation system on the OpenAPI specification, the de\nfacto standard for documenting API metadata in a human-\nand machine-readable format. OpenAPI describes basic API\nmetadata (e.g. title, description, version, contact info), as well\nas key information on the operation of the API endpoints (e.g.\nserver URL, endpoint input parameters, endpoint response\nschemas).\nHowever, this basic OpenAPI specification does not include\nkey domain-specific information that is necessary to facilitate\ndownstream API interoperability. Therefore, we defined an\nOpenAPI extension to capture semantically precise annota-\ntions of each API endpoint. These annotations include the se-\nmantic types and identifier namespaces of biomedical entities\nthat are both used in querying (inputs) and found in the re-\nsponse (outputs), the JSON path to the output identifier val-\nues in the JSON response, and the predicate describing the\nrelationship\nbetween\nthe\ninput\nand\noutput\nentities\n(Supplementary Fig. S1).\nWe also chose a strategy to map arbitrary JSON data struc-\ntures to an established biological data model. In this effort,\nwe mapped API output to the Biolink Model (Unni et al.\n2022), a community-developed data model that was adopted\nand extended by the NCATS Translator consortium (Fecho\net al. 2022). The Biolink model defines the universe of seman-\ntic types that can be used in BioThings Explorer, the allowed\nidentifier systems for each semantic type and the allowed\npredicates. We provided a guide to writing these API annota-\ntions in our GitHub repository (https://github.com/biothings/\nbiothings_explorer/blob/main/docs/README-writing-x-bte.\nmd).\nTo annotate and catalog APIs with our OpenAPI extension,\nwe leveraged the SmartAPI registry (https://smart-api.info/)\n(Zaveri et al. 2017). We created SmartAPI registrations for 34\nAPIs with semantic annotations. The selection of these APIs\nthat are queried by BioThings Explorer is set in a local,\ninstance-specific configuration file. This collection of API\nannotations can be thought of as a “meta-knowledge graph”\n(meta-KG), where the nodes represent types of biomedical en-\ntities (genes, diseases, drugs) and the edges represent APIs that\ndescribe relationships between two types of biomedical enti-\nties. The SmartAPI meta-KG currently contains 35 nodes and\n1817\nedges.\n(The\ncomplete\nmeta-KG\nis\nshown\nin\nSupplementary Table S1 and a partial rendering is shown in\nFig. 1.)\n3 API interoperability using BioThings Explorer\nThe second step in creating our federated biomedical knowl-\nedge graph was to create BioThings Explorer, an engine to au-\ntonomously\nquery\nthe\nSmartAPI\nmeta-KG,\nquery\nthe\nannotated APIs to retrieve associations between biomedical\nentities, and integrate those APIs’ responses. The input to\nBioThings Explorer is a query graph and the syntax for\nencoding the query graph was defined by the NCATS\nTranslator consortium (Fecho et al. 2022). The topology of\nthe query graph and the constraints on its nodes and edges de-\nfine the query (Fig. 2).\nBioThings Explorer executes the query in three distinct\nphases: query-path planning, query-path execution, and inte-\ngration and scoring.\n3.1 Query-path planning\nFor every edge in a query graph, BioThings Explorer consults\nthe SmartAPI registry for APIs that serve those types of associ-\nations (Fig. 2). For example, in Fig. 1, associations between\ndiseases and genes can be found using APIs from the\nComparative Toxicogenomics Database (Davis et al. 2023)\nand the Biolink API from the Monarch Initiative (Mungall\net al. 2017, Unni et al. 2022), while associations between\ngenes and chemicals can be found using MyChem.Info\n(https://mychem.info/) (Lelong et al. 2022). The sequence of\nAPI calls that can satisfy the original query is a “query-path\nplan.”\n3.2 Query-path execution\nIn this phase, BioThings Explorer programmatically and au-\ntonomously executes each query in each query-path plan\nbased on the semantic annotations for each API identified in\nthe previous phase. BioThings Explorer calls each API, using\nthe SmartAPI annotation to construct calls with the correct\nsyntax and appropriate input identifier, and maps the API\nresponses to the Biolink Model (Unni et al. 2022). BioThings\nExplorer also performs ID-to-object translation, which facili-\ntates the chaining of API calls from one step in the query-path\nto the next step. This ID translation step is critical when suc-\ncessive APIs in the query-path plan use different identifiers to\nrepresent the same biomedical entity (e.g. NCBI Gene ID\nversus Ensembl Gene ID). ID translation is currently handled\nby\nthe\nTranslator\nNode\nNormalizer\n(https://github.com/\nNCATSTranslator/Translator-All/wiki/Node-Normalizer). The\noutput of this phase is a set of edges for each step of the query-\npath, which represent the associations between biomedical enti-\nties retrieved from the APIs.\n3.3 Integration and scoring\nIn this final phase, these sets of edges from the API queries are\nassembled into result sub-graphs, each of which matches the\ntopology of the query graph. Each result is then scored based\non a variety of factors including number of paths, length of\npaths, and semantic similarity between concepts based on the\nNormalized Google Distance (Cilibrasi and Vitanyi 2007).\n4 Deployment and usage\nThe BioThings Explorer knowledge graph is entirely com-\nposed from a federated network of APIs. Because there is no\nlocal assembly and storage of a large knowledge graph,\nBioThings Explorer is a very lightweight application that can\nbe easily deployed on almost any standard personal com-\nputer. The ability of every user to create a local instance of\nBioThings Explorer removes centralized bottlenecks associ-\nated with large queries and/or heavy usage. The code reposi-\ntory that describes the installation process is at https://github.\ncom/biothings/biothings_explorer. BioThings Explorer cur-\nrently\nrelies\non\ntwo\nexternal\ndependencies—the\nNode\nNormalizer service and the Biolink Model (Unni et al. 2022).\nHowever, nothing in the BioThings Explorer architecture is\nreliant on these specific tools, so these dependencies can be\nsubstituted for alternatives if desired.\nFor users who prefer not to create a local instance of\nBioThings Explorer, we also maintain a community instance\nfor general use through the NCATS Translator Consortium\n(https://explorer.biothings.io/).\n2\nCallaghan et al.\nDownloaded from https://academic.oup.com/bioinformatics/article/39/9/btad570/7273783 by guest on 11 January 2026", "metadata": {"producer": "Adobe LiveCycle PDF Generator; modified using iTextSharp.LGPLv2.Core 3.7.4.0", "creator": "Arbortext Advanced Print Publisher 9.0.114/W", "creationdate": "2023-09-27T15:50:28+05:30", "source": "/home/donbr/open-biosciences/biosciences-research/data/raw/btad570.pdf", "file_path": "/home/donbr/open-biosciences/biosciences-research/data/raw/btad570.pdf", "total_pages": 4, "format": "PDF 1.3", "title": "OP-CBIO230618 1..4", "author": "", "subject": "", "keywords": "", "moddate": "2026-01-11T01:10:54+00:00", "trapped": "", "modDate": "D:20260111011054+00'00'", "creationDate": "D:20230927155028+05'30'", "page": 1}}
{"page_content": "5 Discussion\nIntegration of existing data from multiple disparate sources is\na key step in assessing the state of current knowledge. There\nare many existing efforts to create biomedical knowledge\ngraphs by integrating locally downloaded data and standard-\nizing it using a common data model (Himmelstein et al. 2017;\nFecho et al. 2021; Mayers et al. 2022; Wood et al. 2022;\nMorris et al. 2023). These efforts result in centralized knowl-\nedge graphs of substantial size, often with millions of nodes\nand tens of millions of edges.\nBioThings Explorer offers a unique strategy for data inte-\ngration, focusing on creating a federated knowledge graph by\nsemantically annotating APIs. Rather than bringing all data\ninto a massive, centralized graph database, this federated de-\nsign instead allows knowledge to remain behind each resour-\nce’s API. Data are retrieved at query time by dynamically\nexecuting API calls and semantically parsing the results. This\narchitecture\nfunctionally\nseparates\ndata\ndissemination\n(through API creation) from data modeling and data integra-\ntion (through semantic annotations).\nThis approach has several advantages. First, by moving the\nrequirements for interoperability from implementation in\ncode to semantic API annotation, we significantly lower the\nbarrier to participation in our API ecosystem. Second, by sep-\narating these roles into distinct layers, we promote the overall\nmodularity of our system. These components can develop and\nevolve in parallel, and these two roles can even be undertaken\nby separate teams (e.g. one team semantically annotates an\nAPI that was created by another team). Third, this design\nfacilitates\nan\niterative\napproach\nto\nAPI\nannotation.\nDevelopers and API annotators can first provide a minimal\nset of API metadata, which can later be extended based on fu-\nture needs and use cases.\nThe federated design of BioThings Explorer also has some\nnotable\nlimitations.\nFirst,\nour\nOpenAPI\nextensions\nin\nSmartAPI to semantically annotate APIs only work on APIs\nthat follow the REST protocol and provide output in JSON\nFigure 1. A visualization of the meta-KG for BioThings Explorer. The nodes in this graph are the semantic types of biomedical entities that BioThings\nExplorer can retrieve associations between (limited to the top eight most common semantic types). The edges between nodes show what associations\nbetween biomedical entities exist in the semantic API network that is accessible through BioThings Explorer. The edge label shows the number of APIs\nthat can retrieve those types of associations, which is also represented by the edge width.\nFigure 2. Deconstruction of a query in BioThings Explorer. (A) A free-text\nrepresentation of a query that can be answered by BioThings Explorer.\n(B) The graph representation of the same query. The exact syntax of this\ngraph query is speciﬁed in the Translator Reasoner API standard\ndescribed in Fecho et al. (2022) and shown in Supplementary Fig. S2.\n(C) The deconstruction of the graph query into multiple API calls by\nconsulting the meta-KG in the SmartAPI registry.\nBioThings Explorer\n3\nDownloaded from https://academic.oup.com/bioinformatics/article/39/9/btad570/7273783 by guest on 11 January 2026", "metadata": {"producer": "Adobe LiveCycle PDF Generator; modified using iTextSharp.LGPLv2.Core 3.7.4.0", "creator": "Arbortext Advanced Print Publisher 9.0.114/W", "creationdate": "2023-09-27T15:50:28+05:30", "source": "/home/donbr/open-biosciences/biosciences-research/data/raw/btad570.pdf", "file_path": "/home/donbr/open-biosciences/biosciences-research/data/raw/btad570.pdf", "total_pages": 4, "format": "PDF 1.3", "title": "OP-CBIO230618 1..4", "author": "", "subject": "", "keywords": "", "moddate": "2026-01-11T01:10:54+00:00", "trapped": "", "modDate": "D:20260111011054+00'00'", "creationDate": "D:20230927155028+05'30'", "page": 2}}
{"page_content": "format. Second, because the entire federated KG is never in-\nstantiated in a single place, reasoning and scoring methods\nthat rely on having the entire knowledge graph in memory\ncannot be used with BioThings Explorer.\nIn sum, we believe that knowledge graphs enable many ex-\nciting use cases in biomedical research (Nicholson and Greene\n2020), and that BioThings Explorer is complementary to\nexisting approaches for assembling knowledge graphs, offer-\ning powerful and unique capabilities for both scientific data\nanalysts and tool developers.\nSupplementary data\nSupplementary data are available at Bioinformatics online.\nConflict of interest\nNone declared.\nFunding\nSupport for this work was provided by the National Center\nfor Advancing Translational Sciences, National Institutes of\nHealth, through the Biomedical Data Translator program,\nawards OT2TR003427 and OT2TR003445.\nData availability\nBioThings Explorer is implemented as a NodeJS application.\nThe primary repository for the BioThings Explorer project is\nat https://github.com/biothings/biothings_explorer, which in\nturn links to and incorporates other repositories as sub-\nmodules. All code is released under the Apache 2.0 open-\nsource software license.\nReferences\nCilibrasi RL, Vitanyi PMB. The google similarity distance. IEEE Trans\nKnowl Data Eng 2007;19:370–83.\nDavis AP, Wiegers TC, Johnson RJ et al. Comparative toxicogenomics\ndatabase\n(CTD):\nupdate\n2023.\nNucleic\nAcids\nRes\n2023;51:\nD1257–62.\nDowell RD, Jokerst RM, Day A et al. The distributed annotation system.\nBMC Bioinformatics 2001;2:7.\nFecho K, Bizon C, Miller F et al. A biomedical knowledge graph system\nto propose mechanistic hypotheses for real-world environmental\nhealth observations: cohort study and informatics application. JMIR\nMed Inform 2021;9:e26714.\nFecho K, Thessen AE, Baranzini SE et al. Progress toward a universal\nbiomedical data translator. Clin Transl Sci 2022;15:1838–47.\nHimmelstein DS, Lizee A, Hessler C et al. Systematic integration\nof biomedical knowledge prioritizes drugs for repurposing. Elife\n2017;6:e26726.\nLelong S, Zhou X, Afrasiabi C et al. BioThings SDK: a toolkit for build-\ning\nhigh-performance\ndata\nAPIs\nin\nbiomedical\nresearch.\nBioinformatics 2022;38:2077–9.\nMayers M, Tu R, Steinecke D et al. Design and application of a knowl-\nedge network for automatic prioritization of drug mechanisms.\nBioinformatics 2022;38:2880–91.\nMorris JH, Soman K, Akbas RE et al. The scalable precision medicine\nopen knowledge engine (SPOKE): a massive knowledge graph of bio-\nmedical information. Bioinformatics 2023;39(2):btad080.\nMungall CJ, McMurry JA, Ko¨hler S et al. The monarch initiative: an in-\ntegrative data and analytic platform connecting phenotypes to geno-\ntypes across species. Nucleic Acids Res 2017;45:D712–22.\nNicholson DN, Greene CS. Constructing knowledge graphs and their\nbiomedical applications. Comput Struct Biotechnol J 2020;18:\n1414–28.\nRambla J, Baudis M, Ariosa R et al. Beacon v2 and Beacon networks: a\n‘lingua franca’ for federated data discovery in biomedical genomics,\nand beyond. Hum Mutat 2022;43:791–9.\nUnni DR, Moxon SAT, Bada M et al. Biolink Model: a universal schema\nfor knowledge graphs in clinical, biomedical, and translational sci-\nence. Clin Transl Sci 2022;15:1848–55.\nWood EC, Glen AK, Kvarfordt LG et al. RTX-KG2: a system for build-\ning a semantically standardized knowledge graph for translational\nbiomedicine. BMC Bioinformatics 2022;23:400.\nZaveri A, Dastgheib S, Wu C et al. smartAPI: towards a more intelligent\nnetwork of web APIs. In: Blomqvist, E., Maynard, D., Gangemi, A.,\net al. (eds) The Semantic Web. ESWC 2017. Lecture Notes in\nComputer Science, 2017, 154–69. https://link.springer.com/chapter/\n10.1007/978-3-319-58451-5_11#chapter-info.\n4\nCallaghan et al.\nDownloaded from https://academic.oup.com/bioinformatics/article/39/9/btad570/7273783 by guest on 11 January 2026", "metadata": {"producer": "Adobe LiveCycle PDF Generator; modified using iTextSharp.LGPLv2.Core 3.7.4.0", "creator": "Arbortext Advanced Print Publisher 9.0.114/W", "creationdate": "2023-09-27T15:50:28+05:30", "source": "/home/donbr/open-biosciences/biosciences-research/data/raw/btad570.pdf", "file_path": "/home/donbr/open-biosciences/biosciences-research/data/raw/btad570.pdf", "total_pages": 4, "format": "PDF 1.3", "title": "OP-CBIO230618 1..4", "author": "", "subject": "", "keywords": "", "moddate": "2026-01-11T01:10:54+00:00", "trapped": "", "modDate": "D:20260111011054+00'00'", "creationDate": "D:20230927155028+05'30'", "page": 3}}
{"page_content": "GigaScience , 2025, 14 , 1–12 \nDOI: 10.1093/gigascience/giae082 \nRESEARCH \nKno wledge gr aph–based thought: a kno wledge \ngraph–enhanced LLM framework for pan-cancer \nquestion ans w ering \nYichun Feng \n1 ,2 ,‡ , Lu Zhou 2 ,‡ , Chao Ma 3 ,‡ , Yikai Zheng 2 , Ruikun He 4 ,5 , * , and Yixue Li 1 ,2 , * \n1 Hangzhou Institute for Advanced Study, University of Chinese Academy of Sciences, 310024 Hangzhou, China \n2 Guangzhou National Laboratory, Guangzhou International Bio Island, 510005 Guangzhou, China \n3 Smartquerier Gene Technology (Shanghai) Co ., Ltd. , 200100 Shanghai, China \n4 BYHEALTH Institute of Nutrition & Health, 510663 Guangzhou, China \n5 Shanghai Institute of Nutrition and Health, Chinese Academy of Sciences Shanghai, 200030 Shanghai, China \n∗Corr espondence addr ess . Ruikun He , BYHEALTH Institute of Nutrition & Health, 510663 Guangzhou, China. E-mail: herk@by-health.com ; Yixue Li, Guangzhou \nNational Laboratory, No. 9 XingDaoHuanBei Road, Guangzhou International Bio Island, 510005 Guangzhou. E-mail: yxli@sibs.ac.cn .\n‡ Contributed equally. \nAbstract \nBac kgr ound: In recent years, large language models (LLMs) have shown promise in various domains, notably in biomedical sciences. \nHowev er, their r eal-world application is often limited by issues like erroneous outputs and hallucinatory responses. \nResults: We developed the knowledge graph–based thought (KGT) framework, an innov ati v e solution that integrates LLMs with knowl- \nedge graphs (KGs) to impr ov e their initial r esponses by utilizing v erifia b le information from KGs, thus significantl y r educing factual \nerr ors in r easoning. The KGT fr amew ork demonstr ates str ong adapta bility and performs well acr oss v arious open-source LLMs. No- \nta b l y, KGT can facilitate the discov er y of new uses for existing drugs through potential drug–cancer associations and can assist in \npr edicting r esistance by anal yzing r elev ant biomarkers and genetic mechanisms. To ev aluate the kno wledge graph question ans wer- \ning task within biomedicine , w e utilize a pan-cancer knowledge graph to develop a pan-cancer question answering benchmark, named \npan-cancer question answering. \nConclusions: The KGT fr amew ork substantiall y impr ov es the accuracy and utility of LLMs in the biomedical field. This study serves \nas a proof of concept, demonstrating its exceptional performance in biomedical question answering. \nKe yw ords: pan-cancer knowledge graph, large language model, knowledge graph question answering, prompt engineering \nKe y P oints: \nr We introduce a framework combining large language \nmodels (LLMs) with knowledge gr a phs (KGs) to impr ov e \nfactual accuracy in LLM reasoning. \nr Our system is a flexible arc hitectur e that seamlessly in- \ntegr ates v arious LLMs. \nr Utilizing a pan-cancer knowledge gr a ph, we hav e pr o- \nposed the first knowledge gr a ph question answering \nbenchmark in the field of biomedicine. \nr Case studies r e v eal our method enhanced LLMs in ad- \ndressing biomedical challenges such as drug reposition- \ning, r esistance r esearc h, individualized tr eatment, and \nbiomarker analysis. \nr The method performs favor abl y in comparison to exist- \ning methods. \nIntroduction \nWith the increasing prominence of large language models (LLMs) \nin the field of artificial intelligence, the advent of influential mod- \nels such as ChatGPT [ 1 ] and Llama [ 2 ] consequently catalyze the \nde v elopment of a wide array of applications in biomedicine and \nhealth care. Ho w ever, LLMs still face the challenge of factual hal- \nlucination, wher e they gener ate incorr ect statements due to lim- \nited inherent knowledge [ 3 ]. Factual hallucination presents a sig- \nnificant challenge for the practical use of LLMs, especially in real- \nworld scenarios where factual accuracy is crucial. Consequently, \nthere is a growing focus on addressing factual hallucinations in \nLLMs within the field of natural language processing (NLP) [ 4 , 5 ]. \nLLMs often struggle to ca ptur e and access factual knowledge, \nprimarily due to 3 aspects: the inability to comprehend ques- \ntions due to the lack of contextual information, the insufficient \nknowledge to generate accurate ans wers , and the incapacity to \nrecall specific facts [ 6 ]. Consequently, researchers consider the \nfine-tuning technique as a solution to address these issues. For \nexample, MedAlpaca [ 7 ] builds upon medical data to fine-tune \nStanford Alpaca for applications related to medical question \nans wering and dialogue . ChatDoctor [ 8 ] is designed to simulate \na conversation between a doctor and a patient by fine-tuning \nRecei v ed: May 27, 2024. Re vised: A ugust 14, 2024. Accepted: October 2, 2024 \n© The Author(s) 2025. Published by Oxford Uni v ersity Pr ess GigaScience. This is an Open Access article distributed under the terms of the Cr eati v e Commons \nAttribution License ( https://cr eati v ecommons.org/licenses/by/4.0/ ), which permits unrestricted reuse, distribution, and reproduction in any medium, provided \nthe original work is pr operl y cited. \nDownloaded from https://academic.oup.com/gigascience/article/doi/10.1093/gigascience/giae082/7943459 by guest on 11 January 2026", "metadata": {"producer": "Acrobat Distiller 24.0 (Windows); modified using iTextSharp.LGPLv2.Core 3.7.4.0", "creator": "OUP", "creationdate": "2025-01-06T10:51:03+00:00", "source": "/home/donbr/open-biosciences/biosciences-research/data/raw/giae082.pdf", "file_path": "/home/donbr/open-biosciences/biosciences-research/data/raw/giae082.pdf", "total_pages": 12, "format": "PDF 1.4", "title": "Knowledge graph&#x2013;based thought: a knowledge graph&#x2013;enhanced LLM framework for pan-cancer question answering", "author": "Feng Yichun, Zhou Lu, Ma Chao, Zheng Yikai, He Ruikun, Li Yixue", "subject": "DOI: 10.1093/gigascience/giae082 GigaScience, 14, 0, 00-00-2025. Abstract: In recent years, large language models (LLMs) have shown promise in various domains, notably in biomedical sciences. However, their real-world application is often limited by issues like erroneous outputs and hallucinatory responses.We developed the knowledge graph&#x2013;based thought (KGT) framework, an innovative solution that integrates LLMs with knowledge graphs (KGs) to improve their initial responses by utilizing verifiable information from KGs, thus significantly reducing factual errors in reasoning. The KGT framework demonstrates strong adaptability and performs well across various open-source LLMs. Notably, KGT can facilitate the discovery of new uses for existing drugs through potential drug&#x2013;cancer associations and can assist in predicting resistance by analyzing relevant biomarkers and genetic mechanisms. To evaluate the knowledge graph question answering task within biomedicine, we utilize a pan-cancer knowledge graph to develop a pan-cancer question answering benchmark, named pan-cancer question answering.The KGT framework substantially improves the accuracy and utility of LLMs in the biomedical field. This study serves as a proof of concept, demonstrating its exceptional performance in biomedical question answering.", "keywords": "", "moddate": "2026-01-11T01:15:31+00:00", "trapped": "", "modDate": "D:20260111011531+00'00'", "creationDate": "D:20250106105103Z", "page": 0}}
{"page_content": "2 | GigaScience , 2025, Vol. 14 \nLLaMA with medical liter atur e. Additionall y, Med-P aLM [ 9 ] shows \npromising performance on the MedQA exam based on clinical \ncor por a and human feedback. Meanwhile, aiming at the Chinese \nmedical domain, LLMs such as BenTsao [ 10 ], DoctorGLM [ 11 ], and \nHuatuoGPT [ 12 ] are developed on the Chinese medical dialogue \ndata. Mor e r ecentl y, Zhongjing [ 13 ] and ChiMed-GPT [ 14 ] adopted \nfull pipeline training from pretraining, SFT, to reinforcement \nlearning with human feedback (RLHF) [ 15 ]. While fine-tuning can \nreduce hallucinations in LLMs, it brings about considerable train- \ning expenses. Additionally, it poses a critical challenge known \nas catastrophic forgetting. This issue manifests when a model \nfor gets its pr e viousl y learned information as a consequence of \nparameter modifications during the acquisition of new tasks. \nThis forgetfulness results in a deterioration of performance on \nprior tasks, consequently constraining the model’s practical \napplicability [ 16 , 17 ]. \nIn addition to fine-tuning, r esearc hers also enhance the out- \nput of LLMs through the field of prompt engineering. Prompt en- \ngineering focuses on the creation and optimization of prompts \nto impr ov e the effectiv eness of LLMs acr oss v arious a pplications \nand r esearc h domains [ 18 ]. It can enhance the ca pabilities of LLMs \nin a wide range of complex tasks, including question answering, \nsentiment classification, and commonsense reasoning. Chain-of- \nthought (CoT) prompts [ 19 ] enable complex reasoning capabili- \nties by incor por ating intermediate r easoning steps . T he Automatic \nPr ompt Engineer (APE) pr oposes an automatic pr ompt gener ation \nmethod aimed at enhancing the performance of LLMs [ 20 ]. Prompt \nengineering offers a straightforw ar d approach to harnessing the \npotential of LLMs without fine-tuning. \nOn the other hand, knowledge gr a phs (KGs) ar e r epositories of \nvast quantities of high-quality structured data, offering the po- \ntential to effectiv el y mitigate the issue of factual hallucinations \nwhen integrated with LLMs . Hence , employing KGs for question \nanswering can enhance the precision of the responses and fur- \nnish a dependable foundation for the factual verification of in- \nformation produced by LLMs. Knowledge gr a ph question answer- \ning (KGQA) has long been a hot r esearc h topic. Befor e the ad- \nvent of LLMs, certain studies [ 21–23 ] typically begin by r etrie ving \na subgr a ph r elated to the question to r educe the searc h space, \nthen perform m ultihop r easoning on this basis . T his r etrie v al- \nplus-r easoning par adigm has shown its adv anta ges ov er dir ect \nr easoning acr oss the entir e KG [ 24 , 25 ]. Additionall y, r esearc hers \ntackle KGQA by parsing the question into a structured query lan- \nguage (e.g., SPARQL) and using a query engine to obtain accu- \nrate answers [ 26 , 27 ]. UniKGQA [ 28 ] introduces a unified fine- \ntuning fr ame work for r etrie v al and r easoning, mor e closel y linking \nthese 2 stages. Ho w ever, traditional KGQA methods usually per- \nform poorly in accurate semantic understanding and high-quality \ntext generation due to the lack of LLMs for r etrie v al and r eason- \ning. Hence, r ecent r esearc h is incr easingl y utilizing external KGs \nto enhance LLMs in addressing KGQA challenges. For instance, \nStructGPT [ 29 ] navigates through knowledge graphs by identify- \ning pathways from an initial seed entity to the target answer en- \ntity, while Think-on-Gr a ph (ToG) [ 30 ] intr oduces iter ativ e explo- \nration of the knowledge gr a ph, whic h can become inefficient with \nv ery lar ge KGs. Additionall y, Reasoning on Gr a phs (RoG) [ 31 ] ne- \ncessitates fine-tuning to accur atel y gener ate and plan the r elation \npaths. KG-GPT [ 32 ] opts for r etrie ving an entire subgraph from the \nknowledge gr a ph and then deduces the answer thr ough infer ence. \nAlthough these methods hav e ac hie v ed gr atifying r esults in gen- \ner al ar eas, as shown in Fig. 1 B, when the intermediate entity in \nthe multihop question is unknown, it is impossible to retrieve the \na ppr opriate knowledge from the KG. \nIn this article, we intr oduce an innov ativ e fr ame work called \nknowledge gr a ph–based thought (KGT), whic h integr ates LLMs \nwith KGs through employing LLMs for reasoning on the schema \nof KGs to mitigate factual hallucinations of LLMs, as shown in \nFig. 1 C. Unlike traditional methods, KGT does not dir ectl y r etrie v e \nfactual information based on the question. Instead, it uses LLMs \nto infer entity information on the schema of the knowledge graph, \ngenerating an optimal subgraph based on k e y information di- \nr ectl y extr acted fr om the question and inferr ed information fr om \nthe sc hema. Subsequentl y, the optimal subgr a ph is used to in- \nfer the answer to the question through LLMs. KGT r equir es no \nfine-tuning, offers seamless integration with multiple LLMs, and \nis plug-and-play, facilitating easy deployment. It demonstrates \ngeneralizability, making it adaptable for use with diverse knowl- \nedge gr a phs . T his fr ame work is tailor ed for wide-r anging a ppli- \ncations in numerous biomedical challenges, such as (i) enhanc- \ning clinical decision-making for physicians and medical organi- \nzations, (ii) delivering medical advice to patients and health care \npro viders , (iii) unco vering crucial biomarkers for early disease de- \ntection and tailored therapy, and (iv) exploring novel therapeutic \napplications for existing medications through insights into their \nmechanisms , side effects , and the biological processes of associ- \nated diseases . Furthermore , we utilize the SmartQuerier Oncol- \nogy Knowledge Gr a ph (SOKG), a pan-cancer knowledge gr a ph de- \nveloped by SmartQuerier, to create a benchmark for the KGQA \ntask within biomedicine, named pan-cancer question answering \n(PcQA). We release this benchmark and its accompanying knowl- \nedge gr a ph, whic h is a subgr a ph of the SOKG, in [ 33 ]. This bench- \nmark is curr entl y the sole question-ans wering dataset a vailable \nin the domain of biomedical knowledge gr a phs. \nMaterials and Methods \nKno wledge gr aph introduction \nIn this w ork, w e tac kle the pr oblem of logical r easoning ov er \nthe KG K : E × R that store entities ( E ) and relations ( R ). With- \nout loss of generality, KG can be organized as a set of triplets \n{ (e 1 , r, e 2 ) } ⊆K, wher e eac h r elation r ∈ R exists between the pair \nof entities (e 1 , e 2 ) ∈ E × E. We define a relational path { (t 1 , r, t 2 ) } as \na sequence of entity types ( T ) and the relation between them, \nwhere (t 1 , t 2 ) ∈ T × T . In contrast, a relational chain { (e 1 , r, e 2 ) } \nrefers to a specific set of relational triplets between entities. To \nfurther enrich the KG, attribute information is included through \npairs (e , at t r ) , where at t r represents an attribute associated with \nan entity e , thereby enhancing the KG’s semantic richness \nand precision by incorporating detailed characteristics of each \nentity. \nWithin the specialized realm of pan-cancer resear ch, w e use \na subgr a ph of the SOKG that pr ovides detailed oncological in- \nformation. As depicted in Table 1 , SOKG includes a collection of \nover 3 million entities, which is substantially larger than the en- \ntity count in the compared knowledge graphs, SynLethKG [ 34 ] and \nSDKG [ 35 ], with 540,012 and 165,062 entities, r espectiv el y. Further- \nmor e, SOKG’s nearl y 6 million unique concept relations exceed \nthose of SynLethKG and SDKG, which have 2,231,921 and 727,318 \nr elations, r espectiv el y. Additionall y, SOKG includes 98 distinct at- \ntribute types, enriching data comprehension and improving the \nefficiency and precision of queries, a capability not matched by \nSynLethKG or SDKG, which do not include comparable attributes. \nFor this r esearc h, we utilize only a subgraph of the SOKG, which \nis available as open data [ 33 ], while the full knowledge graph re- \nmains proprietary. \nDownloaded from https://academic.oup.com/gigascience/article/doi/10.1093/gigascience/giae082/7943459 by guest on 11 January 2026", "metadata": {"producer": "Acrobat Distiller 24.0 (Windows); modified using iTextSharp.LGPLv2.Core 3.7.4.0", "creator": "OUP", "creationdate": "2025-01-06T10:51:03+00:00", "source": "/home/donbr/open-biosciences/biosciences-research/data/raw/giae082.pdf", "file_path": "/home/donbr/open-biosciences/biosciences-research/data/raw/giae082.pdf", "total_pages": 12, "format": "PDF 1.4", "title": "Knowledge graph&#x2013;based thought: a knowledge graph&#x2013;enhanced LLM framework for pan-cancer question answering", "author": "Feng Yichun, Zhou Lu, Ma Chao, Zheng Yikai, He Ruikun, Li Yixue", "subject": "DOI: 10.1093/gigascience/giae082 GigaScience, 14, 0, 00-00-2025. Abstract: In recent years, large language models (LLMs) have shown promise in various domains, notably in biomedical sciences. However, their real-world application is often limited by issues like erroneous outputs and hallucinatory responses.We developed the knowledge graph&#x2013;based thought (KGT) framework, an innovative solution that integrates LLMs with knowledge graphs (KGs) to improve their initial responses by utilizing verifiable information from KGs, thus significantly reducing factual errors in reasoning. The KGT framework demonstrates strong adaptability and performs well across various open-source LLMs. Notably, KGT can facilitate the discovery of new uses for existing drugs through potential drug&#x2013;cancer associations and can assist in predicting resistance by analyzing relevant biomarkers and genetic mechanisms. To evaluate the knowledge graph question answering task within biomedicine, we utilize a pan-cancer knowledge graph to develop a pan-cancer question answering benchmark, named pan-cancer question answering.The KGT framework substantially improves the accuracy and utility of LLMs in the biomedical field. This study serves as a proof of concept, demonstrating its exceptional performance in biomedical question answering.", "keywords": "", "moddate": "2026-01-11T01:15:31+00:00", "trapped": "", "modDate": "D:20260111011531+00'00'", "creationDate": "D:20250106105103Z", "page": 1}}
{"page_content": "Knowledge gr a ph–based thought | 3 \nA\nB\nC\nFigure 1: Illustr ativ e examples contr asting our work with pr e vious efforts. (A) LLM-onl y-based infer ence , ans wering questions solel y thr ough the \ninherent knowledge of LLMs. (B) Subgraph-based inference, enhancing LLMs by retrieving the knowledge from KGs based on the question. If \nintermediate entities are not provided in the multihop question, no appropriate knowledge can be retrieved. (C) Graph schema–based inference, \nenhancing r etrie v al ca pabilities by r easoning intermediary entity types on the sc hema of the KG, using the knowledge of the KG to enhance LLMs’ \nresponses. \nTable 1. Comparison of SOKG with SynLethKG and SDKG \nEntity types \nRelational types \nNodes \nEdges \nAttributes \nSynLethKG \n11 \n24 \n54,012 \n2,231,921 \n0 \nSDKG \n7 \n12 \n165,062 \n727,318 \n0 \nSOKG \n24 \n21 \n3,640,259 \n10,656,273 \n98 \nTask description \nIn order to tackle a div erse arr ay of c hallenges in the field \nof biomedicine, we have designed 4 categories of problems: 1- \nhop pr oblems, m ultihop pr oblems, intersection pr oblems, and at- \ntribute pr oblems, as illustr ated in Table 2 . Based on these 4 types \nof tasks, we le v er a ge the SOKG to establish a benchmark for \nthe KGQA task within biomedicine, named PcQA. Unlike KGQA \ntasks in general domains, such as MetaQA [ 36 ] and FACTKG [ 37 ], \nwhic h typicall y pr ovide the entity types of intermediate entities, \nKGQA problems in the biomedical domain often do not have \nany information about intermediate entities. Instead, the infor- \nmation about intermediate entities must be inferred from the \nquestion itself rather than being dir ectl y pr ovided, as shown in \nSupplementary Table S1 . Additionally, our PcQA dataset includes \nattributes such as whether a drug is tar geted ther a py or if a \nmutated gene is oncogenic. This makes our tasks slightly more \nchallenging and better suited to the actual needs of biomedical \nKGQA. \nOne-hop problems \nOne-hop pr oblems involv e single-r elation c hain r easoning, wher e \nthe objective is to deduce the tail entity T ? given a head entity \nH 1 and a relation R 1 , or to infer the relation R ? when a head en- \ntity H 1 and a tail entity T 1 are known, as depicted in equations ( 1 ) \nand ( 2 ). \nH 1 + R 1 → T ? \n(1) \nH 1 + T 1 → R ? \n(2) \nDownloaded from https://academic.oup.com/gigascience/article/doi/10.1093/gigascience/giae082/7943459 by guest on 11 January 2026", "metadata": {"producer": "Acrobat Distiller 24.0 (Windows); modified using iTextSharp.LGPLv2.Core 3.7.4.0", "creator": "OUP", "creationdate": "2025-01-06T10:51:03+00:00", "source": "/home/donbr/open-biosciences/biosciences-research/data/raw/giae082.pdf", "file_path": "/home/donbr/open-biosciences/biosciences-research/data/raw/giae082.pdf", "total_pages": 12, "format": "PDF 1.4", "title": "Knowledge graph&#x2013;based thought: a knowledge graph&#x2013;enhanced LLM framework for pan-cancer question answering", "author": "Feng Yichun, Zhou Lu, Ma Chao, Zheng Yikai, He Ruikun, Li Yixue", "subject": "DOI: 10.1093/gigascience/giae082 GigaScience, 14, 0, 00-00-2025. Abstract: In recent years, large language models (LLMs) have shown promise in various domains, notably in biomedical sciences. However, their real-world application is often limited by issues like erroneous outputs and hallucinatory responses.We developed the knowledge graph&#x2013;based thought (KGT) framework, an innovative solution that integrates LLMs with knowledge graphs (KGs) to improve their initial responses by utilizing verifiable information from KGs, thus significantly reducing factual errors in reasoning. The KGT framework demonstrates strong adaptability and performs well across various open-source LLMs. Notably, KGT can facilitate the discovery of new uses for existing drugs through potential drug&#x2013;cancer associations and can assist in predicting resistance by analyzing relevant biomarkers and genetic mechanisms. To evaluate the knowledge graph question answering task within biomedicine, we utilize a pan-cancer knowledge graph to develop a pan-cancer question answering benchmark, named pan-cancer question answering.The KGT framework substantially improves the accuracy and utility of LLMs in the biomedical field. This study serves as a proof of concept, demonstrating its exceptional performance in biomedical question answering.", "keywords": "", "moddate": "2026-01-11T01:15:31+00:00", "trapped": "", "modDate": "D:20260111011531+00'00'", "creationDate": "D:20250106105103Z", "page": 2}}
{"page_content": "4 | GigaScience , 2025, Vol. 14 \nTable 2. Four differ ent r easoning types of task. Eac h r easoning type ma y include o v erla pping questions, so the sum acr oss the 4 differ ent \nreasoning types of the task may exceed the total number of questions \nReasoning type \nClaim example \nGraph \nQuestion number \nOne-hop \nWhat types of cancer can be treated with diethylstilbestrol? \n243 \nMultihop \nWhat genetic mutations are present in adenoid cystic \ncarcinoma? \n124 \nIntersection \nWhich drugs are ALK in basaloid large cell carcinoma of \nthe lung sensitivity to? \n37 \nAttribute \nWhat is the maximum age for recruitment of clinical trials \nfor patients with meningioma? \n59 \nMultihop problems \nMultihop pr oblems involv e m ultiple-r elation c hain r easoning that \ncan be br oadl y categorized into 2 types . T he first category in volves \ndeducing potential relationships between entities by navigating \nthr ough indir ect r elations. By examining the indir ect r elations \n( R 1 , R 2 ) between a head entity H 1 and a tail entity T 1 , it is possible \nto infer an unknown or potential relation R ? linking them dir ectl y. \nThis inference process is encapsulated in the following equation: \nH 1 + T 1 → R 1 + R 2 → R ? \n(3) \nThe second category extends the reasoning to include the discov- \nery of entities themselves b y follo wing a path from a head entity \nthrough intermediate relations to a final tail entity. Starting with \na head entity H 1 , coupled with an indir ect r elation R 1 , an inter- \nmediary entity M can be inferred. This intermediary entity M is \nthen applied with an indirect relation R 2 to deduce the final tail \nentity T ? . This infer ence pr ocess is summarized in the following \nequation: \nH 1 + R 1 → M + R 2 → T ? \n(4) \nIntersection problems \nIntersection pr oblems r efer to taking the intersection of multi- \nple relational chains. Two head entities ( H 1 , H 2 ) lead to the deduc- \ntion of 2 types of tail entities ( T 1 , T 2 ) based on different relations \n( R 1 , R 2 ). The final tail entity T ? is determined by intersecting these \n2 types of tail entities ( T 1 , T 2 ). This inference process is summa- \nrized as following: \nH 1 + R 1 → T 1 \n(5) \nH 2 + R 2 → T 2 \n(6) \nT 1 ∩ T 2 → T ? \n(7) \nAttribute problems \nAttribute problems refer to the attribute information of the en- \ntity, where the task involves retrieving the attributes of a known \nhead entity H 1 or determining whether the tail entity T 1 , identified \nthrough a known head entity H 1 and relation R 1 , satisfies the at- \ntributes specified in the query, as illustrated in equations ( 8 ) and \n( 9 ). \nH 1 → P H \n1 \n(8) \nH 1 + R 1 → P T \n1 \n(9) \nDatasets \nIn the continuously evolving field of biomedical r esearc h, the in- \ntegration of LLMs with KGs offers a more efficient and effective \nmethod for knowledge discovery and utilization, particularly in \nadv ancing cancer r esearc h. Nonetheless, we note a scarcity of \na ppr opriate datasets for e v aluating these sophisticated method- \nologies within this field. To address this, we leverage the SOKG \nto establish a benchmark for the KGQA task within biomedicine, \nnamed PcQA. Our questions were carefully crafted by experts \nbased on the content of the knowledge gr a ph. GPT-4 [ 38 ] was \nthen emplo y ed to generate Cypher queries, which were used to re- \ntrie v e answers fr om the knowledge gr a ph. The gener ated Cypher \nqueries and corresponding answers underwent an initial r e vie w \nby a biomedical PhD candidate, who manually verified and cor- \nrected the dataset against the knowledge graph. Finally, the en- \ntire dataset was thoroughly reviewed by 2 biomedical experts to \nensure its accuracy and reliability. This multistep process was \nmeticulously designed to uphold the highest standards of qual- \nity throughout the dataset creation. This dataset, along with the \naccompan ying knowledge gr a ph, is completel y open source [ 33 ]. \nThe PcQA includes 405 data entries, covering a wide range of appli- \ncations in the field of pan-cancer r esearc h, including genetic pre- \ndisposition to cancer, medication tr eatment planning, drug r epo- \nsitioning, identification of potential drug targets, studies on drug \nr esistance, and pr edictions of cancer pr ogr ession and metastasis. \nBy deeply exploring cancer-related reasoning and information re- \ntrie v al c hallenges, this dataset can inspir e r esearc hers and clini- \ncians to gain a deeper understanding of cancer and explore more \neffectiv e tr eatment methods. \nKGT fr ame w ork \nT he o v er all fr ame work of KGT is laid out in Fig. 2 . When users \ninput their question in natural language, the first step is to ana- \nlyze the question, extracting the main information with the goal of \nbreaking down the question into smaller, more manageable units. \nThis main information is then passed to an LLM, which applies \ngr a ph r easoning on the sc hema gr a ph of the knowledge gr a ph, \nyielding the optimal r elational path. Subsequentl y, a r etrie v al \nstatement is generated, and a subgraph is constructed within the \nDownloaded from https://academic.oup.com/gigascience/article/doi/10.1093/gigascience/giae082/7943459 by guest on 11 January 2026", "metadata": {"producer": "Acrobat Distiller 24.0 (Windows); modified using iTextSharp.LGPLv2.Core 3.7.4.0", "creator": "OUP", "creationdate": "2025-01-06T10:51:03+00:00", "source": "/home/donbr/open-biosciences/biosciences-research/data/raw/giae082.pdf", "file_path": "/home/donbr/open-biosciences/biosciences-research/data/raw/giae082.pdf", "total_pages": 12, "format": "PDF 1.4", "title": "Knowledge graph&#x2013;based thought: a knowledge graph&#x2013;enhanced LLM framework for pan-cancer question answering", "author": "Feng Yichun, Zhou Lu, Ma Chao, Zheng Yikai, He Ruikun, Li Yixue", "subject": "DOI: 10.1093/gigascience/giae082 GigaScience, 14, 0, 00-00-2025. Abstract: In recent years, large language models (LLMs) have shown promise in various domains, notably in biomedical sciences. However, their real-world application is often limited by issues like erroneous outputs and hallucinatory responses.We developed the knowledge graph&#x2013;based thought (KGT) framework, an innovative solution that integrates LLMs with knowledge graphs (KGs) to improve their initial responses by utilizing verifiable information from KGs, thus significantly reducing factual errors in reasoning. The KGT framework demonstrates strong adaptability and performs well across various open-source LLMs. Notably, KGT can facilitate the discovery of new uses for existing drugs through potential drug&#x2013;cancer associations and can assist in predicting resistance by analyzing relevant biomarkers and genetic mechanisms. To evaluate the knowledge graph question answering task within biomedicine, we utilize a pan-cancer knowledge graph to develop a pan-cancer question answering benchmark, named pan-cancer question answering.The KGT framework substantially improves the accuracy and utility of LLMs in the biomedical field. This study serves as a proof of concept, demonstrating its exceptional performance in biomedical question answering.", "keywords": "", "moddate": "2026-01-11T01:15:31+00:00", "trapped": "", "modDate": "D:20260111011531+00'00'", "creationDate": "D:20250106105103Z", "page": 3}}
{"page_content": "Knowledge gr a ph–based thought | 5 \nF igure 2: F r ame work of KGT. (A) Question anal ysis. Decompose the question and extr act its k e y information. (B) Gr a ph sc hema–based infer ence. Input \nthe types of the head and tail entities into the gr a ph sc hema of the knowledge gr a ph, complete the gr a ph r easoning, and obtain the optimal r elational \npath. (C) Subgr a ph construction. Gener ate a query statement and r etrie v e the subgr a ph. (D) Infer ence. Complete the final r easoning and output the \nresults in natural language. Note: The symbol “×” r epr esents content that has been filtered out by the LLM, while “✓ ” denotes the optimal content \nselected by the LLM. \nKG thr ough searc h. The r elational c hains and attributes in the \nsubgr a ph ar e then fed back into the LLM to finalize the reason- \ning and generate an output in natural language. \nQuestion analysis \nKey information extraction \nThe user inputs a question text ( Q ) in natural language, which \nis initially deconstructed and parsed. An LLM is applied to ana- \nlyze the question, resulting in the identification of the head entity \nname ( H n ), the tail entity type ( T t ), and the attributes of tail entity \n( T a ). The prompt for the LLM to extract k e y information from the \nquestion is presented in Supplementary Fig. S1 . \nRetrie ving ke y information from the KG \nBased on H n , a fixed Cypher format is set to query the head entity \ntype ( H t ), facilitating subsequent reasoning. \nGraph schema–based inference \nConstruction of a graph based on KG schema \nBased on the entity types ( E t ) and the relations ( R ) between them \nin the SOKG, an undirected graph G is established where E t serve \nas nodes N and R act as edges P. \nCandidate path sear c h \nBr eadth-first searc h (BFS) is emplo y ed to identify the shortest \npaths connecting H t and T t from the constructed graph G. Initi- \nate the search at H t , creating a queue to hold nodes encountered \nalong the way. Sim ultaneousl y, form a set to tr ac k nodes that have \nbeen visited to avoid r e visiting them. Insert H t into the queue. Con- \ntinue processing as long as the queue remains nonempty, remov- \ning a node from the queue at each step. For each of its unvisited \nneighbors, enqueue the neighbor, mark it as visited, and log the \npathway from H t to this neighbor. Upon arrival at T t , use the accu- \nmulated path data to compile the set of shortest paths ( SPs ) from \nH t to T t , with each individual path within the set r eferr ed to as an \nSP . The nodes in each SP represent entity types, while the edges \ndenote the relationships between these entity types. \nOptimal path selection \nBy utilizing embedding technology, textual information is mapped \ninto a low-dimensional space, resulting in N-dimensional real- \nv alue v ectors . T he similarity between each SP and the Q is calcu- \nlated based on their r espectiv e r eal-v alue v ectors, with the SP ex- \nhibiting the highest similarity being selected as the optimal path \n( OP ). \nSimilarity (Q, SP) = \nQ · SP \n∥ Q∥ × ∥ SP∥ \n= \n\u0002 n \ni =1 (Q i × SP i ) \n\u0003 \u0002 n \ni =1 Q 2 \ni ×\n\u0003 \u0002 n \ni =1 SP 2 \ni \n(10) \nOP = max \nQ,SP Similarity (Q, SP) \n(11) \nSubgraph construction \nQuery statement generation \nInput H t , H n , T t , T a , and OP into an LLM to generate a query \nstatement, such as Cypher . T ext2Cypher Prompt is presented in \nSupplementary Fig. S2 . \nSubgr aph gener ation \nEnter the query statement in the KG to obtain a reasonable sub- \ngr a ph. \nDownloaded from https://academic.oup.com/gigascience/article/doi/10.1093/gigascience/giae082/7943459 by guest on 11 January 2026", "metadata": {"producer": "Acrobat Distiller 24.0 (Windows); modified using iTextSharp.LGPLv2.Core 3.7.4.0", "creator": "OUP", "creationdate": "2025-01-06T10:51:03+00:00", "source": "/home/donbr/open-biosciences/biosciences-research/data/raw/giae082.pdf", "file_path": "/home/donbr/open-biosciences/biosciences-research/data/raw/giae082.pdf", "total_pages": 12, "format": "PDF 1.4", "title": "Knowledge graph&#x2013;based thought: a knowledge graph&#x2013;enhanced LLM framework for pan-cancer question answering", "author": "Feng Yichun, Zhou Lu, Ma Chao, Zheng Yikai, He Ruikun, Li Yixue", "subject": "DOI: 10.1093/gigascience/giae082 GigaScience, 14, 0, 00-00-2025. Abstract: In recent years, large language models (LLMs) have shown promise in various domains, notably in biomedical sciences. However, their real-world application is often limited by issues like erroneous outputs and hallucinatory responses.We developed the knowledge graph&#x2013;based thought (KGT) framework, an innovative solution that integrates LLMs with knowledge graphs (KGs) to improve their initial responses by utilizing verifiable information from KGs, thus significantly reducing factual errors in reasoning. The KGT framework demonstrates strong adaptability and performs well across various open-source LLMs. Notably, KGT can facilitate the discovery of new uses for existing drugs through potential drug&#x2013;cancer associations and can assist in predicting resistance by analyzing relevant biomarkers and genetic mechanisms. To evaluate the knowledge graph question answering task within biomedicine, we utilize a pan-cancer knowledge graph to develop a pan-cancer question answering benchmark, named pan-cancer question answering.The KGT framework substantially improves the accuracy and utility of LLMs in the biomedical field. This study serves as a proof of concept, demonstrating its exceptional performance in biomedical question answering.", "keywords": "", "moddate": "2026-01-11T01:15:31+00:00", "trapped": "", "modDate": "D:20260111011531+00'00'", "creationDate": "D:20250106105103Z", "page": 4}}
{"page_content": "6 | GigaScience , 2025, Vol. 14 \nInference \nSubgraph inference \nBased on the relational chains and attribute data in the subgraph, \ndetermine the r ele v ance to the question text. Prune an y err oneous \ninformation, r etaining onl y the corr ect r elational c hains. \nNatural language output \nThe LLM divides the subgr a ph into multiple relational chains, \neach of which outputs a sentence in natural language, and then \nthe LLM generates natural language output. LLMs Inference and \nOutput Prompt is presented in Supplementary Fig. S3 . \nResults \nEv alua tion criteria \nWe use e v aluators based on GPT-4 [ 38 ], BERTScore [ 39 ], and \nROUGE [ 40 ] to assess the accuracy of the generated ans wers . As a \nscoring bot, GPT-4 e v aluates and assigns scores based on the sim- \nilarity in meaning between 2 sentences. GPT-4–based Evaluation \nPr ompt is pr esented in Supplementary Fig. S4 . BERTScor e e v alu- \nates semantic similarity using context-sensiti ve embed dings, of- \nfering a compr ehensiv e e v aluation of langua ge model outputs. \nROUGE, on the other hand, e v aluates the longest common subse- \nquence (LCS) between the generated text and the reference text, \nfocusing on sequence-based similarity to assess the fluency and \nthe pr eserv ation of semantic content. \nBaselines \nTo assess the adv anta ges of our fr ame w ork, w e compare it with \nse v er al a ppr oac hes that can be dir ectl y a pplied for KGQA tasks \nwithout fine-tuning. We introduce a straightforw ar d baseline ap- \npr oac h, named Base, which is similar to KG-GPT [ 32 ], curr entl y \nthe leading method in the KGQA field, excluding the sentence \nsegmentation step of KG-GPT. Initiall y, this involv es le v er a ging \nan LLM to r etrie v e r ele v ant information fr om the KG by gener at- \ning a query statement. Then, another LLM is used to answer the \nquestion with the r etrie v ed information. To enhance the baseline, \nwe incor por ate CoT pr ompting [ 19 ] and in-context learning (ICL) \ntec hniques [ 41 ], collectiv el y r eferr ed to as CoT&ICL. The pr ompts \nfor these methods are illustrated in Supplementary Table S5 . Ad- \nditionally, we implement KG-GPT [ 32 ] to enhance the r etrie v al and \nr easoning ca pabilities of the LLMs. For a fair comparison, all meth- \nods are based on Code-Llama-13B [ 42 ]. \nTo further underscore the efficacy of our fr ame w ork, w e con- \nduct a compar ativ e anal ysis of KGT, whic h is built upon Code- \nLlama-13B, a gainst 2 highl y ca pable lar ge langua ge models that \nar e pr ominent in the general and biomedical domains: ChatGPT- \n3.5 [ 1 ] and Taiyi [ 43 ]. ChatGPT-3.5, a leader in tasks across the \ngeneral domain, has exhibited competitive performance in a wide \nr ange of a pplications. To compensate for its limited biomedi- \ncal kno wledge, w e emplo y 2 methodologies pr e viousl y described, \nBase and CoT&ICL, as advanced baselines to augment ChatGPT- \n3.5’s capabilities. Taiyi, a cutting-edge LLM in biomedicine, pre- \ntrained on 2 trillion tokens, le v er a ges its extensiv e biomedical \nknowledge base for direct question answering, bypassing the need \nfor knowledge gr a ph r etrie v al. \nDue to the scarcity of KGQA datasets within the biomedical \ndomain, all experiments are conducted on our ne wl y pr oposed \nbenchmark, named PcQA. \nCompar a ti v e analysis across different KGQA \nmethods \nWe e v aluated the ca pabilities of v arious methods based on \nCode-Llama-13B, with the experimental results presented in Ta- \nble 3 . The experimental results indicate that the Code-Llama- \n13B model, enhanced with KGT, consistently surpasses competing \nmethods across all metrics assessed. Notabl y, KG-GPT impr ov es \nthe F1 score by 15.7% over previous methods CoT&ICL, while our \nmethod KGT increases the F1 score by 33% over KG-GPT. Because \nKG-GPT overlooks the impact of entity types and attributes on an- \nswers within the biomedical domain, this ac hie v ement positions \nour a ppr oac h as a pioneering benc hmark in biomedical KGQA, \neclipsing pr e viousl y established best pr actices. \nCompar a ti v e analysis across di v erse LLMs \nWe present a comparative study of KGT applied to Code-Llama- \n13B against 2 highly capable LLMs in the general and biomedical \ndomains, with experimental results displayed in Table 4 . Code- \nLlama-13B, enhanced by KGT, significantly outperforms its peers, \nac hie ving the highest marks in e v ery assessment metric: a GPT- \n4 Eval score of 92.4, a BERTScore of 97.7, and a ROUGE F1 score \nof 86.8. Remarkably, our approach’s F1 score surpasses that of \nChatGPT-3.5 with the Base method by 52.7%, the CoT&ICL method \nby 36.3%, and Taiyi’s base model by 67.3%. These results highlight \nKGT’s substantial contribution to improving the performance of \nlar ge langua ge models for the pan-cancer KGQA task. Ev en when \nintegrated with open-source general models, KGT exhibits re- \nmarkable performance, outstripping both the recognized state- \nof-the-art closed-source large language models and those specif- \nicall y tailor ed for the biomedical domain. This showcases KGT’s \nadeptness at parsing and le v er a ging knowledge gr a ph data, set- \nting a new standard for future research and applications in the \nfield. \nAssessing KGT’s effecti v eness on di v erse LLM \nplatforms \nTo underscore the adaptability and effectiveness of our KGT \nfr ame work when applied to a range of large language models, \nwe conduct experiments on se v er al LLMs: Zephyr [ 44 ], Llama-2 \n[ 2 ], and Code-Llama [ 42 ]. The outcomes, illustrated in Fig. 3 , re- \nveal that while the CoT&ICL techniques significantly boost per- \nformance in terms of F1 score, our KGT methodology delivers \ne v en mor e substantial enhancements acr oss all e v aluated mod- \nels . T his demonstrates not only the effectiveness of CoT&ICL as a \nperformance-enhancing strategy but also highlights the superior \nadvancements and impact of KGT, establishing its dominance and \nefficiency in knowledge gr a ph question-answering tasks. \nAblation study for dissecting the components of \nKGT \nIn our effort to illuminate the individual contributions of the com- \nponents that constitute our KGT fr ame work and their collective \nimpact on enhancing the performance of LLMs, we define 4 foun- \ndational modules: (i) question analysis for the extraction of piv- \notal information, (ii) gr a ph sc hema–based infer ence to identify \nthe optimal relational chains in the knowledge graph, (iii) the \ngeneration of query statements to facilitate subgraph construc- \ntion, and (iv) the inference process coupled with the articulation \nof results in natural language . T his ablation study, grounded on \nthe Code-Llama-13B model, is meticulously designed to e v alu- \nate the efficacy of these components. Since gr a ph sc hema–based \ninfer ence r equir es the pr ocess of question anal ysis, the ques- \nDownloaded from https://academic.oup.com/gigascience/article/doi/10.1093/gigascience/giae082/7943459 by guest on 11 January 2026", "metadata": {"producer": "Acrobat Distiller 24.0 (Windows); modified using iTextSharp.LGPLv2.Core 3.7.4.0", "creator": "OUP", "creationdate": "2025-01-06T10:51:03+00:00", "source": "/home/donbr/open-biosciences/biosciences-research/data/raw/giae082.pdf", "file_path": "/home/donbr/open-biosciences/biosciences-research/data/raw/giae082.pdf", "total_pages": 12, "format": "PDF 1.4", "title": "Knowledge graph&#x2013;based thought: a knowledge graph&#x2013;enhanced LLM framework for pan-cancer question answering", "author": "Feng Yichun, Zhou Lu, Ma Chao, Zheng Yikai, He Ruikun, Li Yixue", "subject": "DOI: 10.1093/gigascience/giae082 GigaScience, 14, 0, 00-00-2025. Abstract: In recent years, large language models (LLMs) have shown promise in various domains, notably in biomedical sciences. However, their real-world application is often limited by issues like erroneous outputs and hallucinatory responses.We developed the knowledge graph&#x2013;based thought (KGT) framework, an innovative solution that integrates LLMs with knowledge graphs (KGs) to improve their initial responses by utilizing verifiable information from KGs, thus significantly reducing factual errors in reasoning. The KGT framework demonstrates strong adaptability and performs well across various open-source LLMs. Notably, KGT can facilitate the discovery of new uses for existing drugs through potential drug&#x2013;cancer associations and can assist in predicting resistance by analyzing relevant biomarkers and genetic mechanisms. To evaluate the knowledge graph question answering task within biomedicine, we utilize a pan-cancer knowledge graph to develop a pan-cancer question answering benchmark, named pan-cancer question answering.The KGT framework substantially improves the accuracy and utility of LLMs in the biomedical field. This study serves as a proof of concept, demonstrating its exceptional performance in biomedical question answering.", "keywords": "", "moddate": "2026-01-11T01:15:31+00:00", "trapped": "", "modDate": "D:20260111011531+00'00'", "creationDate": "D:20250106105103Z", "page": 5}}
{"page_content": "Knowledge gr a ph–based thought | 7 \nTable 3. Comparison of results between KGT and other commonly used methods based on the Code-Llama-13B. The best results are \ndisplayed in bold for each indicator \nROUGE (%) \nMethod \nGPT-4 Eval (%) \nBERTScore (%) \nRecall \nPrecision \nF1 score \nBase \n46.6 \n85.3 \n25.3 \n28.5 \n24.5 \nCoT&ICL \n57.9 \n88.8 \n38.9 \n39.4 \n37.6 \nKG-GPT \n68.2 \n93.5 \n55.2 \n55.8 \n53.3 \nKGT (ours) \n92.4 \n97.7 \n87.4 \n87.7 \n86.8 \nTable 4. Comparison of KGT based on Code-Llama-13B with results from other commonly used models . T he best results are displayed \nin bold for each indicator \nROUGE (%) \nModel \nMethod \nGPT-4 Eval (%) \nBERTScore (%) \nRecall \nPrecision \nF1 score \nChatGPT-3.5 \nBase \n65.4 \n91.0 \n42.7 \n32.3 \n34.1 \nCoT&ICL \n70.3 \n93.3 \n57.0 \n50.6 \n50.5 \nTaiyi \n\\ \n40.6 \n85.3 \n15.4 \n39.6 \n19.5 \nCode-Llama-13B \nKGT (ours) \n92.4 \n97.7 \n87.4 \n87.7 \n86.8 \nF igure 3: P erformance of v arious models using differ ent str ategies. \ntion analysis module cannot be removed in isolation; simulta- \nneousl y, subgr a ph construction is indispensable for knowledge \ngr a ph r etrie v al. If the subgr a ph construction module is indepen- \ndently omitted, the outputs of the initial 2 modules will not \nimpact the final results, making the isolated exclusion of this \ncomponent illogical. Ther efor e, we intr oduce 3 specific ablated \nconfigurations for examination: (i) excluding gr a ph sc hema–based \ninference (without GSBI), (ii) omitting both question analysis and \ngr a ph sc hema–based infer ence (without QA&GSBI), and (iii) r e- \nmo ving question analysis , graph schema–based inference , and \nsubgr a ph construction (without QA&GSBI&SC), effectiv el y by- \npassing the structured query of the SOKG and relying solely on \nthe LLM’s inherent knowledge for question answering. \nThe results of the ablation study, as shown in Table 5 , demon- \nstrate that when we remove the GSBI, we observe a 20% decrease \nin the F1 score . Remo ving both GSBI and QA results in an addi- \ntional 8.6% decrease in the F1 score compared to removing GSBI \nalone . Furthermore , remo ving GSBI, QA, and SC together leads to a \n46% decrease in the F1 score compared to removing just GSBI and \nQA. The experiments r e v eal that SC is crucial; its absence forces \nthe LLM to r el y solel y on its inher ent knowledge, significantl y \nr educing effectiv eness. GSBI is also k e y, as it aids in navigating \ncomplex multihop questions by providing necessary intermediate \nentity information for subgr a ph construction. QA is equall y essen- \ntial, ensuring accurate identification of entities and properties for \ncorr ect subgr a ph construction. All these v ariants under perform \nDownloaded from https://academic.oup.com/gigascience/article/doi/10.1093/gigascience/giae082/7943459 by guest on 11 January 2026", "metadata": {"producer": "Acrobat Distiller 24.0 (Windows); modified using iTextSharp.LGPLv2.Core 3.7.4.0", "creator": "OUP", "creationdate": "2025-01-06T10:51:03+00:00", "source": "/home/donbr/open-biosciences/biosciences-research/data/raw/giae082.pdf", "file_path": "/home/donbr/open-biosciences/biosciences-research/data/raw/giae082.pdf", "total_pages": 12, "format": "PDF 1.4", "title": "Knowledge graph&#x2013;based thought: a knowledge graph&#x2013;enhanced LLM framework for pan-cancer question answering", "author": "Feng Yichun, Zhou Lu, Ma Chao, Zheng Yikai, He Ruikun, Li Yixue", "subject": "DOI: 10.1093/gigascience/giae082 GigaScience, 14, 0, 00-00-2025. Abstract: In recent years, large language models (LLMs) have shown promise in various domains, notably in biomedical sciences. However, their real-world application is often limited by issues like erroneous outputs and hallucinatory responses.We developed the knowledge graph&#x2013;based thought (KGT) framework, an innovative solution that integrates LLMs with knowledge graphs (KGs) to improve their initial responses by utilizing verifiable information from KGs, thus significantly reducing factual errors in reasoning. The KGT framework demonstrates strong adaptability and performs well across various open-source LLMs. Notably, KGT can facilitate the discovery of new uses for existing drugs through potential drug&#x2013;cancer associations and can assist in predicting resistance by analyzing relevant biomarkers and genetic mechanisms. To evaluate the knowledge graph question answering task within biomedicine, we utilize a pan-cancer knowledge graph to develop a pan-cancer question answering benchmark, named pan-cancer question answering.The KGT framework substantially improves the accuracy and utility of LLMs in the biomedical field. This study serves as a proof of concept, demonstrating its exceptional performance in biomedical question answering.", "keywords": "", "moddate": "2026-01-11T01:15:31+00:00", "trapped": "", "modDate": "D:20260111011531+00'00'", "creationDate": "D:20250106105103Z", "page": 6}}
{"page_content": "8 | GigaScience , 2025, Vol. 14 \nTable 5. Ablation study of the KGT fr ame work under Code-Llama-13B \nROUGE (%) \nMethod \nGPT-4 Eval (%) \nBERTScore (%) \nRecall \nPrecision \nF1 score \nKGT (ours) \n92.4 \n97.7 \n87.4 \n87.7 \n86.8 \nWithout GSBI \n71.8 \n95.5 \n68.1 \n69.8 \n66.8 \nWithout QA&GSBI \n69.7 \n94.7 \n55.0 \n66.3 \n58.2 \nWithout QA&GSBI&SC \n24.7 \n77.4 \n14.8 \n12.3 \n12.2 \ncompared to the complete KGT, indicating that each of the 3 mod- \nules is vital for the final performance . Furthermore , such obser- \nvations confirm that our KGT can indeed le v er a ge knowledge to \nenhance the final performance of LLMs. \nImplementation settings \nOur knowledge gr a ph is quite large, with a complex schema, and \ntypicall y involv es input tokens within 1,300. Our experiment does \nnot r equir e fine-tuning, and the infer ence time is r elated to the \nmodel size and computational resources. For example, when us- \ning our method, KGT, with the Code-Llama-13B model on an 80 GB \nA100 GPU, it occupies 33 GB of VRAM. Without any acceleration \nfr ame works, the infer ence r equir es 4 passes, eac h taking ar ound \n20 seconds. \nCase studies \nDrug repositioning \nDrug r epositioning emer ges as a pr omising str ategy to acceler ate \nthe process of drug development. This approach involves identi- \nfying new therapeutic uses for existing drugs, thereby saving time \nand r esources typicall y r equir ed for bringing a ne w drug to market \n[ 45 ]. Our system is capable of investigating the potential reposi- \ntioning of carteolol for the treatment of hemangiomas . T he exam- \nple is shown in Supplementary Table S2 and r elational dia gr am is \nshown in Fig. 4 A. Utilizing the system’s knowledge gr a ph, a r ela- \ntional chain is delineated, illustrating that propranolol, another \ninhibitor of ADRB1, is effectiv el y emplo y ed in the treatment of \nhemangiomas . T he system harnesses this insight to formulate a \nhypothesis that carteolol, by virtue of its similar mechanism of \ninhibition, could be potentially repositioning for treating heman- \ngiomas [ 46 ]. This hypothesis would serve as a precursor to clin- \nical trials and r esearc h, potentiall y expediting the availability of \nan additional ther a peutic option for patients with hemangiomas. \nDrug resistance resear c h \nDrug resistance in cancer treatment poses a significant challenge \nin clinical oncology. Understanding the genetic basis of resistance \ncan lead to mor e effectiv e tr eatment str ategies and personalized \nmedicine a ppr oac hes. Researc h in drug r esistance involv es deter- \nmining why certain cancer-carrying mutated genes are not re- \nsponsive to specific drugs and finding ways to overcome this re- \nsistance [ 47 ]. Our system is capable of exploring drug resistance \nin cancer. The example is shown in Supplementary Table S3 , and \na relational diagram is shown in Fig. 4 B. The KG data indicate \nthat the ALK-p.L1196M m utation, whic h is associated with gas- \ntric cancer, has a known resistance to nalatinib [ 48 , 49 ]. The LLM \nprocesses this information and infers that due to this resistance, \nnalatinib might not be an effective medication for treating can- \ncers caused by the ALK-p.L1196M mutation. The case highlights \nthe critical importance of understanding specific gene–drug in- \nteractions in drug resistance research. It demonstrates how cer- \ntain gene mutations could render a drug ineffective, which in turn \ncould guide oncologists in choosing alternative treatments or de- \nv eloping ne w drugs that can bypass or tar get the r esistance mec h- \nanisms. By accelerating the process of understanding drug resis- \ntance, these artificial intelligence–driven systems can contribute \nto impr ov ed patient outcomes and the optimization of cancer \ntr eatment pr otocols. \nIndividualized treatment \nDetails \non \nindividualized \ntr eatment \nar e \npr ovided \nin \nSupplementary Case Studies A . It is important to note that this \nexample is included solely to illustrate the technical capabilities \nof the proposed method. The output generated in this example \nhas not been validated for clinical use, and further validation in \nclinical settings would be r equir ed befor e an y suc h a pplication. \nSelection and understanding of biomarkers \nDetails on selection and understanding of biomarkers are pro- \nvided in Supplementary Case Studies B . \nDiscussion \nIn this article, we introduce a novel framework KGT, which em- \nploys LLMs for reasoning on the schema of KGs, to enhance the \nreasoning abilities of LLMs in areas with missing domain data \nby utilizing domain-specific knowledge gr a phs, suc h as oncology \nknowledge gr a phs, ther eby addr essing the issue of factual hal- \nlucinations in LLMs. Our method excels in extr acting, v alidat- \ning, and refining factual knowledge throughout the LLMs’ rea- \nsoning process. It seamlessly integrates with various LLMs, in- \ncluding open-source models like Code-Llama, and enhances the \ncapabilities of LLMs solely through prompt engineering and in- \ncontext learning without any fine-tuning. This grants it significant \ngeneralizability. \nWe possess an extensive oncology knowledge graph and have \nestablished a benchmark based on it to e v aluate the ca pabilities of \nvarious methods. When tested on PcQA using various open-source \nLLMs, the KGT fr ame work performs exceptionall y well, sur passing \nthe current best methods by 33%. This significant improvement \npositions our a ppr oac h as a pioneering benchmark in biomedical \nKGQA, setting a new standard that advances beyond previously \nestablished best pr actices. Additionall y, thr ough case studies, our \na ppr oac h has been shown to effectiv el y pr ovide ther a peutic plans, \ngener ate v aluable hypotheses for drug r epositioning, identify po- \ntential drug targets, and study drug resistance . T his underscores \nthe pr actical v alue of the KGT fr ame work in deliv ering insight- \nful contributions that aid in the de v elopment and optimization \nof treatment strategies. Each case study’s conclusions are further \nv alidated by e vidence fr om pr e viousl y published r esearc h pa pers, \nenhancing the credibility and impact of our findings. \nHo w e v er, it is important to note that the constructed QA \ndataset and the corresponding published subset of the SOKG \nDownloaded from https://academic.oup.com/gigascience/article/doi/10.1093/gigascience/giae082/7943459 by guest on 11 January 2026", "metadata": {"producer": "Acrobat Distiller 24.0 (Windows); modified using iTextSharp.LGPLv2.Core 3.7.4.0", "creator": "OUP", "creationdate": "2025-01-06T10:51:03+00:00", "source": "/home/donbr/open-biosciences/biosciences-research/data/raw/giae082.pdf", "file_path": "/home/donbr/open-biosciences/biosciences-research/data/raw/giae082.pdf", "total_pages": 12, "format": "PDF 1.4", "title": "Knowledge graph&#x2013;based thought: a knowledge graph&#x2013;enhanced LLM framework for pan-cancer question answering", "author": "Feng Yichun, Zhou Lu, Ma Chao, Zheng Yikai, He Ruikun, Li Yixue", "subject": "DOI: 10.1093/gigascience/giae082 GigaScience, 14, 0, 00-00-2025. Abstract: In recent years, large language models (LLMs) have shown promise in various domains, notably in biomedical sciences. However, their real-world application is often limited by issues like erroneous outputs and hallucinatory responses.We developed the knowledge graph&#x2013;based thought (KGT) framework, an innovative solution that integrates LLMs with knowledge graphs (KGs) to improve their initial responses by utilizing verifiable information from KGs, thus significantly reducing factual errors in reasoning. The KGT framework demonstrates strong adaptability and performs well across various open-source LLMs. Notably, KGT can facilitate the discovery of new uses for existing drugs through potential drug&#x2013;cancer associations and can assist in predicting resistance by analyzing relevant biomarkers and genetic mechanisms. To evaluate the knowledge graph question answering task within biomedicine, we utilize a pan-cancer knowledge graph to develop a pan-cancer question answering benchmark, named pan-cancer question answering.The KGT framework substantially improves the accuracy and utility of LLMs in the biomedical field. This study serves as a proof of concept, demonstrating its exceptional performance in biomedical question answering.", "keywords": "", "moddate": "2026-01-11T01:15:31+00:00", "trapped": "", "modDate": "D:20260111011531+00'00'", "creationDate": "D:20250106105103Z", "page": 7}}
{"page_content": "Knowledge gr a ph–based thought | 9 \nA\nB\nC\nD\nFigure 4: (A), (B), (C), and (D) r espectiv el y r epr esent the r elational dia gr ams of drug r epositioning, drug r esistance r esearc h, individualized tr eatment, \nand selection and understanding of biomarkers. \nwer e specificall y designed to v alidate the effectiv eness of the KGT \nfr ame work within this study. While the dataset is highly relevant \nto biomedical applications, its scope is primarily focused on vali- \ndating the proposed method. T herefore , it ma y not co ver all poten- \ntial use cases. Additionally, our system curr entl y has the dr awbac k \nof not performing fuzzy matching; if a drug name is misspelled by \ne v en 1 letter, it fails to r etrie v e information from the knowledge \ngr a ph. Ther efor e, we plan to impr ov e this aspect in the future to \nenhance the system’s usability and reliability. Our ultimate goal \nis to create a robust framework applicable to the rapidly evolving \ndomain of medical knowledge, supporting health care profession- \nals in delivering personalized, precise medication tailored to the \nindividual needs of each patient. \nFinally, we affirm that this study serves as a proof of concept, \naiming to showcase the technical feasibility and initial efficacy of \nthe method, which has not been validated in actual clinical prac- \ntice. In any clinical or medical decision-making, reliance should \nalways be placed on the judgment and guidance of professional \nhealth care practitioners. \nAdditional Files \nSupplementary Table S1. Comparison of PcQA with MetaQA and \nFACTKG in multihop tasks . T he types of intermediate entities are \nindicated in bold. \nSupplementary Table S2. Example of drug repositioning. \nSupplementary Table S3. Example of drug resistance research. \nSupplementary Table S4. Example of individualized treatment. \nSupplementary Table S5. Example of selection and understand- \ning of biomarkers. \nSupplementary Table S6. Prompts for Base and CoT&ICL. \nSupplementary Fig. S1. Prompt for k e y information extraction. \nSupplementary Fig. S2. Prompt for query statement generation. \nSupplementary Fig. S3. Prompt for LLM inference and output. \nSupplementary Fig. S4. Prompt for GPT-4–based evaluation. \nSupplementary Fig. S5. (A), (B), (C), and (D) r espectiv el y r epr esent \nthe r elational dia gr ams of drug r epositioning, drug r esistance r e- \nsearc h, individualized tr eatment, and selection and understand- \ning of biomarkers. \nDownloaded from https://academic.oup.com/gigascience/article/doi/10.1093/gigascience/giae082/7943459 by guest on 11 January 2026", "metadata": {"producer": "Acrobat Distiller 24.0 (Windows); modified using iTextSharp.LGPLv2.Core 3.7.4.0", "creator": "OUP", "creationdate": "2025-01-06T10:51:03+00:00", "source": "/home/donbr/open-biosciences/biosciences-research/data/raw/giae082.pdf", "file_path": "/home/donbr/open-biosciences/biosciences-research/data/raw/giae082.pdf", "total_pages": 12, "format": "PDF 1.4", "title": "Knowledge graph&#x2013;based thought: a knowledge graph&#x2013;enhanced LLM framework for pan-cancer question answering", "author": "Feng Yichun, Zhou Lu, Ma Chao, Zheng Yikai, He Ruikun, Li Yixue", "subject": "DOI: 10.1093/gigascience/giae082 GigaScience, 14, 0, 00-00-2025. Abstract: In recent years, large language models (LLMs) have shown promise in various domains, notably in biomedical sciences. However, their real-world application is often limited by issues like erroneous outputs and hallucinatory responses.We developed the knowledge graph&#x2013;based thought (KGT) framework, an innovative solution that integrates LLMs with knowledge graphs (KGs) to improve their initial responses by utilizing verifiable information from KGs, thus significantly reducing factual errors in reasoning. The KGT framework demonstrates strong adaptability and performs well across various open-source LLMs. Notably, KGT can facilitate the discovery of new uses for existing drugs through potential drug&#x2013;cancer associations and can assist in predicting resistance by analyzing relevant biomarkers and genetic mechanisms. To evaluate the knowledge graph question answering task within biomedicine, we utilize a pan-cancer knowledge graph to develop a pan-cancer question answering benchmark, named pan-cancer question answering.The KGT framework substantially improves the accuracy and utility of LLMs in the biomedical field. This study serves as a proof of concept, demonstrating its exceptional performance in biomedical question answering.", "keywords": "", "moddate": "2026-01-11T01:15:31+00:00", "trapped": "", "modDate": "D:20260111011531+00'00'", "creationDate": "D:20250106105103Z", "page": 8}}
{"page_content": "10 | GigaScience , 2025, Vol. 14 \nAbbreviations \nAPE: automatic prompt engineer; BFS: breadth-first search; CF: \ncatastr ophic for getting; CoT: c hain of thought; GPT: gener ativ e \npr etr ained tr ansformer; ICL: in-context learning; KG: knowledge \ngr a ph; KGQA: knowledge gr a ph question answering; LLM: large \nlangua ge model; NLP: natur al langua ge pr ocessing; PcQA: pan- \ncancer question answ ering; RLHF: reinfor cement learning with \nhuman feedback; SFT: supervised fine-tuning. \nAvailability of Source Code and \nRequirements \nProject name: bioKGQA-KGT \nr Pr oject homepa ge: https:// github.com/ yichun10/ bioKGQA- \nKGT.git \nr Operating system(s): Linux (Ubuntu) \nr Resource usage in inference ste p: A Lin ux (Ubuntu) system \nwith at least 2 CPU cores and 32 GB of VRAM. The GPU card \nneeds at least 60 GB VRAM (either two 32 GB V100s or one 80 \nGB A100) \nr Pr ogr amming langua ge: Shell Script (Bash) with Python \n3.10.13 \nr Other r equir ements: Python 3.10.13 with GPU/CPU support, \nneo4j 5.13.0 (please see more requirements on GitHub repos- \nitory) \nr Licenses: MIT license \nr Resear ch Resour ce Identifier (#RRID): SCR_025176 \nEthical Statement \nThis study involves the generation of a biomedical question- \nanswer dataset derived from a biomedical knowledge gr a ph de- \nveloped by our team. The knowledge gr a ph has been meticulously \nconstructed using nonpersonalized data obtained from various \ncredible biomedical sources . T he data collection and utilization \npr ocesses strictl y compl y with all r ele v ant legal r egulations and \nethical guidelines, ensuring the highest standards of data secu- \nrity and privacy. The dataset adheres rigorously to data protection \nprinciples and contains no sensitive personal information or iden- \ntifiable individual health data. Furthermore, as the data collection \nand processing activities in this study do not involve human sub- \njects, this r esearc h did not r equir e ethical r e vie w or a ppr ov al. \nAuthor Contributions \nY.F. and L.Z. conceiv ed the pr oject. Y.F. pr oposed a KGQA benc h- \nmark, de v eloped the KGT fr ame work, implemented the code, con- \nducted the experiments, and drafted the manuscript. C.M. con- \ntributed the SmartQuerier Oncology Knowledge Gr a ph. Y.L. and \nL.Z. supervised the study. All authors read and a ppr ov ed the final \nmanuscript. \nFunding \nThis work was supported in part by funds from the Na- \ntional K ey Researc h and De v elopment Pr ogr am of China (Nos. \n2022YFF1202101 and 2023YFC3041600), the Chinese Academy \nof Sciences Research Fund (No. XDB38050200), and the Self- \nsupporting Pr ogr am of Guangzhou National Labor atory (Nos. \nSRPG22001 and SRPG22007). \nDa ta Av ailability \nWe have publicly provided a subset of the SmartQuerier Oncol- \nogy Knowledge Gr a ph necessary for r epr oducing the r esearc h. An \narc hiv al copy of the code and the subgr a ph of the knowledge \ngr a ph used in this r esearc h is av ailable via Softwar e Herita ge [ 33 ], \nand the code and datasets can be accessed via GitHub [ 50 ]. Ad- \nditionall y, the pr ompts used in inter actions with LLMs [ 1 , 2 , 38 , \n42–44 ] during this r esearc h ar e av ailable in the supplemental ma- \nterial. For access to the complete SmartQuerier Oncology Knowl- \nedge Gr a ph data, please contact at service@smartquerier.com. \nCompeting Interests \nChao Ma is emplo y ed b y SmartQuerier Gene Technology (Shang- \nhai) Co., a company active in the biomedical field relevant to the \ncontent of this r esearc h. The SmartQuerier Oncology Knowledge \nGr a ph (SOKG) used in this study is proprietary to SmartQuerier \nGene Technology (Shanghai) Co. The other authors declare that \nthey have no competing interests. \nReferences \n1.\nOpenAI. ChatGPT (Nov 30 version) [large language model]. 30 \nNov 2022. https:// chat.openai.com/ chat .\n2.\nTouvron H, Martin L, Stone K, et al. Llama 2: Open foundation \nand fine-tuned chat models [large language model]. 2023. arXiv \npre print arXi v:230709288. 19 Jul 2023. https:// doi.org/ 10.48550/a \nrXiv.2307.09288 .\n3.\nJi Z, Lee N, Frieske R, et al. Survey of hallucination in natural \nlangua ge gener ation. ACM Comput Surv 2023;55(12):1–38. https: \n// doi.org/ 10.1145/ 3571730 .\n4.\nLiu T, Zheng X, Chang B, et al. To w ar ds faithfulness in open do- \nmain table-to-text generation from an entity-centric view. In: \nYang Qiang, ed. Proceedings of the AAAI Conference on Arti- \nficial Intelligence. AAAI Press. Vol. 35; 2021:13415–423. https: \n// doi.org/ 10.48550/arXiv.2102.08585 .\n5.\nKang D, Hashimoto T. Impr ov ed natur al langua ge gener ation \nvia loss truncation. 2020. arXiv preprint arXiv:200414589. 1 May \n2020. https:// doi.org/ 10.48550/arXiv.2004.14589 .\n6.\nPan S, Luo L, Wang Y, et al. Unifying large language models \nand knowledge gr a phs: a r oadma p. IEEE Tr ans Knowl Data Eng. \n2024;36(7):3580–99. https:// doi.org/ 10.1109/ TKDE.2024.3352100 .\n7.\nHan T, Adams LC, P a paioannou JM, et al. MedAlpaca—an open- \nsource collection of medical conversational AI models and train- \ning data. 2023. arXiv preprint arXiv:230408247. 4 Oct 2023. https: \n// doi.org/ 10.48550/arXiv.2304.08247 .\n8.\nYunxiang L, Zihan L, Kai Z, et al. Chatdoctor: A medical chat \nmodel fine-tuned on Llama model using medical domain knowl- \nedge. Cureus 2023;15(6):e40895. https:// doi.org/ 10.7759/ cureus \n.40895 .\n9.\nSinghal K, Azizi S, Tu T, et al. Lar ge langua ge models encode clin- \nical knowledge. Nature 2023;620:172–80. https:// doi.org/ 10.103 \n8/s41586- 023- 06291- 2 .\n10. Wang H, Liu C, Xi N, et al. Huatuo: tuning Llama model with Chi- \nnese medical knowledge. 2023. arXi v pre print arXi v:230406975. \n14 Apr 2023. https:// doi.org/ 10.48550/arXiv.2304.06975 .\n11. Xiong H, Wang S, Zhu Y, et al. Doctorglm: fine-tuning your \nChinese doctor is not a herculean task. 2023. arXiv preprint \narXiv:230401097. 17 Apr 2023. https:// doi.org/ 10.48550/arXiv.2 \n304.01097 .\n12. Zhang H, Chen J, Jiang F, et al. HuatuoGPT, to w ar ds tam- \ning language model to be a doctor. 2023. arXiv preprint \nDownloaded from https://academic.oup.com/gigascience/article/doi/10.1093/gigascience/giae082/7943459 by guest on 11 January 2026", "metadata": {"producer": "Acrobat Distiller 24.0 (Windows); modified using iTextSharp.LGPLv2.Core 3.7.4.0", "creator": "OUP", "creationdate": "2025-01-06T10:51:03+00:00", "source": "/home/donbr/open-biosciences/biosciences-research/data/raw/giae082.pdf", "file_path": "/home/donbr/open-biosciences/biosciences-research/data/raw/giae082.pdf", "total_pages": 12, "format": "PDF 1.4", "title": "Knowledge graph&#x2013;based thought: a knowledge graph&#x2013;enhanced LLM framework for pan-cancer question answering", "author": "Feng Yichun, Zhou Lu, Ma Chao, Zheng Yikai, He Ruikun, Li Yixue", "subject": "DOI: 10.1093/gigascience/giae082 GigaScience, 14, 0, 00-00-2025. Abstract: In recent years, large language models (LLMs) have shown promise in various domains, notably in biomedical sciences. However, their real-world application is often limited by issues like erroneous outputs and hallucinatory responses.We developed the knowledge graph&#x2013;based thought (KGT) framework, an innovative solution that integrates LLMs with knowledge graphs (KGs) to improve their initial responses by utilizing verifiable information from KGs, thus significantly reducing factual errors in reasoning. The KGT framework demonstrates strong adaptability and performs well across various open-source LLMs. Notably, KGT can facilitate the discovery of new uses for existing drugs through potential drug&#x2013;cancer associations and can assist in predicting resistance by analyzing relevant biomarkers and genetic mechanisms. To evaluate the knowledge graph question answering task within biomedicine, we utilize a pan-cancer knowledge graph to develop a pan-cancer question answering benchmark, named pan-cancer question answering.The KGT framework substantially improves the accuracy and utility of LLMs in the biomedical field. This study serves as a proof of concept, demonstrating its exceptional performance in biomedical question answering.", "keywords": "", "moddate": "2026-01-11T01:15:31+00:00", "trapped": "", "modDate": "D:20260111011531+00'00'", "creationDate": "D:20250106105103Z", "page": 9}}
{"page_content": "Knowledge gr a ph–based thought | 11 \narXiv:230515075. 24 May 2023. https:// doi.org/ 10.48550/arXiv.2 \n305.15075 .\n13. Yang S, Zhao H, Zhu S, et al. Zhongjing: enhancing the Chi- \nnese medical capabilities of large language model through \nexpert feedback and real-world multi-turn dialogue. In: Pro- \nceedings of the AAAI Conference on Artificial Intelligence. \n2023;38(17):19368–76. https:// doi.org/ 10.1609/ aaai.v38i17.2990 \n7 .\n14. Tian Y, Gan R, Song Y, et al. ChiMed-GPT: a Chinese medical large \nlanguage model with full training regime and better alignment \nto human pr efer ences. 2023. arXiv pr e print arXi v:231106025. 15 \nJul 2024. https:// doi.org/ 10.48550/arXiv.2311.06025 .\n15. Ouyang L, Wu J, Jiang X, et al. Tr aining langua ge models \nto follow instructions with human feedback. Adv Neur Inf \nProc Syst 2022;35:27730–44. https:// doi.org/ 10.48550/arXiv.2203. \n02155 .\n16. Luo Y, Yang Z, Meng F, et al. An empirical study of catas- \ntr ophic for getting in lar ge langua ge models during continual \nfine-tuning. 2023. arXi v pre print arXi v:230808747. 17 Aug 2023. \nhttps:// doi.org/ 10.48550/arXiv.2308.08747 .\n17. Li Z, Hoiem D. Learning without forgetting. IEEE Trans Pattern \nAnal Machine Intell 2017;40(12):2935–47. https:// doi.org/ 10.110 \n9/TPAMI.2017.2773081 .\n18. Liu V, Chilton LB. Design guidelines for prompt engineering text- \nto-ima ge gener ativ e models. In: Pr oceedings of the 2022 CHI \nConference on Human Factors in Computing Systems. 2022:1–\n23. https:// doi.org/ 10.1145/ 3491102.3501825 .\n19. W ei J, W ang X, Schuurmans D, et al. Chain-of-thought prompt- \ning elicits reasoning in large language models. Adv Neur Inf Proc \nSyst 2022;35:24824–37. https:// doi.org/ 10.48550/arXiv.2201.1190 \n3 .\n20. Zhou Y, Muresanu AI, Han Z, et al. Large language mod- \nels ar e human-le v el pr ompt engineers. 2022. arXiv pr eprint \narXiv:221101910. 3 Nov 2022. https:// doi.org/ 10.48550/arXiv.221 \n1.01910 .\n21. Sun H, Dhingra B, Zaheer M, et al. Open domain question an- \nswering using early fusion of knowledge bases and text. In: \nRiloff E., Chiang D., Hockenmaier J., and Tsujii J.eds. Proceed- \nings of the 2018 Conference on Empirical Methods in Natural \nLangua ge Pr ocessing. 2018:4231–42. https:// doi.org/ 10.18653/v \n1/D18-1455 .\n22. Sun H, Bedrax-Weiss T, Cohen WW. Pullnet: open domain ques- \ntion answering with iter ativ e r etrie v al on knowledge bases and \ntext. In: Proceedings of the 2019 Conference on Empirical Meth- \nods in Natural Language Processing and the 9th International \nJoint Conference on Natural Language Processing (EMNLP- \nIJCNLP). Association for Computational Linguistics; 2019:2380–\n90. https:// doi.org/ 10.18653/v1/ D19-1242 .\n23. Zhang J, Zhang X, Yu J, et al. Subgr a ph r etrie v al enhanced model \nfor multi-hop knowledge base question answering. 2022. arXiv \npre print arXi v:220213296. 27 Jul 2022. https:// doi.org/ 10.48550/a \nrXiv.2202.13296 .\n24. Chen Y, Wu L, Zaki MJ. Bidir ectional attentiv e memory networks \nfor question answering over knowledge bases. In: Proceedings \nof the 2019 Conference of the North American Chapter of the \nAssociation for Computational Linguistics: Human Language \nTechnologies, Volume 1 (Long and Short Papers); Minneapolis, \nMinnesota: Association for Computational Linguistics; 2913–23. \nhttps:// doi.org/ 10.18653/v1/ N19-1299 .\n25. Saxena A, Tripathi A, Talukdar P. Impr oving m ulti-hop question \nans wering o v er knowledge gr a phs using knowledge base em- \nbeddings . In: J urafsky D., Chai J., Schluter N., and T etreault J .eds. \nProceedings of the 58th Annual Meeting of the Association for \nComputational Linguistics. Association for Computational Lin- \nguistics; 2020:4498–507. https:// doi.org/ 10.18653/v1/ 2020.acl-m \nain.412 .\n26. Lan Y, He G, Jiang J, et al. A survey on complex knowledge base \nquestion ans wering: methods , challenges and solutions . In: Pro- \nceedings of the 30th International Joint Conference on Artificial \nIntelligence (IJC AI-21). IJC AI; 4483–91. https:// doi.org/ 10.24963/i \njcai.2021/611 .\n27. Das R, Zaheer M, Thai D, et al. Case-based reasoning for nat- \nur al langua ge queries ov er knowledge bases. In: Pr oceedings \nof the 2021 Conference on Empirical Methods in Natural Lan- \ngua ge Pr ocessing. Online and Punta Cana: Association for Com- \nputational Linguistics; 2021:9594–611. https:// doi.org/ 10.18653 \n/v1/2021.emnlp-main.755 .\n28. Jiang J, Zhou K, Zhao WX, et al. Unikgqa: unified r etrie v al and \nreasoning for solving multi-hop question ans wering o ver knowl- \nedge gr a ph. 2022. arXiv pr eprint. arXiv:221200959. 2 Dec 2022. \nhttps:// doi.org/ 10.48550/arXiv.2212.00959 .\n29. Jiang J, Zhou K, Dong Z, et al. Structgpt: a gener al fr ame work \nfor lar ge langua ge model to r eason ov er structur ed data. In: Pr o- \nceedings of the 2023 Conference on Empirical Methods in Nat- \nur al Langua ge Pr ocessing. Singa por e. Association for Computa- \ntional Linguistics; 2023:9237–51. https:// doi.org/ 10.18653/v1/ 20 \n23.emnlp-main.574 .\n30. Sun J, Xu C, Tang L, et al. Think-on-gr a ph: deep and responsible \nreasoning of large language model on knowledge graph. In: The \nTwelfth International Conference on Learning Representations. \nVienna, Austria: arXiv; 2024. https:// doi.org/ 10.48550/arXiv.230 \n7.07697 .\n31. Luo L, Li YF, Haf R, et al. Reasoning on gr a phs: faithful and \ninter pr etable lar ge langua ge model r easoning. In: The Twelfth \nInternational Conference on Learning Representations.Vienna, \nAustria. arXiv; 2024. https:// doi.org/ 10.48550/arXiv.2310.01061 .\n32. Kim J, Kwon Y, Jo Y, et al. KG-GPT: A general framework for \nreasoning on knowledge graphs using large language models. \nIn: Bouamor H., Pino J., and Bali K.eds. Findings of the Asso- \nciation for Computational Linguistics: EMNLP. Singa por e: As- \nsociation for Computational Linguistics. 2023; 9410–21. https: \n// doi.org/ 10.48550/arXiv.2310.11220 .\n33. Feng Y, Zhou L, Ma C, et al.. Knowledge gr a ph-based thought: \na knowledge gr a ph enhanced LLMs fr ame work for pan-cancer \nquestion answering (Version 1). 2024 Softwar e Herita ge [Com- \nputer softwar e]. https://arc hive.softwar eheritage.or g/br owse/ \nsna pshot/1906dbbfc88c9d1c8b7acf7deb7495e8002cbafa/dir ect \nory/ ?origin _ url=https:// github.com/yichun10/ bioKGQA-KGT .\n34. W ang J, W u M, Huang X, et al. SynLethDB 2.0: a web-based \nknowledge gr a ph database on synthetic lethality for novel anti- \ncancer drug discovery. Database 2022;2022:baac030. https://doi. \norg/ 10.1093/ database/baac030 .\n35. Zhu C, Yang Z, Xia X, et al. Multimodal reasoning based on \nknowledge gr a ph embedding for specific diseases. Bioinformat- \nics 2022;38(8):2235–45. https:// doi.org/ 10.1093/ bioinformatics/b \ntac085 .\n36. Zhang Y, Dai H, Kozar e v a Z, et al. Variational reasoning \nfor question answering with knowledge gr a ph. In: McIlr aith \nS.A. and Weinberger K.Q. eds. Proceedings of the AAAI confer- \nence on artificial intelligence. Vol. 32. New Orleans, Louisiana, \nUSA: {AAAI} Press; 2018. https:// doi.org/ 10.48550/arXiv.1709.04 \n071 .\n37. Kim J, Park S, Kwon Y, et al. FactKG: fact verification via rea- \nsoning on knowledge gr a phs. In: Rogers A., Boyd-Gr aber J.L., and \nOkazaki N.eds. Proceedings of the 61st Annual Meeting of the \nAssociation for Computational Linguistics (Volume 1: Long Pa- \nDownloaded from https://academic.oup.com/gigascience/article/doi/10.1093/gigascience/giae082/7943459 by guest on 11 January 2026", "metadata": {"producer": "Acrobat Distiller 24.0 (Windows); modified using iTextSharp.LGPLv2.Core 3.7.4.0", "creator": "OUP", "creationdate": "2025-01-06T10:51:03+00:00", "source": "/home/donbr/open-biosciences/biosciences-research/data/raw/giae082.pdf", "file_path": "/home/donbr/open-biosciences/biosciences-research/data/raw/giae082.pdf", "total_pages": 12, "format": "PDF 1.4", "title": "Knowledge graph&#x2013;based thought: a knowledge graph&#x2013;enhanced LLM framework for pan-cancer question answering", "author": "Feng Yichun, Zhou Lu, Ma Chao, Zheng Yikai, He Ruikun, Li Yixue", "subject": "DOI: 10.1093/gigascience/giae082 GigaScience, 14, 0, 00-00-2025. Abstract: In recent years, large language models (LLMs) have shown promise in various domains, notably in biomedical sciences. However, their real-world application is often limited by issues like erroneous outputs and hallucinatory responses.We developed the knowledge graph&#x2013;based thought (KGT) framework, an innovative solution that integrates LLMs with knowledge graphs (KGs) to improve their initial responses by utilizing verifiable information from KGs, thus significantly reducing factual errors in reasoning. The KGT framework demonstrates strong adaptability and performs well across various open-source LLMs. Notably, KGT can facilitate the discovery of new uses for existing drugs through potential drug&#x2013;cancer associations and can assist in predicting resistance by analyzing relevant biomarkers and genetic mechanisms. To evaluate the knowledge graph question answering task within biomedicine, we utilize a pan-cancer knowledge graph to develop a pan-cancer question answering benchmark, named pan-cancer question answering.The KGT framework substantially improves the accuracy and utility of LLMs in the biomedical field. This study serves as a proof of concept, demonstrating its exceptional performance in biomedical question answering.", "keywords": "", "moddate": "2026-01-11T01:15:31+00:00", "trapped": "", "modDate": "D:20260111011531+00'00'", "creationDate": "D:20250106105103Z", "page": 10}}
{"page_content": "12 | GigaScience , 2025, Vol. 14 \npers), Toronto, Canada: Association for Computational Linguis- \ntics; 2023:16190–206.https:// doi.org/ 10.48550/arXiv.2305.06590 .\n38. Achiam J, Adler S, Agarwal S, et al. GPT-4 Technical Report \n(Mar 14 version) [large language model]. 2023. arXiv preprint \narXiv:230308774. 14 Mar 2023. https:// doi.org/ 10.48550/arXiv.2 \n303.08774 .\n39. Zhang T, Kishore V, Wu F, et al. Bertscor e: e v aluating text gen- \neration with bert. In: 8th International Conference on Learn- \ning Re presentations. Ad dis Ababa, Ethiopia. arXi v. 2020; https: \n// doi.org/ 10.48550/arXiv.1904.09675 .\n40. Lin CY. Rouge: a pac ka ge for automatic e v aluation of sum- \nmaries. In: Text Summarization Br anc hes Out. Barcelona, Spain: \nAssociation for Computational Linguistics; 2004:74–81. https: \n// aclanthology.org/W04-1013/ .\n41. Dong Q, Li L, Dai D, et al. A survey for in-context learning. 2022. \narXi v pre print arXi v:230100234. 31 Dec 2022. https:// doi.org/ 10 \n.48550/arXiv.2301.00234 .\n42. Roziere B, Gehring J, Gloeckle F, et al. Code Llama: open founda- \ntion models for code [large language model]. 2023. arXiv preprint \narXiv:230812950. 24 A ug 2023. https:// doi.org/ 10.48550/arXiv.2 \n308.12950 .\n43. Luo L, Ning J, Zhao Y, et al. Taiyi: a bilingual fine-tuned large \nlanguage model for diverse biomedical tasks [large language \nmodel]. J. Am. Medical Informatics Assoc. 2024;31(9):1865–74. \nhttps:// doi.org/ 10.1093/ jamia/ ocae037 . \n44. Tunstall L, Beeching E, Lambert N, et al.. Zephyr: direct distil- \nlation of LM alignment [lar ge langua ge model]. 25 Oct 2023. \nhttps:// doi.org/ 10.48550/arXiv.2310.16944 .\n45. He S, Liu X, Ye X, et al. Analysis of drug repositioning \nand prediction techniques: a concise review. Curr Top Med \nChem. 2022;22(23):1897–906. https:// doi.org/ 10.2174/ 15680266 \n22666220317164016 .\n46. Gan Lq, Wang H, Ni Sl, et al. A pr ospectiv e study of topical car- \nteolol ther a py in Chinese infants with superficial infantile he- \nmangioma. Pediatr Dermatol 2018;35(1):121–25. https://doi.org/ \n10.1111/pde.13361 .\n47. Gottesman MM. Mechanisms of cancer drug resistance. Annu \nRev Med 2002;53(1):615–27. https:// doi.org/ 10.1146/ annurev.me \nd.53.082901.103929 .\n48. Alshareef A, Zhang HF, Huang YH, et al. The use of cellular \nthermal shift assay (CETSA) to study crizotinib resistance in \nALK-expressing human cancers. Sci Rep 2016;6(1):33710. https: \n// doi.org/ 10.1038/ srep33710 .\n49. Simionato F, Frizziero M, Carbone C, et al. Curr ent str ategies to \nov ercome r esistance to ALK-inhibitor a gents. Curr Drug Metab \n2015;16(7):585–96. https:// doi.org/ 10.2174/ 138920021666615081 \n2142059 .\n50. Feng Y, Zhou L, Ma C, et al.. bioKGQA-KGT: knowledge gr a ph- \nbased thought. 10 Feb 2024. https:// github.com/yichun10/ bioK \nGQA-KGT .\nRecei v ed: May 27, 2024. Re vised: A ugust 14, 2024. Accepted: October 2, 2024 \n© The Author(s) 2025. Published by Oxford Uni v ersity Pr ess GigaScience. This is an Open Access article distributed under the terms of the Cr eati v e Commons Attribution License \n( https://creativecommons.org/licenses/by/4.0/ ), which permits unrestricted reuse, distribution, and reproduction in any medium, provided the original work is properly cited.\nDownloaded from https://academic.oup.com/gigascience/article/doi/10.1093/gigascience/giae082/7943459 by guest on 11 January 2026", "metadata": {"producer": "Acrobat Distiller 24.0 (Windows); modified using iTextSharp.LGPLv2.Core 3.7.4.0", "creator": "OUP", "creationdate": "2025-01-06T10:51:03+00:00", "source": "/home/donbr/open-biosciences/biosciences-research/data/raw/giae082.pdf", "file_path": "/home/donbr/open-biosciences/biosciences-research/data/raw/giae082.pdf", "total_pages": 12, "format": "PDF 1.4", "title": "Knowledge graph&#x2013;based thought: a knowledge graph&#x2013;enhanced LLM framework for pan-cancer question answering", "author": "Feng Yichun, Zhou Lu, Ma Chao, Zheng Yikai, He Ruikun, Li Yixue", "subject": "DOI: 10.1093/gigascience/giae082 GigaScience, 14, 0, 00-00-2025. Abstract: In recent years, large language models (LLMs) have shown promise in various domains, notably in biomedical sciences. However, their real-world application is often limited by issues like erroneous outputs and hallucinatory responses.We developed the knowledge graph&#x2013;based thought (KGT) framework, an innovative solution that integrates LLMs with knowledge graphs (KGs) to improve their initial responses by utilizing verifiable information from KGs, thus significantly reducing factual errors in reasoning. The KGT framework demonstrates strong adaptability and performs well across various open-source LLMs. Notably, KGT can facilitate the discovery of new uses for existing drugs through potential drug&#x2013;cancer associations and can assist in predicting resistance by analyzing relevant biomarkers and genetic mechanisms. To evaluate the knowledge graph question answering task within biomedicine, we utilize a pan-cancer knowledge graph to develop a pan-cancer question answering benchmark, named pan-cancer question answering.The KGT framework substantially improves the accuracy and utility of LLMs in the biomedical field. This study serves as a proof of concept, demonstrating its exceptional performance in biomedical question answering.", "keywords": "", "moddate": "2026-01-11T01:15:31+00:00", "trapped": "", "modDate": "D:20260111011531+00'00'", "creationDate": "D:20250106105103Z", "page": 11}}
{"page_content": "An LLM-based Knowledge Synthesis and Scientific Reasoning Framework\nfor Biomedical Discovery\nOskar Wysocki1,2, Magdalena Wysocka2, Danilo S. Carvalho2, Alex Bogatu2,\nDanilo Gusicuma1, Maxime Delmas1, Harriet Unsworth2, André Freitas1,2,3\n1Idiap Research Institute, Switzerland\n2National Biomarker Centre, CRUK-MI, Univ. of Manchester, United Kingdom\n3Department of Computer Science, Univ. of Manchester, United Kingdom\nCorrespondence: firstname.lastname@idiap.ch1firstname.lastname@manchester.ac.uk2\nAbstract\nWe present BioLunar, developed using the Lu-\nnar framework, as a tool for supporting bio-\nlogical analyses, with a particular emphasis\non molecular-level evidence enrichment for\nbiomarker discovery in oncology. The platform\nintegrates Large Language Models (LLMs) to\nfacilitate complex scientific reasoning across\ndistributed evidence spaces, enhancing the ca-\npability for harmonizing and reasoning over\nheterogeneous data sources. Demonstrating\nits utility in cancer research, BioLunar lever-\nages modular design, reusable data access and\ndata analysis components, and a low-code user\ninterface, enabling researchers of all program-\nming levels to construct LLM-enabled scien-\ntific workflows. By facilitating automatic sci-\nentific discovery and inference from heteroge-\nneous evidence, BioLunar exemplifies the po-\ntential of the integration between LLMs, spe-\ncialised databases and biomedical tools to sup-\nport expert-level knowledge synthesis and dis-\ncovery.\n1\nIntroduction\nContemporary biomedical discovery represents a\nprototypical instance of complex scientific reason-\ning, which requires the coordination of controlled\nin-vivo/in-silico interventions, complex multi-step\ndata analysis pipelines and the interpretation of the\nresults under the light of previous evidence (avail-\nable in different curated databases and in the litera-\nture) (Paananen and Fortino, 2019; Nicholson and\nGreene, 2020). This intricacy emerges out of the\ninherent complexity of biological mechanisms un-\nderlying organism responses, which are defined by\na network of multi-scale inter-dependencies (Bog-\ndan et al., 2021). While more granular data is being\ngenerated by the evolution of instruments, assays\nand methods, and the parallel abundance of experi-\nmental interventions (Dryden-Palmer et al., 2020),\nthere a practical barrier for integrating and coher-\ning this evidence space into a specific context of\nanalysis.\nWithin biomedical discovery, the language in-\nterpretation capabilities of Large Language Mod-\nels (LLMs) can provide an integrative framework\nfor harmonising and reasoning over distributed ev-\nidence spaces and tools, systematising and low-\nering the barriers to access and reason over mul-\ntiple structured databases, textual bases such as\nPubMed, enriching the background knowledge\nthrough specialised ontologies and serving as in-\nterfaces to external analytical tools (e.g. mechanis-\ntic/perturbation models, gene enrichment models,\netc). In this context, LLMs can serve as a linguis-\ntic analytical layer which can reduce the syntactic\nimpedance across diverse functional components:\nonce an adapter to an external component is built it\ncan be integrated and reused in different contexts,\ncreating a monotonic increase of functional compo-\nnents. Complementarily, from a Biomedical-NLP\nperspective, in order to address real-world prob-\nlems, LLMs need to be complemented with mech-\nanisms which can deliver contextual control (e.g.\nvia Retrieval Augmented Generation: RAG: ac-\ncess the relevant background knowledge and facts)\nand perform the analytical tasks which are integral\nto contemporary biomedical inference (’toolform-\ning’).\nEmerging LLM-focused coordination frame-\nworks such as LangChain1, Flowise2 and Lunar3\nprovide the capabilities to deliver a composition\nof functional components, some of them under a\nlow-code/no-code use environment, using the ab-\nstraction of workflows. While there are general-\npurpose coordination frameworks, there is a lack of\nspecialised components for addressing biomedical\nanalyses.\nIn this paper we demonstrate BioLunar, a suite of\ncomponents developed over the Lunar environment\n1https://python.langchain.com\n2https://github.com/FlowiseAI/Flowise\n3https://lunarbase.ai\narXiv:2406.18626v1  [q-bio.QM]  26 Jun 2024", "metadata": {"producer": "pdfTeX-1.40.25", "creator": "LaTeX with hyperref", "creationdate": "2024-06-28T00:03:44+00:00", "source": "/home/donbr/open-biosciences/biosciences-research/data/raw/2406.18626v1.pdf", "file_path": "/home/donbr/open-biosciences/biosciences-research/data/raw/2406.18626v1.pdf", "total_pages": 11, "format": "PDF 1.5", "title": "", "author": "", "subject": "", "keywords": "", "moddate": "2024-06-28T00:03:44+00:00", "trapped": "", "modDate": "D:20240628000344Z", "creationDate": "D:20240628000344Z", "page": 0}}
{"page_content": "to support biological analyses. We demonstrate the\nkey functionalities of the platform contextualised\nwithin a real-use case in the context of molecular-\nlevel evidence enrichment for biomarker discovery\nin oncology.\n2\nBioLunar\nBioLunar enables the creation of LLM-based\nbiomedical scientific workflows using software\ncomponents with standardised APIs. A workflow\nis composed of components and subworkflows con-\nnected through input-output relationships, and are\ncapable of handling multiple inputs. In the user in-\nterface, components are clustered according to their\nfunction (see Fig.1). Creating a workflow does not\nrequire programming knowledge since components\nare predefined and merely require data inputs or\nparameter settings. However, for users who wish to\nwrite custom code, ’Python Coder’ and ’R Coder’\ncomponents are provided, enabling the definition\nof custom methods. These custom components can\nbe saved and subsequently accessed in the ’Custom’\ngroup tab.\nIn the paper we describe an exemplar biomed-\nical workflow designed to integrate evidence and\ninfer conclusions from bioinformatics pipeline re-\nsults. Specifically, the biomedical workflow queries\nexpert knowledge bases (KBs) that continuously\ncompile clinical, experimental, and population ge-\nnetic study outcomes, aligning them with assertions\nrelevant to the significance of the observed gene or\nvariant. It then employs Natural Language Infer-\nence (NLI) (via LLM) to integrate and harmonise\nthe evidence space and interpreting the results, cul-\nminating in a comprehensive summary for the en-\ntire gene set input. This interpretation takes into\naccount the bioanalytical context supplied by the\nuser.\n2.1\nExemplar Workflow\nNext-generation sequencing (NGS) assays play a\npivotal role in the precise characterisation of tu-\nmours and patients in experimental cancer treat-\nments. NGS findings are essential to guide the\ndesign of novel biomarkers and cancer treatments.\nNevertheless, the clinical elucidation of NGS find-\nings subsequent to initial bioinformatics analysis\noften requires time-consuming manual analysis pro-\ncedures which are vulnerable to errors. The inter-\npretation of molecular signatures that are typically\nyielded by genome-scale experiments are often\nsupported by pathway-centric approaches through\nwhich mechanistic insights can be gained by point-\ning at a set of biological processes. Moreover,\ngene and variant enrichment benefits from heteroge-\nneous curated data sources which pose challenges\nto seamless integration. Furthermore, there are\ndifferent levels of supporting evidence and there-\nfore prioritising conclusions is crucial. Automating\nevidence interpretation, knowledge synthesis and\nleveraging evidence-rich gene set reports are fun-\ndamental for addressing the challenges in precision\noncology and the discovery of new biomarkers.\n2.2\nUser interface\nThe user interface facilitates an agile workflow\nconstruction by enabling users to select and ar-\nrange components via drag-and-drop from func-\ntionally grouped categories, such as, i.a.: ’Prompt\nQuery’ featuring NLI components, ’Knowledge\nBases’ components, ’Extractors’ for retrieving files\nfrom zip archives or extracting text and tables from\nPDF files, and ’Coders’, which allow for the cre-\nation of custom components using Python or R\nscripts.\nComponents allow for individual execution, edi-\ntion, or configuration adjustment via a visual inter-\nface. Workflows can be executed, saved, or shared.\nEach component has designated input and output\ncapabilities, enabling seamless integration where\nthe output from one can directly feed into another.\nUsers have the flexibility to manually input values\nif no direct connection is established. Additionally,\na component’s output can feed into multiple compo-\nnents. The system’s architecture supports effortless\nexpansion, adding branches and components with-\nout affecting the existing workflow, thus facilitating\nscalable customization to meet changing require-\nments. The user interface with an example of a\nworkflow is presented in Fig.1 and in demo video\nhttps://youtu.be/Hc6pAA_5Xu8.\n2.3\nKnowledge bases\nThe current framework integrates a diverse set\nof knowledge bases which are relevant for pre-\ncision oncology. To identify gene mutations as\nbiomarkers for cancer diagnosis, prognosis, and\ndrug response, we integrated CIViC4 and On-\ncoKB5. CIViC provides molecular profiles (MPs)\nof genes, each linked to clinical evidence, with\n4https://civicdb.org\n5https://www.oncokb.org", "metadata": {"producer": "pdfTeX-1.40.25", "creator": "LaTeX with hyperref", "creationdate": "2024-06-28T00:03:44+00:00", "source": "/home/donbr/open-biosciences/biosciences-research/data/raw/2406.18626v1.pdf", "file_path": "/home/donbr/open-biosciences/biosciences-research/data/raw/2406.18626v1.pdf", "total_pages": 11, "format": "PDF 1.5", "title": "", "author": "", "subject": "", "keywords": "", "moddate": "2024-06-28T00:03:44+00:00", "trapped": "", "modDate": "D:20240628000344Z", "creationDate": "D:20240628000344Z", "page": 1}}
{"page_content": "Figure 1: BioLunar interface. An exemplary workflow of Gene Enrichment with an input gene set, knowledge base\nquery and LLM interpretation components.\na molecular score indicating evidence quality, as-\nsessed by human annotators. The Gene Ontology6\n(GO) offered gene function insights, and the Hu-\nman Protein Atlas7 supplied a list of potential drug\ntargets and transcription factors. We employed\nCOSMIC8 for somatic mutation impacts in can-\ncer, the largest resource in this field. Our analysis\nalso included KEGG9, Reactome10, and WikiPath-\nways11 for pathway information, enriching our in-\nvestigation with scientific literature via PubMed’s\nAPI 12.\nIn the following subsections, we showcase ex-\namples of components, subworkflows, and work-\nflows constructed using the BioLunar framework,\nmotivated by the biomarker discovery/precision on-\ncology themes.\n2.4\nConstruction and reuse of specialised\nprompts\nBioLunar employs standard LLM interfaces, al-\nlowing the use of different models according to\nusers’ preferences. The prompt components allows\nfor the composition of specialised prompt chains\nwhich can be later reused, defining a pragmatic\npathway for specialised Natural Language Infer-\nence (NLI) via prompt decomposition/composition.\nThis approach allows for the creation of reasoning\n6https://geneontology.org\n7https://www.proteinatlas.org\n8https://cancer.sanger.ac.uk/cosmic\n9https://www.kegg.jp/kegg/\n10https://reactome.org\n11https://www.wikipathways.org\n12https://pubmed.ncbi.nlm.nih.gov\nchains that combines user’s instructions with the\nresults of database queries and analyses from spe-\ncialised tools within the context of the study. An\ninstantiated example of the Azure Open AI prompt\nis described in Fig.1.\n2.5\nSubworkflow component\nThe subworkflow component enables the reuse\nof an existing workflow within another workflow,\nfunctioning as a component with specified inputs\nand outputs. This feature simplifies the composi-\ntion of more complex workflows and avoids the\nrepetition of defining identical steps for the same\ntask. Subworkflows can be selected like other com-\nponents from the left panel in the interface, offering\naccess to all available workflows for easy integra-\ntion. Examples of subworkflows are presented in\nFig.2,3.\n2.6\nGene Enrichment subworkflow\nOne example of a specialised subworkflow is the\nGene Enrichment subworkflow (Fig.1,2A) begins\nwith uploading the targeted gene sets.\nThen a\ncomponent accesses a specific KB — such as\nGene Ontology, KEGG, Reactome, or WikiPath-\nways—defined by the user, using gprofiler API13.\nThis component identifies gene groups with a sta-\ntistically significant overlap with the input gene set,\naccording to a Fisher’s test, and calculates p-values,\nrecall, and precision. The user then specifies a vari-\nable to rank these groups and selects the top N for\nfurther analysis. The output includes both a inter-\n13https://biit.cs.ut.ee/gprofiler/page/apis", "metadata": {"producer": "pdfTeX-1.40.25", "creator": "LaTeX with hyperref", "creationdate": "2024-06-28T00:03:44+00:00", "source": "/home/donbr/open-biosciences/biosciences-research/data/raw/2406.18626v1.pdf", "file_path": "/home/donbr/open-biosciences/biosciences-research/data/raw/2406.18626v1.pdf", "total_pages": 11, "format": "PDF 1.5", "title": "", "author": "", "subject": "", "keywords": "", "moddate": "2024-06-28T00:03:44+00:00", "trapped": "", "modDate": "D:20240628000344Z", "creationDate": "D:20240628000344Z", "page": 2}}
{"page_content": "Gene Enrichment\nSort and\nselect top N\nUpload set\nof genes\nInterpret\nresults\n(LLM)\nSave results\nHuman Protein Atlas\nCompute\noverlap and\nstatistics\nUpload set\nof genes\nInterpret\nresults\n(LLM)\nSave results\nQuery\nHuman\nProtein Atlas\nRun Gene\nEnrichment\nProvide\n'Context'\nProvide\n'Context'\nA)\nB)\nFigure 2: A) Gene Enrichment workflow - uses the gprofiler API to access i.a. Gene Ontology, KEGG, WikiPathways,\nReactome; B) Human Protein Atlas workflow. Compares and interprets the input and reference gene sets.\npretation performed by an NLI component (through\nLLM) and a table featuring the names, descriptions,\nand statistics of the top N selected groups.\n2.7\nHuman Protein Atlas subworkflow\nIn the Human Protein Atlas subworkflow, given\na gene set, an associated external KB is queried\nby selecting ‘Transcription factors’ from the HPA\ndatabase using a dedicated query-database connec-\ntor. A reusable component, ’Analyze overlap’, then\nidentifies genes that overlap and calculates relevant\nstatistics. Similarly to the Gene Enrichment sub-\nworkflow, the results are interpreted by an prompt-\nbased NLI component and presented alongside a\ntable summarising the findings (Fig.2B,A.7).\n2.8\nCIVIC subworkflow\nThis subworkflow exemplifies a more complex com-\nposition of components (Fig.3). This subworkflow\ninitiates by querying the CIVIC database for input\ngenes, yielding, among other things, gene descrip-\ntions in clinical contexts, and their variants and\nmolecular profiles (MPs), which are essential for\nthe final interpretation. Additionally, users spec-\nify the analysis context, including aspects such as\ncancer types or subtypes, treatments, populations,\netc. Initially, gene descriptions are analysed by a\nprompt-based NLI component within this defined\ncontext. Subsequently, MPs scored below a prede-\nfined threshold (set at a MP score of 10) are tagged\nas less known, reflecting lower scientific evidence\nand ranking by CIVIC annotators. The evidence\nsupporting these lesser-known MPs is then inter-\npreted by a prompt-based NLI component, con-\nsidering the broader analysis context. Conversely,\nevidence from well-known MPs, scoring above 10,\nundergoes a similar interpretation process.\nFor genes without identified MPs in CIVIC, a\nsequence of components perform further evidence\nretrieval from PubMed. An NLI module gener-\nates context-based keywords for PubMed queries,\nwhich are combined with the names of genes lack-\ning MPs. A ’PubMed search’ component then re-\ntrieves N publications, including metadata, cita-\ntion counts and MeSH terms (used later for context\nalignment validation). The abstracts of these pub-\nlications are interpreted by an NLI module in the\ncontext of the analysis.\nAll clinical evidence interpretations are then suc-\ncinctly summarised by via a prompt component,\ntaking into account the context of the analysis.\nThese interpretations, along with tabular results,\nconstitute the output.\n2.9\nBioworkflow - comprehensive analysis for\na set of genes.\nThe exemplar bioworkflow composes multiple sub-\nworkflows (Fig.4), each dedicated to a specific\nmulti-step and specialised task, which are typically\ndefined by the composition of heterogeneous com-\nponents, most commonly connectors and query in-\nstance components to specialised databases (e.g.", "metadata": {"producer": "pdfTeX-1.40.25", "creator": "LaTeX with hyperref", "creationdate": "2024-06-28T00:03:44+00:00", "source": "/home/donbr/open-biosciences/biosciences-research/data/raw/2406.18626v1.pdf", "file_path": "/home/donbr/open-biosciences/biosciences-research/data/raw/2406.18626v1.pdf", "total_pages": 11, "format": "PDF 1.5", "title": "", "author": "", "subject": "", "keywords": "", "moddate": "2024-06-28T00:03:44+00:00", "trapped": "", "modDate": "D:20240628000344Z", "creationDate": "D:20240628000344Z", "page": 3}}
{"page_content": "Workflow - CivicDB analysis\nDiscovery / new knowledge\nGet genes \ndetails and\nmolecular\nprofiles\nUpload set\nof genes\nSave results\nProvide\n'Context'\nInterpret genes\nin the context\n(LLM)\nIdentify well\nknown\nmolecular\nprofiles\nIdentify\nmolecular\nprofiles without\nevidence\nQuery\nPubMed\nand select\ntop N\nPrepare\nPubMed\nsubquery\n(LLM)\nInterpret\npublications in\nthe context\n(LLM)\nInterpret all\nevidence in the\ncontext (LLM)\nIdentify less\nknown\nmolecular\nprofiles\nInterpret all\nevidence in the\ncontext (LLM)\nSummarize all\ninterpretations in\nthe context\n(LLM)\nQuery CIVIC\ndatabase\nFigure 3: CIVIC evidence analysis workflow. prompt-based NLI components are fed by both the results and context\nof the analysis in order to produce relevant evidence-based conclusions.\nCIVIC, HPA, PubMed, OncoKB), external spe-\ncialised analytical tools (toolformers for gene en-\nrichment analysis) and chains of specialised in-\nterpretation prompts (e.g. selection, filtering, ex-\ntraction, summarisation). This setup forms a com-\nprehensive workflow which exemplifies the close\ndialogue between LLMs and genomic analysis, en-\ncompassing gene enrichment, comparison with ref-\nerence gene sets, and access to evidence within\nan experimental medicine setting. Additionally,\nit queries PubMed publications within the CIVIC\ncomponent to seek evidence for molecular profiles\nnot yet described. Its componentised architecture\nfacilitates the extensibility of the workflow with\nnew sources, prompts and external tools. Conclu-\nsions drawn from each subworkflow are interpreted\nwithin the analysis context, being integrated in a\ncomprehensive summary. All findings are com-\npiled in a report, exported as a PDF file.\n2.10\nSoftware description\nBioLunar uses the LunarVerse backend for its op-\nerations. LunarVerse is downloaded and installed\nby the setup script included with the demonstration\ncode. Some of its components need user specific\nconfiguration to work, such as private API keys,\nwhich are defined in a configuration file indicated in\nthe setup instructions. LunarVerse is distributed un-\nder a open software license. The workflow can also\nbe operated via a graphical interface (LunarFlow)\nRunning a workflow can be done in two ways:\ni) directly, by calling the LunarVerse engine on a\nspecified workflow descriptor file; ii) through the\nWeb interface, by pressing the “Run” button.\nThe first way is the default one in the demonstra-\ntion code. It returns a copy of the workflow descrip-\ntor, with all component output fields filled, which\nis then used to extract and filter the desired outputs,\nbased on the component labels. It is also the best\nway to automate multiple workflow runs and to\nintegrate their outputs into other systems.The sup-\nporting code is available at https://github.com/\nneuro-symbolic-ai/lunar-bioverse-demo.\n2.11\nReport\nThe Bioworkflow, as outlined in point 2.9, gener-\nates a report in PDF (Fig.5) format that begins by\noutlining the context of the study, analysis details,\ndates, and software versions at the top. The report\nis enhanced with hyperlinks for easy navigation to\nspecific sections.\nA \"General Statistics\" table provides a compre-\nhensive overview of key metrics aggregated from\nall components, aiming to consolidate information\nfor each gene throughout the analysis, with hyper-\nlinks directing to the report sections where this\ninformation originates.\nSubsequent sections categorise genes into var-", "metadata": {"producer": "pdfTeX-1.40.25", "creator": "LaTeX with hyperref", "creationdate": "2024-06-28T00:03:44+00:00", "source": "/home/donbr/open-biosciences/biosciences-research/data/raw/2406.18626v1.pdf", "file_path": "/home/donbr/open-biosciences/biosciences-research/data/raw/2406.18626v1.pdf", "total_pages": 11, "format": "PDF 1.5", "title": "", "author": "", "subject": "", "keywords": "", "moddate": "2024-06-28T00:03:44+00:00", "trapped": "", "modDate": "D:20240628000344Z", "creationDate": "D:20240628000344Z", "page": 4}}
{"page_content": "Provide\n'Context'\nCIVIC subworkflow\nConclusion (LLM)\nResults (table)\nReport\nOverall conclusion \n(LLM)\nHPA subworkflow\nConclusion (LLM)\nResults (table)\nCOSMIC subworkflow\nConclusion (LLM)\nResults (table)\nOncoKB subworkflow\nConclusion (LLM)\nResults (table)\nConclusion (LLM)\nGene Enrichment       \nsubworkflow\nConclusion (LLM)\nResults (table)\nConclusion (LLM)\nResults (table)\nConclusion (LLM)\nResults (table)\nResults (table)\nUpload set\nof genes\nGE summary\n(LLM)\nNik-Zainal-93 subworkflow\nConclusion (LLM)\nResults (table)\nFigure 4: Diagram of the Bioworkflow.\nious tables based on biological aspects and the\nKBs consulted. These include Molecular Function\nfor genes sharing ontologies, drug target checks\nbased on the Human Protein Atlas, assessments\nof cancer-related genes, Pathway Analysis and\nMapping via WikiPathways, and classification of\ngene alterations by clinical relevance. By correlat-\ning genes with known functional information, the\nworkflow identifies statistically significant enriched\nterms and summarizes these findings using LLM,\nwhich also furnishes evidence.\nLLM interprets each table, offering textual con-\nclusions relevant to the analysis context. A final\nsummary, crafted using LLM, synthesizes all re-\nsults within the given context. Importantly, all\nLLM interpretations are grounded in concrete ev-\nidence, with sources cited alongside the narrative.\nThis approach underscores the rigor of the analysis\nby highlighting distinct sources that substantiate\nthe relevance of each gene and variant.\n3\nCase study\nTo demonstrate the capabilities of the Biowork-\nflow, we analyzed outputs in two different scenar-\nios, each producing a distinct set of genes from\nseparate bioinformatics analyses. We entered these\ngene sets along with their analysis contexts into\nthe Bioworkflow and executed it. Subsequently,\nwe qualitatively assessed the output reports (see\nFig.A.8,A.9), considering both the statistical data\nand the interpretations provided by the prompt-\nbased NLI modules.\nIn Scenario 1, the user aims to explore the unique\nmolecular characteristics of HER2-low breast can-\ncer to determine if it constitutes a distinct category\nwithin breast cancer types, where the input genes\nare ERBB2, ESR1, PIK3CA, CBFB, SF3B. The\nreport shows genomic alterations and genomic sig-\nnatures that were identified, including ERBB2 am-\nplification, mutations in PIK3CA and ESR1, which\nare important biomarkers in the selection of breast\ncancer treatment. For the remaining two genes,\nevidence was found confirming that these are new,\nsignificantly mutated genes for which there is pre-\nclinical evidence of actionability in clinical prac-\ntice.\nIn Scenario 2, the user aims to discover new\ngenes that could lead to more accurate breast can-\ncer diagnoses, enhancing treatment strategies and\naddressing the disease’s complexity. His numeri-\ncal analysis resulted in a set of genes (DIXDC1,\nDUSP6, PDK4, CXCL12, IRF7, ITGA7, NEK2,\nNR3C1) that require investigation. The report in-\nforms that none of the genes is an oncogene (con-\nfirmation according to OncoKB), two of the genes", "metadata": {"producer": "pdfTeX-1.40.25", "creator": "LaTeX with hyperref", "creationdate": "2024-06-28T00:03:44+00:00", "source": "/home/donbr/open-biosciences/biosciences-research/data/raw/2406.18626v1.pdf", "file_path": "/home/donbr/open-biosciences/biosciences-research/data/raw/2406.18626v1.pdf", "total_pages": 11, "format": "PDF 1.5", "title": "", "author": "", "subject": "", "keywords": "", "moddate": "2024-06-28T00:03:44+00:00", "trapped": "", "modDate": "D:20240628000344Z", "creationDate": "D:20240628000344Z", "page": 5}}
{"page_content": "Cancer type:                Breast Cancer\n  Sample size: 27\n  Cancer type: breast cancer\n  Biopsy site: primary site\n  Center:\n  Informed consent:\n  Sample sent:\nExport report\nAnalysis run date: 2024-02-24\nPipeline version: v2.0. details\n  Context:   Breast cancer (BC) presents a significant global health challenge,\nwith its incidence steadily rising and mortality rates remaining high.\nIts heterogeneous nature complicates treatment strategies,\ncontributing to issues like recurrence and drug resistance. more\nNavigation Menu\nGene set dashboard\nContext\nToolbox\nIndividual gene description\nERBB2\nGene\nGene Info\nOncogene,\nmore\nVariant\nERBB2 L755S\nFunctional relevance evidence\nEvidence (curated):\n>Oncogenic, OncoKB\n>Sensitivity/Response, Resistance, CIViC\nHallmark\nYes, more\nERBB2 D769Y\nEvidence (curated):\n>Oncogenic, OncoKB\n>Sensitivity/Response, CIViC\nERBB2 R143G\nOthers/Inconclusive:\n>Inconclusive/weaker evidence, OncoKB\nHPA:\ndrug targets\nFDA approved\ndrug target,\nmore\nEvidence supporting\nthe variant functional relevance\nCurrent biomarker clinical relevance\nCancer biomarkers\nreported at present\nInvestigational, 1 assertions\nCancer repurposing, 4 assertions\nHypothetical, 2 assertions\nInvestigational, 1 assertions\nHypothetical, 1 assertions\nGeneral Statistics\nGeneral Statistics\nERBB2\nThis gene is considered an oncogene.\nERBB2, a receptor tyrosine kinase, is altered by mutation, amplification and/or\noverexpression in various cancer types, most frequently in breast, esophagogastric and\nendometrial cancers.\nmore gene info\nGene mutations based on previous pan-cancer cohorts\nERBB2 L755S\nhttp://oncokb.org/#/gene/ERBB2/alteration/L755S\nEffect: Oncogenic\nERBB2 L755S\nPlease check the original assertions provided by each\nknowledgebase listed below\nhttp://oncokb.org/#/gene/ERBB2/alteration/Oncogenic\nMutations\nBiomarker: ERBB2 Oncogenic Mutations\nEffect: drug Responsive\nEvidence level: LEVEL_3A\nDrug: Neratinib\nDisease: Breast Cancer\nGene with the hallmark description\nThis gene has a hallmark. more\nThis gene has a stimulating effect via individual cancer\nhallmarks.\nInteractive pop-up windows\nAccess to external resources\nwith the original evidence\nGene-detailed view\n1\n1\n3\n3\n2\n2\n4\n4\nFinal conclusion generated by LLM\nERBB2, commonly referred to as HER2, is amplified and/or overexpressed in 20-30% of invasive breast carcinomas. HER2-positive breast cancer is treated in a separate manner from other subtypes of breast\ncancer and commonly presents as more aggressive disease. Metastatic HER2-positive breast cancer is now commonly treated with HER2-targeted therapy. Apart from being amplified/overexpressed, ERBB2\nactivating mutations have been shown to have clinical importance in HER2-negative breast cancer. These mutations have shown sensitivity to the tyrosine kinase inhibitor neratinib, and highlight the importance\nof clinical sequencing efforts in treating breast cancer. ERBB2 L755S was one of the first ERBB2 variants to be functionally classified (Bose et al. 2013). This mutation was not shown to be an activating mutation,\nunlike many of the other variants queried. This mutation was also shown to confer resistance to the tyrosine kinase inhibitor lapatinib in MCF10A cell lines.\nFinal LLM Conclusion\nLLM Description\nERBB2 (Erb-B2 Receptor Tyrosine\nKinase 2) is a Protein Coding gene.\nAmong its related pathways are\nDrug-mediated inhibition of ERBB2\nsignaling and Signaling by ERBB2\nKD Mutants. Gene Ontology (GO)\nannotations related to this gene\nmore\nLLM Description\nFigure 5: The BioLunar report’s overview, produced by Bioworkflow.\nare potential drug targets and one is FDA approved\ndrug targets. According to the KEGG-based enrich-\nment analysis, these genes were mainly enriched\nthrough several signaling pathways including tu-\nmor necrosis factor (TNF) signaling pathway. Us-\ning LLMs in conjunction with a PubMed search\ncomponent, papers were searched in PubMed that\ndescribe various gene variants and the genes have\nbeen indicated as prospective biomarkers associ-\nated with breast cancer.\nNote that in scenario 2, for genes lacking molec-\nular profiles in the KB, a search in PubMed was\nconducted. This approach enables the workflow to\nautomatically uncover and search for non-obvious\nand previously unknown relationships. Essentially,\nif a gene is absent from the database, it suggests\nthat its relevance is relatively novel and not yet\ndocumented. Therefore, seeking out the most re-\ncent publications that describe this gene within the\nanalysis context represents a significant advantage,\nprovided by the workflow that integrates various\ncomponents.\n4\nRelated Work\nBioinformatics Pipelines Over the past decade,\nthree scientific workflow management systems\nsuch as Galaxy (gal, 2022), Snakemake (Köster\nand Rahmann, 2012), and Nextflow (Di Tommaso\net al., 2017), have been instrumental to bioinfor-\nmaticians to systematise their complex analytical\nprocesses. Nextflow targets bioinformaticians and\nfacilitates gene enrichment analysis, annotate bi-", "metadata": {"producer": "pdfTeX-1.40.25", "creator": "LaTeX with hyperref", "creationdate": "2024-06-28T00:03:44+00:00", "source": "/home/donbr/open-biosciences/biosciences-research/data/raw/2406.18626v1.pdf", "file_path": "/home/donbr/open-biosciences/biosciences-research/data/raw/2406.18626v1.pdf", "total_pages": 11, "format": "PDF 1.5", "title": "", "author": "", "subject": "", "keywords": "", "moddate": "2024-06-28T00:03:44+00:00", "trapped": "", "modDate": "D:20240628000344Z", "creationDate": "D:20240628000344Z", "page": 6}}
{"page_content": "ological sequences, and perform gene expression\nanalysis by including modules supported by various\nbioinformatics tools. These workflow systems are\ncurrently centred around the composition of spe-\ncialised bioinformatics software, configuration pa-\nrameters and supporting datasets, facilitating reuse\nand reproducibility. In contrast, this paper explores\nthe concept on using LLMs within a specialised\nworkflow environment to support the interpretation\nand integration of multiple analytical processes.\n5\nConclusion\nIn this paper we provided a demonstration of a\nscientific workflow based on LLMs to support spe-\ncialised gene analyses using oncology and gene\nenrichment as a driving motivational scenario. The\nframework is built using the Lunar framework\nand allows for the composition of specialised an-\nalytical workflows, integrating external databases\n(Retrieval Augmented Generation), external tools\n(ToolFormers) and contextualised chains of LLM-\nbased interpretation. The paper highlights that\na workflow environment with specialised compo-\nnents for RAG, ToolFormers and a set of spe-\ncialised prompts-based Natural Language Infer-\nence can serve as the foundation for streamlining\nand automating complex analytical process within\na biomedical setting. . We showcase analytical\napplications within the biomedical domain, partic-\nularly in oncology, constructively progressing to-\nwards more complex gene analysis workflows. The\ndeveloped bioworkflow demonstrates the LLMs\ncan be instrumental in enabling a complex end-\nto-end highly-specialised analytical workflow, in\na reproducible manner, supporting the integration\nof heterogeneous evidence, synthesising conclu-\nsions and while simultaneously documenting and\nlinking to the data sources within a comprehensive\noutput report. The proposed workflow is based on\na low-code paradigm that enables domain experts,\nregardless of their programming skills, to construct\nand scientific workflows enabled by generaqtive AI\namethods.\nLimitations\n• The current demonstration uses external LLM-\nbased APIs but can be adapted to open source\nLLM models.\n• The LLM-based inferences require a critical\nsupporting quantitative evaluation and halluci-\nnations are possible. The current workflow is\nmotivated by a hypothesis generation process,\nwhich is fully human supervised and does not\nhave direct clinical applications.\nAcknowledgements\nThis work was partially funded by The Ark\nfoundation, by the European Union’s Horizon\n2020 research and innovation program (grant no.\n965397) through the Cancer Core Europe DART\nproject, and by the Manchester Experimental Can-\ncer Medicine Centre and the NIHR Manchester\nBiomedical Research Centre.\nReferences\n2022. The galaxy platform for accessible, reproducible\nand collaborative biomedical analyses: 2022 update.\nNucleic Acids Research, 50(W1):W345–W351.\nPaul Bogdan, Gustavo Caetano-Anollés, Anna Jolles,\nHyunju Kim, James Morris, Cheryl A Murphy,\nCatherine Royer, Edward H Snell, Adam Steinbren-\nner, and Nicholas Strausfeld. 2021. Biological Net-\nworks across Scales—The Theoretical and Empirical\nFoundations for Time-Varying Complex Networks\nthat Connect Structure and Function across Levels of\nBiological Organization. Integrative and Compara-\ntive Biology, 61(6):1991–2010.\nPaolo Di Tommaso, Maria Chatzou, Evan W Floden,\nPablo Prieto Barja, Emilio Palumbo, and Cedric\nNotredame. 2017. Nextflow enables reproducible\ncomputational workflows.\nNature biotechnology,\n35(4):316–319.\nK.D. Dryden-Palmer, C.S. Parshuram, and W.B. Berta.\n2020. Context, complexity and process in the im-\nplementation of evidence-based innovation: a realist\ninformed review. BMC Health Services Research,\n20(81):1472–6963.\nJohannes Köster and Sven Rahmann. 2012.\nSnake-\nmake—a scalable bioinformatics workflow engine.\nBioinformatics, 28(19):2520–2522.\nDavid N. Nicholson and Casey S. Greene. 2020. Con-\nstructing knowledge graphs and their biomedical ap-\nplications. Computational and Structural Biotechnol-\nogy Journal, 18:1414–1428.\nJussi Paananen and Vittorio Fortino. 2019. An omics\nperspective on drug target discovery platforms. Brief-\nings in Bioinformatics, 21(6):1937–1953.", "metadata": {"producer": "pdfTeX-1.40.25", "creator": "LaTeX with hyperref", "creationdate": "2024-06-28T00:03:44+00:00", "source": "/home/donbr/open-biosciences/biosciences-research/data/raw/2406.18626v1.pdf", "file_path": "/home/donbr/open-biosciences/biosciences-research/data/raw/2406.18626v1.pdf", "total_pages": 11, "format": "PDF 1.5", "title": "", "author": "", "subject": "", "keywords": "", "moddate": "2024-06-28T00:03:44+00:00", "trapped": "", "modDate": "D:20240628000344Z", "creationDate": "D:20240628000344Z", "page": 7}}
{"page_content": "A\nAppendix", "metadata": {"producer": "pdfTeX-1.40.25", "creator": "LaTeX with hyperref", "creationdate": "2024-06-28T00:03:44+00:00", "source": "/home/donbr/open-biosciences/biosciences-research/data/raw/2406.18626v1.pdf", "file_path": "/home/donbr/open-biosciences/biosciences-research/data/raw/2406.18626v1.pdf", "total_pages": 11, "format": "PDF 1.5", "title": "", "author": "", "subject": "", "keywords": "", "moddate": "2024-06-28T00:03:44+00:00", "trapped": "", "modDate": "D:20240628000344Z", "creationDate": "D:20240628000344Z", "page": 8}}
{"page_content": "Scenario 1\nContext:\nThe analysis focuses on HER2-low breast cancer (HLBC), a subtype that\nchallenges traditional classifications based on HER2 expression and ERBB2\namplification. Despite being operationally defined, HLBCs constitute a\nsignificant portion of breast cancers, particularly among estrogen receptor-\npositive tumors. This study aims to elucidate the molecular characteristics\nof HLBCs, examining their mutational and transcriptional profiles. The\nresearch also investigates potential heterogeneity within HLBCs and compares\ntheir genomic landscape with HER2-positive and HER2-negative breast cancers.\nBy providing insights into the distinct molecular features of HLBCs, this\nanalysis seeks to establish whether they represent a unique entity in breast\ncancer pathology.\nList of genes:\nERBB2, ESR1, PIK3CA, CBFB, SF3B1\nScenario 2\nContext:\nBreast cancer (BC) presents a significant global health challenge, with its\nincidence \nsteadily \nrising \nand \nmortality \nrates \nremaining \nhigh. \nIts\nheterogeneous nature complicates treatment strategies, contributing to issues\nlike recurrence and drug resistance. Biomarkers play a crucial role in\ndiagnosing and managing BC, aiding in personalized treatment approaches.\nHowever, existing biomarkers have limitations, necessitating the exploration\nof novel markers, particularly in the realm of molecular and genetic\nanalysis. This study focuses on identifying genes with potential diagnostic\nutility in breast cancer, aiming to contribute to the development of more\neffective biomarkers and therapies, including immunotherapies, to combat this\ndisease.\nList of genes:\nDIXDC1, DUSP6, PDK4, CXCL12, IRF7, ITGA7, NEK2, NR3C1\nFigure A.6: User-defined context of the analysis, including aspects like cancer types or subtypes, treatments,\npopulations, for Scenario 1 and 2.\nFigure A.7: Human Protein Atlas workflow in the BioLunar interface.", "metadata": {"producer": "pdfTeX-1.40.25", "creator": "LaTeX with hyperref", "creationdate": "2024-06-28T00:03:44+00:00", "source": "/home/donbr/open-biosciences/biosciences-research/data/raw/2406.18626v1.pdf", "file_path": "/home/donbr/open-biosciences/biosciences-research/data/raw/2406.18626v1.pdf", "total_pages": 11, "format": "PDF 1.5", "title": "", "author": "", "subject": "", "keywords": "", "moddate": "2024-06-28T00:03:44+00:00", "trapped": "", "modDate": "D:20240628000344Z", "creationDate": "D:20240628000344Z", "page": 9}}
{"page_content": "Cancer type:                Breast Cancer\n  Sample size: 5\n  Cancer type: breast cancer\n  Biopsy site: primary site\n  Center:\n  Informed consent:\n  Sample sent:\nExport report\nAnalysis run date: 2024-02-24\nPipeline version: v2.0. details\n  Context:  \nThe analysis focuses on HER2-low breast cancer (HLBC), a subtype\nthat challenges traditional classifications based on HER2 expression\nand ERBB2 amplification. Despite being operationally defined, HLBCs\nconstitute a significant portion of breast cancers, more\nFigure A.8: The BioLunar report, produced by Biowork-\nflow for Scenario 1\n  Cancer type:                Breast Cancer\n  Sample size: 27\n  Cancer type: breast cancer\n  Biopsy site: primary site\n  Center:\n  Informed consent:\n  Sample sent:\nExport report\nAnalysis run date: 2024-02-24\nPipeline version: v2.0. details\n  Context:   Breast cancer (BC) presents a significant global health challenge,\nwith its incidence steadily rising and mortality rates remaining high.\nIts heterogeneous nature complicates treatment strategies,\ncontributing to issues like recurrence and drug resistance. more\nFigure A.9: The BioLunar report, produced by Biowork-\nflow for Scenario 2.", "metadata": {"producer": "pdfTeX-1.40.25", "creator": "LaTeX with hyperref", "creationdate": "2024-06-28T00:03:44+00:00", "source": "/home/donbr/open-biosciences/biosciences-research/data/raw/2406.18626v1.pdf", "file_path": "/home/donbr/open-biosciences/biosciences-research/data/raw/2406.18626v1.pdf", "total_pages": 11, "format": "PDF 1.5", "title": "", "author": "", "subject": "", "keywords": "", "moddate": "2024-06-28T00:03:44+00:00", "trapped": "", "modDate": "D:20240628000344Z", "creationDate": "D:20240628000344Z", "page": 10}}
{"page_content": "Nucleic Acids Research , 2025, 53 , D730–D737 \nhttps://doi.org/10.1093/nar/gkae1113 \nAdvance access publication date: 18 November 2024 \nDatabase issue \nThe STRING database in 2025: protein networks with \ndirectionality of regulation \nDamian Szklarczyk 1 , 2 , Katerina Nastou \n3 , Mik aela K outrouli \n3 , Rebecca Kirsch 3 , \nF ar rokh Mehryary \n4 , Radja Hachilif 1 , 2 , Dewei Hu 3 , Matteo E. Peluso 1 , 2 , Qingyao Huang 1 , 2 , \nTao Fang \n1 , 2 , Nadezhda T. Doncheva \n3 , Sampo Pyysalo 4 , P eer Bor k 5 , 6 , 7 , * , Lars J. Jensen 3 , * and \nChristian von Mering \n1 , 2 , * \n1 Department of Molecular Life Sciences, University of Zurich, Winterthurerstrasse 190, 8057 Zurich, Switzerland \n2 SIB Swiss Institute of Bioinformatics, Amphipôle, Quartier UNIL-Sorge, 1015 Lausanne, Switzerland \n3 Novo Nordisk Foundation Center for Protein Research, University of Copenhagen, Blegdamsvej 3B, 2200 Copenhagen N, Denmark \n4 TurkuNLP Lab, Department of Computing, University of Turku, Vesilinnantie 5, 20014 Turku, Finland \n5 Structural and Computational Biology Unit, European Molecular Biology Laboratory, Meyerhofstrasse 1, 69117 Heidelberg, Germany \n6 Max Delbrück Centre for Molecular Medicine, Robert-Rössle-Strasse 10, 13125 Berlin, Germany \n7 Department of Bioinformatics, Biozentrum, University of Würzburg, Am Hubland, 97074 Würzburg, Germany \n* To whom correspondence should be addressed. Tel: +49 6221 387 8526; Fax: +49 6221 387 517; Email: bork@embl.de \nCorrespondence may also be addressed to Lars J. Jensen. Email: lars.juhl.jensen@cpr.ku.dk \nCorrespondence may also be addressed to Christian von Mering. Tel: +41 44 6353147; Fax: +41 44 6356864; Email: mering@imls.uzh.ch \nAbstract \nProteins cooperate, regulate and bind each other to achie v e their functions. Understanding the comple x netw ork of their interactions is essential \nf or a sy stems-le v el description of cellular processes. T he S TRING database compiles, scores and integrates protein–protein association informa- \ntion drawn from experimental assays, computational predictions and prior knowledge. Its goal is to create comprehensive and objective global \nnetworks that encompass both ph y sical and functional interactions. A dditionally, S TRING pro vides supplementary tools such as network cluster- \ning and pathw a y enrichment analy sis. T he latest v ersion, S TRING 12.5, introduces a ne w ‘regulatory netw ork’, f or which it gathers evidence on \nthe type and directionality of interactions using curated pathw a y databases and a fine-tuned language model parsing the literature. This update \nenables users to visualize and access three distinct network types—functional, ph y sical and regulatory —separately, each applicable to distinct \nresearch needs. In addition, the pathw a y enrichment detection functionality has been updated, with better false discovery rate corrections, \nredundancy filtering and impro v ed visual displa y s. T he resource now also offers improved annotations of clustered networks and provides users \nwith do wnloadable netw ork embeddings, which f acilitate the use of S TRING netw orks in machine learning and allow cross-species transfer of \nprotein information. The STRING database is available online at https:// string-db.org/ . \nGr aphical abstr act \nReceived: September 15, 2024. Revised: October 18, 2024. Editorial Decision: October 21, 2024. Accepted: October 29, 2024 \n© The Author(s) 2024. Published by Oxford University Press on behalf of Nucleic Acids Research. \nThis is an Open Access article distributed under the terms of the Creative Commons Attribution License (https: // creativecommons.org / licenses / by / 4.0 / ), \nwhich permits unrestricted reuse, distribution, and reproduction in any medium, provided the original work is properly cited. \nDownloaded from https://academic.oup.com/nar/article/53/D1/D730/7903368 by guest on 11 January 2026", "metadata": {"producer": "Acrobat Distiller 24.0 (Windows); modified using iTextSharp 5.5.10 ©2000-2016 iText Group NV (AGPL-version); modified using iTextSharp.LGPLv2.Core 3.7.4.0", "creator": "OUP", "creationdate": "2024-12-30T12:59:32+05:30", "source": "/home/donbr/open-biosciences/biosciences-research/data/raw/gkae1113.pdf", "file_path": "/home/donbr/open-biosciences/biosciences-research/data/raw/gkae1113.pdf", "total_pages": 8, "format": "PDF 1.4", "title": "The STRING database in 2025: protein networks with directionality of regulation", "author": "Szklarczyk Damian, Nastou Katerina, Koutrouli Mikaela, Kirsch Rebecca, Mehryary Farrokh, Hachilif Radja, Hu Dewei, Peluso Matteo E., Huang Qingyao, Fang Tao, Doncheva Nadezhda T., Pyysalo Sampo, Bork Peer, Jensen Lars J., vonMering Christian", "subject": "DOI: 10.1093/nar/gkae1113 , 53, Database issue, 18-11-2024. Abstract: Proteins cooperate, regulate and bind each other to achieve their functions. Understanding the complex network of their interactions is essential for a systems-level description of cellular processes. The STRING database compiles, scores and integrates protein&ndash;protein association information drawn from experimental assays, computational predictions and prior knowledge. Its goal is to create comprehensive and objective global networks that encompass both physical and functional interactions. Additionally, STRING provides supplementary tools such as network clustering and pathway enrichment analysis. The latest version, STRING 12.5, introduces a new &lsquo;regulatory network&rsquo;, for which it gathers evidence on the type and directionality of interactions using curated pathway databases and a fine-tuned language model parsing the literature. This update enables users to visualize and access three distinct network types&mdash;functional, physical and regulatory&mdash;separately, each applicable to distinct research needs. In addition, the pathway enrichment detection functionality has been updated, with better false discovery rate corrections, redundancy filtering&nbsp;and improved visual displays. The resource now also offers improved annotations of clustered networks and provides users with downloadable network embeddings, which facilitate the use of STRING networks in machine learning and allow cross-species transfer of protein information. The STRING database is available online at https:&sol;&sol;string-db.org&sol;.", "keywords": "", "moddate": "2026-01-11T01:02:57+00:00", "trapped": "", "modDate": "D:20260111010257+00'00'", "creationDate": "D:20241230125932+05'30'", "page": 0}}
{"page_content": "Nucleic Acids Research , 2025, Vol. 53, Database issue \nD 731 \nIntroduction \nThe function of every living cell is primarily governed by a \ncomplex network of interacting proteins, with each protein’s \nrole determined not only by its molecular activities but also \nby its position within this network ( 1 ,2 ). Connected proteins \nwork together to contribute to common biological processes \nthrough various interaction types, such as physical binding, \ngenetic interactions and regulatory influences. These interac- \ntions can collectively be categorized as functional associa- \ntions , which serve as fundamental operational units within bi- \nological systems. Unraveling protein networks in their various \nmodalities remains a significant research focus. Consequently, \nnumerous databases have been developed over the years to \nmeet specific research needs. These range from carefully cu- \nrated pathway databases such as Reactome ( 3 ) and KEGG \n( 4 ), to databases of experimental interaction evidence curated \nfrom literature such as BioGRID ( 5 ), IntAct ( 6 ) and MINT \n( 7 ), and to those centered on specific modes of interactions, \nincluding Complex Portal for protein co-complexes ( 8 ) and \nSIGNOR for regulatory interactions ( 9 ). Finally, composite \ndatabases such as STRING ( 10 ), GeneMANIA ( 11 ), FunCoup \n( 12 ) and HumanNet ( 13 ) not only incorporate data from these \nsources but also employ an array of computational methods \nto predict additional associations, striving to provide the most \ncomprehensive views of the interactome. \nAmong these databases, STRING is notable for its many \nsources of evidence, its robust scoring system, user-friendly \ninterface and comprehensive suite of enrichment features. It \nis dedicated to assembling a broad set of associations among \nproteins for the complete proteomes of thousands of organ- \nisms across all domains of life. STRING charts interactions \nranging from highly confident, well-documented associations \nto more speculative ones, which are crucial for exploratory \nand computational analyses. The scoring system ensures that \ndata from diverse sources—including automatic text mining, \nhigh- and low-throughput experimental data, and compu- \ntational predictions—are directly comparable and weighted \nconsistently, regardless of their origin. Additionally, STRING \nextends and maps networks across species by predicting in- \nterologs, thereby broadening the scope of functional associa- \ntions to encompass a diverse range of organisms. This includes \nuncultured novel bacterial species derived from metagenomic \nsamples, and even unpublished proteomes uploaded by users. \nThe development of interaction databases has so far been a \ntrade-off between detailing the exact mode of an interaction—\nwhether physical or functional, signaling or structural, stable \nor transient—and maintaining a comprehensive set of interac- \ntions. Given the limitations of the available data and method- \nologies, STRING has historically focused on broadly defined \nfunctional associations, which provided the most useful set \nof interactions for proteome-wide analysis. While these net- \nworks continue to perform well in downstream tasks ( 14 ,15 ), \nthey often lack fine-grained resolution, leaving more special- \nized databases better equipped to offer detailed insights into \nspecific interactions if needed. However, the growing vol- \nume of data and advancements in text-mining technologies \nhave since enabled the composite, general-purpose databases \nto catch up, offering more detailed maps of the interac- \ntion space. Recently, the STRING database introduced a co- \ncomplex (physical) interaction network mode, which details \nmulti-protein assemblies. With the latest update, the database \nnow also includes the largest set of regulatory (directed) inter- \nactions, highlighting the flow of information within cells. This \nmajor enhancement greatly expands the analytical capabilities \nof the database, enabling new types of studies that were not \npreviously possible with nondirected interaction networks. \nInteraction and pathway databases are frequently used to \ngain insights into the functional context of individual pro- \nteins or to understand the biological organization of an entire \nprotein dataset. Many of these databases enhance the inter- \npretation of protein datasets beyond simple interaction anno- \ntations by incorporating an enrichment analysis toolset. This \ntype of analysis involves comparing observed data against ex- \npected distributions, enabling researchers to identify statisti- \ncally significant features or patterns. Enrichment tools typi- \ncally utilize established ontologies and annotations, such as \nGene Ontology ( 16 ), MSigDB hallmark sets ( 17 ) or OMIM \n( 18 ). A few databases also leverage their annotated datasets \nto help understand the functional context of a user’s input. \nFor example, KEGG ( 4 ) employs pathway and module ab- \nstractions to organize genes into datasets for overrepresenta- \ntion analysis, while Reactome maps interconnected proteins \ninto hierarchically nested modules. Uniquely, STRING uses \nan unsupervised procedure to hierarchically precluster its en- \ntire proteome-wide networks into functional modules. Incor- \nporating these network-derived gene sets into the enrichment \nanalysis facilitates the identification of novel modules, espe- \ncially in areas of the proteome where manual curation has \nbeen less comprehensive so far. \nDatabase content \nThe basic interaction scope in STRING is that of a ‘functional \nassociation’ between pairs of proteins. A functional associa- \ntion is defined as a contribution of two non-identical proteins \nto a common function ( 19 ,20 ). This can take many forms; \nfunctionally associated proteins can be in physical proxim- \nity to each other, regulate each other, exhibit genetic epistasis \nor even work antagonistically (as long as this occurs in the \ncontext of a common function). For the purpose of defining \nfunctional associations, the concept of a common function is \ncrucial, but difficult to define—it should broadly be thought \nof as corresponding to the notion of a ‘pathway’ or ‘function \nmodule’. In practice, STRING roughly follows the functional \ngranularity of the pathway maps in the KEGG database ( 4 ). \nFrom the set of all functional associations in STRING, sub- \nsets are derived that are more specifically annotated with re- \ngard to their mechanism of association. Currently, two such \nmore specific subsets are implemented: ‘physical’ and ‘regula- \ntory’. The physical mode refers to pairs of proteins that either \nbind directly or are at least subunits of the same complex ( 21 ). \nThe regulatory mode refers to associated protein pairs that are \nknown to regulate each other’s activity in at least one direc- \ntion. This mode is described in more detail further below; for \nthe first time in STRING, such connections are annotated not \nonly with confidence scores but also with a directionality. \nAll protein–protein associations in STRING are annotated \nwith ‘confidence scores’. These scores are fully precomputed; \nthey scale between 0 and 1 and describe the estimated like- \nlihood of a postulated association being correct, given the \navailable evidence. Separate confidence scores are provided \nfor physical and regulatory modes—these scores provide es- \ntimates of the likelihood that the proposed association is tak- \ning place and is indeed of the postulated type. The two spe- \nDownloaded from https://academic.oup.com/nar/article/53/D1/D730/7903368 by guest on 11 January 2026", "metadata": {"producer": "Acrobat Distiller 24.0 (Windows); modified using iTextSharp 5.5.10 ©2000-2016 iText Group NV (AGPL-version); modified using iTextSharp.LGPLv2.Core 3.7.4.0", "creator": "OUP", "creationdate": "2024-12-30T12:59:32+05:30", "source": "/home/donbr/open-biosciences/biosciences-research/data/raw/gkae1113.pdf", "file_path": "/home/donbr/open-biosciences/biosciences-research/data/raw/gkae1113.pdf", "total_pages": 8, "format": "PDF 1.4", "title": "The STRING database in 2025: protein networks with directionality of regulation", "author": "Szklarczyk Damian, Nastou Katerina, Koutrouli Mikaela, Kirsch Rebecca, Mehryary Farrokh, Hachilif Radja, Hu Dewei, Peluso Matteo E., Huang Qingyao, Fang Tao, Doncheva Nadezhda T., Pyysalo Sampo, Bork Peer, Jensen Lars J., vonMering Christian", "subject": "DOI: 10.1093/nar/gkae1113 , 53, Database issue, 18-11-2024. Abstract: Proteins cooperate, regulate and bind each other to achieve their functions. Understanding the complex network of their interactions is essential for a systems-level description of cellular processes. The STRING database compiles, scores and integrates protein&ndash;protein association information drawn from experimental assays, computational predictions and prior knowledge. Its goal is to create comprehensive and objective global networks that encompass both physical and functional interactions. Additionally, STRING provides supplementary tools such as network clustering and pathway enrichment analysis. The latest version, STRING 12.5, introduces a new &lsquo;regulatory network&rsquo;, for which it gathers evidence on the type and directionality of interactions using curated pathway databases and a fine-tuned language model parsing the literature. This update enables users to visualize and access three distinct network types&mdash;functional, physical and regulatory&mdash;separately, each applicable to distinct research needs. In addition, the pathway enrichment detection functionality has been updated, with better false discovery rate corrections, redundancy filtering&nbsp;and improved visual displays. The resource now also offers improved annotations of clustered networks and provides users with downloadable network embeddings, which facilitate the use of STRING networks in machine learning and allow cross-species transfer of protein information. The STRING database is available online at https:&sol;&sol;string-db.org&sol;.", "keywords": "", "moddate": "2026-01-11T01:02:57+00:00", "trapped": "", "modDate": "D:20260111010257+00'00'", "creationDate": "D:20241230125932+05'30'", "page": 1}}
{"page_content": "D 732 \nNucleic Acids Research , 2025, Vol. 53, Database issue \ncific network modes are fully consistent with the functional \nnetwork, such that if an interaction is present in either the \nphysical or regulatory network, it will, by definition, also be \npresent in the full functional association network (with an \nequal or higher confidence score). To compute the various con- \nfidence scores, the available evidence supporting a given inter- \naction is first separated into ‘evidence channels’, by the type \nof evidence. For the basic functional association confidence \nscore, seven such channels are used. These include three that \nare based on genomic context predictions—neighborhood, fu- \nsion and gene co-occurrence—as well as channels dedicated to \nco-expression, experimental data, curated databases and text \nmining. \nThe genomic context channels focus on associations in- \nferred from genome sequences alone, detecting evolution- \nary patterns such as gene proximity, fusion events and co- \noccurrence across different organisms. For instance, the neigh- \nborhood channel assigns an association score to gene pairs \nthat are located close to each other on the chromosome in \nthe same orientation (in prokaryotic genomes) ( 22 ). The fu- \nsion channel identifies open reading frames that result from \ngene fusion events ( 23 ), while the co-occurrence channel ex- \namines whether genes have a nontrivial but shared distribu- \ntion across genomes ( 24 ), implying a shared history of hori- \nzontal transfers, losses or duplication events and thus likely \nrelated functions. The co-expression channel compiles data \nfrom gene expression studies, analyzing both transcript and \nprotein abundances across various conditions. By comparing \nexpression profiles, it identifies gene pairs with similar expres- \nsion patterns, suggesting functional linkage ( 10 ,25 ). The ex- \nperiments channel aggregates interaction evidence from labo- \nratory assays, including biochemical, biophysical and genetic \nassays. Data are imported from primary repositories such as \nBioGRID ( 5 ) and the IMEx consortium ( 26 ), and the confi- \ndence scores are estimated by globally benchmarking the accu- \nracy of annotated experimental techniques, as well as within- \ndataset performance and consistency for the case of high- \nthroughput experiments ( 10 ). The database channel is based \non well-described, curated protein–protein associations from \nexpert-compiled resources, such as KEGG ( 4 ), Reactome ( 3 ) \nand Gene Ontology Complexes ( 16 ). These resources provide \nwell-established pathways, complexes and functional relation- \nships, offering a high level of reliability. Unlike other chan- \nnels, the database channel assigns a uniformly high confidence \nscore to associations, reflecting their established nature. Fi- \nnally, the text-mining channel utilizes a large corpus of sci- \nentific literature, including PubMed abstracts and full-text ar- \nticles, to identify co-mentions of protein names ( 27 ). By statis- \ntically analyzing the frequency of these co-mentions in various \ntextual contexts, this channel uncovers potential associations \nthat may not be evident from structured data alone. In addi- \ntion, for the more specific association modes, dedicated large \nlanguage models are employed to detect sentences supporting \neither physical ( 21 ) or regulatory associations (see below). \nFor each evidence channel, the available interaction evi- \ndence is translated into a confidence score by first quantifying \nthe evidence using channel-specific metrics and then convert- \ning these metrics into likelihoods using calibration curves de- \nrived from prior knowledge (from pathway-map memberships \nin KEGG). For the more specific association modes ‘physical’ \nand ‘regulatory’, only channels and evidence that are appli- \ncable to these modes are considered. After this, all channel \nscores that have been computed for a given protein pair in \na given organism are transferred onto related protein pairs \nin other organisms, based on the ‘interolog’ concept ( 27 ,28 ). \nLastly, a final, combined confidence score is computed by inte- \ngrating the channel-specific subscores probabilistically, under \nthe assumption that evidence in different channels is largely \nindependent. Users of STRING can directly rely on this com- \nbined score for browsing and filtering networks, or they can \nalternatively customize their analyses by enabling or disabling \nspecific channels separately, after which the combined score is \nrecomputed according to their chosen settings. \nAll primary evidence underlying a given interaction can \nbe inspected interactively, in dedicated evidence viewers on- \nline. Furthermore, accessory information is available for \neach protein, such as its three-dimensional structure, do- \nmain composition, annotation and cross-references. Apart \nfrom its core protein-network functionality, STRING also \nimplements features that allow extensive analysis of up- \nloaded user data. These include functional enrichment de- \ntection ( 29 ), experimental bias detection ( 21 ), homology \nsearches, and clustering and organizing large query pro- \ntein sets. All data contained in STRING are freely avail- \nable for download, under a Creative Commons BY 4.0 li- \ncense. Apart from its website, STRING can also be accessed \nvia a dedicated Cytoscape plugin ( 30 ), through an applica- \ntion programming interface (API) ( 31 ), as well as through \nan R / Bioconductor package ( https://www.bioconductor.org/ \npackages/ release/ bioc/ html/ STRINGdb.html ). \nRegulat ory netw orks \nMolecular networks, like those available through STRING, \nhave become invaluable tools in biomedical research, offering \npowerful insights into how molecules work together within \ncells. The functional association networks, while useful for \nmany applications, do not specify the interaction type (e.g. \ncomplex formation or transcriptional regulation) nor the di- \nrection of interaction (i.e. who regulates whom). Constructing \nnetworks with more detailed interaction types and directions \ncan significantly expand their utility, particularly for building \nmathematical models of biological systems or for making log- \nical inferences. The directionality of interactions is essential \nfor interpretation of omics data, which typically reveals more \nabout downstream effects of conditions such as diseases than \nit does about the upstream events that led to it and thus could \nbe targeted therapeutically. \nTo incorporate interactions with specific directionality, sign \nand type into STRING, we leveraged the novel dataset and \ndeep learning-based relation extraction method described in \nRegulaTome ( 32 ). The RegulaTome corpus provides a rich \nand diverse dataset that includes 16 961 relations between \n54 951 entities annotated across over 2500 documents. This \ncorpus is utilized to train a deep learning-based method used \nto annotate and extract the regulatory events throughout the \nentire available literature. \nOn top of undirected physical interactions—which are al- \nready included in STRING—the following types of directed \ninteractions can be extracted using the system developed for \nRegulaTome: Regulation , with the signed subtypes of Posi- \ntive Regulation and Negative Regulation , Regulation of Gene \nExpression , Regulation of Degradation and Catalysis of Post- \nTranslational Modifications , covering six subtypes: Catalysis \nof Small Protein Conjugation , Catalysis of Small Protein Re- \nmoval , Catalysis of Phosphorylation , Catalysis of Dephospho- \nDownloaded from https://academic.oup.com/nar/article/53/D1/D730/7903368 by guest on 11 January 2026", "metadata": {"producer": "Acrobat Distiller 24.0 (Windows); modified using iTextSharp 5.5.10 ©2000-2016 iText Group NV (AGPL-version); modified using iTextSharp.LGPLv2.Core 3.7.4.0", "creator": "OUP", "creationdate": "2024-12-30T12:59:32+05:30", "source": "/home/donbr/open-biosciences/biosciences-research/data/raw/gkae1113.pdf", "file_path": "/home/donbr/open-biosciences/biosciences-research/data/raw/gkae1113.pdf", "total_pages": 8, "format": "PDF 1.4", "title": "The STRING database in 2025: protein networks with directionality of regulation", "author": "Szklarczyk Damian, Nastou Katerina, Koutrouli Mikaela, Kirsch Rebecca, Mehryary Farrokh, Hachilif Radja, Hu Dewei, Peluso Matteo E., Huang Qingyao, Fang Tao, Doncheva Nadezhda T., Pyysalo Sampo, Bork Peer, Jensen Lars J., vonMering Christian", "subject": "DOI: 10.1093/nar/gkae1113 , 53, Database issue, 18-11-2024. Abstract: Proteins cooperate, regulate and bind each other to achieve their functions. Understanding the complex network of their interactions is essential for a systems-level description of cellular processes. The STRING database compiles, scores and integrates protein&ndash;protein association information drawn from experimental assays, computational predictions and prior knowledge. Its goal is to create comprehensive and objective global networks that encompass both physical and functional interactions. Additionally, STRING provides supplementary tools such as network clustering and pathway enrichment analysis. The latest version, STRING 12.5, introduces a new &lsquo;regulatory network&rsquo;, for which it gathers evidence on the type and directionality of interactions using curated pathway databases and a fine-tuned language model parsing the literature. This update enables users to visualize and access three distinct network types&mdash;functional, physical and regulatory&mdash;separately, each applicable to distinct research needs. In addition, the pathway enrichment detection functionality has been updated, with better false discovery rate corrections, redundancy filtering&nbsp;and improved visual displays. The resource now also offers improved annotations of clustered networks and provides users with downloadable network embeddings, which facilitate the use of STRING networks in machine learning and allow cross-species transfer of protein information. The STRING database is available online at https:&sol;&sol;string-db.org&sol;.", "keywords": "", "moddate": "2026-01-11T01:02:57+00:00", "trapped": "", "modDate": "D:20260111010257+00'00'", "creationDate": "D:20241230125932+05'30'", "page": 2}}
{"page_content": "Nucleic Acids Research , 2025, Vol. 53, Database issue \nD 733 \nFigure 1. Illustration of the new ‘regulatory network’ mode in STRING, where the network edges visually indicate the direction, confidence and sources \nof each regulatory interaction. Clicking on an edge within the network brings up a pop-up window with a detailed o v ervie w of the a v ailable e vidence and \nthe annotated type of regulation. Users can further explore the data behind the interactions by accessing the specific evidence viewers linked within the \npop-up, including the text-mining evidence viewer. This viewer presents excerpts from literature pertaining to the inspected interactions, as well as an \nautomatically generated large language model summary of the presented e x cerpts. \nrylation , Catalysis of Small Molecule Conjugation and Cataly- \nsis of Small Molecule Removal . To enhance the interoperabil- \nity of the annotations, these chosen relation types align with \nGene Ontology ( 32 ). \nTo identify and classify a wide array of interaction types, we \nfine-tuned the RoBERT a-large-PM-M3-V oc language model, \na transformer-based model well suited for biological tasks. \nThis model was trained on the RegulaTome dataset for multi- \nlabel extraction of the directed, typed and signed interactions \nmentioned above, and achieved an average F 1 score of 73.5% \n(with a precision of 75.2% and a recall of 71.8%) on a held- \nout test set, although the exact performance varied across dif- \nferent types of relationships. This level of accuracy is substan- \ntial, considering the complexity of the underlying task. Using \nthis model, we processed over 1.2 billion sentence-level pairs \nextracted from all available PubMed abstracts and PMC Open \nAccess full-text documents, and assigned 3.5% of them ( ∼43 \nmillion, of which ∼18 million in human) with at least one \npositive label indicating directed (72.9%) or signed (33.1%) \nrelationships among the proteins. Furthermore, we made use \nof the existing curated knowledge of regulatory interactions in \nthe database channel, by parsing regulatory information from \nthe SIGNOR, KEGG and Reactome databases. \nTo integrate these interactions into STRING, we followed \na similar score aggregation and benchmarking approach as \nfor physical interactions ( 21 ). Here, we perform benchmark- \ning in five categories for which we can derive gold stan- \ndard datasets of known human regulatory interactions from \nSIGNOR: regulation , upregulation , downregulation , tran- \nscriptional regulation and phosphorylation . The resulting \ncalibration functions are then applied to extracted regula- \ntions of all types to produce the final regulatory confidence \nscores. \nThe new regulatory network has been seamlessly integrated \ninto the existing STRING framework, complementing the \n‘functional’ and ‘physical’ network types. Directional edges \nwithin this network are visually represented by arrows, depict- \ning both bidirectional and unidirectional relationships. The \nnetwork visualization can show varying confidence levels be- \ntween directions (in the ‘confidence’ view) or the specific di- \nrectionality of the sources (in the ‘evidence’ view). The user \ninterface retains its intuitive design, enabling users to access \nthe type of regulatory event and the evidence for the interac- \ntion by clicking on the edge (Figure 1 ). Additionally, all API \nfunctions have been updated to fully support the new net- \nwork type, which can be accessed by specifying the parameter \nnetw or k_type = regulatory in the API call. \nExpanded co-expression networks \nThe co-expression channel in STRING identifies genes with \nsimilar expression profiles across various tissues and condi- \ntions, revealing their involvement in shared biological pro- \ncesses. It compiles data from large-scale gene expression stud- \nies, analyzing transcript and protein levels to detect gene pairs \nwith coordinated expression. This method highlights poten- \ntial functional relationships between genes. Importantly, the \nanalysis is free from study bias, as it directly uses raw omics \ndata instead of relying on manually curated or experimental \ndatasets, providing a more precise and objective view of gene \ninteractions. \nFor the upcoming version 12.5, we are expanding the \nco-expression channel by generating additional networks \nthrough the application of FAVA (Functional Association of \nVariational Autoencoders) ( 25 ) on a wider range of single- \ncell RNA sequencing data. Specifically, we are incorporating \nsingle-cell data from the cellxgene Atlas ( 33 ) to enhance the \nhuman and mouse networks and from the EBI Single Cell Ex- \npression Atlas to extend coverage to more model organisms \n( 34 ). This expansion will provide more detailed, organism- \nspecific co-expression maps, enabling more precise identifi- \ncation of gene co-expression patterns and cross-species com- \nparisons, particularly for development, immune response and \ndisease-related gene expression dynamics. \nImpro v ed enric hment analysis \nGene set enrichment analysis is a critical component of high- \nthroughput biological studies, providing key insights into the \nbiological characteristics of datasets. This analysis is pivotal in \nidentifying essential aspects such as biological processes, path- \nways and phenotypes, which are crucial for understanding the \nbiological implications of experiments. Typically, overrepre- \nsentation analysis is employed, wherein a subset of data ex- \nceeding a specific cutoff (empirical or customary) is compared \nagainst an experimental background. By default, STRING \nDownloaded from https://academic.oup.com/nar/article/53/D1/D730/7903368 by guest on 11 January 2026", "metadata": {"producer": "Acrobat Distiller 24.0 (Windows); modified using iTextSharp 5.5.10 ©2000-2016 iText Group NV (AGPL-version); modified using iTextSharp.LGPLv2.Core 3.7.4.0", "creator": "OUP", "creationdate": "2024-12-30T12:59:32+05:30", "source": "/home/donbr/open-biosciences/biosciences-research/data/raw/gkae1113.pdf", "file_path": "/home/donbr/open-biosciences/biosciences-research/data/raw/gkae1113.pdf", "total_pages": 8, "format": "PDF 1.4", "title": "The STRING database in 2025: protein networks with directionality of regulation", "author": "Szklarczyk Damian, Nastou Katerina, Koutrouli Mikaela, Kirsch Rebecca, Mehryary Farrokh, Hachilif Radja, Hu Dewei, Peluso Matteo E., Huang Qingyao, Fang Tao, Doncheva Nadezhda T., Pyysalo Sampo, Bork Peer, Jensen Lars J., vonMering Christian", "subject": "DOI: 10.1093/nar/gkae1113 , 53, Database issue, 18-11-2024. Abstract: Proteins cooperate, regulate and bind each other to achieve their functions. Understanding the complex network of their interactions is essential for a systems-level description of cellular processes. The STRING database compiles, scores and integrates protein&ndash;protein association information drawn from experimental assays, computational predictions and prior knowledge. Its goal is to create comprehensive and objective global networks that encompass both physical and functional interactions. Additionally, STRING provides supplementary tools such as network clustering and pathway enrichment analysis. The latest version, STRING 12.5, introduces a new &lsquo;regulatory network&rsquo;, for which it gathers evidence on the type and directionality of interactions using curated pathway databases and a fine-tuned language model parsing the literature. This update enables users to visualize and access three distinct network types&mdash;functional, physical and regulatory&mdash;separately, each applicable to distinct research needs. In addition, the pathway enrichment detection functionality has been updated, with better false discovery rate corrections, redundancy filtering&nbsp;and improved visual displays. The resource now also offers improved annotations of clustered networks and provides users with downloadable network embeddings, which facilitate the use of STRING networks in machine learning and allow cross-species transfer of protein information. The STRING database is available online at https:&sol;&sol;string-db.org&sol;.", "keywords": "", "moddate": "2026-01-11T01:02:57+00:00", "trapped": "", "modDate": "D:20260111010257+00'00'", "creationDate": "D:20241230125932+05'30'", "page": 3}}
{"page_content": "D 734 \nNucleic Acids Research , 2025, Vol. 53, Database issue \nuses the whole genome / proteome background; however, it \nis recommended that users provide a custom background \nlist representing a more realistic universe of genes / proteins \ndetected by their assay ( 35 ). Alternatively, the entire sorted \ndataset can be analyzed using gene set enrichment analysis to \ndetermine whether the distribution of genes at the top, bottom \nor both ends deviates significantly from what would be ex- \npected by chance. STRING supports both of these enrichment \nmethods. The most recent update to STRING introduces vari- \nous improvements, including a new false discovery rate (FDR) \ncorrection, enhanced filtering options and a new graphical vi- \nsualization. \nFDR correction \nIn its analyses, STRING performs a test for each path- \nway (‘term’) within a given pathway collection (‘category’). \nLike many other enrichment tools, STRING employs the \nBenjamini–Hochberg correction ( 36 ) to adjust for multiple \nhypothesis testing. This statistical correction is essential for \nmanaging the FDR effectively. However, the larger the term \ncount and the greater the diversity of the terms, the higher \nthe required correction and the lower the chance of exceed- \ning the alpha level (significance threshold) for a given term. \nOne method used to address this issue involves restricting the \nanalysis to a subset of terms, such as ‘GO slims’ that focus \non broad, high-level terms with a simplified ontology struc- \nture ( 16 ). However, this approach has some disadvantages: it \nmight exclude terms particularly relevant to a tested gene set, \nit may not cover all functions by design and it is mainly ap- \nplicable to hierarchical classification systems such as the Gene \nOntology. \nSTRING utilizes several hierarchical ontologies, such as \nthe Brenda Tissue Ontology, Disease Ontology, Human Phe- \nnotype Ontology and the aforementioned Gene Ontology; \nthey typically contain many more leaf-ward nodes than nodes \ncloser to the root. The parental nodes must contain all the \ngenes included in the child terms, creating an imbalance in \nthe ontology with many more smaller terms and fewer larger \nterms. Removing smaller terms can substantially increase sta- \ntistical power . However , simply eliminating all small terms \ncould negatively impact discoverability, especially for small \nquery gene sets where the smaller terms might be the most rel- \nevant for biological interpretation. The ontologies are usually \nlarge, with > 10 000 terms (and therefore tests) in the Biolog- \nical Process branch of the Gene Ontology tree. Such strong \ncorrections have the effect that for larger query gene sets, or \nfor backgrounds that cover only part of the proteome, it might \nnot be statistically possible to detect enrichments for a subset \nof the terms; these can therefore be removed from considera- \ntion. \nIn its new version, STRING takes a more flexible \napproach—by only testing terms that have a priori any statis- \ntical potential for enrichment. This decision is based on sev- \neral parameters: the size of the term, the size of the query set, \nthe background size and the number of tests conducted. By \ncalculating the smallest and largest term sizes that could the- \noretically be enriched given the FDR correction, STRING can \ndetermine which terms are viable for inclusion in the anal- \nysis. Terms outside these bounds are excluded from testing. \nThis methodical exclusion significantly enhances the statisti- \ncal power of the enrichment analysis, without omitting any \nterms that could be enriched. For smaller query sets, this strat- \negy will maintain all terms, as even the smallest tested sets \n(term size = 2) might still be theoretically enriched. How- \never , for larger , less specific sets or for tests against a cus- \ntom smaller background, this approach markedly improves \nthe statistical power, enabling STRING to perform more fo- \ncused analyses without the drawbacks of increased false posi- \ntive rates or omitting potentially biologically interesting terms. \nAlthough this method leverages the characteristics of ontolo- \ngies, it does not require a hierarchy (parent–child relationships \nbetween the terms) to function and can be applied to any \npathway / term collection with an imbalance of small terms, \nsuch as PMID-derived gene sets and STRING neighborhood \nclusters. \nTerm filtering \nOne key insight from our online user surveys is that the of- \nten overwhelming number of enrichment results can make it \nchallenging to discern the most relevant enriched terms. We \nhave thus enhanced the display of enrichment results, allow- \ning users to filter and sort them based on (i) FDR, (ii) strength, \n(iii) signal, (iv) term size and (v) term similarity. \nThe newest addition to our filtering options are the ‘term \nsimilarity’ and ‘signal’ filters. The ‘term similarity’ filter uses \nthe Jaccard index to measure the similarity between the gene \nsets of terms within the same category. The method proceeds \nby sorting the terms according to their P -values, prioritizing \nthose with the strongest statistical significance. The algorithm \nthen iterates through the list of terms; any term that exhibits \na similarity exceeding a predetermined, user-defined threshold \nrelative to terms already included in the results is subsequently \nexcluded. This approach requires no additional information \nbeyond what is already available in the dataset and can be \nuniformly applied across all categories of terms. The ‘signal’ \nfilter is defined as a weighted harmonic mean between the ra- \ntio of observed to expected gene occurrences in an enriched \nterm and its −log(FDR), respectively. This filter balances the \nimpact of both metrics and provides a more intuitive ordering \nof enriched terms and has been implemented as the default \nsorting criterion. \nThese filters are designed to exclude potentially less relevant \nresults, such as terms that are marginally significant, small or \nshow significant overlap in terms of gene content. This en- \nhancement streamlines the search for relevant observations, \nhelping users focus on the most meaningful results. \nGraphical visualization \nIn response to user feedback requesting more intuitive, graph- \nical visualizations of the detected enrichments, STRING now \nhas adopted an interactive dot plot as a primary display \nmethod for enrichment analysis outcomes. We chose the dot \nplot for its clarity in representing enriched functions across \nthree dimensions: (i) enrichment signal along the X -axis, (ii) \nthe FDR indicated through the color coding of the dots and \n(iii) the term’s protein count in the network represented by the \nsize of each dot. The terms (listed on the Y -axis) are ranked \nby their enrichment signal or, if grouped by similarity, by the \nenrichment signal of their group first (Figure 2 ). This visual- \nization reflects all filter settings chosen by the user to manage \nthe volume of results displayed. These plots can be addition- \nally customized in the interface and the resulting graphs can be \ndownloaded as PNG for immediate use or as SVG for further \nmodifications. \nDownloaded from https://academic.oup.com/nar/article/53/D1/D730/7903368 by guest on 11 January 2026", "metadata": {"producer": "Acrobat Distiller 24.0 (Windows); modified using iTextSharp 5.5.10 ©2000-2016 iText Group NV (AGPL-version); modified using iTextSharp.LGPLv2.Core 3.7.4.0", "creator": "OUP", "creationdate": "2024-12-30T12:59:32+05:30", "source": "/home/donbr/open-biosciences/biosciences-research/data/raw/gkae1113.pdf", "file_path": "/home/donbr/open-biosciences/biosciences-research/data/raw/gkae1113.pdf", "total_pages": 8, "format": "PDF 1.4", "title": "The STRING database in 2025: protein networks with directionality of regulation", "author": "Szklarczyk Damian, Nastou Katerina, Koutrouli Mikaela, Kirsch Rebecca, Mehryary Farrokh, Hachilif Radja, Hu Dewei, Peluso Matteo E., Huang Qingyao, Fang Tao, Doncheva Nadezhda T., Pyysalo Sampo, Bork Peer, Jensen Lars J., vonMering Christian", "subject": "DOI: 10.1093/nar/gkae1113 , 53, Database issue, 18-11-2024. Abstract: Proteins cooperate, regulate and bind each other to achieve their functions. Understanding the complex network of their interactions is essential for a systems-level description of cellular processes. The STRING database compiles, scores and integrates protein&ndash;protein association information drawn from experimental assays, computational predictions and prior knowledge. Its goal is to create comprehensive and objective global networks that encompass both physical and functional interactions. Additionally, STRING provides supplementary tools such as network clustering and pathway enrichment analysis. The latest version, STRING 12.5, introduces a new &lsquo;regulatory network&rsquo;, for which it gathers evidence on the type and directionality of interactions using curated pathway databases and a fine-tuned language model parsing the literature. This update enables users to visualize and access three distinct network types&mdash;functional, physical and regulatory&mdash;separately, each applicable to distinct research needs. In addition, the pathway enrichment detection functionality has been updated, with better false discovery rate corrections, redundancy filtering&nbsp;and improved visual displays. The resource now also offers improved annotations of clustered networks and provides users with downloadable network embeddings, which facilitate the use of STRING networks in machine learning and allow cross-species transfer of protein information. The STRING database is available online at https:&sol;&sol;string-db.org&sol;.", "keywords": "", "moddate": "2026-01-11T01:02:57+00:00", "trapped": "", "modDate": "D:20260111010257+00'00'", "creationDate": "D:20241230125932+05'30'", "page": 4}}
{"page_content": "Nucleic Acids Research , 2025, Vol. 53, Database issue \nD 735 \nFigure 2. Enrichment analysis interface from the STRING database website. Lef t: Sc hematic of the enrichment analysis tab highlighting various sections \nof the webpage. Top right: A zoomed-in view of the analysis table with two user-highlighted terms (colored lines). Bottom right: A visualization of the \nenrichment depicting the two terms highlighted in corresponding colors. \nFurthermore, a distinctive feature of STRING’s visualiza- \ntion is the incorporation of similarity groups, which visually \ngroup related terms on the plot, adding an additional dimen- \nsion to the enrichment results. These groups utilize computed \nsimilarities to cluster terms together, highlighting relationships \nbetween the terms and aiding the analysis. The clustering em- \nploys the average linkage method based on the Jaccard index, \nwith a user-specified cutoff applied to form groups that, on \naverage, share the specified similarity. The groups are sorted \nby the maximum signal of their terms, with each term within a \ngroup subsequently sorted by its individual signal. This group- \ning is visually highlighted on the right side of the plot. \nS TRING clust ering and g ene set descriptions \nSTRING offers network clustering options for user-submitted \ngene lists. This feature visually connects nodes that are more \ninterconnected to each other than to other nodes, reflecting \nthe functional modularity of the user’s gene list. By grouping \nproteins that closely interact, clustering aids in the discovery \nof functional modules and facilitates hypothesis generation. \nClustering is based on the connectivity between nodes, incor- \nporating edge weights (combined scores) reflecting the confi- \ndence level of the interactions. Only visible edges—those cor- \nresponding to active channels with confidence higher than the \nuser-specified cutoff—are considered, ensuring that clustering \nis consistent with the network being shown. \nUsers have the choice of two different clustering algorithms. \n‘ K -means clustering’ is implemented via the k -medoids algo- \nrithm [pyclustering package ( 37 )], allowing users to specify \nthe desired number of clusters ( k ). This method forms clusters \nbased on a distance matrix derived from the most probable \npaths between nodes. In case of multiple disconnected compo- \nnents within the network, STRING aims for a balanced distri- \nbution of cluster sizes by iteratively dividing the largest com- \nponents first. In contrast, ‘MCL clustering’, implemented with \nstandalone binaries (release 14-137), automatically generates \nnatural divisions based on the network’s inherent structure. It \nutilizes the Markov cluster algorithm ( 38 ) to simulate stochas- \ntic flow in graphs, identifying groups of highly interconnected \nnodes as clusters based on their combined STRING scores. \nThe user-controlled inflation parameter influences the granu- \nlarity of the clustering. This method is particularly effective \nin capturing the true modular nature of biological networks, \nallowing for intuitive groupings that correspond to biological \nfunctions and relationships. \nResearchers often analyze the biological context of the ob- \ntained clusters to interpret and draw conclusions from net- \nwork data. To aid this process, we have developed a novel \ngene set naming feature that significantly enhances the inter- \npretability of cluster analyses. This feature automatically as- \nsigns the best description for each cluster based on STRING’s \nrobust statistical enrichment analysis, simplifying the often \ncumbersome task of manually interpreting cluster data. It \nranks the enriched terms by their enrichment signal (see \nthe ‘Improved enrichment analysis’ section), choosing the pri- \nmary, secondary and tertiary names among various enrich- \nment categories. This prioritization ensures that the names re- \nflect the most statistically significant and biologically pertinent \nattributes of each cluster, providing a clear, immediate under- \nstanding of its functional characteristics. The enrichment pri- \nmarily draws from categories such as Gene Ontology Biolog- \nical Processes, while incorporating a range of other categories \nfor broader annotations. In case the enrichment analysis yields \nno significant results or if the cluster contains only one gene, \nSTRING assigns canonical gene names as descriptions to en- \nDownloaded from https://academic.oup.com/nar/article/53/D1/D730/7903368 by guest on 11 January 2026", "metadata": {"producer": "Acrobat Distiller 24.0 (Windows); modified using iTextSharp 5.5.10 ©2000-2016 iText Group NV (AGPL-version); modified using iTextSharp.LGPLv2.Core 3.7.4.0", "creator": "OUP", "creationdate": "2024-12-30T12:59:32+05:30", "source": "/home/donbr/open-biosciences/biosciences-research/data/raw/gkae1113.pdf", "file_path": "/home/donbr/open-biosciences/biosciences-research/data/raw/gkae1113.pdf", "total_pages": 8, "format": "PDF 1.4", "title": "The STRING database in 2025: protein networks with directionality of regulation", "author": "Szklarczyk Damian, Nastou Katerina, Koutrouli Mikaela, Kirsch Rebecca, Mehryary Farrokh, Hachilif Radja, Hu Dewei, Peluso Matteo E., Huang Qingyao, Fang Tao, Doncheva Nadezhda T., Pyysalo Sampo, Bork Peer, Jensen Lars J., vonMering Christian", "subject": "DOI: 10.1093/nar/gkae1113 , 53, Database issue, 18-11-2024. Abstract: Proteins cooperate, regulate and bind each other to achieve their functions. Understanding the complex network of their interactions is essential for a systems-level description of cellular processes. The STRING database compiles, scores and integrates protein&ndash;protein association information drawn from experimental assays, computational predictions and prior knowledge. Its goal is to create comprehensive and objective global networks that encompass both physical and functional interactions. Additionally, STRING provides supplementary tools such as network clustering and pathway enrichment analysis. The latest version, STRING 12.5, introduces a new &lsquo;regulatory network&rsquo;, for which it gathers evidence on the type and directionality of interactions using curated pathway databases and a fine-tuned language model parsing the literature. This update enables users to visualize and access three distinct network types&mdash;functional, physical and regulatory&mdash;separately, each applicable to distinct research needs. In addition, the pathway enrichment detection functionality has been updated, with better false discovery rate corrections, redundancy filtering&nbsp;and improved visual displays. The resource now also offers improved annotations of clustered networks and provides users with downloadable network embeddings, which facilitate the use of STRING networks in machine learning and allow cross-species transfer of protein information. The STRING database is available online at https:&sol;&sol;string-db.org&sol;.", "keywords": "", "moddate": "2026-01-11T01:02:57+00:00", "trapped": "", "modDate": "D:20260111010257+00'00'", "creationDate": "D:20241230125932+05'30'", "page": 5}}
{"page_content": "D 736 \nNucleic Acids Research , 2025, Vol. 53, Database issue \nsure that clusters are easily identifiable. This functionality ex- \ntends beyond cluster analysis and is applicable to any gene set. \nAs such, it is also available through an API, complementing \nour suite of other API methods. The new API function, named \ngeneset_description , requires only a set of genes as input and \nautomatically performs enrichment analysis. It returns up to \nthree descriptions—primary, secondary and tertiary—based \non relevance and availability, identical to the cluster naming \nin the user interface. These descriptions are filled sequentially, \nwith the ‘primary’ always representing the most relevant term. \nOne application of this API is in the stringApp Cytoscape plu- \ngin, which utilizes it to automatically overlay the assigned de- \nscriptions onto each network cluster after clustering is exe- \ncuted. \nNetwork and protein embeddings \nTo facilitate the use of STRING in machine learning applica- \ntions, users can now directly download precomputed ProtT5 \n( 39 ) sequence and cross-species protein network embeddings \nderived from STRING, for all eukaryotes, and utilize the em- \nbeddings on their own labeled datasets. The embeddings en- \ncode information from two aspects. Sequence embeddings can \ncapture protein domains and shorter sequence motifs, while \nnetwork embeddings can complement these with information \non cellular organization such as protein complexes and path- \nways that may not be evident from sequence data alone. \nWhile protein sequence embeddings are inherently compa- \nrable across species due to the universal nature of amino acid \nsequences, the primary obstacle to using network embeddings \nis ensuring that network embeddings from different species \nare directly comparable. This challenge arises because pro- \ntein networks are independent for each species, and tradi- \ntional network embedding methods are not designed to ad- \ndress cross-species comparability. The lack of comparability \nbetween network embeddings prevents the effective transfer \nof knowledge and findings across different species. To address \nthis, we developed a method to align eukaryotic network em- \nbeddings across species using orthologous relationships based \non FedCoder ( 40 ). This alignment technique creates a uni- \nfied embedding space where proteins from different eukary- \notic species can be directly compared. As a result, it enhances \ncross-species protein predictions, particularly in tasks such as \nsubcellular localization and function prediction. \nIn the protein machine learning field, having precomputed \nsequence embeddings and cross-species compatible network \nembeddings from STRING enables researchers to use these re- \nsources directly, eliminating the need to calculate embeddings \nthemselves. This strategy not only reduces the energy footprint \nresulting from redundant calculations across research groups, \nbut also enhances reproducibility in computational biology re- \nsearch promoting more sustainable and consistent scientific \npractices. \nData availability \nThe STRING database is freely available online at https:// \nstring-db.org/. \nA c kno wledg ements \nThe authors wish to thank Yan P. Yuan (EMBL Heidelberg) \nand Dandan Xue (University of Copenhagen) for IT support. \nThomas Rattei (University of Vienna) is thanked for extensive \ncomputational work toward resolving orthology relations. We \nthank the CSC—IT Center for Science, Finland, for generous \ncomputational resources. \nFunding \nSwiss Institute of Bioinformatics; Novo Nordisk Founda- \ntion [NNF14CC0001, NNF20SA0035590]; European Molec- \nular Biology Laboratory (EMBL Heidelberg); HORIZON \nEUROPE Marie Skłodowska-Curie Actions [101023676 to \nK.N.]; Academy of Finland [332844 to F.M. and S.P.]. Fund- \ning for open access charge: University of Zurich. \nConflict of interest statement \nNone declared. \nReferences \n1. Marsh, J.A. and Teichmann, S.A. (2015) Structure, dynamics, \nassembly, and evolution of protein complexes. Annu. Rev. \nBiochem., 84 , 551–575.\n2. Cesnik, A. , Schaffer, L.V. , Gaur, I. , Jain, M. , Ideker, T. and \nLundberg,E. (2024) Mapping the multiscale proteomic \norganization of cellular and disease phenotypes. Annu. Rev. \nBiomed. Data Sci., 7 , 369–389.\n3. Milacic, M. , Beavers, D. , Conley, P. , Gong, C. , Gillespie, M. , Griss, J. , \nHaw, R. , Jassal, B. , Matthews, L. , May, B. , et al. (2024) The \nReactome Pathway Knowledgebase 2024. Nucleic Acids Res., 52 , \nD672–D678.\n4. Kanehisa, M. , Furumichi, M. , Sato, Y. , Kawashima, M. and \nIshiguro-Watanabe,M. (2023) KEGG for taxonomy-based analysis \nof pathways and genomes. Nucleic Acids Res. , 51 , D587–D592. \n5. Oughtred, R. , Stark, C. , Breitkreutz, B.-J. , Rust, J. , Boucher, L. , \nChang, C. , Kolas, N. , O’Donnell, L. , Leung, G. , McAdam, R. , et al. \n(2019) The BioGRID interaction database: 2019 update. Nucleic \nAcids Res., 47 , D529–D541.\n6. Del Toro, N. , Shrivastava, A. , Ragueneau, E. , Meldal, B. , Combe, C. , \nBarrera, E. , Perfetto, L. , How, K. , Ratan, P. , Shirodkar, G. , et al. \n(2022) The IntAct database: efficient access to fine-grained \nmolecular interaction data. Nucleic Acids Res. , 50 , D648–D653. \n7. Licata, L. , Briganti, L. , Peluso, D. , Perfetto, L. , Iannuccelli, M. , \nGaleota, E. , Sacco, F. , Palma, A. , Nardozza, A.P. , Santonico, E. , et al. \n(2012) MINT, the molecular interaction database: 2012 update. \nNucleic Acids Res., 40 , D857–D861.\n8. Meldal, B.H.M. , Perfetto, L. , Combe, C. , Lubiana, T. , Ferreira \nCavalcante, J.V. , Bye-A-Jee, H. , Waagmeester, A. , Del-Toro, N. , \nShrivastava, A. , Barrera, E. , et al. (2022) Complex Portal 2022: \nnew curation frontiers. Nucleic Acids Res. , 50 , D578–D586. \n9. Lo Surdo, P. , Iannuccelli, M. , Contino, S. , Castagnoli, L. , Licata, L. , \nCesareni, G. and Perfetto, L. (2023) SIGNOR 3.0, the SIGnaling \nnetwork open resource 3.0: 2022 update. Nucleic Acids Res., 51 , \nD631–D637.\n10. Szklarczyk, D. , Kirsch, R. , Koutrouli, M. , Nastou, K. , Mehryary, F. , \nHachilif, R. , Gable, A.L. , Fang, T. , Doncheva, N.T. , Pyysalo, S. , et al. \n(2023) The STRING database in 2023: protein–protein \nassociation networks and functional enrichment analyses for any \nsequenced genome of interest. Nucleic Acids Res. , 51 , D638–D646. \n11. Warde-Farley, D. , Donaldson, S.L. , Comes, O. , Zuberi, K. , \nBadrawi, R. , Chao, P. , Franz, M. , Grouios, C. , Kazi, F. , Lopes, C.T. , \net al. (2010) The GeneMANIA prediction server: biological \nnetwork integration for gene prioritization and predicting gene \nfunction. Nucleic Acids Res. , 38 , W214–W220. \n12. Persson, E. , Castresana-Aguirre, M. , Buzzao, D. , Guala, D. and \nSonnhammer,E.L.L. (2021) FunCoup 5: functional association \nDownloaded from https://academic.oup.com/nar/article/53/D1/D730/7903368 by guest on 11 January 2026", "metadata": {"producer": "Acrobat Distiller 24.0 (Windows); modified using iTextSharp 5.5.10 ©2000-2016 iText Group NV (AGPL-version); modified using iTextSharp.LGPLv2.Core 3.7.4.0", "creator": "OUP", "creationdate": "2024-12-30T12:59:32+05:30", "source": "/home/donbr/open-biosciences/biosciences-research/data/raw/gkae1113.pdf", "file_path": "/home/donbr/open-biosciences/biosciences-research/data/raw/gkae1113.pdf", "total_pages": 8, "format": "PDF 1.4", "title": "The STRING database in 2025: protein networks with directionality of regulation", "author": "Szklarczyk Damian, Nastou Katerina, Koutrouli Mikaela, Kirsch Rebecca, Mehryary Farrokh, Hachilif Radja, Hu Dewei, Peluso Matteo E., Huang Qingyao, Fang Tao, Doncheva Nadezhda T., Pyysalo Sampo, Bork Peer, Jensen Lars J., vonMering Christian", "subject": "DOI: 10.1093/nar/gkae1113 , 53, Database issue, 18-11-2024. Abstract: Proteins cooperate, regulate and bind each other to achieve their functions. Understanding the complex network of their interactions is essential for a systems-level description of cellular processes. The STRING database compiles, scores and integrates protein&ndash;protein association information drawn from experimental assays, computational predictions and prior knowledge. Its goal is to create comprehensive and objective global networks that encompass both physical and functional interactions. Additionally, STRING provides supplementary tools such as network clustering and pathway enrichment analysis. The latest version, STRING 12.5, introduces a new &lsquo;regulatory network&rsquo;, for which it gathers evidence on the type and directionality of interactions using curated pathway databases and a fine-tuned language model parsing the literature. This update enables users to visualize and access three distinct network types&mdash;functional, physical and regulatory&mdash;separately, each applicable to distinct research needs. In addition, the pathway enrichment detection functionality has been updated, with better false discovery rate corrections, redundancy filtering&nbsp;and improved visual displays. The resource now also offers improved annotations of clustered networks and provides users with downloadable network embeddings, which facilitate the use of STRING networks in machine learning and allow cross-species transfer of protein information. The STRING database is available online at https:&sol;&sol;string-db.org&sol;.", "keywords": "", "moddate": "2026-01-11T01:02:57+00:00", "trapped": "", "modDate": "D:20260111010257+00'00'", "creationDate": "D:20241230125932+05'30'", "page": 6}}
{"page_content": "Nucleic Acids Research , 2025, Vol. 53, Database issue \nD 737 \nnetworks in all domains of life, supporting directed links and \ntissue-specificity. J. Mol. Biol., 433 , 166835.\n13. Kim, C.Y. , Baek, S. , Cha, J. , Yang, S. , Kim, E. , Marcotte, E.M. , Hart, T. \nand Lee,I. (2022) HumanNet v3: an improved database of human \ngene networks for disease research. Nucleic Acids Res., 50 , \nD632–D639.\n14. Huang, J.K. , Carlin, D.E. , Yu, M.K. , Zhang, W. , Kreisberg, J.F. , \nTamayo, P. and Ideker, T. (2018) Systematic evaluation of molecular \nnetworks for discovery of disease genes. Cell Syst. , 6 , 484–495.e5. \n15. Wright, S.N. , Colton, S. , Schaffer, L.V. , Pillich, R.T. , Churas, C. , \nPratt, D. and Ideker, T. (2024) State of the interactomes: an \nevaluation of molecular networks for generating biological \ninsights. bioRxiv doi: \nhttps:// doi.org/ 10.1101/ 2024.04.26.587073 , 29 April 2024, \npreprint: not peer reviewed.\n16. Ontology Consortium, G. , Aleksander, S.A. , Balhoff, J. , Carbon, S. , \nCherry, J.M. , Drabkin, H.J. , Ebert, D. , Feuermann, M. , Gaudet, P. , \nHarris, N.L. , et al. (2023) The Gene Ontology knowledgebase in \n2023. Genetics , 224 , iyad031.\n17. Liberzon, A. , Birger, C. , Thorvaldsdóttir, H. , Ghandi, M. , \nMesirov, J.P. and Tamayo, P. (2015) The Molecular Signatures \nDatabase hallmark gene set collection. Cell Syst. , 1 , 417–425. \n18. Amberger, J.S. , Bocchini, C.A. , Schiettecatte, F. , Scott, A.F. and \nHamosh,A. (2015) OMIM.org: Online Mendelian Inheritance in \nMan (OMIM ®), an online catalog of human genes and genetic \ndisorders. Nucleic Acids Res. , 43 , D789–D798. \n19. Snel, B. , Bork, P. and Huynen, M.A. (2002) The identification of \nfunctional modules from the genomic association of genes. Proc. \nNatl Acad. Sci. U.S.A., 99 , 5890–5895.\n20. Guala, D. , Ogris, C. , Müller, N. and Sonnhammer, E.L.L. (2020) \nGenome-wide functional association networks: background, data \n& state-of-the-art resources. Brief. Bioinform., 21 , 1224.\n21. Szklarczyk, D. , Gable, A.L. , Nastou, K.C. , Lyon, D. , Kirsch, R. , \nPyysalo, S. , Doncheva, N.T. , Legeay, M. , Fang, T. , Bork, P. , et al. \n(2021) The STRING database in 2021: customizable \nprotein–protein networks, and functional characterization of \nuser-uploaded gene / measurement sets. Nucleic Acids Res., 49 , \nD605–D612.\n22. Mering,C.V. (2003) STRING: a database of predicted functional \nassociations between proteins. Nucleic Acids Res. , 31 , 258–261. \n23. Enright, A.J. , Iliopoulos, I. , Kyrpides, N.C. and Ouzounis, C.A. \n(1999) Protein interaction maps for complete genomes based on \ngene fusion events. Nature , 402 , 86–90.\n24. Franceschini, A. , Lin, J. , von Mering, C. and Jensen, L.J. (2016) \nSVD-phy: improved prediction of protein functional associations \nthrough singular value decomposition of phylogenetic profiles. \nBioinformatics , 32 , 1085–1087.\n25. Koutrouli, M. , Nastou, K. , Piera Líndez, P. , Bouwmeester, R. , \nRasmussen, S. , Martens, L. and Jensen, L.J. (2024) FAVA: \nhigh-quality functional association networks inferred from \nscRNA-seq and proteomics data. Bioinformatics , 40 , btae010.\n26. Porras, P. , Barrera, E. , Bridge, A. , Del-Toro, N. , Cesareni, G. , \nDuesbury, M. , Hermjakob, H. , Iannuccelli, M. , Jurisica, I. , \nKotlyar, M. , et al. (2020) Towards a unified open access dataset of \nmolecular interactions. Nat. Commun., 11 , 6144.\n27. Franceschini, A. , Szklarczyk, D. , Frankild, S. , Kuhn, M. , \nSimonovic, M. , Roth, A. , Lin, J. , Minguez, P. , Bork, P. , von Mering, C. , \net al. (2013) STRING v9.1: protein–protein interaction networks, \nwith increased coverage and integration. Nucleic Acids Res., 41 , \nD808–D815.\n28. Yu, H. , Luscombe, N.M. , Lu, H.X. , Zhu, X. , Xia, Y. , Han, J .-D.J ., \nBertin, N. , Chung, S. , V idal, M. and Gerstein, M. (2004) Annotation \ntransfer between genomes: protein–protein interologs and \nprotein–DNA regulogs. Genome Res. , 14 , 1107–1118. \n29. Szklarczyk, D. , Gable, A.L. , Lyon, D. , Junge, A. , Wyder, S. , \nHuerta-Cepas, J. , Simonovic, M. , Doncheva, N.T. , Morris, J.H. , \nBork, P. , et al. (2019) STRING v11: protein–protein association \nnetworks with increased coverage, supporting functional discovery \nin genome-wide experimental datasets. Nucleic Acids Res., 47 , \nD607–D613.\n30. Doncheva, N.T. , Morris, J.H. , Gorodkin, J. and Jensen, L.J. (2019) \nCytoscape StringApp: network analysis and visualization of \nproteomics data. J. Proteome Res., 18 , 623–632.\n31. Jensen, L.J. , Kuhn, M. , Stark, M. , Chaffron, S. , Creevey, C. , Muller, J. , \nDoerks, T. , Julien, P. , Roth, A. , Simonovic, M. , et al. (2009) STRING \n8—a global view on proteins and their functional interactions in \n630 organisms. Nucleic Acids Res. , 37 , D412–D416. \n32. Nastou, K. , Mehryary, F. , Ohta, T. , Luoma, J. , Pyysalo, S. and \nJensen,L.J. (2024) RegulaTome: a corpus of typed, directed, and \nsigned relations between biomedical entities in the scientific \nliterature. Database , 2024 , baae095.\n33. Megill, C. , Martin, B. , Weaver, C. , Bell, S. , Prins, L. , Badajoz, S. , \nMcCandless, B. , Pisco, A.O. , Kinsella, M. , Griffin, F. , et al. (2021) \ncellxgene: a performant, scalable exploration platform for high \ndimensional sparse matrices. bioRxiv doi: \nhttps:// doi.org/ 10.1101/ 2021.04.05.438318 , 6 April 2021, \npreprint: not peer reviewed.\n34. Moreno, P. , Fexova, S. , George, N. , Manning, J.R. , Miao, Z. , \nMohammed, S. , Muñoz-Pomer, A. , Fullgrabe, A. , Bi, Y. , Bush, N. , \net al. (2022) Expression Atlas update: gene and protein expression \nin multiple species. Nucleic Acids Res. , 50 , D129–D140. \n35. T immons, J.A. , Szkop, K.J. and Gallagher, I.J. (2015) Multiple \nsources of bias confound functional enrichment analysis of global \n-omics data. Genome Biol. , 16 , 186. \n36. Benjamini, Y. and Hochberg, Y. (1995) Controlling the false \ndiscovery rate: a practical and powerful approach to multiple \ntesting. J. R. Stat. Soc. Ser. B Stat. Methodol., 57 , 289–300.\n37. Novikov,A. (2019) PyClustering: data mining library. J. Open \nSource Softw., 4 , 1230.\n38. Van Dongen,S. (2008) Graph clustering via a discrete uncoupling \nprocess. SIAM J. Matrix Anal. Appl., 30 , 121–141.\n39. Elnaggar, A. , Heinzinger, M. , Dallago, C. , Rehawi, G. , Wang, Y. , \nJones, L. , Gibbs, T. , Feher, T. , Angerer, C. , Steinegger, M. , et al. \n(2022) ProtTrans: toward understanding the language of life \nthrough self-supervised learning. IEEE Trans. Pattern Anal. Mach. \nIntell., 44 , 7112–7127.\n40. Baumgartner, M. , Dell’Aglio, D. , Paulheim, H. and Bernstein, A. \n(2023) Towards the Web of Embeddings: integrating multiple \nknowledge graph embedding spaces with FedCoder. J. Web \nSemant., 75 , 100741.\nReceived: September 15, 2024. Revised: October 18, 2024. Editorial Decision: October 21, 2024. Accepted: October 29, 2024 \n© The Author(s) 2024. Published by Oxford University Press on behalf of Nucleic Acids Research. \nThis is an Open Access article distributed under the terms of the Creative Commons Attribution License (https: // creativecommons.org / licenses / by / 4.0 / ), which permits unrestricted reuse, \ndistribution, and reproduction in any medium, provided the original work is properly cited. \nDownloaded from https://academic.oup.com/nar/article/53/D1/D730/7903368 by guest on 11 January 2026", "metadata": {"producer": "Acrobat Distiller 24.0 (Windows); modified using iTextSharp 5.5.10 ©2000-2016 iText Group NV (AGPL-version); modified using iTextSharp.LGPLv2.Core 3.7.4.0", "creator": "OUP", "creationdate": "2024-12-30T12:59:32+05:30", "source": "/home/donbr/open-biosciences/biosciences-research/data/raw/gkae1113.pdf", "file_path": "/home/donbr/open-biosciences/biosciences-research/data/raw/gkae1113.pdf", "total_pages": 8, "format": "PDF 1.4", "title": "The STRING database in 2025: protein networks with directionality of regulation", "author": "Szklarczyk Damian, Nastou Katerina, Koutrouli Mikaela, Kirsch Rebecca, Mehryary Farrokh, Hachilif Radja, Hu Dewei, Peluso Matteo E., Huang Qingyao, Fang Tao, Doncheva Nadezhda T., Pyysalo Sampo, Bork Peer, Jensen Lars J., vonMering Christian", "subject": "DOI: 10.1093/nar/gkae1113 , 53, Database issue, 18-11-2024. Abstract: Proteins cooperate, regulate and bind each other to achieve their functions. Understanding the complex network of their interactions is essential for a systems-level description of cellular processes. The STRING database compiles, scores and integrates protein&ndash;protein association information drawn from experimental assays, computational predictions and prior knowledge. Its goal is to create comprehensive and objective global networks that encompass both physical and functional interactions. Additionally, STRING provides supplementary tools such as network clustering and pathway enrichment analysis. The latest version, STRING 12.5, introduces a new &lsquo;regulatory network&rsquo;, for which it gathers evidence on the type and directionality of interactions using curated pathway databases and a fine-tuned language model parsing the literature. This update enables users to visualize and access three distinct network types&mdash;functional, physical and regulatory&mdash;separately, each applicable to distinct research needs. In addition, the pathway enrichment detection functionality has been updated, with better false discovery rate corrections, redundancy filtering&nbsp;and improved visual displays. The resource now also offers improved annotations of clustered networks and provides users with downloadable network embeddings, which facilitate the use of STRING networks in machine learning and allow cross-species transfer of protein information. The STRING database is available online at https:&sol;&sol;string-db.org&sol;.", "keywords": "", "moddate": "2026-01-11T01:02:57+00:00", "trapped": "", "modDate": "D:20260111010257+00'00'", "creationDate": "D:20241230125932+05'30'", "page": 7}}
{"page_content": "Research and Applications\nImproving large language model applications in \nbiomedicine with retrieval-augmented generation: a \nsystematic review, meta-analysis, and clinical \ndevelopment guidelines\nSiru Liu, PhD\u0003,1,2, Allison B. McCoy \n, PhD1, Adam Wright \n, PhD1,3 \n1Department of Biomedical Informatics, Vanderbilt University Medical Center, Nashville, TN 37212, United States, 2Department of Computer \nScience, Vanderbilt University, Nashville, TN 37212, United States, 3Department of Medicine, Vanderbilt University Medical Center, \nNashville, TN 37212, United States\n\u0003Corresponding author: Siru Liu, PhD, Department of Biomedical Informatics, Vanderbilt University Medical Center, 2525 West End Ave #1475, Nashville, \nTN 37212, United States (siru.liu@vumc.org)\nAbstract\nObjective: The objectives of this study are to synthesize findings from recent research of retrieval-augmented generation (RAG) and large lan­\nguage models (LLMs) in biomedicine and provide clinical development guidelines to improve effectiveness.\nMaterials and Methods: We conducted a systematic literature review and a meta-analysis. The report was created in adherence to the Pre­\nferred Reporting Items for Systematic Reviews and Meta-Analyses 2020 analysis. Searches were performed in 3 databases (PubMed, Embase, \nPsycINFO) using terms related to “retrieval augmented generation” and “large language model,” for articles published in 2023 and 2024. We \nselected studies that compared baseline LLM performance with RAG performance. We developed a random-effect meta-analysis model, using \nodds ratio as the effect size.\nResults: Among 335 studies, 20 were included in this literature review. The pooled effect size was 1.35, with a 95% confidence interval of \n1.19-1.53, indicating a statistically significant effect (P ¼ .001). We reported clinical tasks, baseline LLMs, retrieval sources and strategies, as \nwell as evaluation methods.\nDiscussion: Building on our literature review, we developed Guidelines for Unified Implementation and Development of Enhanced LLM Appli­\ncations with RAG in Clinical Settings to inform clinical applications using RAG.\nConclusion: Overall, RAG implementation showed a 1.35 odds ratio increase in performance compared to baseline LLMs. Future research \nshould focus on (1) system-level enhancement: the combination of RAG and agent, (2) knowledge-level enhancement: deep integration of \nknowledge into LLM, and (3) integration-level enhancement: integrating RAG systems within electronic health records.\nKey words: large language model; retrieval augmented generation; systematic review; meta-analysis. \nIntroduction\nLarge language models (LLMs) have reported remarkable \nperformance in question-answering, summarization, and text \ngeneration.1 Given this, researchers have explored its poten­\ntial in biomedical areas.2 For example, several studies \nreported the ability of using LLM to answer patient mes­\nsages,3 to analyze alert logic in clinical decision support,4 and \nto make discharge summaries more readable to patients.5\nHowever, several challenges remain.\nLLMs are trained on fixed datasets, which restrict their \nknowledge to information available up to the training cut-off \ndate. For example, GPT-4o’s training data only includes \ninformation up to October 2023, making it unable to \nrespond accurately to findings that emerged afterward. LLM \ntraining datasets are also generally broad and lack the specif­\nicity required for biomedical applications. Finally, not all \nsources used to train the LLMs are reliable and trustworthy. \nTo address these limitations, researchers have performed \nfine-tuning and retrieval-augmented generation (RAG) tech­\nniques. Fine-tuning can adapt LLMs to specific domains, but \nit is resource-intensive and does not allow for real-time \nupdates. In contrast, RAG maintains the original LLM archi­\ntecture while incorporating relevant context directly into \nqueries, offering more flexibility and control. In addition, \nRAG’s unique advantage in biomedical applications lies in its \nability to adapt to dynamic environments by delivering up-to- \ndate information and efficiently integrating external knowl­\nedge sources with high interpretability.6\nAnother limitation of using LLMs directly is the risk of hal­\nlucination, where the model generates incorrect or fabricated \ninformation.7 To mitigate such issues, researchers have pro­\nposed RAG as a solution that integrates up-to-date, relevant \ninformation, enhancing both the accuracy and reliability of \nLLM generated responses.8,9 For example, when ChatGPT \nReceived: November 19, 2024; Revised: December 17, 2024; Editorial Decision: January 2, 2025; Accepted: January 3, 2025 \n© The Author(s) 2025. Published by Oxford University Press on behalf of the American Medical Informatics Association.  \nThis is an Open Access article distributed under the terms of the Creative Commons Attribution-NonCommercial License (https://creativecommons.org/licenses/ \nby-nc/4.0/), which permits non-commercial re-use, distribution, and reproduction in any medium, provided the original work is properly cited. For commercial \nre-use, please contact reprints@oup.com for reprints and translation rights for reprints. All other permissions can be obtained through our RightsLink service via \nthe Permissions link on the article page on our site—for further information please contact journals.permissions@oup.com. \nJournal of the American Medical Informatics Association, 2025, 32(4), 605–615 \nhttps://doi.org/10.1093/jamia/ocaf008 \nAdvance access publication 15 January 2025 \nResearch and Applications \nDownloaded from https://academic.oup.com/jamia/article/32/4/605/7954485 by guest on 11 January 2026", "metadata": {"producer": "PDFlib+PDI 9.0.7p3 (C++/Win32); modified using iTextSharp.LGPLv2.Core 3.7.4.0", "creator": "Servigistics Arbortext Advanced Print Publisher 11.1.4667/W", "creationdate": "2025-03-22T11:01:16+05:30", "source": "/home/donbr/open-biosciences/biosciences-research/data/raw/ocaf008.pdf", "file_path": "/home/donbr/open-biosciences/biosciences-research/data/raw/ocaf008.pdf", "total_pages": 11, "format": "PDF 1.5", "title": "Improving large language model applications in biomedicine with retrieval-augmented generation: a systematic review, meta-analysis, and clinical development guidelines", "author": "", "subject": "Doi: 10.1093/jamia/ocaf008 Journal of the American Medical Informatics Association, 32, 4, 2025 Publication Date: 15/01/2025 Abstract Objective The objectives of this study are to synthesize findings from recent research of retrieval-augmented generation (RAG) and large language models (LLMs) in biomedicine and provide clinical development guidelines to improve effectiveness.Materials and Methods We conducted a systematic literature review and a meta-analysis. The report was created in adherence to the Preferred Reporting Items for Systematic Reviews and Meta-Analyses 2020 analysis. Searches were performed in 3 databases (PubMed, Embase, PsycINFO) using terms related to “retrieval augmented generation” and “large language model,” for articles published in 2023 and 2024. We selected studies that compared baseline LLM performance with RAG performance. We developed a random-effect meta-analysis model, using odds ratio as the effect size.Results Among 335 studies, 20 were included in this literature review. The pooled effect size was 1.35, with a 95% confidence interval of 1.19-1.53, indicating a statistically significant effect (P = .001). We reported clinical tasks, baseline LLMs, retrieval sources and strategies, as well as evaluation methods.Discussion Building on our literature review, we developed Guidelines for Unified Implementation and Development of Enhanced LLM Applications with RAG in Clinical Settings to inform clinical applications using RAG.Conclusion Overall, RAG implementation showed a 1.35 odds ratio increase in performance compared to baseline LLMs. Future research should focus on (1) system-level enhancement: the combination of RAG and agent, (2) knowledge-level enhancement: deep integration of knowledge into LLM, and (3) integration-level enhancement: integrating RAG systems within electronic health records.", "keywords": "large language model; retrieval augmented generation; systematic review; meta-analysis", "moddate": "2026-01-11T01:13:27+00:00", "trapped": "", "modDate": "D:20260111011327+00'00'", "creationDate": "D:20250322110116+05'30'", "page": 0}}
{"page_content": "was asked about medications for peripheral artery disease \npatients without increased bleeding risk, it initially omitted \nlow-dose rivaroxaban. After integrating retrieved text from \nthe 2024 American College of Cardiology / American Heart \nAssociation Guideline for the Management of Lower Extrem­\nity Peripheral Artery Disease,10 the model correctly recom­\nmended rivaroxaban.\nSeveral guidelines exist for evaluating Artificial Intelligence \n(AI) applications and LLMs in healthcare, including \nDECIDE-AI (Developmental and Exploratory Clinical Inves­\ntigations of DEcision support systems driven by Artificial \nIntelligence),11 CLAIM (Checklist for Artificial Intelligence in \nMedical Imaging),12\nand CONSORT-AI (Consolidated \nStandards of Reporting Trials-AI).13 In addition, Tam et al. \nintroduced QUEST, a framework specifically for human eval­\nuation of LLMs in healthcare.14 However, these guidelines \ndo not cover RAG applications in clinical settings, emphasiz­\ning the need for a more specific guideline.\nDespite the promise of RAG in improving LLM performance \nin clinical settings, there is limited understanding of its overall \neffectiveness comparing with the baseline LLM, adoption in \nclinical domains, and optimal strategies for its development in \nbiomedical applications. The aim of this study is to synthesize \nfindings from recent research of RAG and LLM in biomedicine \nand provide clinical development guidelines to improve effec­\ntiveness as well as transparency in future research.\nMaterials and methods\nStudy design\nWe conducted a systematic literature review. The report was \ncreated in adherence to the Preferred Reporting Items for Sys­\ntematic Reviews and Meta-Analyses 2020 analysis checklist \n(File S1).15 We also performed a meta-analysis following the \nCochrane Handbook guidelines.16\nScreening papers\nWe searched in 3 databases (PubMed, Embase, and Psy­\ncINFO), using terms related to “retrieval augmented gener­\nation” and “large language model.” PubMed and Embase \nwere selected based on recommendations from the Cochrane \nHandbook, as they are widely recognized for their extensive \ncoverage of biomedical literature.17 PsycINFO was included \nto capture articles from the psychological and behavioral sci­\nences. To maintain a high standard of quality and reliability, \nwe focused on peer-reviewed articles and excluded preprints. \nThe specific search terms used for each database are provided \nin File S2. Given that ChatGPT was released on November \n30, 2022, we set the publication filter to search papers pub­\nlished in 2023 and 2024. The search was performed on \nDecember 12, 2024. The inclusion criteria were: (1) the study \nmust compare baseline LLM performance with RAG per­\nformance and (2) the study must address a biomedical ques­\ntion. The exclusion criteria were: (1) literature reviews, \neditorial comments, or viewpoint papers, (2) studies focusing \non LLMs in languages other than English, or (3) studies cen­\ntered on a multi-agent system without a focus on RAG. SL \nscreened titles and abstracts, then conducted a full-text \nreview of papers meeting the criteria.\nData extraction\nFor each included study, we extracted the following informa­\ntion: author, title, publication year, journal, clinical task, and \nspecialty. Regarding RAG techniques, we gathered details \nabout the baseline LLM, retrieval sources, and strategies used \nin the pre-retrieval, retrieval, and post-retrieval stages. For \nevaluation, we extracted the evaluation method (human, \nautomated, or a combination of both), the number of evalua­\ntors, the evaluation dataset, and the evaluation metrics.\nMeta-analysis\nEffect size was defined as a metric quantifying the relation­\nship between variables, including both direction and magni­\ntude.18 For each included study, we calculated the effect size \nbetween baseline LLM performance and RAG-enhanced \nLLM performance. The outcomes focused on the perform­\nance of generation results, such as accuracy and usefulness. \nMetrics related to the retrieval process, cost, or speed were \nnot included as outcomes in the meta-analysis. For continu­\nous outcomes, we used Cohen’s d, standardized between- \ngroup mean difference (SMD), calculated as the difference in \nmeans divided by the pooled standard deviation. The stand­\nard error (SE) of SMD was calculated using the following for­\nmula (1), where n1 and n2 represent the sample sizes of each \ngroup.19 For dichotomous measurements, we calculated the \nlog-odds ratio, obtained by transforming the odds ratio (OR) \nwith the natural logarithm, and the associated SE was calcu­\nlated using formula (2), where a, b, c, and d represent the \nnumber of successful and failed events in the baseline LLM \nand RAG-enhanced LLM approaches. For studies reporting \nmultiple outcomes, we used the overall outcome to calculate \neffect size. If no overall outcome was reported, we averaged \nthe effect sizes of all reported outcomes. We excluded out­\ncomes with a sample size of less than 30 to avoid small- \nsample bias. \nSESMD ¼\nffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi\nn1 þ n2\nn1n2\nþ\nSMD2\n2 n1 þ n2\nð\nÞ\ns\n(1) \nSElogOR ¼\nffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi\n1\na þ 1\nb þ 1\nc þ 1\nd\nr\n(2) \nWe developed a random-effect meta-analysis model, \nbecause of the variability in RAG architectures and evalua­\ntion datasets among the included studies. The random-effect \nmodel was used when individual study effects likely con­\ntained additional sources of variance beyond sampling error. \nBetween-study heterogeneity was assessed using Higgins & \nThompson’s I2 statistic, where 25% indicated low heteroge­\nneity, 50% moderate, and 75% substantial.20\nWe conducted subgroup analyses to explore performance \nvariations across different factors. First, we analyzed the \ninfluence of the baseline LLM, referring to the foundation \nmodel (eg, GPT-4 or Llama2) that provides the core architec­\nture for the system. Second, we examined data retrieval strat­\negies, categorizing them as simple or complex. Simple \nstrategies included fixed-length chunking and basic similarity \nsearch, and we performed a subgroup analysis to compare \nthese with complex retrieval strategies. Third, we analyzed \ndifferences based on evaluation methods, distinguishing \nbetween human evaluations, such as Likert scale ratings for \nhelpfulness and accuracy, and automatic evaluation metrics, \nincluding ROUGE-1 and BLEU. Finally, we conducted a sub­\ngroup analysis based on the type of task, classifying studies \n606                                                                                                      \nJournal of the American Medical Informatics Association, 2025, Vol. 32, No. 4 \nDownloaded from https://academic.oup.com/jamia/article/32/4/605/7954485 by guest on 11 January 2026", "metadata": {"producer": "PDFlib+PDI 9.0.7p3 (C++/Win32); modified using iTextSharp.LGPLv2.Core 3.7.4.0", "creator": "Servigistics Arbortext Advanced Print Publisher 11.1.4667/W", "creationdate": "2025-03-22T11:01:16+05:30", "source": "/home/donbr/open-biosciences/biosciences-research/data/raw/ocaf008.pdf", "file_path": "/home/donbr/open-biosciences/biosciences-research/data/raw/ocaf008.pdf", "total_pages": 11, "format": "PDF 1.5", "title": "Improving large language model applications in biomedicine with retrieval-augmented generation: a systematic review, meta-analysis, and clinical development guidelines", "author": "", "subject": "Doi: 10.1093/jamia/ocaf008 Journal of the American Medical Informatics Association, 32, 4, 2025 Publication Date: 15/01/2025 Abstract Objective The objectives of this study are to synthesize findings from recent research of retrieval-augmented generation (RAG) and large language models (LLMs) in biomedicine and provide clinical development guidelines to improve effectiveness.Materials and Methods We conducted a systematic literature review and a meta-analysis. The report was created in adherence to the Preferred Reporting Items for Systematic Reviews and Meta-Analyses 2020 analysis. Searches were performed in 3 databases (PubMed, Embase, PsycINFO) using terms related to “retrieval augmented generation” and “large language model,” for articles published in 2023 and 2024. We selected studies that compared baseline LLM performance with RAG performance. We developed a random-effect meta-analysis model, using odds ratio as the effect size.Results Among 335 studies, 20 were included in this literature review. The pooled effect size was 1.35, with a 95% confidence interval of 1.19-1.53, indicating a statistically significant effect (P = .001). We reported clinical tasks, baseline LLMs, retrieval sources and strategies, as well as evaluation methods.Discussion Building on our literature review, we developed Guidelines for Unified Implementation and Development of Enhanced LLM Applications with RAG in Clinical Settings to inform clinical applications using RAG.Conclusion Overall, RAG implementation showed a 1.35 odds ratio increase in performance compared to baseline LLMs. Future research should focus on (1) system-level enhancement: the combination of RAG and agent, (2) knowledge-level enhancement: deep integration of knowledge into LLM, and (3) integration-level enhancement: integrating RAG systems within electronic health records.", "keywords": "large language model; retrieval augmented generation; systematic review; meta-analysis", "moddate": "2026-01-11T01:13:27+00:00", "trapped": "", "modDate": "D:20260111011327+00'00'", "creationDate": "D:20250322110116+05'30'", "page": 1}}
{"page_content": "into \nclinical \ndecision-making \nand \nmedical \nquestion- \nanswering. These analyses provided insights into how varia­\ntions in model architecture, retrieval strategies, evaluation \nmethods, and task types affect system outcomes.\nTo visualize the meta-analysis outcomes, we generated a \nforest plot. This plot displayed the effect size, confidence inter­\nval for each study, as well as the pooled effect and predicted \neffect size. We evaluated the publication bias using a contour- \nenhanced funnel plot to investigate small-study effects. This \nscatter plot had the effect size on the x-axis and the inverted \nSE on the y-axis, with contours indicating P-values (<.1, .05, \nand .01).21 Symmetry in the funnel plot suggested no publica­\ntion bias, and asymmetry was quantified using Egger’s regres­\nsion test.22 We used the “meta” package in R to conduct the \nmeta-analysis and perform statistical analyses.\nResults\nStudy selection\nA total of 335 studies were identified from 3 databases: \nPubMed, Embase, and PsycINFO. After removing duplicates, \n251 studies were screened. Of these, 20 studies were included \nin this literature review, all of which were published in 2024. \nOne of the included studies was a conference paper.23 The \nflow diagram depicting the study selection process is shown \nin Figure 1. For each included study, their author, title, publi­\ncation year, journal, clinical task, specialty, and retrieval \nsources are listed in Table S1 of File S2.\nMeta-analysis\nThe pooled effect size was 1.35, with a 95% confidence inter­\nval of 1.19-1.53, indicating a statistically significant effect \n(P ¼ .001). All outcomes and associated SEs are listed in File \nS2. The I2 value was 37%, indicating low to moderate heter­\nogeneity among the studies. The prediction interval ranged \nfrom 1.01 to 1.8. The forest plot is shown in Figure 2. The \ncontour-enhanced funnel plot is presented in File S2. In \nEgger’s regression test, the intercept (β0) was 1.1, with a 95% \nconfidence interval of [0.56, 1.64] and a P-value of .001, \nindicating the presence of small-study effects and potential \npublication bias.\nFigure 1. Flow diagram of included studies illustrating the systematic review process. Out of 335 records identified from PubMed, Embase, and \nPsycINFO, 84 duplicates were removed, leaving 251 records screened. After excluding 200 records, 51 reports were assessed for eligibility. 31 reports \nwere excluded for reasons such as lack of RAG-LLM comparison, non-clinical focus, or commentary. Ultimately, 20 studies were included in the final \nreview. LLM, large language model; RAG, retrieval-augmented generation.\nJournal of the American Medical Informatics Association, 2025, Vol. 32, No. 4                                                                                                      \n607 \nDownloaded from https://academic.oup.com/jamia/article/32/4/605/7954485 by guest on 11 January 2026", "metadata": {"producer": "PDFlib+PDI 9.0.7p3 (C++/Win32); modified using iTextSharp.LGPLv2.Core 3.7.4.0", "creator": "Servigistics Arbortext Advanced Print Publisher 11.1.4667/W", "creationdate": "2025-03-22T11:01:16+05:30", "source": "/home/donbr/open-biosciences/biosciences-research/data/raw/ocaf008.pdf", "file_path": "/home/donbr/open-biosciences/biosciences-research/data/raw/ocaf008.pdf", "total_pages": 11, "format": "PDF 1.5", "title": "Improving large language model applications in biomedicine with retrieval-augmented generation: a systematic review, meta-analysis, and clinical development guidelines", "author": "", "subject": "Doi: 10.1093/jamia/ocaf008 Journal of the American Medical Informatics Association, 32, 4, 2025 Publication Date: 15/01/2025 Abstract Objective The objectives of this study are to synthesize findings from recent research of retrieval-augmented generation (RAG) and large language models (LLMs) in biomedicine and provide clinical development guidelines to improve effectiveness.Materials and Methods We conducted a systematic literature review and a meta-analysis. The report was created in adherence to the Preferred Reporting Items for Systematic Reviews and Meta-Analyses 2020 analysis. Searches were performed in 3 databases (PubMed, Embase, PsycINFO) using terms related to “retrieval augmented generation” and “large language model,” for articles published in 2023 and 2024. We selected studies that compared baseline LLM performance with RAG performance. We developed a random-effect meta-analysis model, using odds ratio as the effect size.Results Among 335 studies, 20 were included in this literature review. The pooled effect size was 1.35, with a 95% confidence interval of 1.19-1.53, indicating a statistically significant effect (P = .001). We reported clinical tasks, baseline LLMs, retrieval sources and strategies, as well as evaluation methods.Discussion Building on our literature review, we developed Guidelines for Unified Implementation and Development of Enhanced LLM Applications with RAG in Clinical Settings to inform clinical applications using RAG.Conclusion Overall, RAG implementation showed a 1.35 odds ratio increase in performance compared to baseline LLMs. Future research should focus on (1) system-level enhancement: the combination of RAG and agent, (2) knowledge-level enhancement: deep integration of knowledge into LLM, and (3) integration-level enhancement: integrating RAG systems within electronic health records.", "keywords": "large language model; retrieval augmented generation; systematic review; meta-analysis", "moddate": "2026-01-11T01:13:27+00:00", "trapped": "", "modDate": "D:20260111011327+00'00'", "creationDate": "D:20250322110116+05'30'", "page": 2}}
{"page_content": "Clinical applications of RAG\nRAG techniques have been applied across a broad range of \nmedical specialties, as shown in Table 1. These applications \ninclude clinical decision-making and medical question- \nanswering. In clinical decision making, RAG has supported \npersonalized treatment,23,24 emergency triage,25 and disease \nmanagement.26,27 For medical question-answering, RAG’s \ncapability has been explored to address complex treatment \nguidelines questions,28 as well as queries focused on specific \nareas, such as head and neck surgery-related questions,29 and \npatient questions regarding diabetes.30 In the subgroup anal­\nysis, 13 studies focused on clinical decision-making (OR \n1.46, 95% CI [1.16, 1.71]) and 7 studies focused on medical \nquestion-answering (OR 1.32, 95% CI [1.08, 1.63]), with no \nstatistically significant difference observed between these 2 \ngroups.\nBaseline LLMs\nThe baseline LLMs varied across studies, with GPT-4 being \nthe most common, used in 14 studies, (OR: 1.58, 95% CI: \n1.21-2.04). GPT-3.5, used in 6 studies, showed an OR of \n1.43 (95% CI: 1.06-1.93). Llama2 was applied in 5 studies \n(OR: 1.25, 95% CI: 1.08-1.44).\nRetrieval sources\nRetrieval sources were categorized as pre-stored documents \nand real-time online browsing. Regarding pre-stored docu­\nments, 6 studies used clinical guidelines, such as the Emer­\ngency Severity Index (ESI) Ver.3 Field Triage.25 Five studies \nused academic articles from sources like PubMed abstracts or \nfull texts, or document sets such as the Radiographics Top 10 \nReading List on Gastrointestinal Imaging.31 Three studies \nused specialized knowledge bases, including ChatENT, \nOncoKB, and RareDis Corpus, while one study employed a \ngeneral biomedical knowledge graph (Scalable Precision \nMedicine Open Knowledge Engine [SPOKE]). SPOKE \nTable 1. Distribution of RAG applications by medical specialty and task \ntype.\nMedical Specialty\nFrequency\nInternal medicine\n4\nGeneral medicine\n3\nOncology\n3\nEmergency medicine\n2\nGastroenterology\n2\nOtolaryngology\n1\nHepatology\n1\nRare diseases\n1\nOrthopedics\n1\nNeurology\n1\nOphthalmology\n1\nTask\nFrequency (%)\nClinical decision-making\n13 (65%)\nMedical question-answering\n7 (35%)\nFigure 2. A forest plot showing the odds ratio (OR) of total impacts of the RAG-enhanced system compared with the baseline LLM system in clinical \ntasks. The left panel lists individual studies (e.g., Zakka et al., Murugan et al., Long et al.) along with their corresponding ORs and 95% confidence \nintervals (CIs). The right panel visualizes these ORs, with square markers proportional to study weights and horizontal lines representing CIs. The overall \npooled OR is 1.35 (95% CI: 1.19–1.53), represented by a diamond at the bottom.\n608                                                                                                      \nJournal of the American Medical Informatics Association, 2025, Vol. 32, No. 4 \nDownloaded from https://academic.oup.com/jamia/article/32/4/605/7954485 by guest on 11 January 2026", "metadata": {"producer": "PDFlib+PDI 9.0.7p3 (C++/Win32); modified using iTextSharp.LGPLv2.Core 3.7.4.0", "creator": "Servigistics Arbortext Advanced Print Publisher 11.1.4667/W", "creationdate": "2025-03-22T11:01:16+05:30", "source": "/home/donbr/open-biosciences/biosciences-research/data/raw/ocaf008.pdf", "file_path": "/home/donbr/open-biosciences/biosciences-research/data/raw/ocaf008.pdf", "total_pages": 11, "format": "PDF 1.5", "title": "Improving large language model applications in biomedicine with retrieval-augmented generation: a systematic review, meta-analysis, and clinical development guidelines", "author": "", "subject": "Doi: 10.1093/jamia/ocaf008 Journal of the American Medical Informatics Association, 32, 4, 2025 Publication Date: 15/01/2025 Abstract Objective The objectives of this study are to synthesize findings from recent research of retrieval-augmented generation (RAG) and large language models (LLMs) in biomedicine and provide clinical development guidelines to improve effectiveness.Materials and Methods We conducted a systematic literature review and a meta-analysis. The report was created in adherence to the Preferred Reporting Items for Systematic Reviews and Meta-Analyses 2020 analysis. Searches were performed in 3 databases (PubMed, Embase, PsycINFO) using terms related to “retrieval augmented generation” and “large language model,” for articles published in 2023 and 2024. We selected studies that compared baseline LLM performance with RAG performance. We developed a random-effect meta-analysis model, using odds ratio as the effect size.Results Among 335 studies, 20 were included in this literature review. The pooled effect size was 1.35, with a 95% confidence interval of 1.19-1.53, indicating a statistically significant effect (P = .001). We reported clinical tasks, baseline LLMs, retrieval sources and strategies, as well as evaluation methods.Discussion Building on our literature review, we developed Guidelines for Unified Implementation and Development of Enhanced LLM Applications with RAG in Clinical Settings to inform clinical applications using RAG.Conclusion Overall, RAG implementation showed a 1.35 odds ratio increase in performance compared to baseline LLMs. Future research should focus on (1) system-level enhancement: the combination of RAG and agent, (2) knowledge-level enhancement: deep integration of knowledge into LLM, and (3) integration-level enhancement: integrating RAG systems within electronic health records.", "keywords": "large language model; retrieval augmented generation; systematic review; meta-analysis", "moddate": "2026-01-11T01:13:27+00:00", "trapped": "", "modDate": "D:20260111011327+00'00'", "creationDate": "D:20250322110116+05'30'", "page": 3}}
{"page_content": "integrates over 40 publicly available biomedical knowledge \nsources across separate domains, such as genes, proteins, \ndrugs, compounds, and diseases, along with their known \nrelationships.32 Two studies used textbooks, such as Harri­\nson's Principles of Internal Medicine, while 3 others utilized \nelectronic health record (EHR) data. Additionally, Zakka \net al. added over 500 markdown files from MDCalc to \nimprove clinical calculation capabilities in LLM.28 Two stud­\nies employed real-time online browsing to search academic \nsites, such as PubMed and UpToDate. The amount of \nretrieval resources varied across studies, ranging from a small \ndataset specific to 6 osteoarthritis guidelines to a large dataset \nof EHR data from 7 hospitals.\nRetrieval strategies\nIdentified retrieval strategies were grouped based on the RAG \nstages: pre-retrieval, retrieval, and post-retrieval. Figure 3 \npresents an example of how RAG is applied and lists identi­\nfied strategies within each stage.\nIn the pre-retrieval stage, 50% of studies (n ¼ 10) reported \nstrategies, such as query rewriting, document preprocessing, \nand assessing the necessity of retrieval. Zakka et al. simplified \nqueries by rephrasing text into search terms that are better \nsuited for website browsing,28 while Wang et al. focused on \ntechniques such as correcting errors, expanding abbrevia­\ntions, and matching synonyms in user queries.30 Soman et al. \nextracted disease entities in queries and retrieved correspond­\ning nodes from a knowledge graph.33 Document preprocess­\ning involved removing non-textual elements from PMC \npapers (eg, figures, references, and author disclosures),30\nextracted tables from PDFs using pdfplumber, structured the \ncontent with pydantic for seamless integration.25 In addition \nto query modification and document preprocessing, Jeong \net al. fine-tuned a model to determine whether retrieval was \nnecessary for a given query.34\nDuring the data retrieval stage, 85% of studies (n ¼ 17) \nreported strategies regarding indexing, aligning queries with \ndocuments, and ranking retrieval chunks. Chunking methods \nranged from fixed-size chunks35 to recursive splits.36 Embed­\nding models such as Text-embedding-ada-002,24,28–30,36,37\nMiniLM, and PubMedBERT33 were commonly used to con­\nvert sentences into vectors. Cosine similarity was the primary \nmetric for measuring query-document alignment. Two stud­\nies adopted Maximal Marginal Relevance for search and \nhighlighted its improved performance over similarity-based \nmethods.24,35 A domain-specific retriever, MedCPT, was \nused in one study.34 Another study used the multi-vector \nretriever that leveraged summarized document sections to \nidentify the original content for final answer generation.25\nThe retrieval cutoff parameters varied widely, with probabil­\nity thresholds up to 0.83 and the number of retrieved chunks \nranging from 3 to 90.28,36,38 Vector databases like FAISS and \nChroma were frequently reported, and LangChain was \nwidely used for document processing and retrieval.23,25,35,38\nIn the subgroup analysis, 12 studies used simple data retrieval \nstrategies (OR 1.30, 95% CI [1.16, 1.45]), while 5 studies \nused complex data retrieval strategies (OR 1.30, 95% CI \n[1.07, 1.24]), with no statistically significant difference \nobserved between the 2 approaches.\nIn the post-retrieval stage, 65% of studies (n ¼ 13) imple­\nmented specific strategies to refine outputs. Murugan et al. \ntailored prompts by providing clear context, defining roles \n(eg, distinguishing between healthcare providers and patients \nto deliver appropriately detailed information), and incorpo­\nrating relevant citations from retrieval sources such as the \nClinical \nPharmacogenetics \nImplementation \nConsortium \nguidelines and Food and Drug Administration (FDA) label­\ning.24 Soman et al. utilized prompt engineering to integrate \naccurate knowledge sources and statistical evidence, such as \nP-values and z-scores, from the SPOKE knowledge graph \ninto their outputs.33 Wang et al. outlined a detailed process \nin the post-retrieval stage using prompt engineering, which \ninvolved decomposing retrieved text into individual claims, \nverifying each claim with external knowledge sources, \nFigure 3. An example of using RAG in clinical applications, with identified strategies in 3 stages: pre-retrieval, retrieval, and post-retrieval. The user query \nseeks recommended medications for symptomatic peripheral artery disease without increased bleeding risk. The system retrieves evidence from clinical \nguidelines, processes it through embeddings and a vector database, and outputs a response, including rivaroxaban with low-dose aspirin, as recommended \nby the retrieved guideline information. In this example, GPT-4 suggested a dose of 75 mg Aspirin, but the common low-dose Aspirin is 81 mg.\nJournal of the American Medical Informatics Association, 2025, Vol. 32, No. 4                                                                                                      \n609 \nDownloaded from https://academic.oup.com/jamia/article/32/4/605/7954485 by guest on 11 January 2026", "metadata": {"producer": "PDFlib+PDI 9.0.7p3 (C++/Win32); modified using iTextSharp.LGPLv2.Core 3.7.4.0", "creator": "Servigistics Arbortext Advanced Print Publisher 11.1.4667/W", "creationdate": "2025-03-22T11:01:16+05:30", "source": "/home/donbr/open-biosciences/biosciences-research/data/raw/ocaf008.pdf", "file_path": "/home/donbr/open-biosciences/biosciences-research/data/raw/ocaf008.pdf", "total_pages": 11, "format": "PDF 1.5", "title": "Improving large language model applications in biomedicine with retrieval-augmented generation: a systematic review, meta-analysis, and clinical development guidelines", "author": "", "subject": "Doi: 10.1093/jamia/ocaf008 Journal of the American Medical Informatics Association, 32, 4, 2025 Publication Date: 15/01/2025 Abstract Objective The objectives of this study are to synthesize findings from recent research of retrieval-augmented generation (RAG) and large language models (LLMs) in biomedicine and provide clinical development guidelines to improve effectiveness.Materials and Methods We conducted a systematic literature review and a meta-analysis. The report was created in adherence to the Preferred Reporting Items for Systematic Reviews and Meta-Analyses 2020 analysis. Searches were performed in 3 databases (PubMed, Embase, PsycINFO) using terms related to “retrieval augmented generation” and “large language model,” for articles published in 2023 and 2024. We selected studies that compared baseline LLM performance with RAG performance. We developed a random-effect meta-analysis model, using odds ratio as the effect size.Results Among 335 studies, 20 were included in this literature review. The pooled effect size was 1.35, with a 95% confidence interval of 1.19-1.53, indicating a statistically significant effect (P = .001). We reported clinical tasks, baseline LLMs, retrieval sources and strategies, as well as evaluation methods.Discussion Building on our literature review, we developed Guidelines for Unified Implementation and Development of Enhanced LLM Applications with RAG in Clinical Settings to inform clinical applications using RAG.Conclusion Overall, RAG implementation showed a 1.35 odds ratio increase in performance compared to baseline LLMs. Future research should focus on (1) system-level enhancement: the combination of RAG and agent, (2) knowledge-level enhancement: deep integration of knowledge into LLM, and (3) integration-level enhancement: integrating RAG systems within electronic health records.", "keywords": "large language model; retrieval augmented generation; systematic review; meta-analysis", "moddate": "2026-01-11T01:13:27+00:00", "trapped": "", "modDate": "D:20260111011327+00'00'", "creationDate": "D:20250322110116+05'30'", "page": 4}}
{"page_content": "conducting safety checks by applying 24 predefined rules to \nensure ethical and factual accuracy, and summarizing the \nresults.30 Glicksberg et al. developed an ensemble model that \ncombined structured and unstructured data to predict hospi­\ntal admission probabilities. These predicted probabilities, \nalong with similar historical cases, were incorporated into the \nprompt to enhance the performance of LLM.37 Chen et al. \nused Chain-of-Thought (CoT) prompting to improve LLM \nreasoning capabilities.39 Kresevic et al. customized prompts \nto help the model interpret structured guidelines, combined \nwith few-shot learning using 54 question-answer pairs.27\nJeong et al. fine-tuned LLMs to assess the relevance of \nretrieved evidence, ensure all statements were evidence-based, \nand confirm that the response effectively addressed the \nquery.34\nEvaluation\nNine studies used human evaluation, 8 relied on automated \nevaluation (eg, similarity comparisons between generated \nsentences and original answers), and 3 used a mix of both. \nOutcomes from human evaluation showed an overall OR of \n1.65 (95% CI: 1.36-2.03), while automatic evaluation \nresulted in an OR of 1.20 (95% CI: 1.1-1.41). The differen­\nces between the 2 were statistically significant (P < .01). \nThere were 4 human evaluators on average, with the range \nspanning from 1 to 10. Most human evaluators were \nphysicians from relevant specialties according to the \nstudy focus. In one case, 3 diabetic patients were involved in \nevaluating the understandability of diabetes-related patient \nqueries.30\nTwelve studies used self-curated datasets focused on \nresearch tasks. Examples included the ClinicalQA bench­\nmark, which comprised 314 open-ended questions about \ntreatment guidelines and clinical calculations generated by \nphysicians,28 and 43 diabetes-related questions sourced from \nthe National Institute of Diabetes and Digestive and Kidney \nDiseases website.30 Simulated cases from medical examina­\ntions were also utilized.25\nThree studies used EHR \ndata.35,37,40 Six studies used public benchmark datasets, such \nUS board exam practice questions, MedMCQA29,34 and \nlongform question-answering benchmarks (eg, LiveQA, Med­\nicationQA).34 The self-curated datasets averaged 76 ques­\ntions, ranging from 7 to 314. The length of public benchmark \ndatasets varied significantly, from 102 questions in the \nLiveQA dataset28 to 194 000 questions in the MedMCQA \ndataset.34\nMost studies reported evaluation metrics for the final \nresponse generation, while 4 (25%) also included specific \nmetrics to evaluate the retrieval process. For instance, 1 study \nmeasured recall in context retrieval,24 another evaluated \nretrieval accuracy,33 and a fine-tuned LLM was developed to \nassess the relevance of retrieved information to the user’s \nquery.34 Additionally, 1 study evaluated the accuracy of \nusing LLMs to extract text from figures and tables during \ndocument preprocessing.27 The final evaluation metrics \nfocused on the generated responses, consistent with those \nused in LLM-only systems. These metrics could be catego­\nrized as accuracy, completeness, user perception, safety, hal­\nlucination, citation, bias, and language. Accuracy was the \nmost frequently reported metric, covering Likert scale rat­\nings, match rates, correct treatment percentages,9 AUC, \nAUPRC, and F1 scores, as well as text similarity metrics like \nROUGE \n(ROUGE-1, \nROUGE-2, \nROUGE-L), \nBLEU, \nMETEOR, and BERTScore,21\nwhich compared LLM- \ngenerated responses to expert-provided answers. Complete­\nness metrics assessed whether responses included all neces­\nsary information, typically using Likert scales. User \nperception captured subjective feedback from both healthcare \nproviders and patients on understandability, helpfulness, and \nwhether responses met user intent, usually using Likert scales. \nSafety metrics focused both on user-related and system- \nrelated aspects. These metrics assessed potential harm, adver­\nsarial safety, and risk management,24 ensuring that outputs \nwere free of harmful content or risks. Scientific validity and \nadherence to evidence were also evaluated.29 One study used \nadversarial prompting, defined as intentionally adding harm­\nful directives to a prompt, to evaluate the safety of the RAG \nsystem.28 Hallucinations were primarily identified through \nmanual review, with definitions varying across studies. Some \nstudies defined hallucinations as nonfactual information, \nwhile one study added 2 other types of hallucinations: input- \nconflicting (content deviating from user-provided input) and \ncontextual-conflicting (content conflicting with previously \ngenerated information).27,41 Citation metrics measured the \naccuracy of provided references, with valid references consid­\nered those that pointed to established publications, guide­\nlines, or research. Bias and language were evaluated for \nclarity and neutrality, ensuring responses were unbiased and \nempathetic to patient concerns.24\nDiscussion\nThis study presents a systematic review of current research \non RAG for clinical tasks. Overall, RAG implementation \nincreased outcomes by 1.35 times compared to baseline \nLLM. We analyzed clinical tasks, baseline LLMs, retrieval \nsources and strategies, as well as evaluation methods. Despite \nthe potential benefits of RAG systems, there remains room \nfor improvement. Building on our literature review, we devel­\noped GUIDE-RAG (Guidelines for Unified Implementation \nand Development of Enhanced LLM Applications with RAG \nin Clinical Settings) for future clinical applications using \nRAG (Figure 4).\nGUIDE-RAG:\n1) Define clear clinical tasks and evaluation datasets. \nFuture research should clearly define clinical tasks and \nquestions to maximize the effectiveness of RAGs. Ambi­\nguity in questions can hinder performance, particularly \nin less powerful LLMs, making it challenging to achieve \nsignificant improvements in responses generation, even \nwith improved knowledge selection.42 For example, one \nstudy in the review constructed the evaluation dataset \nusing 30 case reports on rare diseases from PubMed.38\nThe authors did not report human performance on the \nself-build dataset. The questions themselves might have \nbeen inherently challenging or ambiguous. As expected, \nthe reported performance showed modest improvement, \nwith an OR of 1.31. \n2) Identify appropriate external resources for specific clini­\ncal tasks. \nThe first step in developing a RAG-based clinical system \nis to identify external resources that fill the knowledge \ngaps of the baseline LLM in relation to specific clinical \ntasks. The external knowledge should complement the \nLLM’s existing capabilities to effectively address task \n610                                                                                                      \nJournal of the American Medical Informatics Association, 2025, Vol. 32, No. 4 \nDownloaded from https://academic.oup.com/jamia/article/32/4/605/7954485 by guest on 11 January 2026", "metadata": {"producer": "PDFlib+PDI 9.0.7p3 (C++/Win32); modified using iTextSharp.LGPLv2.Core 3.7.4.0", "creator": "Servigistics Arbortext Advanced Print Publisher 11.1.4667/W", "creationdate": "2025-03-22T11:01:16+05:30", "source": "/home/donbr/open-biosciences/biosciences-research/data/raw/ocaf008.pdf", "file_path": "/home/donbr/open-biosciences/biosciences-research/data/raw/ocaf008.pdf", "total_pages": 11, "format": "PDF 1.5", "title": "Improving large language model applications in biomedicine with retrieval-augmented generation: a systematic review, meta-analysis, and clinical development guidelines", "author": "", "subject": "Doi: 10.1093/jamia/ocaf008 Journal of the American Medical Informatics Association, 32, 4, 2025 Publication Date: 15/01/2025 Abstract Objective The objectives of this study are to synthesize findings from recent research of retrieval-augmented generation (RAG) and large language models (LLMs) in biomedicine and provide clinical development guidelines to improve effectiveness.Materials and Methods We conducted a systematic literature review and a meta-analysis. The report was created in adherence to the Preferred Reporting Items for Systematic Reviews and Meta-Analyses 2020 analysis. Searches were performed in 3 databases (PubMed, Embase, PsycINFO) using terms related to “retrieval augmented generation” and “large language model,” for articles published in 2023 and 2024. We selected studies that compared baseline LLM performance with RAG performance. We developed a random-effect meta-analysis model, using odds ratio as the effect size.Results Among 335 studies, 20 were included in this literature review. The pooled effect size was 1.35, with a 95% confidence interval of 1.19-1.53, indicating a statistically significant effect (P = .001). We reported clinical tasks, baseline LLMs, retrieval sources and strategies, as well as evaluation methods.Discussion Building on our literature review, we developed Guidelines for Unified Implementation and Development of Enhanced LLM Applications with RAG in Clinical Settings to inform clinical applications using RAG.Conclusion Overall, RAG implementation showed a 1.35 odds ratio increase in performance compared to baseline LLMs. Future research should focus on (1) system-level enhancement: the combination of RAG and agent, (2) knowledge-level enhancement: deep integration of knowledge into LLM, and (3) integration-level enhancement: integrating RAG systems within electronic health records.", "keywords": "large language model; retrieval augmented generation; systematic review; meta-analysis", "moddate": "2026-01-11T01:13:27+00:00", "trapped": "", "modDate": "D:20260111011327+00'00'", "creationDate": "D:20250322110116+05'30'", "page": 5}}
{"page_content": "requirements. For instance, in question-answering tasks \nrelated to broad medical exams for physicians, clinical \nguidelines (eg, StatPearls) and textbooks proved more \nuseful than PubMed abstracts as external sources.43\nAnother example from our review involved a task \nfocused on medical question-answering in internal medi­\ncine. The study used a single source—Harrison’s Princi­\nples of Internal Medicine—as the knowledge retrieval \nsource, and the reported improvement was marginal \n(OR: 1.14).36 Expanding the knowledge base to include \nadditional resources, such as clinical guidelines, could \npotentially enhance the performance of the RAG system \nfor such tasks. \n3) Preprocess multi-modal content in documents. \nClinical guidelines and medical literature often contain \ncomplex information presented through flowcharts, \ngraphs, and tables. Accurately parsing this multi-modal \ncontent is essential for effective retrieval. Relying solely \non LLMs for text extraction may be insufficient; a pre­\nliminary study found that GPT-4 Turbo had only a \n16% accuracy rate in extracting table data.27 Compre­\nhensive document preprocessing should systematically \nextract relevant information from text, tables, and fig­\nures to ensure accuracy and clarity. Only 3 studies in \nour review explicitly mentioned extracting text from \ntables or figures during the pre-retrieval process.25,27,44\n4) Optimize \nchunking \nand \nindexing \nbased \ncorpus \nstructure. \nThe structure of clinical knowledge corpora should be \ncarefully considered during chunking and indexing. \nFixed-length chunking can introduce noise by fragment­\ning related information, which can reduce retrieval \naccuracy. Researchers should optimize the chunking \ngranularity based on a thorough review of the clinical \nknowledge corpus, to ensure the completeness of \nretrieved information. An alternative approach is \ndynamic chunking, which adjusts chunk boundaries \nbased \non \nsemantic \nsimilarity \nchanges.45\nOther \napproaches include recursive chunking, which hierarchi­\ncally divides text into smaller chunks using delimiters \nlike headings, subheadings, paragraphs, and sentences. \nSliding window chunking enables layered retrieval by \noverlapping chunks of text, allowing the system to cap­\nture and merge contextually related information across \ndifferent segments.46 Context enriched chucking enhan­\nces retrieval by incorporating concise summaries within \neach segment to provide additional context for down­\nstream tasks.47 In indexing, while dense indexing (con­\nverting text to vectors) is widely used, it may miss global \ninformation. The structure of a clinical knowledge cor­\npora such as some headings, keywords, can be used as \nsparse indexing and further combined with dense index­\ning. This hybrid approach that combines dense and \nsparse indexing can improve retrieval performance by \ncapturing both global and local information.48,49\n5) Explore advanced retrieval strategies, including iterative \nand graph-based retrieval. \nIterative retrieval improves accuracy by refining results \nthrough multiple rounds. Parameters such as the num­\nber of retrieved chunks or cutoff thresholds should be \noptimized based on specific clinical questions, as \nretrieval needs can vary—some questions may not \nrequire external knowledge at all. Researchers should \nevaluate retrieval requirements in advance and adapt \nretrieval parameters accordingly. Graph-based retrieval, \nwhich structures entities and relationships into a graph, \ncan improve information synthesis from multiple sour­\nces. For example, GraphRAG identified entities and \nrelationships from documents and developed a graph \nusing LLM. Then, they used clustering algorithm to \nFigure 4. Overview of GUIDE-RAG: This framework streamlines RAG in clinical applications through three iterative stages. In the pre-retrieval stage, it \nfocuses on defining tasks, identifying relevant resources, and preprocessing content. The retrieval stage enhances data retrieval with optimized chunking, \nindexing, and advanced strategies such as graph-based retrieval. The post-retrieval stage emphasizes system evaluation, monitoring, knowledge updates, \nand implementing few-shot learning for complex clinical reasoning, ensuring robust and adaptive performance. CoT, chain-of-thought.\nJournal of the American Medical Informatics Association, 2025, Vol. 32, No. 4                                                                                                      \n611 \nDownloaded from https://academic.oup.com/jamia/article/32/4/605/7954485 by guest on 11 January 2026", "metadata": {"producer": "PDFlib+PDI 9.0.7p3 (C++/Win32); modified using iTextSharp.LGPLv2.Core 3.7.4.0", "creator": "Servigistics Arbortext Advanced Print Publisher 11.1.4667/W", "creationdate": "2025-03-22T11:01:16+05:30", "source": "/home/donbr/open-biosciences/biosciences-research/data/raw/ocaf008.pdf", "file_path": "/home/donbr/open-biosciences/biosciences-research/data/raw/ocaf008.pdf", "total_pages": 11, "format": "PDF 1.5", "title": "Improving large language model applications in biomedicine with retrieval-augmented generation: a systematic review, meta-analysis, and clinical development guidelines", "author": "", "subject": "Doi: 10.1093/jamia/ocaf008 Journal of the American Medical Informatics Association, 32, 4, 2025 Publication Date: 15/01/2025 Abstract Objective The objectives of this study are to synthesize findings from recent research of retrieval-augmented generation (RAG) and large language models (LLMs) in biomedicine and provide clinical development guidelines to improve effectiveness.Materials and Methods We conducted a systematic literature review and a meta-analysis. The report was created in adherence to the Preferred Reporting Items for Systematic Reviews and Meta-Analyses 2020 analysis. Searches were performed in 3 databases (PubMed, Embase, PsycINFO) using terms related to “retrieval augmented generation” and “large language model,” for articles published in 2023 and 2024. We selected studies that compared baseline LLM performance with RAG performance. We developed a random-effect meta-analysis model, using odds ratio as the effect size.Results Among 335 studies, 20 were included in this literature review. The pooled effect size was 1.35, with a 95% confidence interval of 1.19-1.53, indicating a statistically significant effect (P = .001). We reported clinical tasks, baseline LLMs, retrieval sources and strategies, as well as evaluation methods.Discussion Building on our literature review, we developed Guidelines for Unified Implementation and Development of Enhanced LLM Applications with RAG in Clinical Settings to inform clinical applications using RAG.Conclusion Overall, RAG implementation showed a 1.35 odds ratio increase in performance compared to baseline LLMs. Future research should focus on (1) system-level enhancement: the combination of RAG and agent, (2) knowledge-level enhancement: deep integration of knowledge into LLM, and (3) integration-level enhancement: integrating RAG systems within electronic health records.", "keywords": "large language model; retrieval augmented generation; systematic review; meta-analysis", "moddate": "2026-01-11T01:13:27+00:00", "trapped": "", "modDate": "D:20260111011327+00'00'", "creationDate": "D:20250322110116+05'30'", "page": 6}}
{"page_content": "offer global information based on user query, offering \nbetter performance than naïve RAG on the traditional \nvector databases.50\n6) Implement few-shot learning with CoT for complex clin­\nical reasoning. \nFew-shot learning has been shown to enhance LLMs’ \nreasoning capabilities by teaching specific reasoning that \nmay not have been included in their original training. \nSimilarly, CoT techniques can improve complex reason­\ning in clinical tasks.51,52 Researchers should generate \nhigh-quality examples and incorporate CoT strategies \ninto the final query to refine specialized reasoning. \n7) Evaluate all stages of the RAG system development. \nMost current studies focus only on final performance, \noverlooking the importance of evaluating each stage of \ndevelopment. It is crucial to formally assess and report \nperformance at the pre-retrieval, retrieval, and post- \nretrieval stages. Evaluating the knowledge boundaries of \nthe baseline LLM, potential conflicts between the LLM \nand external knowledge, and the accuracy and coverage \nof retrieved information helps ensure replicability and \ntransparency. This level of evaluation enables other \nresearchers to understand why a RAG system works (or \ndoes not) and facilitates reproducibility. \n8) Consider cost and time constraints. \nAdvanced retrieval strategies can improve performance \nbut often increase processing time and computational \ncosts. For example, graph-based RAG requires substan­\ntial resources for developing knowledge graphs, and \nresponses from global summaries may take longer than \nwith naïve RAG methods.50 Another example is to fine- \ntune LLMs to evaluate the needs and the quality of \nretrieval.34 In terms of computational cost, this process \nis expensive, especially when scaling the method to \nlarger datasets or deploying it in a real-time system. \nAlso, a set of extra processes will make the whole speed \nslow. The long response time might have a nonignorable \nnegative impact in situations that need a quick answer, \nespecially common in clinical settings. Researchers \nshould balance performance improvements with time \nand cost considerations. Only 1 study in our review, \nwhich focused on gastrointestinal radiology diagnosis \nbased on imaging descriptions, compared the cost and \nresponse time between LLMs and LLMs with RAG.31\nThe mean response time was 29.8 s for LLM with RAG \nvs 15.7s for LLM alone, with costs of $0.15 and $0.02 \nper case, respectively. Another study used EHR to pre­\ndict cognitive decline only reported cost, with LLM: \n$4.49; RAG: $12.51. Another study that used EHR \ndata to predict cognitive decline reported costs of $4.49 \nfor LLM alone and $12.51 for LLM with RAG.53\n9) Establish monitoring and knowledge update strategies. \nAn important concept in AI applications in healthcare, \nalgorithmovigilance, which defined as “scientific meth­\nods and activities relating to the evaluation, monitoring, \nunderstanding, and prevention of adverse effects of \nalgorithms in health care,” 54 should also be considered \nin the RAG applications. Researchers need to develop \nlong-term monitoring strategies for the RAG system per­\nformance, especially in clinical applications. In addition, \ncurrent studies use fixed external datasets. Researchers \nshould update external knowledge sources as latest \ninformation becomes available. Clear strategies for \nupdating knowledge should be defined, specifying when \nand how updates will occur. \nFor future studies, the first direction could be the system- \nlevel enhancement, the combination of RAG and LLM- \npowered agents. LLM-powered agents are AI systems that \nuse LLMs with complex reasoning and planning capabilities, \nmemory management, interactive capabilities with the envi­\nronment, and actions to execute tasks.55,56 Recent research \npoints to the emerging trend of combination of RAG and \nLLM-powered agents, where agents can assist in planning \nand decision making for complex tasks, rather than simple \nretrieval.57 For example, clinicians and patients have diverse \ninformation access needs, some needing to analyze text from \na knowledge base, others needing to incorporate structured \ndata from an EHR. RAG will eventually only become one of \nthe methods for agents to access information. Moreover, \nfuture research could focus on the usage of internal and \nexternal functions and tools, long-term and short-term mem­\nory module, self-learning module. For example, a study \ndeveloped an agent to answer questions related to rare dis­\neases by expanding beyond RAG with additional tool func­\ntions, such as querying phenotypes and performing web \nsearches. This approach improved the overall correctness \nfrom 0.48 to 0.75 compared to the GPT-4 baseline LLM.58\nThe second future direction could focus on the knowledge- \nlevel enhancement: deep integration of external knowledge \ninto LLM. LLM exhibits the knowledge boundaries. RAG \napproaches retrieving external knowledge and then integrates \nit into LLMs in the forms of prompts for the final generation \nto enhance the capabilities of LLMs in perceiving knowledge \nboundaries.59 However, the integration of external knowl­\nedge into LLM reasoning is typically limited to providing the \nretrieved data as additional context for the LLM’s query dur­\ning generation. This approach keeps retrieval and generation \nloosely connected, and the LLM’s output can still be influ­\nenced by its inherent knowledge boundaries or by noise in the \nretrieved text, leading to incorrect answers. Additionally, \nwhen the external knowledge source is EHR data, this \nenhancement becomes even more important. Current EHR \ndata is organized in a “problem-oriented medical record” \n(POMR) format, which collects and displays information in a \nstructured manner.60 LLMs excel in free-form contexts, and \ntheir ability to perform clinical tasks depends on access to \nunstructured text that provides a comprehensive view of the \npatient. Achieving this within the structured POMR format \nin modern EHR systems poses a significant challenge.61\nTherefore, investigating how to realize the deep integration \nof external knowledge with LLM reasoning is an important \ndirection for future research in clinical applications.\nThe final direction is the integration-level enhancement, \nfocusing on integrating RAG systems within EHRs. Current \nresearch has primarily focused on development and testing \noutside of EHR systems. To seamlessly provide support for \nhealthcare providers and patients, future efforts should pri­\noritize embedding RAG systems into EHR interfaces. This \nrequires collaboration with EHR vendors to ensure the neces­\nsary infrastructure is available. Researchers also can facilitate \nthis integration using data exchange frameworks, such as \nSMART on FHIR.62\n612                                                                                                      \nJournal of the American Medical Informatics Association, 2025, Vol. 32, No. 4 \nDownloaded from https://academic.oup.com/jamia/article/32/4/605/7954485 by guest on 11 January 2026", "metadata": {"producer": "PDFlib+PDI 9.0.7p3 (C++/Win32); modified using iTextSharp.LGPLv2.Core 3.7.4.0", "creator": "Servigistics Arbortext Advanced Print Publisher 11.1.4667/W", "creationdate": "2025-03-22T11:01:16+05:30", "source": "/home/donbr/open-biosciences/biosciences-research/data/raw/ocaf008.pdf", "file_path": "/home/donbr/open-biosciences/biosciences-research/data/raw/ocaf008.pdf", "total_pages": 11, "format": "PDF 1.5", "title": "Improving large language model applications in biomedicine with retrieval-augmented generation: a systematic review, meta-analysis, and clinical development guidelines", "author": "", "subject": "Doi: 10.1093/jamia/ocaf008 Journal of the American Medical Informatics Association, 32, 4, 2025 Publication Date: 15/01/2025 Abstract Objective The objectives of this study are to synthesize findings from recent research of retrieval-augmented generation (RAG) and large language models (LLMs) in biomedicine and provide clinical development guidelines to improve effectiveness.Materials and Methods We conducted a systematic literature review and a meta-analysis. The report was created in adherence to the Preferred Reporting Items for Systematic Reviews and Meta-Analyses 2020 analysis. Searches were performed in 3 databases (PubMed, Embase, PsycINFO) using terms related to “retrieval augmented generation” and “large language model,” for articles published in 2023 and 2024. We selected studies that compared baseline LLM performance with RAG performance. We developed a random-effect meta-analysis model, using odds ratio as the effect size.Results Among 335 studies, 20 were included in this literature review. The pooled effect size was 1.35, with a 95% confidence interval of 1.19-1.53, indicating a statistically significant effect (P = .001). We reported clinical tasks, baseline LLMs, retrieval sources and strategies, as well as evaluation methods.Discussion Building on our literature review, we developed Guidelines for Unified Implementation and Development of Enhanced LLM Applications with RAG in Clinical Settings to inform clinical applications using RAG.Conclusion Overall, RAG implementation showed a 1.35 odds ratio increase in performance compared to baseline LLMs. Future research should focus on (1) system-level enhancement: the combination of RAG and agent, (2) knowledge-level enhancement: deep integration of knowledge into LLM, and (3) integration-level enhancement: integrating RAG systems within electronic health records.", "keywords": "large language model; retrieval augmented generation; systematic review; meta-analysis", "moddate": "2026-01-11T01:13:27+00:00", "trapped": "", "modDate": "D:20260111011327+00'00'", "creationDate": "D:20250322110116+05'30'", "page": 7}}
{"page_content": "Limitations\nThis study was limited to peer-reviewed publications avail­\nable in biomedical databases (eg, PubMed, Embase), exclud­\ning \npreprint \narticles \nfrom \nrepositories \nlike \nArXiv. \nAdditionally, only studies in English language were included, \nwhich might have excluded relevant studies in other lan­\nguages. We did not include sources such as IEEE Xplore or \nGoogle Scholar, which might have additional relevant stud­\nies. However, our focus was on biomedicine, and we priori­\ntized databases specifically tailored to biomedical research to \nmaintain the relevance and quality of the included studies. \nFurthermore, we used free-text searches in the databases, \nwhich activated automatic mapping to Medical Subject \nHeadings (MeSH) and Emtree terms, improving retrieval \naccuracy. However, the limitations of automatic term map­\nping cannot be ignored, as it may introduce variability if the \nunderlying algorithms change. To address this, we have docu­\nmented all identified papers from our search. The title, publi­\ncation year, PMID, PUI, and database source for each study \nare provided in File S3.\nConclusion\nWe conducted a systematic literature review of studies \nexploring the use of RAG and LLM in clinical tasks. RAG \nimplementation showed a 1.35 odds ratio increase in per­\nformance compared to baseline LLMs. To improve perform­\nance and transparency in future studies, we developed \nguidelines for improving clinical RAG applications based on \ncurrent research findings. Future research could focus on \nthese 3 directions: (1) system-level enhancement: the combi­\nnation of RAG and agent, (2) knowledge-level enhancements: \ndeep integration of knowledge into LLM, and (3) integration- \nlevel enhancements: integrating RAG systems within EHRs.\nAuthor contributions\nSiru Liu (Conceptualization, Data curation, Formal analysis, \nInvestigation, Methodology, Software, Visualization, Writing \n– original draft), Allison B. McCoy (Conceptualization, Writ­\ning – review & editing), Adam Wright (Conceptualization, \nWriting – review & editing).\nSupplementary material\nSupplementary material is available at Journal of the Ameri­\ncan Medical Informatics Association online.\nFunding\nThis work was supported by National Institutes of Health \ngrants: R00LM014097-02 and R01LM013995-01.\nConflicts of interest\nThe authors do not have conflicts of interest related to this \nstudy.\nData availability\nThe characteristics and outcomes for each included study \nwere reported in the File S2.\nReferences\n01. Raiaan MAK, Mukta MSH, Fatema K, et al. A review on large lan­\nguage models: architectures, applications, taxonomies, open issues \nand challenges. IEEE Access. 2024;12:26839-26874. https://doi. \norg/10.1109/ACCESS.2024.3365742\n02. Thirunavukarasu AJ, Ting DSJ, Elangovan K, et al. Large language \nmodels in medicine. Nat Med. 2023;29:1930-1940. https://doi. \norg/10.1038/s41591-023-02448-8\n03. Ayers JW, Poliak A, Dredze M, et al. Comparing physician and arti­\nficial intelligence Chatbot responses to patient questions posted to a \npublic social media forum. JAMA Intern Med. 2023;183:589-596. \nhttps://doi.org/10.1001/jamainternmed.2023.1838\n04. Liu S, Wright AP, Patterson BL, et al. Using AI-generated sugges­\ntions from ChatGPT to optimize clinical decision support. J Am \nMed Inform Assoc. 2023;30:1237-1245. https://doi.org/10.1093/ \njamia/ocad072\n05. Zaretsky J, Kim JM, Baskharoun S, et al. Generative artificial intel­\nligence to transform inpatient discharge summaries to patient- \nfriendly language and format. JAMA Netw Open. 2024;7: \ne240357. https://doi.org/10.1001/jamanetworkopen.2024.0357\n06. Gao Y, Xiong Y, Gao X, et al. Retrieval-augmented generation for \nlarge language models: a survey. 2023, preprint: not peer reviewed. \nhttps://arxiv.org/abs/2312.10997\n07. Xu Z, Jain S, Kankanhalli M. Hallucination is inevitable: an innate \nlimitation of large language models. January 22, 2024, preprint: \nnot peer reviewed. https://arxiv.org/abs/2401.11817\n08. Shuster K, Poff S, Chen M, et al. Retrieval augmentation reduces \nhallucination in conversation. Findings of the Association for Com­\nputational Linguistics, Findings of ACL: EMNLP 2021. Associa­\ntion for Computational Linguistics (ACL); 2021:3784-3803.\n09. Ayala  O, Bechard P. Reducing hallucination in structured outputs \nvia Retrieval-Augmented Generation. Proceedings of the 2024 \nConference of the North American Chapter of the Association for \nComputational Linguistics: Human Language Technologies. \nAssociation for Computational Linguistics; 2024;228-38. https:// \ndoi.org/10.18653/v1/2024.naacl-industry.19\n10. Gornik HL, Aronow HD, Goodney PP, et al. 2024 ACC/AHA/ \nAACVPR/APMA/ABC/SCAI/SVM/SVN/SVS/SIR/VESS guideline \nfor the management of lower extremity peripheral artery disease: a \nreport of the American College of Cardiology/American Heart \nAssociation Joint Committee on Clinical Practice Guidelines. Cir­\nculation. 2024;149:e1313-e1410. https://doi.org/10.1161/CIR. \n0000000000001251\n11. Vasey B, Nagendran M, Campbell B, et al. Reporting guideline for \nthe early stage clinical evaluation of decision support systems \ndriven by artificial intelligence: DECIDE-AI. BMJ. 2022;377: \ne070904. https://doi.org/10.1136/bmj-2022-070904\n12. Mongan J, Moy L, Kahn CE. Checklist for Artificial Intelligence in \nMedical Imaging (CLAIM): a guide for authors and reviewers. \nRadiol Artif Intell. 2020;2:e200029. https://doi.org/10.1148/ryai. \n2020200029\n13. Martindale APL, Ng B, Ngai V, et al. Concordance of randomised \ncontrolled trials for artificial intelligence interventions with the \nCONSORT-AI \nreporting \nguidelines. \nNat \nCommun. \n2024;15:6376-6311. https://doi.org/10.1038/s41467-024-45355-3\n14. Tam TYC, Sivarajkumar S, Kapoor S, et al. A framework for \nhuman evaluation of large language models in healthcare derived \nfrom literature review. NPJ Digit Med. 2024;7:258. https://doi. \norg/10.1038/s41746-024-01258-7\n15. Shamseer L, Moher D, Clarke M, et al. Preferred reporting items \nfor systematic review and meta-analysis protocols (PRISMA-p) \n2015: elaboration and explanation. BMJ. 2015;350:g7647. \nhttps://doi.org/10.1136/bmj.g7647\n16. Higgins JPT, Thomas J, Chandler J, et al. (editors). Cochrane Hand­\nbook for Systematic Reviews of Interventions version 6.5 (updated \nAugust 2024). Cochrane, 2024. Available from www.training. \ncochrane.org/handbook. Date accessed December 11, 2024.\nJournal of the American Medical Informatics Association, 2025, Vol. 32, No. 4                                                                                                      \n613 \nDownloaded from https://academic.oup.com/jamia/article/32/4/605/7954485 by guest on 11 January 2026", "metadata": {"producer": "PDFlib+PDI 9.0.7p3 (C++/Win32); modified using iTextSharp.LGPLv2.Core 3.7.4.0", "creator": "Servigistics Arbortext Advanced Print Publisher 11.1.4667/W", "creationdate": "2025-03-22T11:01:16+05:30", "source": "/home/donbr/open-biosciences/biosciences-research/data/raw/ocaf008.pdf", "file_path": "/home/donbr/open-biosciences/biosciences-research/data/raw/ocaf008.pdf", "total_pages": 11, "format": "PDF 1.5", "title": "Improving large language model applications in biomedicine with retrieval-augmented generation: a systematic review, meta-analysis, and clinical development guidelines", "author": "", "subject": "Doi: 10.1093/jamia/ocaf008 Journal of the American Medical Informatics Association, 32, 4, 2025 Publication Date: 15/01/2025 Abstract Objective The objectives of this study are to synthesize findings from recent research of retrieval-augmented generation (RAG) and large language models (LLMs) in biomedicine and provide clinical development guidelines to improve effectiveness.Materials and Methods We conducted a systematic literature review and a meta-analysis. The report was created in adherence to the Preferred Reporting Items for Systematic Reviews and Meta-Analyses 2020 analysis. Searches were performed in 3 databases (PubMed, Embase, PsycINFO) using terms related to “retrieval augmented generation” and “large language model,” for articles published in 2023 and 2024. We selected studies that compared baseline LLM performance with RAG performance. We developed a random-effect meta-analysis model, using odds ratio as the effect size.Results Among 335 studies, 20 were included in this literature review. The pooled effect size was 1.35, with a 95% confidence interval of 1.19-1.53, indicating a statistically significant effect (P = .001). We reported clinical tasks, baseline LLMs, retrieval sources and strategies, as well as evaluation methods.Discussion Building on our literature review, we developed Guidelines for Unified Implementation and Development of Enhanced LLM Applications with RAG in Clinical Settings to inform clinical applications using RAG.Conclusion Overall, RAG implementation showed a 1.35 odds ratio increase in performance compared to baseline LLMs. Future research should focus on (1) system-level enhancement: the combination of RAG and agent, (2) knowledge-level enhancement: deep integration of knowledge into LLM, and (3) integration-level enhancement: integrating RAG systems within electronic health records.", "keywords": "large language model; retrieval augmented generation; systematic review; meta-analysis", "moddate": "2026-01-11T01:13:27+00:00", "trapped": "", "modDate": "D:20260111011327+00'00'", "creationDate": "D:20250322110116+05'30'", "page": 8}}
{"page_content": "17. Lefebvre C, Glanville J, Briscoe S, et al. Chapter 4: searching for \nand selecting studies. Cochrane Handbook for Systematic Reviews \nof Interventions Version, Vol. 6. Cochrane, 2024. https://training. \ncochrane.org/handbook/current/chapter-04\n18. Chapter 3 Effect Sizes j Doing Meta-Analysis in R. Accessed Octo­\nber 13, 2024. https://bookdown.org/MathiasHarrer/Doing_Meta_ \nAnalysis_in_R/effects.html\n19. Borenstein M, Hedges LV, Higgins JPT, et al. Introduction to \nMeta-Analysis. John Wiley & Sons; 2011.\n20. Higgins JPT, Thompson SG. Quantifying heterogeneity in a meta- \nanalysis. Stat Med. 2002;21:1539-1558. https://doi.org/10.1002/ \nsim.1186\n21. Peters JL, Sutton AJ, Jones DR, et al. Contour-enhanced meta- \nanalysis funnel plots help distinguish publication bias from other \ncauses of asymmetry. J Clin Epidemiol. 2008;61:991-996. https:// \ndoi.org/10.1016/j.jclinepi.2007.11.010\n22. Egger M, Smith GD, Schneider M, et al. Bias in meta-analysis detected \nby a simple, graphical test measures of funnel plot asymmetry. BMJ. \n1997;315:629-634. https://doi.org/10.1136/bmj.315.7109.629\n23. Kreimeyer K, Canzoniero JV, Fatteh M, et al. Using retrieval- \naugmented generation to capture molecularly-driven treatment \nrelationships for precision oncology. Stud Health Technol Inform. \n2024;316:983-987. https://doi.org/10.3233/SHTI240575\n24. Murugan M, Yuan B, Venner E, et al. Empowering personalized \npharmacogenomics with generative AI solutions. J Am Med Inform \nAssoc. 2024;31:1356-1366. https://doi.org/10.1093/jamia/ocae039\n25. Yazaki M, Maki S, Furuya T, et al. Emergency patient triage \nimprovement through a retrieval-augmented generation enhanced \nlarge-scale language model. Prehosp Emerg Care. 2024;1-7. \nhttps://doi.org/10.1080/10903127.2024.2374400\n26. Malik S, Kharel H, Dahiya DS, et al. Assessing ChatGPT4 with \nand without retrieval-augmented generation in anticoagulation \nmanagement for gastrointestinal procedures. Ann Gastroenterol. \n2024;37:514-526. https://doi.org/10.20524/aog.2024.0907\n27. Kresevic S, Giuffr\u0012e M, Ajcevic M, et al. Optimization of hepatolog­\nical clinical guidelines interpretation by large language models: a \nretrieval augmented generation-based framework. NPJ Digit Med. \n2024;7:102-109. https://doi.org/10.1038/s41746-024-01091-y\n28. Zakka C, Shad R, Chaurasia A, et al. Almanac–retrieval-aug­\nmented language models for clinical medicine. NEJM AI. 2024;1 \n(2):10.1056/aioa2300068. https://doi.org/10.1056/aioa2300068\n29. Long C, Subburam D, Lowe K, et al. ChatENT: augmented large \nlanguage model for expert knowledge retrieval in otolaryngology– \nhead and neck surgery. Otolaryngol Head Neck Surg. \n2024;171:1042-1051. https://doi.org/10.1002/ohn.864\n30. Wang D, Liang J, Ye J, et al. Enhancement of the performance of \nlarge language models in diabetes education through retrieval-aug­\nmented generation: comparative study. J Med Internet Res. \n2024;26:e58041. https://doi.org/10.2196/58041\n31. Rau S, Rau A, Nattenm€uller J, et al. A retrieval-augmented chatbot \nbased on GPT-4 provides appropriate differential diagnosis in gas­\ntrointestinal radiology: a proof of concept study. Eur Radiol Exp. \n2024;8:60. https://doi.org/10.1186/s41747-024-00457-x\n32. Morris JH, Soman K, Akbas RE, et al. The scalable precision \nmedicine open knowledge engine (SPOKE): a massive knowl­\nedge \ngraph \nof \nbiomedical \ninformation. \nBioinformatics. \n2023;39(2):btad080. \nhttps://doi.org/10.1093/BIOINFOR­\nMATICS/BTAD080\n33. Soman K, Rose PW, Morris JH, et al. Biomedical knowl­\nedge graph-optimized prompt generation for large language \nmodels. Commun ACM. 2023;66:7-7. https://doi.org/10.1145/ \n3606337\n34. Jeong M, Sohn J, Sung M, et al. Improving medical reasoning \nthrough retrieval and self-reflection with retrieval-augmented large \nlanguage models. Bioinformatics. 2024;40:i119-i129. https://doi. \norg/10.1093/bioinformatics/btae238\n35. Alkhalaf M, Yu P, Yin M, et al. Applying generative AI with \nretrieval augmented generation to summarize and extract key \nclinical information from electronic health records. J Biomed \nInform. \n2024;156:104662. \nhttps://doi.org/10.1016/j.jbi.2024. \n104662\n36. Tarabanis C, Zahid S, Mamalis M, et al. Performance of publicly \navailable large language models on internal medicine board-style \nquestions. PLOS Digit Health. 2024;3:e0000604. https://doi.org/ \n10.1371/journal.pdig.0000604\n37. Glicksberg BS, Timsina P, Patel D, et al. Evaluating the accuracy of \na state-of-the-art large language model for prediction of admis­\nsions from the emergency room. J Am Med Inform Assoc. \n2024;31:1921-1928. https://doi.org/10.1093/jamia/ocae103\n38. Zelin C, Chung WK, Jeanne M, et al. Rare disease diagnosis using \nknowledge guided retrieval augmentation for ChatGPT. J Biomed \nInform. 2024;157:104702. https://doi.org/10.1016/j.jbi.2024.104702\n39. Chen X, Wang L, You MK, et al. Evaluating and enhancing large \nlanguage models’ performance in domain-specific medicine: devel­\nopment and usability study with DocOA. J Med Internet Res. \n2024;26:e58158. https://doi.org/10.2196/58158\n40. Du X, Novoa-Laurentiev J, Plasaek JM, et al. Enhancing early \ndetection of cognitive decline in the elderly: a comparative study \nutilizing large language models in clinical notes. medRxiv. \n2024;109:105401. https://doi.org/10.1101/2024.04.03.24305298\n41. Zhang Y, Li Y, Cui L, et al. Siren’s song in the AI Ocean: a survey \non hallucination in large language models. 2023, preprint: not \npeer reviewed. https://arxiv.org/abs/2309.01219\n42. Li X, Ouyang JA. Systematic investigation of knowledge retrieval \nand selection for retrieval augmented generation. 2024, preprint: \nnot peer reviewed. https://arxiv.org/abs/2410.13258\n43. Xiong G, Jin Q, Lu Z, et al. Benchmarking retrieval-augmented \ngeneration for medicine. Findings of the Association for Computa­\ntional Linguistics: ACL 2024, 6233-6251. Bangkok, Thailand: \nAssociation for Computational Linguistics. \n44. Hewitt KJ, Wiest IC, Carrero ZI, et al. Large language models as a \ndiagnostic support tool in neuropathology. J Pathol Clin Res. \n2024;10:e70009. https://doi.org/10.1002/2056-4538.70009\n45. Allahverdiyev R, Taha M, Akalin A, et al. ChunkRAG: novel \nLLM-chunk filtering method for RAG systems. October 25, 2024, \npreprint: not peer reviewed. https://arxiv.org/abs/2410.19572\n46. Cai B, Zhang FL, Wang C. Research on chunking algorithms of \ndata de-duplication. Advances in Intelligent Systems and Comput­\ning, Vol. 181. 2013:1019-1025. Springer, Berlin, Heidelberg. \nhttps://doi.org/10.1007/978-3-642-31698-2_144\n47. Optimizing RAG with Advanced Chunking Techniques. Accessed \nDecember 15, 2024. https://antematter.io/blogs/optimizing-rag- \nadvanced-chunking-techniques-study\n48. Chen J, Xiao S, Zhang P, et al. M3-Embedding: multi-lingual, \nmulti-functionality, multi-granularity text embeddings through \nself-knowledge distillation. Findings of the Association for Com­\nputational Linguistics ACL 2024. 2024:2318-2335. Association \nfor Computational Linguistics. https://doi.org/10.18653/V1/2024. \nFINDINGS-ACL.137\n49. Sawarkar K, Mangal A, Solanki SR. Blended RAG: improving \nRAG (Retriever-Augmented Generation) accuracy with semantic \nsearch and hybrid query-based retrievers. 2024 IEEE 7th Interna­\ntional Conference on Multimedia Information Processing and \nRetrieval (MIPR), 155-161. San Jose, CA, USA: IEEE. https://doi. \norg/10.1109/MIPR62202.2024.00031\n50. Edge D, Trinh H, Cheng N, et al. From local to global: a graph \nRAG approach to query-focused summarization. 2024, preprint: \nnot peer reviewed. https://arxiv.org/abs/2404.16130\n51. Wu Z, Hasan A, Wu J, et al. KnowLab_AIMed at MEDIQA- \nCORR 2024: Chain-of-Though (CoT) prompting strategies for \nmedical error detection and correction. Proceedings of the 6th \nClinical Natural Language Processing Workshop. 2024:353-359. \nAssociation for Computational Linguistics. 10.18653/v1/2024. \nclinicalnlp-1.33\n52. Kwon T, Tzu-Iunn Ong K, Kang D, et al. Large language models \nare clinical reasoners: reasoning-aware diagnosis framework with \n614                                                                                                      \nJournal of the American Medical Informatics Association, 2025, Vol. 32, No. 4 \nDownloaded from https://academic.oup.com/jamia/article/32/4/605/7954485 by guest on 11 January 2026", "metadata": {"producer": "PDFlib+PDI 9.0.7p3 (C++/Win32); modified using iTextSharp.LGPLv2.Core 3.7.4.0", "creator": "Servigistics Arbortext Advanced Print Publisher 11.1.4667/W", "creationdate": "2025-03-22T11:01:16+05:30", "source": "/home/donbr/open-biosciences/biosciences-research/data/raw/ocaf008.pdf", "file_path": "/home/donbr/open-biosciences/biosciences-research/data/raw/ocaf008.pdf", "total_pages": 11, "format": "PDF 1.5", "title": "Improving large language model applications in biomedicine with retrieval-augmented generation: a systematic review, meta-analysis, and clinical development guidelines", "author": "", "subject": "Doi: 10.1093/jamia/ocaf008 Journal of the American Medical Informatics Association, 32, 4, 2025 Publication Date: 15/01/2025 Abstract Objective The objectives of this study are to synthesize findings from recent research of retrieval-augmented generation (RAG) and large language models (LLMs) in biomedicine and provide clinical development guidelines to improve effectiveness.Materials and Methods We conducted a systematic literature review and a meta-analysis. The report was created in adherence to the Preferred Reporting Items for Systematic Reviews and Meta-Analyses 2020 analysis. Searches were performed in 3 databases (PubMed, Embase, PsycINFO) using terms related to “retrieval augmented generation” and “large language model,” for articles published in 2023 and 2024. We selected studies that compared baseline LLM performance with RAG performance. We developed a random-effect meta-analysis model, using odds ratio as the effect size.Results Among 335 studies, 20 were included in this literature review. The pooled effect size was 1.35, with a 95% confidence interval of 1.19-1.53, indicating a statistically significant effect (P = .001). We reported clinical tasks, baseline LLMs, retrieval sources and strategies, as well as evaluation methods.Discussion Building on our literature review, we developed Guidelines for Unified Implementation and Development of Enhanced LLM Applications with RAG in Clinical Settings to inform clinical applications using RAG.Conclusion Overall, RAG implementation showed a 1.35 odds ratio increase in performance compared to baseline LLMs. Future research should focus on (1) system-level enhancement: the combination of RAG and agent, (2) knowledge-level enhancement: deep integration of knowledge into LLM, and (3) integration-level enhancement: integrating RAG systems within electronic health records.", "keywords": "large language model; retrieval augmented generation; systematic review; meta-analysis", "moddate": "2026-01-11T01:13:27+00:00", "trapped": "", "modDate": "D:20260111011327+00'00'", "creationDate": "D:20250322110116+05'30'", "page": 9}}
{"page_content": "prompt-generated rationales. Proceedings of the AAAI Conference \non Artificial Intelligence. Association for the Advancement of Arti­\nficial Intelligence; 2024:18417-18425.\n53. Du X, Novoa-Laurentiev J, Plasek JM, et al. Enhancing early \ndetection of cognitive decline in the elderly: a comparative study \nutilizing large language models in clinical notes. EBioMedicine. \n2024;109:105401. https://doi.org/10.1016/j.ebiom.2024.105401\n54. Embi PJ. Algorithmovigilance—advancing methods to analyze and \nmonitor artificial intelligence–driven health care for effectiveness \nand equity. JAMA Netw Open. 2021;4:e214622. https://doi.org/ \n10.1001/jamanetworkopen.2021.4622\n55. Xi Z, Chen W, Guo X, et al. The rise and potential of large lan­\nguage model based agents: a survey. September 14, 2023, preprint: \nnot peer reviewed.\n56. Wang L, Ma C, Feng X, et al. A survey on large language model \nbased autonomous agents. Front Comput Sci. 2024;18:1-26. \nhttps://doi.org/10.1007/S11704-024-40231-1/METRICS\n57. Li X, Wang S, Zeng S, et al. A survey on LLM-based multi-agent \nsystems: workflow, infrastructure, and challenges. Vicinagearth. \n2024;1:9. https://doi.org/10.1007/s44336-024-00009-2\n58. Yang J, Shu L, Duan H, et al. RDguru: a conversational intelligent \nagent for rare diseases. IEEE J Biomed Health Inform. Published \nonline September 19, 2024. https://doi.org/10.1109/JBHI.2024. \n3464555\n59. Ren R, Wang Y, Qu Y, et al. Investigating the factual knowledge \nboundary of large language models with retrieval augmentation. \n2023, preprint: not peer reviewed. https://arxiv.org/abs/2307. \n11019\n60. Weed LL. Medical records that guide and teach. N Engl J Med. \n1968;278:593-600. https://doi.org/10.1056/NEJM19680314278 \n1105/ASSET/9EE62BDC-88EB-469C-BCDC-DB379C2CAE47/ \nASSETS/IMAGES/MEDIUM/NEJM196803142781105_F2.GIF\n61. McCoy LG, Manrai AK, Rodman A. Large language mod­\nels and the degradation of the medical record. N Engl J \nMed. \n2024;391:1561-1564. \nhttps://doi.org/10.1056/ \nNEJMP2405999\n62. Mandel JC, Kreda DA, Mandl KD, et al. SMART on FHIR: a \nstandards-based, interoperable apps platform for electronic health \nrecords. J Am Med Inform Assoc. 2016;23:899-908. https://doi. \norg/10.1093/jamia/ocv189\n© The Author(s) 2025. Published by Oxford University Press on behalf of the American Medical Informatics Association.\nThis is an Open Access article distributed under the terms of the Creative Commons Attribution-NonCommercial License (https://creativecommons.org/licenses/by-nc/ \n4.0/), which permits non-commercial re-use, distribution, and reproduction in any medium, provided the original work is properly cited. For commercial re-use, please \ncontact reprints@oup.com for reprints and translation rights for reprints. All other permissions can be obtained through our RightsLink service via the Permissions link \non the article page on our site—for further information please contact journals.permissions@oup.com.\nJournal of the American Medical Informatics Association, 2025, 32, 605–615\nhttps://doi.org/10.1093/jamia/ocaf008\nResearch and Applications\nJournal of the American Medical Informatics Association, 2025, Vol. 32, No. 4                                                                                                      \n615 \nDownloaded from https://academic.oup.com/jamia/article/32/4/605/7954485 by guest on 11 January 2026", "metadata": {"producer": "PDFlib+PDI 9.0.7p3 (C++/Win32); modified using iTextSharp.LGPLv2.Core 3.7.4.0", "creator": "Servigistics Arbortext Advanced Print Publisher 11.1.4667/W", "creationdate": "2025-03-22T11:01:16+05:30", "source": "/home/donbr/open-biosciences/biosciences-research/data/raw/ocaf008.pdf", "file_path": "/home/donbr/open-biosciences/biosciences-research/data/raw/ocaf008.pdf", "total_pages": 11, "format": "PDF 1.5", "title": "Improving large language model applications in biomedicine with retrieval-augmented generation: a systematic review, meta-analysis, and clinical development guidelines", "author": "", "subject": "Doi: 10.1093/jamia/ocaf008 Journal of the American Medical Informatics Association, 32, 4, 2025 Publication Date: 15/01/2025 Abstract Objective The objectives of this study are to synthesize findings from recent research of retrieval-augmented generation (RAG) and large language models (LLMs) in biomedicine and provide clinical development guidelines to improve effectiveness.Materials and Methods We conducted a systematic literature review and a meta-analysis. The report was created in adherence to the Preferred Reporting Items for Systematic Reviews and Meta-Analyses 2020 analysis. Searches were performed in 3 databases (PubMed, Embase, PsycINFO) using terms related to “retrieval augmented generation” and “large language model,” for articles published in 2023 and 2024. We selected studies that compared baseline LLM performance with RAG performance. We developed a random-effect meta-analysis model, using odds ratio as the effect size.Results Among 335 studies, 20 were included in this literature review. The pooled effect size was 1.35, with a 95% confidence interval of 1.19-1.53, indicating a statistically significant effect (P = .001). We reported clinical tasks, baseline LLMs, retrieval sources and strategies, as well as evaluation methods.Discussion Building on our literature review, we developed Guidelines for Unified Implementation and Development of Enhanced LLM Applications with RAG in Clinical Settings to inform clinical applications using RAG.Conclusion Overall, RAG implementation showed a 1.35 odds ratio increase in performance compared to baseline LLMs. Future research should focus on (1) system-level enhancement: the combination of RAG and agent, (2) knowledge-level enhancement: deep integration of knowledge into LLM, and (3) integration-level enhancement: integrating RAG systems within electronic health records.", "keywords": "large language model; retrieval augmented generation; systematic review; meta-analysis", "moddate": "2026-01-11T01:13:27+00:00", "trapped": "", "modDate": "D:20260111011327+00'00'", "creationDate": "D:20250322110116+05'30'", "page": 10}}
{"page_content": "Joy et. al \n1 \n \nFederated Knowledge Retrieval Elevates Large Language Model Performance on \nBiomedical Benchmarks \nJanet Joy1,2, Andrew I. Su1,2 \n1Department of Integrative Structural and Computational Biology, Scripps Research, La \nJolla, CA, USA. \n2Correspondence: Janet Joy (jjoy@scripps.edu) and Andrew I. Su (asu@scripps.edu) \nMain Figures: 4 \nSupplemental Figures: 11 \nSupplemental Table: 1 \nAbstract \n \nBackground \nLarge language models (LLMs) have significantly advanced natural language \nprocessing in biomedical research, however, their reliance on implicit, statistical \nrepresentations often results in factual inaccuracies or hallucinations, posing significant \nconcerns in high-stakes biomedical contexts. \nResults \nTo overcome these limitations, we developed BTE-RAG, a retrieval-augmented \ngeneration framework that integrates the reasoning capabilities of advanced language \nmodels with explicit mechanistic evidence sourced from BioThings Explorer, an API \nfederation of more than sixty authoritative biomedical knowledge sources. We \nsystematically evaluated BTE-RAG in comparison to traditional LLM-only methods \nacross three benchmark datasets that we created from DrugMechDB. These datasets \nspecifically targeted gene-centric mechanisms (798 questions), metabolite effects (201 \nquestions), and drug–biological process relationships (842 questions). On the gene-\ncentric task, BTE-RAG increased accuracy from 51% to 75.8% for GPT-4o mini and \nfrom 69.8% to 78.6% for GPT-4o. In metabolite-focused questions, the proportion of \nresponses with cosine similarity scores of at least 0.90 rose by 82% for GPT-4o mini \nand 77% for GPT-4o. While overall accuracy was consistent in the drug–biological \nprocess benchmark, the retrieval method enhanced response concordance, producing a \ngreater than 10% increase in high-agreement answers (from 129 to 144) using GPT-4o. \nConclusion \nFederated knowledge retrieval provides transparent improvements in accuracy for large \nlanguage models, establishing BTE-RAG as a valuable and practical tool for \nmechanistic exploration and translational biomedical research. \n.\nCC-BY 4.0 International license\navailable under a\n(which was not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made \nThe copyright holder for this preprint\nthis version posted August 2, 2025. \n; \nhttps://doi.org/10.1101/2025.08.01.668022\ndoi: \nbioRxiv preprint", "metadata": {"producer": "Microsoft® Word for Microsoft 365", "creator": "Appligent AppendPDF Pro 5.5", "creationdate": "2025-07-31T21:14:35-07:00", "source": "/home/donbr/open-biosciences/biosciences-research/data/raw/2025.08.01.668022v1.full.pdf", "file_path": "/home/donbr/open-biosciences/biosciences-research/data/raw/2025.08.01.668022v1.full.pdf", "total_pages": 31, "format": "PDF 1.7", "title": "32182735", "author": "janet", "subject": "", "keywords": "", "moddate": "2026-01-10T17:11:54-08:00", "trapped": "", "modDate": "D:20260110171154-08'00'", "creationDate": "D:20250731211435-07'00'", "page": 0}}
{"page_content": "Joy et. al \n2 \n \n1 Introduction \nLarge language models (LLMs) have rapidly advanced the state of natural-language \nprocessing, reaching or surpassing expert performance across a wide range of \nbiomedical tasks, including cell type annotation, protein-structure prediction and \nautomated synthesis of clinical-trial results 1–6. However, the underlying generative \nmethodology of these models, which sequentially predict tokens based on statistical \npatterns learned from massive text corpora, renders them susceptible to hallucinations, \ndefined as outputs that are syntactically fluent yet factually incorrect 7,8. Such \ninaccuracies pose significant risks in biomedicine, where even minor errors can \nmisdirect research efforts, delay critical therapeutic discoveries, or compromise patient \nsafety 7,9–11. Indeed, recent assessments underscore that hallucination rates remain too \nhigh for safe and effective deployment in clinical and research-intensive environments \n12,13. \nEfforts to mitigate these hallucinations through domain-specific pre-training and prompt \nengineering have yielded only incremental improvements, as these approaches \ncontinue to embed knowledge implicitly within opaque model parameters and fail to \nreliably surface evidence provenance 14–16. Retrieval-augmented generation (RAG) has \nemerged as a promising solution, explicitly grounding model-generated responses by \ndynamically incorporating external, verifiable evidence into prompts 17–19. Within \nbiomedical question-answering contexts, RAG approaches consistently reduce \nhallucinations and elevate factual accuracy compared to parameter-only models. \nNonetheless, the efficacy of RAG hinges critically on the precision, comprehensiveness, \nand currency of the retrieved contextual evidence 20–22. \nKnowledge graphs (KGs) are particularly compelling resources for RAG because they \nexplicitly represent biological entities and their relationships, support multi-hop \nmechanistic reasoning, and maintain persistent identifiers that simplify provenance \ntracking 23–26. Yet most biomedical KGs are tuned to a narrow slice of biology (for \nexample, protein–protein interactions) or require extensive curation to remain current, \nlimiting their utility for cross-domain mechanistic reasoning. To address these \nchallenges, BioThings Explorer (BTE) integrates and federates 61 authoritative \nbiomedical APIs into a continuously updated meta-knowledge graph that encompasses \ngenes, pathways, drugs, diseases, phenotypes, and more 27. The API-centric framework \nof BTE returns structured JSON triples annotated with semantic types and evidence \ncitations from reputable biomedical databases such as Gene Ontology, DrugBank, and \nPubmed central using Translator Reasoner API (TRAPI) specification 28–30. \nHere, we introduce BTE–RAG (BioThings Explorer–Retrieval-Augmented Generation), \na novel framework that integrates the conversational fluency and reasoning capabilities \nof advanced LLMs with the explicit, multi-domain mechanistic knowledge captured by \nBTE. BTE–RAG dynamically executes targeted, query-focused graph traversals to \nretrieve concise, mechanistically pertinent evidence, formulates this evidence into \ndeclarative context statements, and augments model prompts accordingly. \nTo rigorously assess the performance of BTE-RAG in biomedical question answering, \nwe systematically created three specialized benchmark datasets from DrugMechDB, a \ncurated knowledge base containing 5,666 expert-annotated mechanistic pathways with \nliterature validation 31. These datasets consist of gene-centric (n = 798), metabolite-\n.\nCC-BY 4.0 International license\navailable under a\n(which was not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made \nThe copyright holder for this preprint\nthis version posted August 2, 2025. \n; \nhttps://doi.org/10.1101/2025.08.01.668022\ndoi: \nbioRxiv preprint", "metadata": {"producer": "Microsoft® Word for Microsoft 365", "creator": "Appligent AppendPDF Pro 5.5", "creationdate": "2025-07-31T21:14:35-07:00", "source": "/home/donbr/open-biosciences/biosciences-research/data/raw/2025.08.01.668022v1.full.pdf", "file_path": "/home/donbr/open-biosciences/biosciences-research/data/raw/2025.08.01.668022v1.full.pdf", "total_pages": 31, "format": "PDF 1.7", "title": "32182735", "author": "janet", "subject": "", "keywords": "", "moddate": "2026-01-10T17:11:54-08:00", "trapped": "", "modDate": "D:20260110171154-08'00'", "creationDate": "D:20250731211435-07'00'", "page": 1}}
{"page_content": "Joy et. al \n3 \n \ncentric (n = 201), and drug-centric (n = 842) question–answer pairs, each explicitly \nreflecting the causal flow from drug through intermediate biological nodes to disease \noutcomes. Across all three DrugMechDB-derived benchmarks, BTE–RAG robustly \nimproves factual grounding, accelerates convergence to correct responses over diverse \nbiomedical entities relative to an LLM-only baseline. \nCollectively, these findings establish BTE–RAG as a powerful, practical tool for reducing \nhallucination risks and enhancing mechanistic clarity, significantly advancing the \ntransparency, reliability, and utility of language model-driven biomedical discovery and \nclinical decision-making. \n  \n2 Materials & Methods \n2.1 BTE–RAG Framework and Baseline Comparison \nThe BTE–RAG framework systematically compares two distinct inference routes to \nevaluate the impact of structured, mechanistic context on large language model (LLM) \noutputs (Figure 1A). The first inference route, labeled \"LLM-only,\" directly submits user-\ngenerated questions to the language model without external context augmentation. The \nsecond route, labeled \"BTE–RAG,\" integrates structured mechanistic evidence retrieved \nfrom BioThings Explorer prior to submitting an enriched, evidence-supported prompt to \nthe same language model. This dual-path design allows rigorous evaluation of how \nexplicitly retrieved context influences both answer accuracy and the factual grounding of \nmodel-generated responses. The BTE–RAG architecture comprises three key phases: \nentity recognition, knowledge-graph-based retrieval via BTE, and generative inference \nutilizing context-augmented LLM prompting. \nEntity Recognition \nThe retrieval phase begins with precise identification of biomedical entities mentioned \nwithin each input question. For the current benchmarks, entities such as drugs, \ndiseases, metabolites, and biological processes were pre-annotated and standardized \nto established knowledge graph identifiers, enabling automated recognition at runtime. \nAdditionally, the framework includes a zero-shot entity extraction module that leverages \na specialized task-oriented prompting approach. This module is currently optimized for \nretrieving drugs and diseases from queries, with potential to extend extraction \ncapabilities to include other biomedical entities as needed. \nKnowledge Graph Retrieval \nIdentified biomedical entities are translated into structured queries interfacing directly \nwith BTE. BTE integrates 61 authoritative biomedical databases under a unified \nknowledge graph schema, accessible via the programmatic API endpoint (/v1/query). \nEach query to BTE specifies an input entity (e.g., disease, drug, or biological process) \nalong with desired output entity categories, following the TRAPI query format. In \nresponse, BTE returns structured JSON data that includes a detailed knowledge graph \ncontaining two key components: \"nodes,\" which describe biomedical entities along with \ntheir semantic categories and standardized names; and \"edges,\" which specify the \n.\nCC-BY 4.0 International license\navailable under a\n(which was not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made \nThe copyright holder for this preprint\nthis version posted August 2, 2025. \n; \nhttps://doi.org/10.1101/2025.08.01.668022\ndoi: \nbioRxiv preprint", "metadata": {"producer": "Microsoft® Word for Microsoft 365", "creator": "Appligent AppendPDF Pro 5.5", "creationdate": "2025-07-31T21:14:35-07:00", "source": "/home/donbr/open-biosciences/biosciences-research/data/raw/2025.08.01.668022v1.full.pdf", "file_path": "/home/donbr/open-biosciences/biosciences-research/data/raw/2025.08.01.668022v1.full.pdf", "total_pages": 31, "format": "PDF 1.7", "title": "32182735", "author": "janet", "subject": "", "keywords": "", "moddate": "2026-01-10T17:11:54-08:00", "trapped": "", "modDate": "D:20260110171154-08'00'", "creationDate": "D:20250731211435-07'00'", "page": 2}}
{"page_content": "Joy et. al \n4 \n \nexplicit relationships (predicates) between pairs of entities, supplemented by \nprovenance details indicating the primary knowledge sources. \nFor each benchmark dataset, targeted queries were structured to retrieve \nmechanistically relevant context. Specifically, in the gene-centric benchmark, queries \nseparately utilized disease and drug entities to retrieve directly linked gene and protein \nnodes. In the metabolite-centric benchmark, disease and chemical (drug) entities were \nqueried independently to identify connected biochemical entities. For the drug–\nbiological process benchmark, separate queries using disease entities and biological \n \nFigure 1: Retrieval‑Augmented Generation workflow and derivation of mechanistic \nevaluation benchmarks. \n(A) Schematic of the BTE-RAG pipeline, which augments large language model (LLM) \nresponses with context retrieved from the BioThings Explorer (BTE) knowledge graph. In \nthe LLM-only pathway, the model generates a response using only the input question. In \ncontrast, BTE-RAG operates in two phases: a Retrieval Phase, where relevant entities \nare extracted from the question and queried against BTE to collect mechanistically \nrelevant subject–predicate–object triples, and a Generative Phase, where this curated \ncontext is appended to the input question and passed to the LLM. The resulting outputs: \nLLM-only or BTE-RAG, can be directly compared to assess the impact of knowledge-\naugmented generation. \n(B) Construction of benchmark datasets from DrugMechDB, a curated biomedical \nknowledge graph of drug–disease mechanisms. Directed paths connecting a drug to a \ndisease were mined and transformed into structured questions targeting different \nmechanistic facets: (i) gene nodes (Mechanistic Gene Benchmark), (ii) biochemical \nentities or metabolites (Metabolite Benchmark), and (iii) drug–biological process–\ndisease paths (Drug Benchmark). Each benchmark provides paired questions and \ngold‑standard labels for rigorous, domain‑specific evaluation of retrieval‑augmented \ngeneration. \n.\nCC-BY 4.0 International license\navailable under a\n(which was not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made \nThe copyright holder for this preprint\nthis version posted August 2, 2025. \n; \nhttps://doi.org/10.1101/2025.08.01.668022\ndoi: \nbioRxiv preprint", "metadata": {"producer": "Microsoft® Word for Microsoft 365", "creator": "Appligent AppendPDF Pro 5.5", "creationdate": "2025-07-31T21:14:35-07:00", "source": "/home/donbr/open-biosciences/biosciences-research/data/raw/2025.08.01.668022v1.full.pdf", "file_path": "/home/donbr/open-biosciences/biosciences-research/data/raw/2025.08.01.668022v1.full.pdf", "total_pages": 31, "format": "PDF 1.7", "title": "32182735", "author": "janet", "subject": "", "keywords": "", "moddate": "2026-01-10T17:11:54-08:00", "trapped": "", "modDate": "D:20260110171154-08'00'", "creationDate": "D:20250731211435-07'00'", "page": 3}}
{"page_content": "Joy et. al \n5 \n \nprocess entities were conducted to retrieve associated chemical entities (drugs). Upon \nreceiving the structured knowledge graph responses from BTE, both node and edge \ninformation were systematically processed. Nodes were extracted along with their \nsemantic categories and descriptive names, while edges were parsed to identify \nsubject-object pairs, predicates, and associated primary knowledge sources. Nodes and \nedges were subsequently merged to construct coherent statements that succinctly \ndescribe each mechanistic relationship (e.g., \"drug X inhibits gene Y\"). These concise, \nnatural-language context statements collectively formed the mechanistic evidence \nprovided to the language models during the generative inference phase, significantly \nenhancing the transparency, interpretability, and accuracy of the generated outputs. \nSupplementary Figure S1 provides a detailed schematic illustrating the complete BTE–\nRAG pipeline workflow, demonstrating a representative query and the subsequent \nprocessing and integration steps. \nContext Selection \nTwo distinct evidence-inclusion strategies were systematically assessed for each \nquestion. The first strategy incorporates the entire set of sentences retrieved by BTE, \nleveraging the extensive 128,000-token context window of GPT-4o 32. The second \nstrategy employs sentence-level cosine similarity filtering using ‘S-PubMedBert-MS-\nMARCO’ embeddings, retaining only sentences whose similarity scores with the query \nexceed a predefined percentile threshold 33. Running these two strategies concurrently \nenables a direct evaluation of the impact of comprehensive versus selectively pruned \ncontextual evidence under identical experimental conditions. \nGenerative Inference \nFor the generative phase, selected context sentences and the original query were \nconcatenated to form an enriched prompt submitted to both GPT-4o and GPT-4o-mini \nmodels. Models were configured deterministically (temperature set to 0) to produce \nreproducible outputs. Parallel runs of the LLM-only baseline used identical questions \nwithout the BTE-derived context. To streamline downstream analyses and ensure \nobjective comparisons, language models were instructed explicitly to output structured \nJSON responses devoid of extraneous explanatory text. Detailed system prompts are \nshown in Supplementary Table S1. \nEvaluation \nGenerated outputs were benchmarked against curated ground-truth annotations from \nthe constructed mechanistic datasets. For entity-specific tasks (e.g., gene identification), \ncorrectness was evaluated via exact, case-insensitive string matching. For semantically \nnuanced responses (e.g., metabolites and drugs), BioBERT-based embeddings \n(‘BioBERT-mnli-snli-scinli-scitail-mednli-stsb’) quantified the semantic similarity between \nmodel-generated outputs and reference answers 34. Answers surpassing a \npredetermined similarity threshold were classified as accurate. Collectively, these \nstandardized evaluation methodologies ensure scalable, objective, and reproducible \nassessment of the fidelity and biological coherence of model predictions, rigorously \ntesting the utility and impact of knowledge graph-enhanced prompting in biomedical \nreasoning contexts. \n.\nCC-BY 4.0 International license\navailable under a\n(which was not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made \nThe copyright holder for this preprint\nthis version posted August 2, 2025. \n; \nhttps://doi.org/10.1101/2025.08.01.668022\ndoi: \nbioRxiv preprint", "metadata": {"producer": "Microsoft® Word for Microsoft 365", "creator": "Appligent AppendPDF Pro 5.5", "creationdate": "2025-07-31T21:14:35-07:00", "source": "/home/donbr/open-biosciences/biosciences-research/data/raw/2025.08.01.668022v1.full.pdf", "file_path": "/home/donbr/open-biosciences/biosciences-research/data/raw/2025.08.01.668022v1.full.pdf", "total_pages": 31, "format": "PDF 1.7", "title": "32182735", "author": "janet", "subject": "", "keywords": "", "moddate": "2026-01-10T17:11:54-08:00", "trapped": "", "modDate": "D:20260110171154-08'00'", "creationDate": "D:20250731211435-07'00'", "page": 4}}
{"page_content": "Joy et. al \n6 \n \n2.2 Datasets from DMDB \n2.2 Construction of Mechanistic Question–Answer Benchmarks from \nDrugMechDB  \nDrugMechDB is a rigorously curated biomedical knowledge graph designed to represent \ntherapeutic mechanisms through explicit stepwise paths. These pathways originate from \ndrug nodes, traverse biologically meaningful intermediate entities, and culminate at \ndisease nodes, collectively delineating mechanisms underlying drug–disease \ninteractions 31. The current version of DrugMechDB contains 5,666 curated mechanistic \npathways, providing comprehensive coverage for 4,583 distinct drug–disease \nindications. Each node within DrugMechDB is systematically mapped to a standardized \nBiolink category and anchored to stable identifiers, while each relationship (edge) is \nannotated with a controlled predicate 35. This structured, granular, and provenance-rich \nresource enables robust benchmarking of computational models focused on \nmechanistic inference rather than simple associative or co-occurrence patterns. \nTo comprehensively evaluate the BTE–RAG framework across multiple levels of \nbiological resolution, DrugMechDB was systematically transformed into three \ncomplementary mechanistic question–answer (QA) benchmarks, each highlighting a \ndistinct biological focus: genes, metabolites, and drugs (Figure 1B). \nGene-Centric Benchmark: Mechanistic pathways were initially filtered to retain those \ncontaining exactly one internal node annotated as a Gene entity. Gene identifiers were \nresolved into standardized HGNC symbols using MyGene.info services; pathways \ncontaining deprecated or ambiguous identifiers were systematically excluded 36. Each \nremaining mechanistic pathway was converted into a structured question of the form: \n\"Which gene plays the most significant mechanistic role in how Drug 'X' treats or \nimpacts Disease 'Y'?\" The corresponding HGNC gene symbol served as the definitive \nground truth. Following deduplication across different indications, this dataset \ncomprised 798 unique QA pairs. \nMetabolite-Centric Benchmark: To capture downstream biochemical effects, pathways \nexclusively containing taxonomic relationships (such as “subclass” predicates) were \nremoved to ensure mechanistic specificity. Selected pathways included exactly one \nmetabolite node, identified specifically by filtering node identifiers prefixed with \"CHEBI:\" \nto denote biochemical entities. Records containing multiple mechanistic pathways were \nexcluded to maintain dataset simplicity and clarity. Each qualifying pathway was \nformulated into the structured question: \"Which biochemical entity is affected by Drug 'X' \nvia its mechanism of action in treating Disease 'Y'?\" The metabolite node identified via \nCHEBI identifiers served as the ground truth answer, yielding a final dataset of 201 \nunique QA pairs. \nDrug-Centric Benchmark: A third benchmark dataset was developed to evaluate the \nability of computational models to infer therapeutic agents when provided with a disease \nand a mediating biological process. Pathways were selected specifically if they included \nexactly one BiologicalProcess node, and drugs lacking resolvable identifiers from \nDrugBank or MESH databases were excluded to ensure accurate and standardized \nidentification. Each qualifying path was structured into the question: \"Which drug can be \n.\nCC-BY 4.0 International license\navailable under a\n(which was not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made \nThe copyright holder for this preprint\nthis version posted August 2, 2025. \n; \nhttps://doi.org/10.1101/2025.08.01.668022\ndoi: \nbioRxiv preprint", "metadata": {"producer": "Microsoft® Word for Microsoft 365", "creator": "Appligent AppendPDF Pro 5.5", "creationdate": "2025-07-31T21:14:35-07:00", "source": "/home/donbr/open-biosciences/biosciences-research/data/raw/2025.08.01.668022v1.full.pdf", "file_path": "/home/donbr/open-biosciences/biosciences-research/data/raw/2025.08.01.668022v1.full.pdf", "total_pages": 31, "format": "PDF 1.7", "title": "32182735", "author": "janet", "subject": "", "keywords": "", "moddate": "2026-01-10T17:11:54-08:00", "trapped": "", "modDate": "D:20260110171154-08'00'", "creationDate": "D:20250731211435-07'00'", "page": 5}}
{"page_content": "Joy et. al \n7 \n \nused in the treatment of Disease 'Y' by targeting Biological Process 'P'?\" The \ncorresponding drug node served as the ground truth. After thorough harmonization and \nstringent quality control measures, this benchmark comprised 842 unique QA pairs. \nThe resulting benchmarks thus offer a robust, multiscale evaluation platform specifically \ndesigned to probe the mechanistic inference capabilities of knowledge-graph-\naugmented language models comprehensively and rigorously. \n2.3 Use of Large Language Models: \nAll natural-language processing steps were carried out with two OpenAI models, \nGPT-4o-mini (snapshot 2024-07-18) and GPT-4o (snapshot 2024-08-06) 32,37. Both \nmodels were invoked through the OpenAI API. The temperature parameter was fixed at \n0.0 for every request, thereby forcing deterministic decoding and facilitating \nreproducible evaluation. Each model accepts up to 128,000 input tokens and can return \na maximum of 16,384 completion tokens. Although GPT-4o-mini is substantially smaller \nin parameter count than GPT-4o, both models share the same context window size, \npermitting a controlled comparison of model capacity while holding prompt length \nconstant 32,37. At the time the experiments were executed, GPT-4o-mini was priced at \n0.15 USD per million input tokens and 0.60 USD per million output tokens. The \ncorresponding prices for GPT-4o were 2.50 USD and 10.00 USD, respectively. Model \nversions were pinned by explicit snapshot identifiers to eliminate the possibility of \nversion drift during the study period. Snapshot documentation is archived at \n https://platform.openai.com/docs/models/gpt-4o-mini and \n https://platform.openai.com/docs/models/gpt-4o. ChatGPT was used to assist with \ngrammar correction and to improve conciseness in the manuscript. \nPrompt engineering \nEach request began with a concise system prompt defining the model’s role 38–40. Two \ndistinct system prompts were prepared per dataset: one for the standalone LLM \nbaseline, and one tailored for the retrieval-augmented BTE–RAG workflow. Queries \nwere provided directly to the model without additional contextual examples, employing a \nzero-shot prompting approach. To facilitate efficient and accurate downstream \nprocessing, the model was instructed to produce responses strictly in a predefined \nJSON format, omitting supplementary explanatory text. \n3 Results \nWe developed BTE–RAG, a retrieval-augmented generation framework designed to \nenhance large language models (LLMs) by integrating mechanistic evidence from \nBioThings Explorer (BTE), a federated biomedical knowledge graph. BTE–RAG embeds \nstructured, graph-derived context into prompts to improve mechanistic accuracy, ensure \nexplicit provenance, and facilitate higher-order reasoning. We benchmarked the \nperformance of BTE–RAG versus an LLM-only baseline across three distinct \nmechanistic reasoning tasks: gene identification, drug–metabolite interactions, and \ndrug–biological-process relationships. \n3.1 Mechanistic Gene Prediction \n.\nCC-BY 4.0 International license\navailable under a\n(which was not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made \nThe copyright holder for this preprint\nthis version posted August 2, 2025. \n; \nhttps://doi.org/10.1101/2025.08.01.668022\ndoi: \nbioRxiv preprint", "metadata": {"producer": "Microsoft® Word for Microsoft 365", "creator": "Appligent AppendPDF Pro 5.5", "creationdate": "2025-07-31T21:14:35-07:00", "source": "/home/donbr/open-biosciences/biosciences-research/data/raw/2025.08.01.668022v1.full.pdf", "file_path": "/home/donbr/open-biosciences/biosciences-research/data/raw/2025.08.01.668022v1.full.pdf", "total_pages": 31, "format": "PDF 1.7", "title": "32182735", "author": "janet", "subject": "", "keywords": "", "moddate": "2026-01-10T17:11:54-08:00", "trapped": "", "modDate": "D:20260110171154-08'00'", "creationDate": "D:20250731211435-07'00'", "page": 6}}
{"page_content": "Joy et. al \n8 \n \nWe first assessed the effect of knowledge graph augmentation on gene-level \nmechanistic inference using 798 curated drug–disease pairs from DrugMechDB. \nQueries were structured as: \"Which gene plays the most significant mechanistic role in \nhow Drug 'X' treats or impacts Disease 'Y'?\" Two models, GPT-4o and the smaller GPT-\n4o-mini were evaluated in two experimental conditions: (i) LLM-only, providing no \nadditional context, and (ii) BTE–RAG, incorporating evidence retrieved via BTE. \n \nUnder the LLM-only condition, GPT-4o-mini correctly answered 407 queries \n(Supplementary Figure S2B), achieving an accuracy of 51%. Augmenting prompts with \nBTE-derived evidence markedly increased accuracy to 75.8% (Figure 2A), a substantial \nabsolute improvement of 24.8 percentage points (Supplementary Figure S2C). The \nlarger GPT-4o model demonstrated an accuracy of 69.8% in the baseline condition, \nwhich increased to 78.6% (627 correct answers, Supplementary Figure S3B) when \nsupplemented with BTE context (Figure 2B), reflecting an absolute gain of 8.8 \npercentage points (Supplementary Figure S3C).  \nBecause knowledge‑graph queries can return superfluous triples, we evaluated a \nsimple similarity‑based pruning strategy. Specifically, both the user queries and the \ncontext statements were embedded using the sentence embedding model 'S-\nPubMedBert-MS-MARCO' 33. Context statements were then ranked based on cosine \n \nFigure 2: Retrieval‑augmented generation with BTE-RAG markedly improves factual \naccuracy of gene-centric benchmark using GPT‑4o models. \n(A) For the compact gpt‑4o‑mini model, introducing the BTE-RAG retrieval layer raised \noverall accuracy from 51% (hatched bar, LLM‑only baseline) to 75.8 % (solid bar). \n(B) The same intervention applied to the larger gpt‑4o model increased accuracy from \n69.8% to 78.6 %. Accuracy was calculated as the proportion of correct answers across the \ncomposite biomedical question‑answering benchmark described in Methods. \n.\nCC-BY 4.0 International license\navailable under a\n(which was not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made \nThe copyright holder for this preprint\nthis version posted August 2, 2025. \n; \nhttps://doi.org/10.1101/2025.08.01.668022\ndoi: \nbioRxiv preprint", "metadata": {"producer": "Microsoft® Word for Microsoft 365", "creator": "Appligent AppendPDF Pro 5.5", "creationdate": "2025-07-31T21:14:35-07:00", "source": "/home/donbr/open-biosciences/biosciences-research/data/raw/2025.08.01.668022v1.full.pdf", "file_path": "/home/donbr/open-biosciences/biosciences-research/data/raw/2025.08.01.668022v1.full.pdf", "total_pages": 31, "format": "PDF 1.7", "title": "32182735", "author": "janet", "subject": "", "keywords": "", "moddate": "2026-01-10T17:11:54-08:00", "trapped": "", "modDate": "D:20260110171154-08'00'", "creationDate": "D:20250731211435-07'00'", "page": 7}}
{"page_content": "Joy et. al \n9 \n \nsimilarity scores relative to the embedded query, and those statements falling within the \nlowest 10% similarity scores were removed to retain only the most relevant context \nlines. This lightweight filtering strategy preserved, and in some cases slightly enhanced \nperformance across all evaluated accuracy metrics (Supplementary Figure S2A, S3A), \nsuggesting that excluding the least relevant context statements can beneficially impact \nthe accuracy of gene-level reasoning tasks. Cross-tabulation provided quantitative detail \non the effect of retrieval augmentation, illustrating that BTE context flipped 245 \npreviously incorrect answers to correct for GPT-4o-mini and 119 for GPT-4o \n(Supplementary Figure S2D, S3D). \n \nTogether, these findings illustrate that structured mechanistic context provided through \nBTE significantly enhances gene-level reasoning performance, particularly amplifying \nthe capabilities of smaller-scale models such as GPT-4o-mini. The accuracy \nimprovements observed even in GPT-4o highlight that state-of-the-art models retain \nlatent knowledge gaps effectively bridged by integrating curated biomedical graphs and \nselectively pruning irrelevant content. \n3.2 Prediction of Drug–Metabolite Relationships \nTo gauge whether retrieval augments the mechanistic fidelity of metabolite‑level \nreasoning, we posed 201 queries of the form “Which biochemical entity is affected by \nDrug X via its mechanism of action in treating Disease Y?” using the DrugMechDB‑\nderived Drug → Metabolite → Disease paths. Because metabolite names are much less \nstandardized than gene names, we scored the answer quality by computing a semantic \nconcordance between each model answer and the gold standard metabolite. Semantic \nconcordance was based on cosine similarity of text embeddings using the BioBERT-\nSTSB text embedding model, a metric that rewards graded lexical and semantic overlap \nrather than exact string identity 34. \nRank‑ordered similarity curves in Figure 3A immediately reveal the effect of \naugmentation: for both gpt‑4o‑mini (orange) and gpt‑4o (blue), the BTE‑RAG trace \n(solid line) departs from the prompt‑only baseline (dashed line) after ~130 ranked \nquestions (cosine ≈ 0.70) and widens steadily, nearly doubling the number of answers \nthat reach the high‑fidelity zone (cosine ≥ 0.90). \nHistograms for the prompt‑only condition (Figure 3B, gpt4o; Supplementary Figure S4, \ngpt-4o-mini) reveal a pronounced left‑skew: both gpt‑4o‑mini and gpt‑4o peak in the \n0.15–0.30 similarity bins, with medians below 0.30. Only 15 % of answers fall in the \nhigh‑similarity regime (≥ 0.90), indicating that the LLMs frequently retrieve metabolites \nthat are semantically distant from the curated ground truth. \nAppending BTE evidence shifts the distributions rightward across similarity bins (Figure \n3C (gpt-4o), Supplementary Figure S5, S6). For GPT-4o-mini, applying a stringent \ncontext similarity threshold (>80th percentile) increased the number of high-fidelity \nanswers (cosine similarity 0.90–1.00) from 28 to 51 (+82%). Similarly, GPT-4o exhibited \nan increase from 30 to 53 (+77%) under the same conditions. Simultaneously, counts in \nthe mid‑similarity interval (0.40–0.70) contract (Supplementary Figure S5, S6), \n.\nCC-BY 4.0 International license\navailable under a\n(which was not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made \nThe copyright holder for this preprint\nthis version posted August 2, 2025. \n; \nhttps://doi.org/10.1101/2025.08.01.668022\ndoi: \nbioRxiv preprint", "metadata": {"producer": "Microsoft® Word for Microsoft 365", "creator": "Appligent AppendPDF Pro 5.5", "creationdate": "2025-07-31T21:14:35-07:00", "source": "/home/donbr/open-biosciences/biosciences-research/data/raw/2025.08.01.668022v1.full.pdf", "file_path": "/home/donbr/open-biosciences/biosciences-research/data/raw/2025.08.01.668022v1.full.pdf", "total_pages": 31, "format": "PDF 1.7", "title": "32182735", "author": "janet", "subject": "", "keywords": "", "moddate": "2026-01-10T17:11:54-08:00", "trapped": "", "modDate": "D:20260110171154-08'00'", "creationDate": "D:20250731211435-07'00'", "page": 8}}
{"page_content": "Joy et. al \n10 \n \nconfirming that retrieval largely converts borderline predictions into highly concordant \nhits rather than merely redistributing low‑score failures.  \nBecause voluminous context can inflate token budgets, we assessed performance \nwhen progressively discarding lower‑ranked context lines (10th to 90th percentile \ncut‑offs). Rank‑ordered similarity traces (Supplementary Figure S7) show that the \nBTE‑RAG curves remain above or coincide with the prompt‑only baseline throughout \nthe distribution even when 90 % of context is withheld. Histograms (Supplementary \nFigure S5, S6) reinforce this observation: the ≥ 0.90 similarity bin consistently retains \n \nFigure 3: Retrieval‑augmented context increases semantic concordance with \nground‑truth metabolites. \n(A) Cosine‑similarity scores between each generated answer and the corresponding \nreference metabolite (sentence‑transformer embeddings; see Methods) are plotted \nfor all 201 questions in the Metabolite Benchmark, ordered from lowest to highest \nsimilarity. Dashed traces represent the LLM‑only baseline, whereas solid traces \ninclude BioThings Explorer (BTE) retrieval‑augmented context. Orange curves \ndenote gpt‑4o‑mini; blue curves denote gpt‑4o. For both model sizes, BTE‑RAG \nsystematically shifts the similarity distribution upward, indicating improved semantic \nalignment with the curated biochemical ground truth. \n(B) Score distribution GPT-4o, LLM-only. Histogram of cosine-similarity scores for \nGPT-4o answers generated without external context. Bar heights and numeric labels \ndenote the number of questions (n = 201) falling in each bin; the overlaid KDE line \nsummarizes the distribution. \n(C) Score distribution GPT-4o + BTE-RAG. Same format as panel B but for GPT-4o \nanswers generated with BTE-RAG's context. The right-shifted, more peaked \ndistribution highlights the improvement in semantic alignment achieved by \nretrieval-augmented generation. \n.\nCC-BY 4.0 International license\navailable under a\n(which was not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made \nThe copyright holder for this preprint\nthis version posted August 2, 2025. \n; \nhttps://doi.org/10.1101/2025.08.01.668022\ndoi: \nbioRxiv preprint", "metadata": {"producer": "Microsoft® Word for Microsoft 365", "creator": "Appligent AppendPDF Pro 5.5", "creationdate": "2025-07-31T21:14:35-07:00", "source": "/home/donbr/open-biosciences/biosciences-research/data/raw/2025.08.01.668022v1.full.pdf", "file_path": "/home/donbr/open-biosciences/biosciences-research/data/raw/2025.08.01.668022v1.full.pdf", "total_pages": 31, "format": "PDF 1.7", "title": "32182735", "author": "janet", "subject": "", "keywords": "", "moddate": "2026-01-10T17:11:54-08:00", "trapped": "", "modDate": "D:20260110171154-08'00'", "creationDate": "D:20250731211435-07'00'", "page": 9}}
{"page_content": "Joy et. al \n11 \n \n≥ 40 hits for both models across all pruning levels, demonstrating that a concise subset \nof top‑ranked evidence lines is sufficient to drive the bulk of the performance gains. \n3.3 Drug–Biological Process Reasoning \nWe next asked 842 DrugMechDB questions of the form “Which drug can be used in the \ntreatment of Disease Y by targeting Biological Process P?”. Answer fidelity was again \nscored with BioBERT‑STSB cosine similarity 34. In rank‑ordered plots (Figure 4A), the \nprompt‑only (dashed) and BTE‑RAG (solid) curves for both gpt‑4o‑mini (orange) and \n \nFigure 4: Retrieval-augmented generation maintains overall parity yet excels in the \nhigh-fidelity regime of drug-centric mechanistic answers. \n(A) Cosine-similarity scores (sentence-transformer embeddings; see Methods) between \neach generated answer and the reference drug→biological-process pathway are plotted \nfor all 842 questions in the Drug Benchmark, ordered from lowest to highest similarity. \nDashed traces (LLM-only) and solid traces (BTE-RAG) follow nearly overlapping \ntrajectories across most of the distribution, indicating broadly comparable performance \nbetween the two inference modes. However, above a cosine similarity threshold of ≈ 0.7, \nboth gpt-4o-mini (orange) and gpt-4o (blue) curves generated with BTE context surge \nahead of their prompt-only counterparts, revealing a marked advantage in producing \nhighly concordant mechanistic explanations. \n(B) Score distribution GPT-4o, LLM-only. Histogram of cosine-similarity scores for GPT-\n4o answers generated without external context. The hatched bar at 0.90–1.00 marks the \nhigh-fidelity zone, capturing 129 near-perfect matches produced by the baseline model. \n(C) Score distribution GPT-4o + BTE-RAG. Same format as panel B but for GPT-4o \nanswers produced with BTE-RAG's context. The distribution is right-shifted, and the solid \nbar in the 0.90–1.00 high-fidelity zone now contains 144 answers, highlighting the \nenrichment of top-tier mechanistic concordance achieved through retrieval-augmented \ngeneration. \n.\nCC-BY 4.0 International license\navailable under a\n(which was not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made \nThe copyright holder for this preprint\nthis version posted August 2, 2025. \n; \nhttps://doi.org/10.1101/2025.08.01.668022\ndoi: \nbioRxiv preprint", "metadata": {"producer": "Microsoft® Word for Microsoft 365", "creator": "Appligent AppendPDF Pro 5.5", "creationdate": "2025-07-31T21:14:35-07:00", "source": "/home/donbr/open-biosciences/biosciences-research/data/raw/2025.08.01.668022v1.full.pdf", "file_path": "/home/donbr/open-biosciences/biosciences-research/data/raw/2025.08.01.668022v1.full.pdf", "total_pages": 31, "format": "PDF 1.7", "title": "32182735", "author": "janet", "subject": "", "keywords": "", "moddate": "2026-01-10T17:11:54-08:00", "trapped": "", "modDate": "D:20260110171154-08'00'", "creationDate": "D:20250731211435-07'00'", "page": 10}}
{"page_content": "Joy et. al \n12 \n \ngpt‑4o (blue) are nearly super‑imposable through the first ≈ 600 ranked queries \n(cosine < 0.70). Beyond this inflection point, the BTE‑augmented traces bend upward \nmore steeply, yielding a clear margin in the high‑fidelity zone (cosine ≥ 0.80). Thus, \nretrieval does not alter overall parity but selectively boosts the most mechanistically \ndemanding subset of questions. \nPrompt‑only histograms (Figure 4B; Supplementary Figure S8, gpt-4o-mini) peak in the \n0.20–0.35 range, with ~15 % of answers falling in the ≥ 0.90 bin. Appending the full BTE \ncontext nudges the entire distribution rightward (Figure 4C; Supplementary Figure S9-\ntop‑left panels). The ≥ 0.90 bin increases by ≈ 5–10 % for both model sizes. These \nshifts, though smaller than those seen for gene‑ and metabolite tasks, account for the \nlate‑stage separation observed in Figure 4A. \nUnlike the previous tasks, performance here depends on retaining a broad evidentiary \nwindow. When the lowest‑ranked 10–20 % of context lines are removed, the uplift in the \n≥ 0.90 bin attenuates, and the rank‑ordered curves progressively converge toward the \nbaseline (Supplementary Figure S9, S10 & S11). Deeper cuts (> 40 %) essentially erase \nthe retrieval advantage. This suggests that pathway‑level questions draw on a more \ndiffuse set of graph triples than gene or metabolite queries, and aggressive trimming \ncan discard critical relational clues. For drug → biological‑process reasoning, BTE‑RAG \ndelivers targeted gains in the top decile of similarity scores, provided the complete \nknowledge‑graph context is supplied. \nThese findings reinforce that optimal evidence granularity is task‑dependent: concise, \nhigh‑relevance snippets suffice for gene‑ and metabolite‑level inference, whereas \npathway‑level queries benefit from a richer contextual fabric. By grounding LLM outputs \nwithin curated, biologically meaningful pathways, BTE–RAG consistently accelerates \naccurate inference, reduces residual errors, and demonstrates considerable promise for \nadvancing automated biomedical hypothesis generation and therapeutic repurposing \nworkflows. \n4 Discussion \nThe rapid advancement of large language models (LLMs) has profoundly reshaped \nbiomedical natural language processing 41. Despite these advances, current LLMs \npredominantly operate as opaque systems with implicit knowledge representation, \nrendering their factual accuracy challenging to verify and limiting their applicability in \nhigh-stakes biomedical environments. Recent efforts, such as the knowledge-graph \naugmented retrieval approach 21, have successfully enhanced biomedical reasoning by \nintegrating disease-specific embeddings from specialized knowledge graphs such as \nSPOKE 42. We developed BTE–RAG, a novel retrieval-augmented generation pipeline \nthat strategically incorporates explicit mechanistic evidence from BTE 27. By leveraging \nthe extensive and federated biomedical knowledge graph of BTE, our method \nsubstantially broadens the applicability of knowledge-graph augmented strategies to \naddress diverse query types, including those involving genes, proteins, metabolites, \nbiological processes, diseases and chemical substances. This capability allows BTE–\nRAG to support complex, multi-domain biomedical inquiries, significantly extending \nbeyond disease-centric queries alone. Our comparative analysis, utilizing a direct \"LLM-\n.\nCC-BY 4.0 International license\navailable under a\n(which was not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made \nThe copyright holder for this preprint\nthis version posted August 2, 2025. \n; \nhttps://doi.org/10.1101/2025.08.01.668022\ndoi: \nbioRxiv preprint", "metadata": {"producer": "Microsoft® Word for Microsoft 365", "creator": "Appligent AppendPDF Pro 5.5", "creationdate": "2025-07-31T21:14:35-07:00", "source": "/home/donbr/open-biosciences/biosciences-research/data/raw/2025.08.01.668022v1.full.pdf", "file_path": "/home/donbr/open-biosciences/biosciences-research/data/raw/2025.08.01.668022v1.full.pdf", "total_pages": 31, "format": "PDF 1.7", "title": "32182735", "author": "janet", "subject": "", "keywords": "", "moddate": "2026-01-10T17:11:54-08:00", "trapped": "", "modDate": "D:20260110171154-08'00'", "creationDate": "D:20250731211435-07'00'", "page": 11}}
{"page_content": "Joy et. al \n13 \n \nonly\" approach versus the BTE-augmented strategy (Figure 1A) across three rigorously \nconstructed DrugMechDB benchmarks (Figure 1B), demonstrates that incorporating \nexplicit, structured context significantly elevates answer accuracy, enhances \ntransparency, and allows smaller, more computationally efficient models to perform \ncompetitively with leading-edge systems. The granularity, explicit mechanistic \ngrounding, and high-quality source attribution inherent in these benchmarks uniquely \nposition them for probing the causal inference capabilities of language models. \nComparable mechanistically focused datasets remain scarce in the biomedical domain, \nas existing resources like PubMedQA or Natural Questions predominantly target \ndocument-level retrieval or summarization rather than deep mechanistic inference 43,44. \nTraditional LLMs accumulate domain-specific knowledge implicitly during pre-training by \nstatistically modeling large collections of biomedical texts. Although this method yields \nlinguistically coherent responses, it inherently exposes models to the risk of \nhallucinations, particularly in scenarios involving sparse biomedical facts or multi-step \nmechanistic reasoning. By contrast, retrieval-augmented generation explicitly anchors \nmodel predictions in verifiable external sources, constraining generation to well-\nsubstantiated evidence. BTE–RAG advances this paradigm by dynamically federating \n61 authoritative biomedical APIs into a single cohesive meta-graph, thereby enabling \nreal-time inclusion of newly curated knowledge in generated responses and ensuring \nreproducible benchmarking through cached retrievals. \nFour critical design principles underpin the efficacy of the BTE–RAG framework. First, \nthe framework leverages an API-centric federation layer that integrates trusted \nbiomedical data sources, including MyGene.info, Gene Ontology, CTDbase, Pubmed \ncentral, CHEBI, disease-ontology, DrugBank and more, through unified interface of BTE \n29,36,45–47. Second, it employs semantic query templates aligned with the Translator \nReasoner API (TRAPI) standard, selectively retrieving only the most relevant \nrelationships for each question, thereby avoiding extraneous contextual noise. Third, \nretrieved knowledge graph triples are translated into succinct, directionally explicit \ndeclarative statements, seamlessly integrating structured knowledge with natural-\nlanguage prompts. Fourth, BTE–RAG incorporates flexible context-selection strategies; \nfull-context utilization and cosine similarity-based pruning for scenarios requiring \nconcise, highly relevant context subsets. \nAcross diverse mechanistic tasks, including gene-centric, metabolite-centric, and drug-\ncentric benchmarks derived from DrugMechDB 31, BTE-augmented prompting \nconsistently outperformed the LLM-only approach. Notably, the smaller GPT-4o-mini \nmodel achieved over sixty-percentage improvement in accuracy on the gene-centric \ntask and eighty-two percent improvement on the metabolite task, when provided with \nstructured BTE evidence. Even GPT-4o, the larger flagship model, demonstrated \nsubstantial accuracy gains, underscoring that high-quality, explicit mechanistic context \ncan effectively mitigate the need for extremely large model sizes, suggesting a cost-\nefficient pathway toward domain-specific accuracy. \nWhile BTE offers comprehensive coverage across numerous biomedical domains, \ncertain areas such as single-cell data, epigenomic profiles, and microbiome interactions \nremain sparsely represented. Furthermore, variations in curation quality across \n.\nCC-BY 4.0 International license\navailable under a\n(which was not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made \nThe copyright holder for this preprint\nthis version posted August 2, 2025. \n; \nhttps://doi.org/10.1101/2025.08.01.668022\ndoi: \nbioRxiv preprint", "metadata": {"producer": "Microsoft® Word for Microsoft 365", "creator": "Appligent AppendPDF Pro 5.5", "creationdate": "2025-07-31T21:14:35-07:00", "source": "/home/donbr/open-biosciences/biosciences-research/data/raw/2025.08.01.668022v1.full.pdf", "file_path": "/home/donbr/open-biosciences/biosciences-research/data/raw/2025.08.01.668022v1.full.pdf", "total_pages": 31, "format": "PDF 1.7", "title": "32182735", "author": "janet", "subject": "", "keywords": "", "moddate": "2026-01-10T17:11:54-08:00", "trapped": "", "modDate": "D:20260110171154-08'00'", "creationDate": "D:20250731211435-07'00'", "page": 12}}
{"page_content": "Joy et. al \n14 \n \nfederated APIs could inadvertently propagate erroneous edges into model-generated \ncontexts. Although our evaluation leveraged the meticulously curated, high-confidence \nknowledge graph of DrugMechDB, real-world applications may require strategies for \nmanaging lower-confidence or conflicting evidence. Our study employed deterministic \nprompting to maintain comparability; exploring guided, chain-of-thought prompting \nstrategies could further enhance complex reasoning capabilities but may simultaneously \nreintroduce hallucinatory risks. \nFuture developments of BTE–RAG may involve integration into autonomous agent \nsystems capable of iterative querying, generation, self-critiquing, and re-querying, thus \nfacilitating automated self-verification workflows. Expanding the underlying knowledge \ngraph to incorporate resources such as LINCS transcriptomic signatures, tissue-specific \ninteraction networks, and multi-omics datasets would further enrich the mechanistic \ncoverage and broaden applicability 48. Expanding benchmarking efforts beyond \nDrugMechDB to encompass open-world biomedical queries could rigorously evaluate \nand strengthen the capacity BTE–RAG for reliable, contextually grounded inference. \nFurthermore, adopting frameworks like the Model Context Protocol could harmonize \ncomparisons across diverse generative models, facilitate rigorous auditing, and support \nreal-time decision-making in clinical and regulatory contexts. \nIn conclusion, BTE–RAG demonstrates the substantial value derived from strategically \nintegrating explicit mechanistic evidence into biomedical language modeling workflows. \nBy significantly improving answer accuracy, interpretability, and computational efficiency, \nthis approach provides a scalable, transparent, and robust foundation for future \nbiomedical AI systems, effectively balancing accuracy, affordability, and trustworthiness. \n \nSupplementary File: \nSupplementary Figure S1: Detailed pipeline for BTE-RAG \nSupplementary Figure S2: Performance of BTE‑RAG versus an LLM‑only baseline on \nthe gene‑centric benchmark using gpt‑4o‑mini. \nSupplementary Figure S3: Performance of BTE‑RAG versus an LLM‑only baseline on \nthe gene‑centric benchmark using gpt‑4o. \nSupplementary Figure S4: Cosine-similarity profile for the metabolite-centric \nbenchmark using GPT-4o-mini in LLM-only mode. \nSupplementary Figure S5: Distribution of answer similarities for the metabolite-centric \nbenchmark using GPT-4o-mini in BTE-RAG mode. \nSupplementary Figure S6: Distribution of answer similarities for the metabolite-centric \nbenchmark using GPT-4o in BTE-RAG mode. \nSupplementary Figure S7: Rank-ordered cosine similarities between model \npredictions and ground-truth answers on the metabolite-centric benchmark, across \ncontext filtering thresholds. \n.\nCC-BY 4.0 International license\navailable under a\n(which was not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made \nThe copyright holder for this preprint\nthis version posted August 2, 2025. \n; \nhttps://doi.org/10.1101/2025.08.01.668022\ndoi: \nbioRxiv preprint", "metadata": {"producer": "Microsoft® Word for Microsoft 365", "creator": "Appligent AppendPDF Pro 5.5", "creationdate": "2025-07-31T21:14:35-07:00", "source": "/home/donbr/open-biosciences/biosciences-research/data/raw/2025.08.01.668022v1.full.pdf", "file_path": "/home/donbr/open-biosciences/biosciences-research/data/raw/2025.08.01.668022v1.full.pdf", "total_pages": 31, "format": "PDF 1.7", "title": "32182735", "author": "janet", "subject": "", "keywords": "", "moddate": "2026-01-10T17:11:54-08:00", "trapped": "", "modDate": "D:20260110171154-08'00'", "creationDate": "D:20250731211435-07'00'", "page": 13}}
{"page_content": "Joy et. al \n15 \n \nSupplementary Figure S8: Cosine-similarity profile for the drug-centric benchmark \nusing GPT-4o-mini in LLM-only mode. \nSupplementary Figure S9: Distribution of answer similarities for the drug-centric \nbenchmark using GPT-4o-mini in BTE-RAG mode. \nSupplementary Figure S10: Distribution of answer similarities for the drug-centric \nbenchmark using GPT-4o in BTE-RAG mode.  \nSupplementary Figure S11: Rank-ordered cosine similarities between model \npredictions and ground-truth answers on the drug-centric benchmark, across context \nfiltering thresholds.  \nSupplementary Table S1: System prompts used for each task and model \n \nAvailability of Data and Materials \nThe source code, datasets, and analysis workflows described in this manuscript are \npublicly available in the GitHub repository: Project Name: BTE-RAG. Repository URL: \nhttps://github.com/janjoy/BTE-RAG \n \nAuthor contributions \nJ.J. and A.S. conceived the project and proposed the benchmark curation. J.J. \nimplemented the code, created the benchmarks and wrote the manuscript. A.S. \nsupervised the study. All authors read and approved the final manuscript. \n \nAcknowledgements \nWe thank Jackson Callaghan, Mikhael Astorga, and Karthik Soman for insightful \ndiscussions, and Everaldo Rodolpho for technical support with high-performance \ncomputing resources and server infrastructure. \n \nCompeting Interests \nThe authors declare no competing interests. \n \nFunding \nSupport for this work was provided by the National Institute on Aging (award \nR01AG066750) and by the National Center for Advancing Translational Sciences \nthrough the Biomedical Data Translator program (awards 1OT2TR003427 and \n1OT2TR005710). Any opinions expressed in this document do not necessarily reflect \n.\nCC-BY 4.0 International license\navailable under a\n(which was not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made \nThe copyright holder for this preprint\nthis version posted August 2, 2025. \n; \nhttps://doi.org/10.1101/2025.08.01.668022\ndoi: \nbioRxiv preprint", "metadata": {"producer": "Microsoft® Word for Microsoft 365", "creator": "Appligent AppendPDF Pro 5.5", "creationdate": "2025-07-31T21:14:35-07:00", "source": "/home/donbr/open-biosciences/biosciences-research/data/raw/2025.08.01.668022v1.full.pdf", "file_path": "/home/donbr/open-biosciences/biosciences-research/data/raw/2025.08.01.668022v1.full.pdf", "total_pages": 31, "format": "PDF 1.7", "title": "32182735", "author": "janet", "subject": "", "keywords": "", "moddate": "2026-01-10T17:11:54-08:00", "trapped": "", "modDate": "D:20260110171154-08'00'", "creationDate": "D:20250731211435-07'00'", "page": 14}}
{"page_content": "Joy et. al \n16 \n \nthe views of NIA, NCATS, NIH, individual Translator team members, or affiliated \norganizations and institutions. \n \nReferences \n1. \nHou, W. & Ji, Z. Assessing GPT-4 for cell type annotation in single-cell RNA-seq \nanalysis. Nature Methods 2024 21:8 21, 1462–1465 (2024). \n2. \nRives, A. et al. Biological structure and function emerge from scaling \nunsupervised learning to 250 million protein sequences. Proc Natl Acad Sci U S A \n118, e2016239118 (2021). \n3. \nLin, Z. et al. Evolutionary-scale prediction of atomic-level protein structure with a \nlanguage model. Science (1979) 379, 1123–1130 (2023). \n4. \nMeier, J. et al. Language models enable zero-shot prediction of the effects of \nmutations on protein function. in Proceedings of the 35th International Conference \non Neural Information Processing Systems (Curran Associates Inc., Red Hook, \nNY, USA, 2021). \n5. \nZheng, Y. et al. Large language models in drug discovery and development: From \ndisease mechanisms to clinical trials. arxiv.orgY Zheng, HY Koh, M Yang, L Li, LT \nMay, GI Webb, S Pan, G ChurcharXiv preprint arXiv:2409.04481, 2024•arxiv.org. \n6. \nMiller, K. et al. Dynamic few-shot prompting for clinical note section classification \nusing lightweight, open-source large language models. Journal of the American \nMedical Informatics Association 32, 1164–1173 (2025). \n7. \nJi, Z. et al. Survey of Hallucination in Natural Language Generation. ACM Comput \nSurv 55, (2023). \n8. \nVaswani, A. et al. Attention Is All You Need (Transformer Architecture). (2023). \n9. \nKim, Y. et al. Medical Hallucinations in Foundation Models and Their Impact on \nHealthcare. (2025). \n10. \nIdnay, B. et al. Environment scan of generative AI infrastructure for clinical and \ntranslational science. npj Health Systems 2, 1–11 (2025). \n11. \nIbrahim, M. et al. Generative AI for synthetic data across multiple medical \nmodalities: A systematic review of recent developments and challenges. Comput \nBiol Med 189, 109834 (2025). \n12. \nMaynez, J., Narayan, S., Bohnet, B. & McDonald, R. On Faithfulness and \nFactuality in Abstractive Summarization. Proceedings of the Annual Meeting of the \n.\nCC-BY 4.0 International license\navailable under a\n(which was not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made \nThe copyright holder for this preprint\nthis version posted August 2, 2025. \n; \nhttps://doi.org/10.1101/2025.08.01.668022\ndoi: \nbioRxiv preprint", "metadata": {"producer": "Microsoft® Word for Microsoft 365", "creator": "Appligent AppendPDF Pro 5.5", "creationdate": "2025-07-31T21:14:35-07:00", "source": "/home/donbr/open-biosciences/biosciences-research/data/raw/2025.08.01.668022v1.full.pdf", "file_path": "/home/donbr/open-biosciences/biosciences-research/data/raw/2025.08.01.668022v1.full.pdf", "total_pages": 31, "format": "PDF 1.7", "title": "32182735", "author": "janet", "subject": "", "keywords": "", "moddate": "2026-01-10T17:11:54-08:00", "trapped": "", "modDate": "D:20260110171154-08'00'", "creationDate": "D:20250731211435-07'00'", "page": 15}}
{"page_content": "Joy et. al \n17 \n \nAssociation for Computational Linguistics 1906–1919 (2020) \ndoi:10.18653/v1/2020.acl-main.173. \n13. \nYang Bs, Y., Jin, Q., Huang Phd, F. & Lu, Z. Adversarial Attacks on Large \nLanguage Models in Medicine. (2024). \n14. \nLuo, R. et al. BioGPT: generative pre-trained transformer for biomedical text \ngeneration and mining. Brief Bioinform 23, (2022). \n15. \nKojima, T., Gu, S. S., Reid, M., Matsuo, Y. & Iwasawa, Y. Large Language Models \nare Zero-Shot Reasoners. Adv Neural Inf Process Syst 35, (2022). \n16. \nBrown, T. B. et al. Language Models are Few-Shot Learners. Adv Neural Inf \nProcess Syst 2020-December, (2020). \n17. \nLewis, P. et al. Retrieval-Augmented Generation for Knowledge-Intensive NLP \nTasks. Adv Neural Inf Process Syst 2020-December, (2020). \n18. \nIzacard, G. & Grave, E. Leveraging Passage Retrieval with Generative Models for \nOpen Domain Question Answering. EACL 2021 - 16th Conference of the \nEuropean Chapter of the Association for Computational Linguistics, Proceedings \nof the Conference 874–880 (2020) doi:10.18653/v1/2021.eacl-main.74. \n19. \nKarpukhin, V. et al. Dense Passage Retrieval for Open-Domain Question \nAnswering. EMNLP 2020 - 2020 Conference on Empirical Methods in Natural \nLanguage Processing, Proceedings of the Conference 6769–6781 (2020) \ndoi:10.18653/v1/2020.emnlp-main.550. \n20. \nZhang, G. et al. Leveraging long context in retrieval augmented language models \nfor medical question answering. NPJ Digit Med 8, 239 (2025). \n21. \nSoman, K. et al. Biomedical knowledge graph-optimized prompt generation for \nlarge language models. Bioinformatics 40, (2024). \n22. \nHou, W., BioRxiv, Z. J.- & 2023, undefined. GeneTuring tests GPT models in \ngenomics. biorxiv.orgW Hou, Z JiBioRxiv, 2023•biorxiv.org \ndoi:10.1101/2023.03.11.532238.ABSTRACT. \n23. \nBizon, C. et al. ROBOKOP KG AND KGB: Integrated Knowledge Graphs from \nFederated Sources. J Chem Inf Model 59, 4968 (2019). \n24. \nMungall, C. J. et al. The Monarch Initiative: An integrative data and analytic \nplatform connecting phenotypes to genotypes across species. Nucleic Acids Res \n45, D712–D722 (2017). \n25. \nPan, S. et al. Unifying Large Language Models and Knowledge Graphs: A \nRoadmap. IEEE Trans Knowl Data Eng 36, 3580–3599 (2024). \n.\nCC-BY 4.0 International license\navailable under a\n(which was not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made \nThe copyright holder for this preprint\nthis version posted August 2, 2025. \n; \nhttps://doi.org/10.1101/2025.08.01.668022\ndoi: \nbioRxiv preprint", "metadata": {"producer": "Microsoft® Word for Microsoft 365", "creator": "Appligent AppendPDF Pro 5.5", "creationdate": "2025-07-31T21:14:35-07:00", "source": "/home/donbr/open-biosciences/biosciences-research/data/raw/2025.08.01.668022v1.full.pdf", "file_path": "/home/donbr/open-biosciences/biosciences-research/data/raw/2025.08.01.668022v1.full.pdf", "total_pages": 31, "format": "PDF 1.7", "title": "32182735", "author": "janet", "subject": "", "keywords": "", "moddate": "2026-01-10T17:11:54-08:00", "trapped": "", "modDate": "D:20260110171154-08'00'", "creationDate": "D:20250731211435-07'00'", "page": 16}}
{"page_content": "Joy et. al \n18 \n \n26. \nEvangelista, J. E. et al. Toxicology knowledge graph for structural birth defects. \nCommunications Medicine 2023 3:1 3, 1–14 (2023). \n27. \nCallaghan, J. et al. BioThings Explorer: a query engine for a federated knowledge \ngraph of biomedical APIs. Bioinformatics 39, (2023). \n28. \nCarbon, S. et al. The Gene Ontology resource: enriching a GOld mine. Nucleic \nAcids Res 49, D325–D334 (2021). \n29. \nKnox, C. et al. DrugBank 6.0: the DrugBank Knowledgebase for 2024. Nucleic \nAcids Res 52, D1265–D1275 (2024). \n30. \nFecho, K. et al. Progress toward a universal biomedical data translator. Clin Transl \nSci 15, 1838–1847 (2022). \n31. \nGonzalez-Cavazos, A. C. et al. DrugMechDB: A Curated Database of Drug \nMechanisms. Sci Data 10, 1–7 (2023). \n32. \nOpenAI. GPT-4o System Card. (2024). \n33. \nDeka, P., Jurek-Loughrey, A. & Padmanabhan, D. Improved methods to aid \nunsupervised evidence-based fact checking for online health news. Journal of \nData Intelligence 3, 474–505 (2022). \n34. \nDeka, P., Jurek-Loughrey, A. & P, D. Evidence extraction to validate medical \nclaims in fake news detection. SpringerP Deka, A Jurek-Loughrey, DPInternational \nconference on health information science, 2022•Springer 13705 LNCS, 3–15 \n(2022). \n35. \nUnni, D. R. et al. Biolink Model: A universal schema for knowledge graphs in \nclinical, biomedical, and translational science. Clin Transl Sci 15, 1848 (2022). \n36. \nWu, C., MacLeod, I. & Su, A. I. BioGPS and MyGene.info: organizing online, \ngene-centric information. Nucleic Acids Res 41, D561–D565 (2013). \n37. \nOpenAI et al. GPT-4 Technical Report. (2023). \n38. \nChen, Y. et al. Iterative Prompt Refinement for Mining Gene Relationships from \nChatGPT. bioRxiv 2023.12.23.573201 (2023) doi:10.1101/2023.12.23.573201. \n39. \nMarvin, G., Hellen, N., Jjingo, D. & Nakatumba-Nabende, J. Prompt Engineering \nin Large Language Models. 387–402 (2024) doi:10.1007/978-981-99-7962-2_30. \n40. \nSahoo, P. et al. A Systematic Survey of Prompt Engineering in Large Language \nModels: Techniques and Applications. (2024). \n41. \nLi, M., Kilicoglu, H., Xu, H. & Zhang, R. BiomedRAG: A retrieval augmented large \nlanguage model for biomedicine. J Biomed Inform 162, 104769 (2025). \n.\nCC-BY 4.0 International license\navailable under a\n(which was not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made \nThe copyright holder for this preprint\nthis version posted August 2, 2025. \n; \nhttps://doi.org/10.1101/2025.08.01.668022\ndoi: \nbioRxiv preprint", "metadata": {"producer": "Microsoft® Word for Microsoft 365", "creator": "Appligent AppendPDF Pro 5.5", "creationdate": "2025-07-31T21:14:35-07:00", "source": "/home/donbr/open-biosciences/biosciences-research/data/raw/2025.08.01.668022v1.full.pdf", "file_path": "/home/donbr/open-biosciences/biosciences-research/data/raw/2025.08.01.668022v1.full.pdf", "total_pages": 31, "format": "PDF 1.7", "title": "32182735", "author": "janet", "subject": "", "keywords": "", "moddate": "2026-01-10T17:11:54-08:00", "trapped": "", "modDate": "D:20260110171154-08'00'", "creationDate": "D:20250731211435-07'00'", "page": 17}}
{"page_content": "Joy et. al \n19 \n \n42. \nMorris, J. H. et al. The scalable precision medicine open knowledge engine \n(SPOKE): a massive knowledge graph of biomedical information. Bioinformatics \n39, (2023). \n43. \nJin, Q. et al. Pubmedqa: A dataset for biomedical research question answering. \narxiv.orgQ Jin, B Dhingra, Z Liu, WW Cohen, X LuarXiv preprint \narXiv:1909.06146, 2019•arxiv.org. \n44. \nKwiatkowski, T. et al. Natural Questions: A Benchmark for Question Answering \nResearch. Trans Assoc Comput Linguist 7, 452–466 (2019). \n45. \nDavis, A. P. et al. Comparative Toxicogenomics Database (CTD): update 2023. \nNucleic Acids Res 51, D1257–D1262 (2023). \n46. \nSchriml, L. M. et al. Disease Ontology: a backbone for disease semantic \nintegration. Nucleic Acids Res 40, D940–D946 (2012). \n47. \nBateman, A. et al. UniProt: the Universal Protein Knowledgebase in 2025. Nucleic \nAcids Res 53, D609–D617 (2025). \n48. \nPilarczyk, M. et al. Connecting omics signatures and revealing biological \nmechanisms with iLINCS. Nat Commun 13, 4678 (2022). \n  \n \n \n.\nCC-BY 4.0 International license\navailable under a\n(which was not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made \nThe copyright holder for this preprint\nthis version posted August 2, 2025. \n; \nhttps://doi.org/10.1101/2025.08.01.668022\ndoi: \nbioRxiv preprint", "metadata": {"producer": "Microsoft® Word for Microsoft 365", "creator": "Appligent AppendPDF Pro 5.5", "creationdate": "2025-07-31T21:14:35-07:00", "source": "/home/donbr/open-biosciences/biosciences-research/data/raw/2025.08.01.668022v1.full.pdf", "file_path": "/home/donbr/open-biosciences/biosciences-research/data/raw/2025.08.01.668022v1.full.pdf", "total_pages": 31, "format": "PDF 1.7", "title": "32182735", "author": "janet", "subject": "", "keywords": "", "moddate": "2026-01-10T17:11:54-08:00", "trapped": "", "modDate": "D:20260110171154-08'00'", "creationDate": "D:20250731211435-07'00'", "page": 18}}
{"page_content": "Joy et. al \n \n1 \n \nSupplementary Figures \n \n \n \n \nFigure S1: Detailed pipeline for BTE-RAG \nSupplementary Figure S1 depicts the end-to-end workflow through which the BTE-RAG retrieval module converts \na biomedical question into the evidence snippets ultimately supplied to the language-model reasoner. Beginning \nwith an example query, “Which gene plays the most significant mechanistic role in how the \ndrug hydrochlorothiazide treats or impacts the disease hypertension?”, the system first performs named-entity \nrecognition, normalizing the detected concepts to controlled identifiers (Drug: MESH:D006852; Disease: \nMESH:D006973). Each entity is then submitted to BioThings Explorer (BTE) as part of a query graph that \nrequests mechanistically relevant genes and proteins; independent queries are executed for the drug and for the \ndisease. BTE returns knowledge-graph sub-graphs whose nodes and edges represent subject-predicate-object \ntriples grounded in the biomedical literature. These triples are linearized into plain-text sentences, yielding two \npreliminary corpora (one for the disease, one for the drug) that list, for example, genes such as INS, REN, \nSLC12A3 and PTGS2 with their associated predicates. Finally, the complete set of sentences or a \npercentile-filtered subset is ranked by cosine similarity to the original question, and the highest-scoring lines are \nselected as the “retrieved context” passed forward for answer generation.  \n.\nCC-BY 4.0 International license\navailable under a\n(which was not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made \nThe copyright holder for this preprint\nthis version posted August 2, 2025. \n; \nhttps://doi.org/10.1101/2025.08.01.668022\ndoi: \nbioRxiv preprint", "metadata": {"producer": "Microsoft® Word for Microsoft 365", "creator": "Appligent AppendPDF Pro 5.5", "creationdate": "2025-07-31T21:14:35-07:00", "source": "/home/donbr/open-biosciences/biosciences-research/data/raw/2025.08.01.668022v1.full.pdf", "file_path": "/home/donbr/open-biosciences/biosciences-research/data/raw/2025.08.01.668022v1.full.pdf", "total_pages": 31, "format": "PDF 1.7", "title": "32182735", "author": "janet", "subject": "", "keywords": "", "moddate": "2026-01-10T17:11:54-08:00", "trapped": "", "modDate": "D:20260110171154-08'00'", "creationDate": "D:20250731211435-07'00'", "page": 19}}
{"page_content": "Joy et. al \n \n2 \n \n \n \n \n \n \nFigure S2: Performance of BTE‑RAG versus an LLM‑only baseline on the gene‑centric \nbenchmark using gpt‑4o‑mini. \n(A) Overall accuracy as a function of how much of the retrieved context is retained. Bars show accuracy when \nonly context lines above a given cosine‑similarity percentile are supplied to the model (10 th–90 th) as well as \nwhen the full context is used.  \n(B) Breakdown of prediction counts for the 798 benchmark questions. The hatched bars represent the LLM‑only \ncondition; solid bars represent BTE‑RAG.  \n(C) BTE‑RAG outperforms the LLM‑only run by +24.8 percentage points, confirming that targeted \nknowledge‑graph snippets materially improve answer quality. \n(D) Confusion matrix comparing the two methods. The upper‑left cell (360 cases) denotes questions both methods \nanswer correctly; the lower‑left cell (245) highlights errors that BTE‑RAG fixes; the upper‑right cell (47) shows \ninstances where retrieval introduces an error; and the lower‑right cell (146) comprises questions neither approach \nresolves. \n.\nCC-BY 4.0 International license\navailable under a\n(which was not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made \nThe copyright holder for this preprint\nthis version posted August 2, 2025. \n; \nhttps://doi.org/10.1101/2025.08.01.668022\ndoi: \nbioRxiv preprint", "metadata": {"producer": "Microsoft® Word for Microsoft 365", "creator": "Appligent AppendPDF Pro 5.5", "creationdate": "2025-07-31T21:14:35-07:00", "source": "/home/donbr/open-biosciences/biosciences-research/data/raw/2025.08.01.668022v1.full.pdf", "file_path": "/home/donbr/open-biosciences/biosciences-research/data/raw/2025.08.01.668022v1.full.pdf", "total_pages": 31, "format": "PDF 1.7", "title": "32182735", "author": "janet", "subject": "", "keywords": "", "moddate": "2026-01-10T17:11:54-08:00", "trapped": "", "modDate": "D:20260110171154-08'00'", "creationDate": "D:20250731211435-07'00'", "page": 20}}
{"page_content": "Joy et. al \n \n3 \n \n \n \n \n \n \nFigure S3: Performance of BTE‑RAG versus an LLM‑only baseline on the gene‑centric \nbenchmark using gpt‑4o. \n(A) Overall accuracy as a function of how much of the retrieved context is retained. Bars show accuracy when \nonly context lines above a given cosine‑similarity percentile are supplied to the model (10 th–90 th) as well as \nwhen the full context is used.  \n(B) Breakdown of prediction counts for the 798 benchmark questions. The hatched bars represent the LLM‑only \ncondition; solid bars represent BTE‑RAG.  \n(C) BTE‑RAG outperforms the LLM‑only run by +8.8 percentage points, confirming that targeted \nknowledge‑graph snippets materially improve answer quality. \n(D) Confusion matrix comparing the two methods. The upper‑left cell (508 cases) denotes questions both \nmethods answer correctly; the lower‑left cell (119) highlights errors that BTE‑RAG fixes; the upper‑right cell (49) \nshows instances where retrieval introduces an error; and the lower‑right cell (122) comprises questions neither \napproach resolves. \n \n.\nCC-BY 4.0 International license\navailable under a\n(which was not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made \nThe copyright holder for this preprint\nthis version posted August 2, 2025. \n; \nhttps://doi.org/10.1101/2025.08.01.668022\ndoi: \nbioRxiv preprint", "metadata": {"producer": "Microsoft® Word for Microsoft 365", "creator": "Appligent AppendPDF Pro 5.5", "creationdate": "2025-07-31T21:14:35-07:00", "source": "/home/donbr/open-biosciences/biosciences-research/data/raw/2025.08.01.668022v1.full.pdf", "file_path": "/home/donbr/open-biosciences/biosciences-research/data/raw/2025.08.01.668022v1.full.pdf", "total_pages": 31, "format": "PDF 1.7", "title": "32182735", "author": "janet", "subject": "", "keywords": "", "moddate": "2026-01-10T17:11:54-08:00", "trapped": "", "modDate": "D:20260110171154-08'00'", "creationDate": "D:20250731211435-07'00'", "page": 21}}
{"page_content": "Joy et. al \n \n4 \n \n \n \n \n \n \n \n \n \n \n \n \nFigure S4: Cosine-similarity profile for the metabolite-centric benchmark using \nGPT-4o-mini in LLM-only mode. \nHistogram shows the frequency of cosine-similarity scores (bin width = 0.05) between model \nanswers and ground truth answers across 201 metabolite-related queries when using gpt-4o-\nmini. A smoothed kernel-density curve traces the overall score profile. \n.\nCC-BY 4.0 International license\navailable under a\n(which was not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made \nThe copyright holder for this preprint\nthis version posted August 2, 2025. \n; \nhttps://doi.org/10.1101/2025.08.01.668022\ndoi: \nbioRxiv preprint", "metadata": {"producer": "Microsoft® Word for Microsoft 365", "creator": "Appligent AppendPDF Pro 5.5", "creationdate": "2025-07-31T21:14:35-07:00", "source": "/home/donbr/open-biosciences/biosciences-research/data/raw/2025.08.01.668022v1.full.pdf", "file_path": "/home/donbr/open-biosciences/biosciences-research/data/raw/2025.08.01.668022v1.full.pdf", "total_pages": 31, "format": "PDF 1.7", "title": "32182735", "author": "janet", "subject": "", "keywords": "", "moddate": "2026-01-10T17:11:54-08:00", "trapped": "", "modDate": "D:20260110171154-08'00'", "creationDate": "D:20250731211435-07'00'", "page": 22}}
{"page_content": "Joy et. al \n \n5 \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nFigure S5: Distribution of answer similarities for the metabolite-centric benchmark \nusing GPT-4o-mini in BTE-RAG mode. \nEach panel shows the cosine similarity between model predictions and ground-truth answers when either the full \nretrieved context is used (top left) or when context lines are filtered above increasing cosine similarity percentiles \n(10th to 90th). \n.\nCC-BY 4.0 International license\navailable under a\n(which was not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made \nThe copyright holder for this preprint\nthis version posted August 2, 2025. \n; \nhttps://doi.org/10.1101/2025.08.01.668022\ndoi: \nbioRxiv preprint", "metadata": {"producer": "Microsoft® Word for Microsoft 365", "creator": "Appligent AppendPDF Pro 5.5", "creationdate": "2025-07-31T21:14:35-07:00", "source": "/home/donbr/open-biosciences/biosciences-research/data/raw/2025.08.01.668022v1.full.pdf", "file_path": "/home/donbr/open-biosciences/biosciences-research/data/raw/2025.08.01.668022v1.full.pdf", "total_pages": 31, "format": "PDF 1.7", "title": "32182735", "author": "janet", "subject": "", "keywords": "", "moddate": "2026-01-10T17:11:54-08:00", "trapped": "", "modDate": "D:20260110171154-08'00'", "creationDate": "D:20250731211435-07'00'", "page": 23}}
{"page_content": "Joy et. al \n \n6 \n \n \n \n \n \n \n \n \n \n \n \n \n \nFigure S6: Distribution of answer similarities for the metabolite-centric benchmark using \nGPT-4o in BTE-RAG mode. \nEach panel shows the cosine similarity between model predictions and ground-truth answers \nwhen either the full retrieved context is used (top left) or when context lines are filtered above \nincreasing cosine similarity percentiles (10th to 90th). \n.\nCC-BY 4.0 International license\navailable under a\n(which was not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made \nThe copyright holder for this preprint\nthis version posted August 2, 2025. \n; \nhttps://doi.org/10.1101/2025.08.01.668022\ndoi: \nbioRxiv preprint", "metadata": {"producer": "Microsoft® Word for Microsoft 365", "creator": "Appligent AppendPDF Pro 5.5", "creationdate": "2025-07-31T21:14:35-07:00", "source": "/home/donbr/open-biosciences/biosciences-research/data/raw/2025.08.01.668022v1.full.pdf", "file_path": "/home/donbr/open-biosciences/biosciences-research/data/raw/2025.08.01.668022v1.full.pdf", "total_pages": 31, "format": "PDF 1.7", "title": "32182735", "author": "janet", "subject": "", "keywords": "", "moddate": "2026-01-10T17:11:54-08:00", "trapped": "", "modDate": "D:20260110171154-08'00'", "creationDate": "D:20250731211435-07'00'", "page": 24}}
{"page_content": "Joy et. al \n \n7 \n \n \n \n \n \n \n \n \n \n \n \n \nFigure S7: Rank-ordered cosine similarities between model predictions and ground-\ntruth answers on the metabolite-centric benchmark, across context filtering thresholds. \nEach panel displays results from four model–method combinations (GPT-4o-mini-Prompt (LLM-only), GPT-4o-mini \n+ BTE-RAG, GPT-4o Prompt, GPT-4o + BTE-RAG) under either full context or filtered context lines exceeding the \nindicated cosine similarity percentile (10th to 90th). Question predictions are sorted by similarity, revealing how \ncontext filtering and model selection affect semantic alignment with ground truth. \n \n.\nCC-BY 4.0 International license\navailable under a\n(which was not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made \nThe copyright holder for this preprint\nthis version posted August 2, 2025. \n; \nhttps://doi.org/10.1101/2025.08.01.668022\ndoi: \nbioRxiv preprint", "metadata": {"producer": "Microsoft® Word for Microsoft 365", "creator": "Appligent AppendPDF Pro 5.5", "creationdate": "2025-07-31T21:14:35-07:00", "source": "/home/donbr/open-biosciences/biosciences-research/data/raw/2025.08.01.668022v1.full.pdf", "file_path": "/home/donbr/open-biosciences/biosciences-research/data/raw/2025.08.01.668022v1.full.pdf", "total_pages": 31, "format": "PDF 1.7", "title": "32182735", "author": "janet", "subject": "", "keywords": "", "moddate": "2026-01-10T17:11:54-08:00", "trapped": "", "modDate": "D:20260110171154-08'00'", "creationDate": "D:20250731211435-07'00'", "page": 25}}
{"page_content": "Joy et. al \n \n8 \n \n \n \n \n \n \n \n \n \n \n \n \nFigure S8: Cosine-similarity profile for the drug-centric benchmark using GPT-4o-mini in \nLLM-only mode. \n \nHistogram shows the frequency of cosine-similarity scores (bin width = 0.05) between model answers and ground \ntruth answers across 842 drug-biological process queries when using gpt-4o-mini. A smoothed kernel-density \ncurve traces the overall score profile. \n.\nCC-BY 4.0 International license\navailable under a\n(which was not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made \nThe copyright holder for this preprint\nthis version posted August 2, 2025. \n; \nhttps://doi.org/10.1101/2025.08.01.668022\ndoi: \nbioRxiv preprint", "metadata": {"producer": "Microsoft® Word for Microsoft 365", "creator": "Appligent AppendPDF Pro 5.5", "creationdate": "2025-07-31T21:14:35-07:00", "source": "/home/donbr/open-biosciences/biosciences-research/data/raw/2025.08.01.668022v1.full.pdf", "file_path": "/home/donbr/open-biosciences/biosciences-research/data/raw/2025.08.01.668022v1.full.pdf", "total_pages": 31, "format": "PDF 1.7", "title": "32182735", "author": "janet", "subject": "", "keywords": "", "moddate": "2026-01-10T17:11:54-08:00", "trapped": "", "modDate": "D:20260110171154-08'00'", "creationDate": "D:20250731211435-07'00'", "page": 26}}
{"page_content": "Joy et. al \n \n9 \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nFigure S9: Distribution of answer similarities for the drug-centric benchmark using GPT-\n4o-mini in BTE-RAG mode. \nEach panel shows the cosine similarity between model predictions and ground-truth answers when either the full \nretrieved context is used (top left) or when context lines are filtered above increasing cosine similarity percentiles \n(10th to 90th). \n.\nCC-BY 4.0 International license\navailable under a\n(which was not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made \nThe copyright holder for this preprint\nthis version posted August 2, 2025. \n; \nhttps://doi.org/10.1101/2025.08.01.668022\ndoi: \nbioRxiv preprint", "metadata": {"producer": "Microsoft® Word for Microsoft 365", "creator": "Appligent AppendPDF Pro 5.5", "creationdate": "2025-07-31T21:14:35-07:00", "source": "/home/donbr/open-biosciences/biosciences-research/data/raw/2025.08.01.668022v1.full.pdf", "file_path": "/home/donbr/open-biosciences/biosciences-research/data/raw/2025.08.01.668022v1.full.pdf", "total_pages": 31, "format": "PDF 1.7", "title": "32182735", "author": "janet", "subject": "", "keywords": "", "moddate": "2026-01-10T17:11:54-08:00", "trapped": "", "modDate": "D:20260110171154-08'00'", "creationDate": "D:20250731211435-07'00'", "page": 27}}
{"page_content": "Joy et. al \n \n10 \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nFigure S10: Distribution of answer similarities for the drug-centric benchmark using \nGPT-4o in BTE-RAG mode. \nEach panel shows the cosine similarity between model predictions and ground-truth answers \nwhen either the full retrieved context is used (top left) or when context lines are filtered above \nincreasing cosine similarity percentiles (10th to 90th). \n.\nCC-BY 4.0 International license\navailable under a\n(which was not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made \nThe copyright holder for this preprint\nthis version posted August 2, 2025. \n; \nhttps://doi.org/10.1101/2025.08.01.668022\ndoi: \nbioRxiv preprint", "metadata": {"producer": "Microsoft® Word for Microsoft 365", "creator": "Appligent AppendPDF Pro 5.5", "creationdate": "2025-07-31T21:14:35-07:00", "source": "/home/donbr/open-biosciences/biosciences-research/data/raw/2025.08.01.668022v1.full.pdf", "file_path": "/home/donbr/open-biosciences/biosciences-research/data/raw/2025.08.01.668022v1.full.pdf", "total_pages": 31, "format": "PDF 1.7", "title": "32182735", "author": "janet", "subject": "", "keywords": "", "moddate": "2026-01-10T17:11:54-08:00", "trapped": "", "modDate": "D:20260110171154-08'00'", "creationDate": "D:20250731211435-07'00'", "page": 28}}
{"page_content": "Joy et. al \n \n11 \n \n \n \n \n \n \n \n \n \n \n \n \nFigure S11: Rank-ordered cosine similarities between model predictions and ground-\ntruth answers on the drug-centric benchmark, across context filtering thresholds. \nEach panel displays results from four model–method combinations (GPT-4o-mini Prompt (LLM-only), GPT-4o-mini \n+ BTE-RAG, GPT-4o Prompt, GPT-4o + BTE-RAG) under either full context or filtered context lines exceeding the \nindicated cosine similarity percentile (10th to 90th). Question predictions are sorted by similarity, revealing how \ncontext filtering and model selection affect semantic alignment with ground truth. \n \n  \n.\nCC-BY 4.0 International license\navailable under a\n(which was not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made \nThe copyright holder for this preprint\nthis version posted August 2, 2025. \n; \nhttps://doi.org/10.1101/2025.08.01.668022\ndoi: \nbioRxiv preprint", "metadata": {"producer": "Microsoft® Word for Microsoft 365", "creator": "Appligent AppendPDF Pro 5.5", "creationdate": "2025-07-31T21:14:35-07:00", "source": "/home/donbr/open-biosciences/biosciences-research/data/raw/2025.08.01.668022v1.full.pdf", "file_path": "/home/donbr/open-biosciences/biosciences-research/data/raw/2025.08.01.668022v1.full.pdf", "total_pages": 31, "format": "PDF 1.7", "title": "32182735", "author": "janet", "subject": "", "keywords": "", "moddate": "2026-01-10T17:11:54-08:00", "trapped": "", "modDate": "D:20260110171154-08'00'", "creationDate": "D:20250731211435-07'00'", "page": 29}}
{"page_content": "Joy et. al \n \n12 \n \n \n \nTable S1: System Prompts  \nGene-centric benchmark \nLLM-only \nBTE-RAG \nYou are an expert biomedical researcher. Please \nprovide your answer (only gene name) in the \nfollowing JSON format for the Question asked: \n{ \n  \"answer\": <correct answer> \n} \n \nYou are an advanced biomedical researcher. Use \nyour most recent knowledge in addition to the \nContext provided when needed to answer \naccurately. \n*Answer Format*: Provide your answer (only the \ngene name) in the following JSON format: \n{ \n  \"answer\": <correct answer> \n} \n \nMetabolite-centric benchmark \nLLM-only \nBTE-RAG \nYou are an expert biomedical researcher. Please \nprovide your answer (only the biochemical entity \nname) in the following JSON format for the \nQuestion asked: \n{{ \n  \"answer\": <correct answer> \n}} \nYou are an advanced biochemistry researcher. \nUse your most recent knowledge in addition to the \nContext provided when needed to answer \naccurately. \n*Answer Format*: Provide your answer (only the \nbiochemical entity name) in the following JSON \nformat: \n{ \n  \"answer\": <correct answer> \n} \n \nDrug-centric benchmark \nLLM-only \nBTE-RAG \nYou are an expert biomedical researcher. Please \nprovide your answer (only Drug names) in the \nfollowing JSON format for the Question asked: \n{{ \n  \"answer\": <correct answer> \n}} \nYou are an **advanced biomedical research AI**, \nspecializing in answering biological and \nbiomedical questions with accuracy. Use the \nprovided CONTEXT along with your most recent \nknowledge to answer the question. \n*Answer Format*: Provide your answer (only Drug \nname) in the following JSON format: \n{ \n  \"answer\": <correct answer> \n} \n \n.\nCC-BY 4.0 International license\navailable under a\n(which was not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made \nThe copyright holder for this preprint\nthis version posted August 2, 2025. \n; \nhttps://doi.org/10.1101/2025.08.01.668022\ndoi: \nbioRxiv preprint", "metadata": {"producer": "Microsoft® Word for Microsoft 365", "creator": "Appligent AppendPDF Pro 5.5", "creationdate": "2025-07-31T21:14:35-07:00", "source": "/home/donbr/open-biosciences/biosciences-research/data/raw/2025.08.01.668022v1.full.pdf", "file_path": "/home/donbr/open-biosciences/biosciences-research/data/raw/2025.08.01.668022v1.full.pdf", "total_pages": 31, "format": "PDF 1.7", "title": "32182735", "author": "janet", "subject": "", "keywords": "", "moddate": "2026-01-10T17:11:54-08:00", "trapped": "", "modDate": "D:20260110171154-08'00'", "creationDate": "D:20250731211435-07'00'", "page": 30}}
{"page_content": "∗Corresponding author \nEmail address: suhardivi@hss.edu (Vincentius Jeremy Suhardi) \nEvaluating Large Language Models for Gene-to-\nPhenotype Mapping: The Critical Role of Full-Text \nDatabase Access \nNicolas Matthew Suhardia, Anastasia Oktarinaa, Julia Retzkya, Damanpreet Dhillond, \nDona Ninane, Mathias P.G. Bostroma,b,c, Xu Yanga,c, Vincentius Jeremy Suhardia,b,∗ \n \naSkeletal Health and Orthopedic Research Program, Hospital for Special Surgery, New York, NY \n10021, USA \nbDepartment of Orthopedic Surgery, Hospital for Special Surgery, New York, NY 10021, US \n cDepartment of Orthopedic Surgery, Weill Cornell Medicine, New York, NY 10065, USA \n dQueens College, City University of New York, 65-30 Kissena Boulevard, Flushing, NY 11367, \nUSA \n eAlbany College of Pharmacy and Health Sciences, 106 New Scotland Avenue, Albany, NY \n12208, USA \nAbstract \nTransformer-based large language models (LLMs) have demonstrated significant \npotential in the biological and medical fields due to their ability to effectively learn from \nlarge-scale, diverse datasets and perform a wide range of downstream tasks. However, \nLLMs are limited by issues such as information processing inaccuracies and data \nconfabulation. These limitations hinder their utility for literature searches and other \ntasks requiring accurate and comprehensive extraction of information from extensive \nscientific literature. In this study, we evaluated the performance of various LLMs in \naccurately retrieving peer-reviewed literature and mapping correlations between 198 \ngenes and six phenotypes: bone formation, cartilage formation, fibrosis, cell \nproliferation, tendon formation, and ligament formation. Our analysis included three \ntypes of models. First, standard transformer-based LLMs (ChatGPT-4o and Gemini 1.5 \nPro). Second, specialized LLMs with dedicated custom databases containing peer-\nreviewed articles (SciSpace and ScholarAI). Third, specialized LLMs without dedicated \ndatabases (PubMedGPT and ScholarGPT). Using human-curated gene-to-phenotype \nmappings as the ground truth, we found that specialized LLMs with dedicated \ndatabases achieved the highest accuracy (>80%) in gene-to-phenotype mapping. \nAdditionally, these models were able to provide relevant peer-reviewed publications \nsupporting each gene-to-phenotype correlation. These findings underscore the \nimportance of database augmentation and specialization in enhancing the reliability and \nutility of LLMs for biomedical research applications. \n \n(which was not certified by peer review) is the author/funder. All rights reserved. No reuse allowed without permission. \nThe copyright holder for this preprint\nthis version posted June 12, 2025. \n; \nhttps://doi.org/10.1101/2025.06.11.659165\ndoi: \nbioRxiv preprint", "metadata": {"producer": "Acrobat Distiller 8.1.0 (Windows)", "creator": "PScript5.dll Version 5.2.2", "creationdate": "2025-06-11T13:55:55-07:00", "source": "/home/donbr/open-biosciences/biosciences-research/data/raw/2025.06.11.659165v1.full.pdf", "file_path": "/home/donbr/open-biosciences/biosciences-research/data/raw/2025.06.11.659165v1.full.pdf", "total_pages": 24, "format": "PDF 1.4", "title": "Evaluating Large Language Models for Gene-to-Phenotype Mapping: The Critical Role of Full-Text Database Access", "author": "Administrator", "subject": "", "keywords": "", "moddate": "2026-01-10T17:13:47-08:00", "trapped": "", "modDate": "D:20260110171347-08'00'", "creationDate": "D:20250611135555-07'00'", "page": 0}}
{"page_content": "2 \n \nIntroduction \nUnderstanding the relationship between genes and their cellular phenotypes is crucial \nin the biomedical field. It provides insights into how genetic perturbations impact cellular \nbehavior[1]. This knowledge is essential for determining how gene function relates to \ncellular phenotypes. The process of mapping genetic perturbations to phenotypes \ntypically involves two approaches. The first approach identifies the gene(s) associated \nwith a specific observed phenotype. The second approach evaluates the phenotypic \nchanges resulting from specific gene perturbations. Accurate correlations between \ngenes and phenotypes are foundational for various applications, including cellular type \nclassification in single-cell omics analysis[2]. However, the traditional approach of \nperturbing one gene at a time is labor-intensive and time-consuming. The vast number \nof genes within the genome, coupled with the virtually limitless combinations of \nphenotypes resulting from perturbation of a single gene—and further compounded by \nthe exponential growth of research publications—has made the task of accurately \nmapping genes to phenotypes increasingly challenging[3]. \nGiven these limitations of traditional approaches, large language models (LLMs), \nparticularly transformer-based architectures, have garnered significant interest in \nbiology and medicine. Transformer-based LLMs effectively learn from extensive and \ndiverse datasets, facilitating a wide range of downstream applications. These models \nfrequently surpass the performance of task-specific models trained from scratch [4, 5]. \nSpecifically, LLMs have demonstrated substantial utility in elucidating complex \nrelationships within genomic (DNA sequences), transcriptomic (RNA sequences), and \nproteomic (protein sequences and structures) datasets [6]. Central to transformer \narchitectures is the self-attention mechanism [7], which enables the model to capture \ncontext and manage long-range dependencies more effectively than recurrent neural \nnetworks (RNNs) or long short-term memory (LSTM) models [8]. Moreover, the multi-\nhead attention mechanism inherent in transformers allows parallel processing of \ninformation, significantly enhancing computational efficiency and reducing processing \ntime relative to RNNs and LSTMs[7]. \nDespite their significant potential and growing use in biomedical research, large \nlanguage models (LLMs) have notable limitations. Previous studies have underscored \nissues concerning their accuracy [9] and tendency to generate false or misleading \ninformation, often termed hallucinations [10]. Additionally, the lack of transparency \nregarding training data and inherent biases can perpetuate errors and misinformation \n[11]. Given the expanding use of LLMs in biomedical research, particularly for mapping \ngene-phenotype relationships[2, 12],  it is essential to assess their accuracy rigorously \nin this specific task. \n \n(which was not certified by peer review) is the author/funder. All rights reserved. No reuse allowed without permission. \nThe copyright holder for this preprint\nthis version posted June 12, 2025. \n; \nhttps://doi.org/10.1101/2025.06.11.659165\ndoi: \nbioRxiv preprint", "metadata": {"producer": "Acrobat Distiller 8.1.0 (Windows)", "creator": "PScript5.dll Version 5.2.2", "creationdate": "2025-06-11T13:55:55-07:00", "source": "/home/donbr/open-biosciences/biosciences-research/data/raw/2025.06.11.659165v1.full.pdf", "file_path": "/home/donbr/open-biosciences/biosciences-research/data/raw/2025.06.11.659165v1.full.pdf", "total_pages": 24, "format": "PDF 1.4", "title": "Evaluating Large Language Models for Gene-to-Phenotype Mapping: The Critical Role of Full-Text Database Access", "author": "Administrator", "subject": "", "keywords": "", "moddate": "2026-01-10T17:13:47-08:00", "trapped": "", "modDate": "D:20260110171347-08'00'", "creationDate": "D:20250611135555-07'00'", "page": 1}}
{"page_content": "3 \n \nTo address these concerns and evaluate LLM performance in the critical area of gene-\nphenotype identification, we assessed transformer-based LLMs—specifically ChatGPT-\n4o and Gemini 1.5 Pro. These models were evaluated based on their embedded \nbiological knowledge, as well as their ability to leverage information from PubMed and \nother open-access literature sources. We utilized six key phenotypes as representative \nproxies: cell proliferation, bone formation, fibrosis, cartilage formation, tendon \nformation, and ligament formation. Human-curated, peer-reviewed literature searches \nserved as our benchmark for accuracy. \nBoth ChatGPT-4o and Gemini 1.5 Pro demonstrated moderate accuracy in identifying \ngene-phenotype relationships. Importantly, granting specialized access to abstracts and \nfull-length articles significantly improved the accuracy of these models. These findings \nsuggest that transformer-based LLMs, when specialized with direct literature access, \nhold promise as reliable tools for accurately mapping gene-phenotype relationships. \nMethods \nTransformer-Based Large Language Models \nPublicly available transformer-based large language models (LLMs) ChatGPT-4o (“o” \nfor “omni”) and Gemini 1.5 Pro were used through their user interface. As of May 2025, \nChatGPT-4o had around 1.76 trillion parameters [13] and has a context window of \n128,000 tokens. Gemini 1.5 Pro has a context window of 2 million tokens, though the \nexact number of model parameters has not been officially disclosed by Google. Both \nChatGPT-4o and Gemini 1.5 Pro were able to access metadata of PubMed abstracts \nbut unable to directly access the PubMed database. \nModel Access and Implementation \nAll models evaluated in this study are closed-source models and were accessed \nthrough their respective web interfaces without any fine-tuning, training, or modification. \nEach model was used in its standard, publicly available configuration through browser-\nbased interfaces. No custom code was developed for this study; all data collection and \nanalysis were performed manually using spreadsheet software. \nData Collection Methodology \nThis study employed a completely manual approach to data collection and analysis to \nensure reproducibility and transparency. The entire experimental protocol was \nconducted without automated scripts or custom software development. The following \nstep-by-step methodology was used: \nQuery Execution: For each of the 1,188 gene-phenotype combinations per model, \nqueries were systematically submitted through the web interfaces of the respective \nlarge language models (LLMs). Each query adhered rigorously to the standardized \n(which was not certified by peer review) is the author/funder. All rights reserved. No reuse allowed without permission. \nThe copyright holder for this preprint\nthis version posted June 12, 2025. \n; \nhttps://doi.org/10.1101/2025.06.11.659165\ndoi: \nbioRxiv preprint", "metadata": {"producer": "Acrobat Distiller 8.1.0 (Windows)", "creator": "PScript5.dll Version 5.2.2", "creationdate": "2025-06-11T13:55:55-07:00", "source": "/home/donbr/open-biosciences/biosciences-research/data/raw/2025.06.11.659165v1.full.pdf", "file_path": "/home/donbr/open-biosciences/biosciences-research/data/raw/2025.06.11.659165v1.full.pdf", "total_pages": 24, "format": "PDF 1.4", "title": "Evaluating Large Language Models for Gene-to-Phenotype Mapping: The Critical Role of Full-Text Database Access", "author": "Administrator", "subject": "", "keywords": "", "moddate": "2026-01-10T17:13:47-08:00", "trapped": "", "modDate": "D:20260110171347-08'00'", "creationDate": "D:20250611135555-07'00'", "page": 2}}
{"page_content": "4 \n \nprompt format detailed in the LLM Prompts section. Queries were individually executed \nvia manual entry into browser-based interfaces, ensuring consistent methodology for \neach gene-phenotype combination. \nResponse Collection: Responses generated by the LLMs were systematically collected \nby manually transferring data from the web interface to Microsoft Excel spreadsheets. \nSeparate spreadsheets were maintained for each LLM model evaluated. Each \nspreadsheet entry comprehensively recorded the gene name, associated phenotype, \nthe \nfull \nLLM \nresponse, \ncategorized \noutcomes \n(\"increase,\" \n\"decrease,\" \nor \n\"inconclusive\"), provided citations, and relevant journal impact factor information. \nReference Verification: URLs provided by the LLMs linking to peer-reviewed \npublications were individually verified through direct manual access. Each URL was \ncritically evaluated to ensure it directed accurately to the referenced publication, \nconfirmed congruence with the publication title and authorship details provided by the \nLLM, and assessed its relevance to the specific gene-phenotype relationship queried. \nURLs or reference titles that led to non-existent publications were categorized as \n\"hallucination.\" URLs or reference titles that correctly directed to a publication but were \nunrelated to the queried gene-phenotype relationship were categorized as \"irrelevant.\" \nScientific-Focused Large Language Models \nFor scientific-focused large language models (LLMs), three types of GPT-based \nsystems were utilized: \n1. LLMs with access to dedicated databases of full-length articles: These include \nmodels like GPT-4-based SciSpace (Typeset.io) and ScholarAI. \n2. LLMs without dedicated databases but with direct access to publicly available \nsources: Examples include \n3. GPT-4-based ScholarGPT, which can access various scholarly databases and \nacademic search engines. \n4. LLMs without access to full-length articles, customized to draw information from \nabstracts: An example is PubMedGPT, which primarily utilizes abstracts from the \nPubMed database. \n \n \n(which was not certified by peer review) is the author/funder. All rights reserved. No reuse allowed without permission. \nThe copyright holder for this preprint\nthis version posted June 12, 2025. \n; \nhttps://doi.org/10.1101/2025.06.11.659165\ndoi: \nbioRxiv preprint", "metadata": {"producer": "Acrobat Distiller 8.1.0 (Windows)", "creator": "PScript5.dll Version 5.2.2", "creationdate": "2025-06-11T13:55:55-07:00", "source": "/home/donbr/open-biosciences/biosciences-research/data/raw/2025.06.11.659165v1.full.pdf", "file_path": "/home/donbr/open-biosciences/biosciences-research/data/raw/2025.06.11.659165v1.full.pdf", "total_pages": 24, "format": "PDF 1.4", "title": "Evaluating Large Language Models for Gene-to-Phenotype Mapping: The Critical Role of Full-Text Database Access", "author": "Administrator", "subject": "", "keywords": "", "moddate": "2026-01-10T17:13:47-08:00", "trapped": "", "modDate": "D:20260110171347-08'00'", "creationDate": "D:20250611135555-07'00'", "page": 3}}
{"page_content": "5 \n \nTechnical Implementation: These specialized models are implemented as closed-\nsource, publicly available, custom GPTs. Each model utilizes the ChatGPT platform’s \ninternet search capabilities and custom API integrations as tools. SciSpace and \nScholarAI employ API calls to their respective proprietary databases and search \nsystems, while ScholarGPT uses API connections to established scholarly indexes and \njournal databases. PubMedGPT operates without external API calls, relying on its \nspecialized instructions to extract information from its training data focused on PubMed \nabstracts. The base models (ChatGPT-4o and Gemini 1.5 Pro) utilize the standard \ninternet search tools provided by their respective platforms. \nGPT-4-based SciSpace: SciSpace’s general knowledge cutoff was October 2023. \nHowever, it can retrieve, analyze, and synthesize information from its proprietary \ndatabase of over 300 million research papers. Through its integration with this \ndatabase, SciSpace provides real-time access to research papers up to 2025. \nAdditionally, SciSpace can access publicly available databases, including CrossRef, \nPubMed, arXiv, Springer, Elsevier, IEEE Xplore, Google Scholar, and patent databases. \nGPT-4-based ScholarAI: Like SciSpace, ScholarAI’s general knowledge cutoff was \nOctober 2023. It has access to the dedicated ScholarAI database, which contains more \nthan 200 million research papers, and can also draw from publicly available sources \nsuch as CrossRef, PubMed, arXiv, Springer, Elsevier, IEEE Xplore, Google Scholar, \nand patent databases. \nGPT-4-based ScholarGPT: ScholarGPT had a general knowledge cutoff in October \n2023 and lacked a dedicated database. Instead, it relies on real-time access to \nabstracts and open-access full-text articles from various publicly available scholarly \ndatabases and academic search engines. \nPubMedGPT: PubMedGPT was not trained on full-text manuscripts from PubMed or \nother databases. Its training included publicly available data up to 2023. PubMedGPT is \ndesigned to provide citations from PubMed and integrate findings from multiple studies \nto synthesize conclusions. \n \n \n(which was not certified by peer review) is the author/funder. All rights reserved. No reuse allowed without permission. \nThe copyright holder for this preprint\nthis version posted June 12, 2025. \n; \nhttps://doi.org/10.1101/2025.06.11.659165\ndoi: \nbioRxiv preprint", "metadata": {"producer": "Acrobat Distiller 8.1.0 (Windows)", "creator": "PScript5.dll Version 5.2.2", "creationdate": "2025-06-11T13:55:55-07:00", "source": "/home/donbr/open-biosciences/biosciences-research/data/raw/2025.06.11.659165v1.full.pdf", "file_path": "/home/donbr/open-biosciences/biosciences-research/data/raw/2025.06.11.659165v1.full.pdf", "total_pages": 24, "format": "PDF 1.4", "title": "Evaluating Large Language Models for Gene-to-Phenotype Mapping: The Critical Role of Full-Text Database Access", "author": "Administrator", "subject": "", "keywords": "", "moddate": "2026-01-10T17:13:47-08:00", "trapped": "", "modDate": "D:20260110171347-08'00'", "creationDate": "D:20250611135555-07'00'", "page": 4}}
{"page_content": "6 \n \nQueries \nSix phenotypes were utilized in this study: bone formation, fibrosis, cartilage formation, \ncell proliferation, tendon formation, and ligament formation. These phenotypes were \nchosen as representative topics of interest for skeletal and connective tissue \nresearchers, reflecting both mineralized and soft tissue outcomes relevant to \nmusculoskeletal biology. \nA power analysis was conducted to determine the minimum number of genes required \nto detect at least a 20% difference with a power of 80% and a significance level (p-\nvalue) of 0.05. The analysis indicated that at least 91 genes would be needed. To \nensure robust statistical power and comprehensive coverage of biological pathways, \nwe expanded our dataset to 198 genes. \nTo ensure comprehensive representation, 198 genes previously reported in peer-\nreviewed studies to impact at least one of the six phenotypes were selected as genes \nof interest (Table 1). These 198 genes were selected to encompass various genetic \npathways. \nThese \ninclude \nbone \nmorphogenetic \npathway \n(BMP \nPathway), \nwingless/integrated pathway (WNT Pathway), transforming-growth factor beta pathway \n(TGFβ pathway), Hedgehog pathway, NF-kB pathway, fibroblast growth factor pathway \n(FGF pathway), and additional pathways relevant to connective tissue biology. The \nexpanded gene set includes established markers of skeletal development (e.g., \nRUNX2, SOX9, COL1A1), tissue remodeling factors (e.g., MMP13, ADAMTS5), \nmechanosensitive channels (e.g., PIEZO1, PIEZO2), and tissue-specific transcription \nfactors (e.g., SCX for tendon, GDF5 for cartilage). For each gene, six distinct LLM \nqueries were created, corresponding to the six phenotypes of interest: bone formation, \nfibrosis, cartilage formation, cell proliferation, tendon formation, and ligament formation. \nThis resulted in 1,188 total queries per LLM model tested. \n \n \n(which was not certified by peer review) is the author/funder. All rights reserved. No reuse allowed without permission. \nThe copyright holder for this preprint\nthis version posted June 12, 2025. \n; \nhttps://doi.org/10.1101/2025.06.11.659165\ndoi: \nbioRxiv preprint", "metadata": {"producer": "Acrobat Distiller 8.1.0 (Windows)", "creator": "PScript5.dll Version 5.2.2", "creationdate": "2025-06-11T13:55:55-07:00", "source": "/home/donbr/open-biosciences/biosciences-research/data/raw/2025.06.11.659165v1.full.pdf", "file_path": "/home/donbr/open-biosciences/biosciences-research/data/raw/2025.06.11.659165v1.full.pdf", "total_pages": 24, "format": "PDF 1.4", "title": "Evaluating Large Language Models for Gene-to-Phenotype Mapping: The Critical Role of Full-Text Database Access", "author": "Administrator", "subject": "", "keywords": "", "moddate": "2026-01-10T17:13:47-08:00", "trapped": "", "modDate": "D:20260110171347-08'00'", "creationDate": "D:20250611135555-07'00'", "page": 5}}
{"page_content": "7 \n \nTable 1: List of genes used in this study \nPhenotype\nGene Names\nFibrosis \nPiezo1, En1, Bglap, Osterix, Wnt1, Wnt3a, Osteopontin, Pth1R, \nLRP5, LRP6, RSPO1, RSPO2, RSPO3, Sclerostin, Dkk1, Dkk2, \nPiezo2, Spry2, Dkk3, Dkk4, Gli1, Gli2, Gli3, BMP1, BMP2, \nCol2a1, Prx1, Sox2, WIF-1, Sp7, Embigin \nBone \nAlp, Col1a1, Runx2, Krm1, Smad2, Smad4, Smad3, Runx1, IGF-\n1, IGF1R, Slit3, Shn3, Osteolectin, Fgf2, Fgfr2, Fgf23, Acta2, \nIkbkb, Lats2, Galectin1, Pdgfra, Pdgfrb, Tgfbr2, Asxl1, Cxcl4, \nCxcr2, Smpd3, Sox9, Nfkb1, Ctgf, Timp1, Pkd1, Tnfsrf11b, Tnfa, \nBmp7 \nCartilage \nPlcg1, Alk1, Alk5, Smad1, Smad8, Pinch1, Pinch2, Mek1, Mekk2, \nTgfbr1, CCn2, Fgfr1, Mef2c, Epas1, Msx2, Sox5, Sox6, Ddr2, \nScn9a, Erg, Rela, Mmp13, Adamts5, Prg4, Gdf5, Frzb, Ctnnb, \nEgfr, Nfatc2, Nfatc1, Egr1, Sparc, Sost \nCell Proliferation \nGrem1, Trpv4, Creb5, Notch2, MKX, EGR2, HIF1A, NRF2, \nSIRT1, TSC1, AKT1, CAV1, DICER1, ESR1, GDF6, GDF7, TGF-\nβ2, TGF-β3, Noggin, ACVR1, SMO, IL-4, IL-10, IL-13, IL-17A, IL-\n33, CCL2, CXCL12, CXCR4, TLR4, NOS2, NOS3 \nTendon Formation \nSCX, HO-1, FLII, SFRP2, Substance P, COX-2, LGALS3, \nPPARG, COL3A1, COL5A1, COL10A1, COL12A1, COL14A1, \nDCN, BGN, FMOD, LUM, VCAN, TNC, POSTN, THBS1, THBS2, \nTHBS3, THBS4, COMP, MGP, FN1, CLEC3B, TSG-6, IGFBP5, \nCDH11, MMP3, MMP8, MMP9 \nLigament \nFormation \nMMP10, MMP12, MMP14, MMP2, ADAMTS2, LOX, LOXL2, \nPLG, PLAU, SERPINE1, MKL1, YAP1, WWTR1, SMAD7, ILK, \nITGB1, ITGA5, PDGFB, VEGFA, ANGPT1, ANGPT2, CDKN1A, \nCDKN2A, TGFB1I1, PTEN, MAPK14, MAPK1, MAPK3, MAPK8, \nRELB, STAT3, IGF2, TIMP2 \n \n \n \n(which was not certified by peer review) is the author/funder. All rights reserved. No reuse allowed without permission. \nThe copyright holder for this preprint\nthis version posted June 12, 2025. \n; \nhttps://doi.org/10.1101/2025.06.11.659165\ndoi: \nbioRxiv preprint", "metadata": {"producer": "Acrobat Distiller 8.1.0 (Windows)", "creator": "PScript5.dll Version 5.2.2", "creationdate": "2025-06-11T13:55:55-07:00", "source": "/home/donbr/open-biosciences/biosciences-research/data/raw/2025.06.11.659165v1.full.pdf", "file_path": "/home/donbr/open-biosciences/biosciences-research/data/raw/2025.06.11.659165v1.full.pdf", "total_pages": 24, "format": "PDF 1.4", "title": "Evaluating Large Language Models for Gene-to-Phenotype Mapping: The Critical Role of Full-Text Database Access", "author": "Administrator", "subject": "", "keywords": "", "moddate": "2026-01-10T17:13:47-08:00", "trapped": "", "modDate": "D:20260110171347-08'00'", "creationDate": "D:20250611135555-07'00'", "page": 6}}
{"page_content": "8 \n \nLLM Prompts \nOne-shot prompting approach was utilized to ensure consistent and structured \nresponses across all models. The prompt design included model-specific instructions to \ndirect each LLM to use its appropriate database or search capabilities, followed by a \nstandardized query structure and output format. \nThe prompt consisted of three main components. First, a model-specific instruction \ndirecting the LLM to use its designated database or search tool. Second, the core query \nasking whether a specific gene increases, decreases, or has an inconclusive \nrelationship with a given phenotype based on peer-reviewed literature. Third, detailed \nformatting instructions requiring a single-word answer followed by supporting \nreferences and impact factor information. \nTo enhance response consistency and accuracy, an example of the expected output \nformat was included as port of the prompt (Figure 1). The one-shot example was \nincluded to allow the LLM to return consistent structured answer: a bracketed single-\nword answer categorized as increase, decrease, or inconclusive, followed by numbered \nreference links, and the highest impact factor among cited journals.  \nAll links to peer-reviewed manuscripts provided by the LLM prompts were manually \nverified to ensure they correspond to the intended and relevant manuscripts. \n \n(which was not certified by peer review) is the author/funder. All rights reserved. No reuse allowed without permission. \nThe copyright holder for this preprint\nthis version posted June 12, 2025. \n; \nhttps://doi.org/10.1101/2025.06.11.659165\ndoi: \nbioRxiv preprint", "metadata": {"producer": "Acrobat Distiller 8.1.0 (Windows)", "creator": "PScript5.dll Version 5.2.2", "creationdate": "2025-06-11T13:55:55-07:00", "source": "/home/donbr/open-biosciences/biosciences-research/data/raw/2025.06.11.659165v1.full.pdf", "file_path": "/home/donbr/open-biosciences/biosciences-research/data/raw/2025.06.11.659165v1.full.pdf", "total_pages": 24, "format": "PDF 1.4", "title": "Evaluating Large Language Models for Gene-to-Phenotype Mapping: The Critical Role of Full-Text Database Access", "author": "Administrator", "subject": "", "keywords": "", "moddate": "2026-01-10T17:13:47-08:00", "trapped": "", "modDate": "D:20260110171347-08'00'", "creationDate": "D:20250611135555-07'00'", "page": 7}}
{"page_content": "9 \n \nFigure 1. Standardized prompt template used for querying large language \nmodels. The prompt consists of three main components: (1) Model-specific instructions \n(part 1-5) directing each LLM to use its designated database or search capabilities, with \nonly the relevant instruction provided to each model; (2) Core query structure (part 6) \nfollowing a standardized format asking whether a specific gene increases, decreases, \nor has an inconclusive relationship with a given phenotype based on peer-reviewed \n(1) Use the SciSpace database/connector to retrieve relevant article and \ncitations to answer the following question. Do NOT use your own internet \nsearching tool. \n(2) Use the ScholarAI database/connector to retrieve relevant article and \ncitations to answer the following question. Do NOT use your own internet \nsearching tool. \n(3) Use the ScholarGPT(Sider) database/connector. \n(4) Use your ability to find PubMed sources to answer the following \nquestion. \n(5) Use your internet search tool to answer the following  \nquestion: \n(6) Does {gene} increase or decrease {phenotype} according to \navailable peer reviewed papers? \n(7) INSTRUCTION: \n1. In the first line of your answer, please answer with only 1 word: \nincrease/decrease/inconclusive surrounded by a pair of square \nbracket. \n2. In the following line of answers, please provide the links of \npaperthat support your answer. \n3. If you answered inconclusive, you don’t need to provide any \nlinks. \n4. (Ignore this step if answer is inconclusive) If you provide \ncitations, use your search tool to find the citations with the \nhighest journal impact factor \n5. (Ignore this step if answer is inconclusive) Provide also the \nnumerical value of the impact factor of the journal that you \nchoose in step 4. \n(8) EXAMPLE: \n[increase / decrease / inconclusive] \nReference: \n1.https://pubmed.ncbi.nlm.nih.gov/1234/ \n2.https://www.mdpi.com/1234567/12/3/456 \nHighest Impact Factor Journal Cited: \nJournal of ABC. Impact factor: 12.34 \n(which was not certified by peer review) is the author/funder. All rights reserved. No reuse allowed without permission. \nThe copyright holder for this preprint\nthis version posted June 12, 2025. \n; \nhttps://doi.org/10.1101/2025.06.11.659165\ndoi: \nbioRxiv preprint", "metadata": {"producer": "Acrobat Distiller 8.1.0 (Windows)", "creator": "PScript5.dll Version 5.2.2", "creationdate": "2025-06-11T13:55:55-07:00", "source": "/home/donbr/open-biosciences/biosciences-research/data/raw/2025.06.11.659165v1.full.pdf", "file_path": "/home/donbr/open-biosciences/biosciences-research/data/raw/2025.06.11.659165v1.full.pdf", "total_pages": 24, "format": "PDF 1.4", "title": "Evaluating Large Language Models for Gene-to-Phenotype Mapping: The Critical Role of Full-Text Database Access", "author": "Administrator", "subject": "", "keywords": "", "moddate": "2026-01-10T17:13:47-08:00", "trapped": "", "modDate": "D:20260110171347-08'00'", "creationDate": "D:20250611135555-07'00'", "page": 8}}
{"page_content": "10 \n \nliterature; and (3) Detailed response formatting instructions (part 7) requiring a single-\nword answer in square brackets followed by supporting references and impact factor \ninformation. The example output (part 8) demonstrates the expected response format \nto ensure consistency across all 1,188 queries per model. This structured one-shot \nprompting approach minimized response variability and facilitated systematic data \nextraction across all six LLM models tested. \n \nReference Quality Assessment \nTo rigorously evaluate the quality and reliability of references provided by each \nLLM, we employed four specific metrics: \n1. Accuracy Assessment: Responses for each gene-phenotype query were \nassessed against ground truth determinations established by manual \ncuration of peer-reviewed literature by the senior author (VJS). A response \nwas considered accurate if the LLM’s determination (increase, decrease, \nor inconclusive) matched the manually curated classification. \n2. Topic-Irrelevant Citations: For queries in which references were \nprovided, we assessed relevance to both the specified gene and the \nphenotype queried. A citation was categorized as topic-irrelevant if the \ncited paper did not contain information directly supporting the gene-\nphenotype relationship. The topic-irrelevant citation rate was calculated as \nthe proportion of queries where all provided references failed to \nsubstantiate the claimed gene-phenotype association. \n3. Hallucination: References provided by LLMs were verified to determine if \nthey corresponded to actual publications. A reference was classified as \nhallucination if: (a) the provided URL directed to a non-existent page or a \ndifferent publication than indicated, or (b) the title and authors listed by the \nLLM did not match the actual publication accessible through the URL. The \nhallucination rate was calculated as the proportion of queries where all \ncited references were either non-existent or inaccurately represented. \n4. Impact Factor Analysis: The journal impact factor was recorded for all \nvalid peer-reviewed references using the most recent Journal Citation \nReports®. For queries providing multiple valid references, we utilized the \nhighest impact factor among cited journals. Queries resulting in \ninconclusive determinations or lacking valid peer-reviewed citations were \nassigned an impact factor of zero. \n(which was not certified by peer review) is the author/funder. All rights reserved. No reuse allowed without permission. \nThe copyright holder for this preprint\nthis version posted June 12, 2025. \n; \nhttps://doi.org/10.1101/2025.06.11.659165\ndoi: \nbioRxiv preprint", "metadata": {"producer": "Acrobat Distiller 8.1.0 (Windows)", "creator": "PScript5.dll Version 5.2.2", "creationdate": "2025-06-11T13:55:55-07:00", "source": "/home/donbr/open-biosciences/biosciences-research/data/raw/2025.06.11.659165v1.full.pdf", "file_path": "/home/donbr/open-biosciences/biosciences-research/data/raw/2025.06.11.659165v1.full.pdf", "total_pages": 24, "format": "PDF 1.4", "title": "Evaluating Large Language Models for Gene-to-Phenotype Mapping: The Critical Role of Full-Text Database Access", "author": "Administrator", "subject": "", "keywords": "", "moddate": "2026-01-10T17:13:47-08:00", "trapped": "", "modDate": "D:20260110171347-08'00'", "creationDate": "D:20250611135555-07'00'", "page": 9}}
{"page_content": "11 \n \n \nStatistical Analysis \nLogistic regression was performed to analyze the performance of large language \nmodels (LLMs) in providing correct answers, using human responses as the ground \ntruth. ChatGPT-4o (base model) was treated as the reference model. The p-value was \ncalculated to test the null hypothesis that the log-odds difference between the tested \nmodel and the reference model is zero (i.e., the performance of the tested model is the \nsame as that of the reference model). Statistical analysis was conducted using \nGraphPad Prism v9.5, with a significance threshold set at p<0.05. \n \nResults \nChatGPT-4o and Google Gemini 1.5 Pro demonstrated comparable \nmoderate accuracy in mapping genes to phenotype \nWe investigated the performance of an OpenAI-based large language model (LLM) \n(ChatGPT-4o) in comparison to the Gemini 1.5 Pro LLM to map genes to phenotype \nbased on peer-reviewed literature. To evaluate the ability of the LLMs to map genes to \nphenotypes, each model was tasked with mapping 198 genes (as described in the \nMethods section) to six phenotypes: bone formation, cartilage formation, fibrosis, cell \nproliferation, tendon formation, and ligament formation. This resulted in a total of six \nqueries per gene and 1,188 queries per LLM. Manual mapping of genes to phenotypes \nby the senior author (VJS), based on peer-reviewed journal articles, was used as the \nground truth for comparison. \n \nChatGPT-4o demonstrated significantly higher accuracy in gene-to-phenotype mapping \ncompared to Gemini 1.5 Pro (59.3% vs. 55.8%, respectively; p = 0.04, Figure 2a). \nAlthough prompts explicitly requested references from peer-reviewed articles, not all \nqueries yielded relevant citations. Notably, ChatGPT-4o provided a significantly greater \nproportion of irrelevant citations compared to Gemini 1.5 Pro (18.4% vs. 5.5%, \nrespectively; p = 0.0001, Figure 2b). Furthermore, ChatGPT-4o exhibited a higher \nhallucination rate than Gemini 1.5 Pro (16.4% vs. 7.1%, respectively; p = 0.0001, \nFigure 2b). Performance varied across the six queried phenotypes, with both models \nachieving their highest accuracy in fibrosis-related queries (ChatGPT-4o: 73.2%, \nGemini 1.5 Pro: 69.2%, Supplementary Figure 1). Conversely, the lowest accuracy \nwas observed in cartilage formation queries for ChatGPT-4o (45.9%) and ligament \nformation queries for Gemini 1.5 Pro (44.4%). \nAmong relevant peer-reviewed articles, the Wilcoxon paired signed-rank test indicated \nsignificantly higher impact factors per correct answer between ChatGPT-4o and Gemini \n(which was not certified by peer review) is the author/funder. All rights reserved. No reuse allowed without permission. \nThe copyright holder for this preprint\nthis version posted June 12, 2025. \n; \nhttps://doi.org/10.1101/2025.06.11.659165\ndoi: \nbioRxiv preprint", "metadata": {"producer": "Acrobat Distiller 8.1.0 (Windows)", "creator": "PScript5.dll Version 5.2.2", "creationdate": "2025-06-11T13:55:55-07:00", "source": "/home/donbr/open-biosciences/biosciences-research/data/raw/2025.06.11.659165v1.full.pdf", "file_path": "/home/donbr/open-biosciences/biosciences-research/data/raw/2025.06.11.659165v1.full.pdf", "total_pages": 24, "format": "PDF 1.4", "title": "Evaluating Large Language Models for Gene-to-Phenotype Mapping: The Critical Role of Full-Text Database Access", "author": "Administrator", "subject": "", "keywords": "", "moddate": "2026-01-10T17:13:47-08:00", "trapped": "", "modDate": "D:20260110171347-08'00'", "creationDate": "D:20250611135555-07'00'", "page": 10}}
{"page_content": "12 \n \n1.5 Pro (14.6 vs. 14.0, respectively; p = 0.001, p=0.0001, Figure 2c). Human-provided \nreferences had a statistically significantly higher impact factor than both ChatGPT-4o \nand Gemini 1.5 Pro (p = 0.0001 for human vs. ChatGPT-4o, and p = 0.0001 for human \nvs. Gemini 1.5 Pro; Figure 2d).  \n \nThe scientific-focused GPT-based LLM without its own database \nunderperformed the base GPT-based LLM in mapping genes to phenotype \nBuilding on these baseline comparisons between standard LLMs, we investigated \nwhether LLMs specialized for scientific purposes could improve gene-phenotype \nmapping performance. To address this question, we utilized publicly available \nChatGPT-4o-based, scientifically specialized LLMs, including PubMedGPT, SciSpace, \nScholarGPT, and ScholarAI. These scientific LLMs were categorized into three main \ngroups. First, LLMs with access to dedicated databases of full-length articles (SciSpace \nand ScholarAI). Second, LLMs without dedicated databases but with direct access to \npublicly available sources (ScholarGPT). Third, LLMs without access to full-length \narticles, customized to extract information from PubMed abstracts (PubMedGPT). \nPubmedGPT, an LLM customized to extract information from PubMed abstracts, \ndemonstrated similar accuracy compared to ChatGPT-4o (57.6% for PubmedGPT vs \n59.3% for ChatGPT-4o, p = 0.23; Figure 3a).  PubmedGPT had a comparable \nhallucination rate compared to ChatGPT-4o (16.4 % vs 16.2%, respectively; Figure 3b). \nHowever, PubmedGPT had a significantly lower percentage of irrelevant references \ncompared to ChatGPT-4o (5.4% vs. 18.3%, respectively; p = 0.0001; Figure 3b). \nAmong the relevant peer-reviewed articles and using the Wilcoxon pair-matched signed \nrank test, the impact factor of references provided by PubmedGPT had significantly \nlower impact factors to those from ChatGPT-4o and human (11.6 ± 11.9 for \nPubmedGPT, 13.1 ± 15.3 for ChatGPT-4o, 22.9 ± 25.0 for human, p = 0.02 for \nPubmedGPT vs ChatGPT-4o and p=0.0001 for PubmedGPT vs Human; Figure 3c-d). \nThe phenotype-specific analysis revealed that despite its specialization in biomedical \nabstracts, PubMedGPT showed consistently low accuracy across all phenotypes (42.4 - \n65.2% range), with particularly poor accuracy in cartilage formation (42.4%, \nSupplementary Figure 1). \n \nScholarGPT with public database access showed limited improvement over \nbase ChatGPT-4o \nTo further explore the role of database access beyond abstract-only models, we \ninvestigated whether GPT-based LLMs with direct access to publicly available scholarly \nsources could improve performance. We compared the ability of ScholarGPT to the \n(which was not certified by peer review) is the author/funder. All rights reserved. No reuse allowed without permission. \nThe copyright holder for this preprint\nthis version posted June 12, 2025. \n; \nhttps://doi.org/10.1101/2025.06.11.659165\ndoi: \nbioRxiv preprint", "metadata": {"producer": "Acrobat Distiller 8.1.0 (Windows)", "creator": "PScript5.dll Version 5.2.2", "creationdate": "2025-06-11T13:55:55-07:00", "source": "/home/donbr/open-biosciences/biosciences-research/data/raw/2025.06.11.659165v1.full.pdf", "file_path": "/home/donbr/open-biosciences/biosciences-research/data/raw/2025.06.11.659165v1.full.pdf", "total_pages": 24, "format": "PDF 1.4", "title": "Evaluating Large Language Models for Gene-to-Phenotype Mapping: The Critical Role of Full-Text Database Access", "author": "Administrator", "subject": "", "keywords": "", "moddate": "2026-01-10T17:13:47-08:00", "trapped": "", "modDate": "D:20260110171347-08'00'", "creationDate": "D:20250611135555-07'00'", "page": 11}}
{"page_content": "13 \n \nbase ChatGPT-4o and human to map genes to phenotype based on relevant peer-\nreviewed articles. \nScholarGPT had comparable accuracy compared to ChatGPT-4o (58.6 % for \nScholarGPT vs 59.3 % for ChatGPT-4o, p = 0.064; Figure 4a). Compared to ChatGPT-\n4o, ScholarGPT demonstrated significantly lower hallucination rate (5.9% for \nScholarGPT vs 16.4% for ChatGPT-4o, p=0.0001, Figure 4b).  ScholarGPT provided \nsignificantly lower irrelevant reference, with only 2.1% topic-irrelevant citations, \nsignificantly lower than ChatGPT-4o’s 18.4% (p = 0.0001). Among the relevant peer-\nreviewed articles provided and using the Wilcoxon pair-matched signed rank test, the \nimpact factor of references provided by ScholarGPT were comparable to the base \nChatGPT-4o (13.1 ± 15.3 for ChatGPT-4o, 13.4 ± 14.3 for ScholarGPT, p = 0.35, Figure \n4c) but significantly lower compared to human (13.4 ± 14.3 ScholarGPT and 22.9 ± 25.0 \nfor human, p= 0.0001, Figure 4d). The phenotype-specific heatmap demonstrated that \nScholarGPT maintained consistent performance across all phenotypes  with moderate \naccuracy (54.5 - 72.7% , Supplementary Figure 1). \n \nThe scientific-focused GPT-based LLMs with their own dedicated \ndatabases outperformed the base GPT LLM in mapping genes to \nphenotype \nWe next assessed whether large language models (LLMs) integrated with dedicated \ndatabases could enhance performance in gene-phenotype mapping tasks. Models \naugmented with dedicated databases containing full-length articles, specifically \nSciSpace and ScholarAI, exhibited significantly improved accuracy compared to \nChatGPT-4o (74.9% for SciSpace vs. 59.3% for ChatGPT-4o, p = 0.0001; 81.0% for \nScholarAI vs. 59.3% for ChatGPT-4o; p=0.0001, Figure 5a). Additionally, these \ndatabase-enhanced models outperformed other LLMs lacking dedicated databases, \nincluding those relying solely on publicly available resources (e.g., ScholarGPT) and \nthose tailored specifically for retrieving information from PubMed abstracts (e.g., \nPubMedGPT) (Figure 5b). \nMoreover, SciSpace and ScholarAI demonstrated significantly lower rates of \nhallucination compared to ChatGPT-4o (1.5% for ScholarAI, 2.86% for SciSpace, and \n16.4% for ChatGPT-4o; p = 0.0001; Figure 5c). Similarly, both SciSpace and ScholarAI \nshowed markedly lower rates of topic-irrelevant citations relative to ChatGPT-4o (3.1% \nfor ScholarAI, 5.55% for SciSpace, and 18.3% for ChatGPT-4o; p = 0.0001; Figure 5c). \nThe average impact factors of references cited by ScholarAI were significantly higher \nthan those cited by SciSpace, although both were notably lower compared to human-\nprovided references (10.3 ± 12.7 for ScholarAI, 8.5 ± 8.0 for SciSpace, and 22.9 ± 25.0 \n(which was not certified by peer review) is the author/funder. All rights reserved. No reuse allowed without permission. \nThe copyright holder for this preprint\nthis version posted June 12, 2025. \n; \nhttps://doi.org/10.1101/2025.06.11.659165\ndoi: \nbioRxiv preprint", "metadata": {"producer": "Acrobat Distiller 8.1.0 (Windows)", "creator": "PScript5.dll Version 5.2.2", "creationdate": "2025-06-11T13:55:55-07:00", "source": "/home/donbr/open-biosciences/biosciences-research/data/raw/2025.06.11.659165v1.full.pdf", "file_path": "/home/donbr/open-biosciences/biosciences-research/data/raw/2025.06.11.659165v1.full.pdf", "total_pages": 24, "format": "PDF 1.4", "title": "Evaluating Large Language Models for Gene-to-Phenotype Mapping: The Critical Role of Full-Text Database Access", "author": "Administrator", "subject": "", "keywords": "", "moddate": "2026-01-10T17:13:47-08:00", "trapped": "", "modDate": "D:20260110171347-08'00'", "creationDate": "D:20250611135555-07'00'", "page": 12}}
{"page_content": "14 \n \nfor human; p = 0.0016 for ScholarAI vs. SciSpace; p = 0.001 for human vs. SciSpace \nand human vs. ScholarAI; Figure 5d). The phenotype-specific heatmap revealed that \nScholarAI and SciSpace achieved remarkably consistent high performance across all \nphenotypes (79.3 - 82.3% for ScholarAI and 71.2 - 80.8% for SciSpace, \nSupplementary Figure 1). \n \nDiscussion \nAccurate gene-to-phenotype mapping at cellular and tissue levels is critical for single-\ncell omics analyses[14], elucidating disease correlations[15], assessing genetic \nperturbations[2], \nand \nadvancing \nthe \nmechanistic \nunderstanding \nof \ndisease \npathophysiology[16]. Currently, manual human curation from peer-reviewed literature \nremains the gold standard. However, given the exponential growth in scientific \npublications, there is a pressing need for automated or minimally supervised methods \ncapable of efficiently handling this task. \nAn ideal automated system for gene-to-phenotype mapping must meet three critical \nrequirements: (1) accurate retrieval of relevant literature, (2) precise extraction and \nsynthesis of relevant information, and (3) generation of reliable gene-to-phenotype \ncorrelations from aggregated data. Meeting these requirements would markedly \nenhance efficiency and accuracy while significantly reducing reliance on manual \ncuration, thus providing practical benefits across biomedical research, clinical practice, \npharmaceutical industries, and educational applications. \nTransformer-based large language models (LLMs) have emerged as promising tools to \naddress this need due to their impressive capabilities in information synthesis and \nnatural language processing. Indeed, these models have demonstrated notable \nsuccesses, such as passing medical licensing exams (USMLE)[17] and performing \ncompetitively in medical subspecialty assessments [18, 19, 20]. Despite these \nstrengths, LLMs have notable limitations, particularly concerning inaccuracies in \ninformation retrieval and the generation of fabricated content (hallucinations). Studies by \nBhattacharyya et al. (2023), Walters et al. (2023), and Chelli et al. (2024) highlight these \nissues, showing high rates of inaccuracies and fabricated references across various \nLLMs including GPT-3.5, GPT-4, and Bard [21, 22, 23]. \nConsistent with these concerns, our results identified similar limitations. ChatGPT-4o \nhad 16.4 % hallucination rate, produced 18.4% topic-irrelevant citations, and achieved \nonly 59.3% accuracy in gene-to-phenotype mapping. Gemini 1.5 Pro, another \ntransformer-based LLM, demonstrated slightly lower mapping accuracy (55.8%) but \nproduced fewer irrelevant citations and fabricated references, suggesting variability in \nperformance among models. \n(which was not certified by peer review) is the author/funder. All rights reserved. No reuse allowed without permission. \nThe copyright holder for this preprint\nthis version posted June 12, 2025. \n; \nhttps://doi.org/10.1101/2025.06.11.659165\ndoi: \nbioRxiv preprint", "metadata": {"producer": "Acrobat Distiller 8.1.0 (Windows)", "creator": "PScript5.dll Version 5.2.2", "creationdate": "2025-06-11T13:55:55-07:00", "source": "/home/donbr/open-biosciences/biosciences-research/data/raw/2025.06.11.659165v1.full.pdf", "file_path": "/home/donbr/open-biosciences/biosciences-research/data/raw/2025.06.11.659165v1.full.pdf", "total_pages": 24, "format": "PDF 1.4", "title": "Evaluating Large Language Models for Gene-to-Phenotype Mapping: The Critical Role of Full-Text Database Access", "author": "Administrator", "subject": "", "keywords": "", "moddate": "2026-01-10T17:13:47-08:00", "trapped": "", "modDate": "D:20260110171347-08'00'", "creationDate": "D:20250611135555-07'00'", "page": 13}}
{"page_content": "15 \n \nThe suboptimal accuracy of these general-purpose LLMs likely results from their \ninability to directly access the peer-reviewed articles, restricting their literature retrieval \ncapabilities and accuracy in synthesizing gene-to-phenotype relationships [24]. \nMoreover, their inherent design for linguistic rather than information processing \npredisposes these models to generate inaccuracies and fabrications [22]. \nTo overcome these limitations and improve practical applicability, specialized scientific \nLLMs have been developed. These specialized models incorporate enhanced features \nspecifically designed to improve literature retrieval and information extraction accuracy, \nsuch as abstract retrieval (PubMedGPT), integration with academic databases \n(ScholarGPT), and augmentation with proprietary databases containing extensive \ncollections of full-length articles (ScholarAI and SciSpace). \nPerformance varied notably among these specialized models. PubMedGPT, despite \ntargeted access to PubMed abstracts and open-access full texts via PubMed Central, \nhad moderate accuracy (57.6%) and low irrelevant citations (5.4%), but suffer from high \nhallucination rate similar to the base ChatGPT-4o model (16.2%). Conversely, models \nwith extensive access to full-text articles (ScholarAI and SciSpace) markedly \noutperformed others, achieving accuracies of 81.1% and 74.9%, respectively, with the \nlowest rate of hallucination rate and irrelevant references among all the models tested in \nthis study. These results underscore the critical role of comprehensive database access \nin enhancing LLM performance, with potential to benefit researchers through expedited \nliterature reviews, clinicians via rapid candidate gene identification, pharmaceutical \ncompanies by streamlining drug target validation, and educational settings through \nauthoritative learning resources. \nOur findings extend and complement previous studies in this field. Toufiq et al. (2023) \ndemonstrated LLM utility in knowledge-driven gene prioritization, albeit within \npredefined modules rather than direct gene-to-phenotype mapping [26]. Kim et al. \n(2024) found limited accuracy of GPT-4 in phenotype-driven gene prioritization for rare \ndiseases, noting biases toward highly studied genes [27]. Our results expand upon \nthese findings by demonstrating significantly improved accuracy when LLMs are \naugmented with comprehensive full-text databases, highlighting the importance of \nintegrating extensive domain-specific knowledge sources for genomic applications. \nFuture research should build upon these findings through systematic optimization of \nprompt engineering, exploring strategies such as multi-shot prompting, chain-of-thought \nreasoning, and domain-specific prompt libraries to enhance accuracy and reliability. \nAdditionally, exploring reverse phenotype-to-gene mapping, broadening the scope and \nscale of evaluation across diverse biological domains, incorporating temporal aspects of \ngene expression and phenotype development, and integrating with established \ndatabases like OMIM, ClinVar, and GWAS catalogs would significantly advance the \n(which was not certified by peer review) is the author/funder. All rights reserved. No reuse allowed without permission. \nThe copyright holder for this preprint\nthis version posted June 12, 2025. \n; \nhttps://doi.org/10.1101/2025.06.11.659165\ndoi: \nbioRxiv preprint", "metadata": {"producer": "Acrobat Distiller 8.1.0 (Windows)", "creator": "PScript5.dll Version 5.2.2", "creationdate": "2025-06-11T13:55:55-07:00", "source": "/home/donbr/open-biosciences/biosciences-research/data/raw/2025.06.11.659165v1.full.pdf", "file_path": "/home/donbr/open-biosciences/biosciences-research/data/raw/2025.06.11.659165v1.full.pdf", "total_pages": 24, "format": "PDF 1.4", "title": "Evaluating Large Language Models for Gene-to-Phenotype Mapping: The Critical Role of Full-Text Database Access", "author": "Administrator", "subject": "", "keywords": "", "moddate": "2026-01-10T17:13:47-08:00", "trapped": "", "modDate": "D:20260110171347-08'00'", "creationDate": "D:20250611135555-07'00'", "page": 14}}
{"page_content": "16\n \napplicability of these models. Furthermore, the development of standardized protocols\nfor clinical integration, quality control, and interpretation frameworks is essential for\npractical, safe, and effective real-world use. \nDespite these promising outcomes, our study has limitations, including the evaluation of\na limited subset of existing specialized GPT models and relatively small sets of genes\nand phenotypes compared to the entire human genome. Future studies addressing\nthese limitations through expanded evaluations will be vital for validating and\ngeneralizing our findings. \n \nConclusion \nThis study represents the first systematic and comprehensive analysis of both base \ntransformer-based large language models (LLMs) and specialized LLMs built on the \nGPT-4 architecture. Specifically, we evaluated their ability to retrieve query-relevant \nscientific literature and synthesize this information to generate accurate gene-to-\nphenotype mappings. The GPT-4-based LLMs, particularly when equipped with direct \naccess to databases, demonstrated significant improvement in accurately mapping \ngene-to-phenotypes. This integration not only enabled the generation of highly accurate \ngene-to-phenotype mappings but also ensured that the outputs were supported by \nrelevant, peer-reviewed scientific articles. By combining cutting-edge natural language \nprocessing capabilities with access to high-quality data sources, GPT-4-based LLMs \nhighlight their potential as powerful tools for facilitating complex biomedical research \ntasks. This bridges the gap between vast genomic datasets and actionable biological \ninsights \nFigures \n6 \ns \nr \nf \ns \ng \nd \n \n(which was not certified by peer review) is the author/funder. All rights reserved. No reuse allowed without permission. \nThe copyright holder for this preprint\nthis version posted June 12, 2025. \n; \nhttps://doi.org/10.1101/2025.06.11.659165\ndoi: \nbioRxiv preprint", "metadata": {"producer": "Acrobat Distiller 8.1.0 (Windows)", "creator": "PScript5.dll Version 5.2.2", "creationdate": "2025-06-11T13:55:55-07:00", "source": "/home/donbr/open-biosciences/biosciences-research/data/raw/2025.06.11.659165v1.full.pdf", "file_path": "/home/donbr/open-biosciences/biosciences-research/data/raw/2025.06.11.659165v1.full.pdf", "total_pages": 24, "format": "PDF 1.4", "title": "Evaluating Large Language Models for Gene-to-Phenotype Mapping: The Critical Role of Full-Text Database Access", "author": "Administrator", "subject": "", "keywords": "", "moddate": "2026-01-10T17:13:47-08:00", "trapped": "", "modDate": "D:20260110171347-08'00'", "creationDate": "D:20250611135555-07'00'", "page": 15}}
{"page_content": "17\n \nFigure 2. Accuracy and impact factor of references provided by ChatGPT4o and\nGeminiAI. (a) Accuracy of correct gene-to-phenotype mappings by ChatGPT4o and GeminiAI\nEach dot represents a single gene-to-phenotype query, with a value of 1 indicating agreement\nbetween the tested LLM and human answers, and 0 indicating a discrepancy. Data are\npresented as mean ± s.e.m., analyzed using the Wilcoxon pair-matched signed-rank test. (b)\nProportion of hallucinations (red bars) and irrelevant references (blue bars) provided by\nChatGPT4o and GeminiAI. Data are presented as mean ± s.e.m., analyzed using the Chi-\nsquare test.(c-d) Impact factor of references provided by human, ChatGPT4o, and GeminiAI\nEach dot represents the impact factor of a single gene-to-phenotype query. Data are presented\nas mean ± s.e.m., analyzed using the Wilcoxon pair-matched signed-rank test. *p<0.05\n**p<0.01, ***p<0.001, ****p<0.0001. \n \n \n  \nFigure 3. Accuracy and impact factor of references provided by PubmedGPT. (a) Accuracy\nof gene-to-phenotype mappings by ChatGPT4o and PubmedGPT. Each dot represents a single\ngene-to-phenotype query, with a value of 1 indicating agreement between the tested LLM and\nhuman answers, and 0 indicating a discrepancy. Data are presented as mean ± s.e.m.\nanalyzed using the Wilcoxon pair-matched signed-rank test. (b) Proportion of hallucinations (red\nbars) and irrelevant references (blue bars) provided by ChatGPT4o and PubmedGPT. Data are\npresented as mean ± s.e.m., analyzed using the Chi-square test.(c-d) Impact factor of\nreferences provided by human, ChatGPT4o, and PubmedGPT. Each dot represents the impact\nfactor of a single gene-to-phenotype query. Data are presented as mean ± s.e.m., analyzed\nusing the Wilcoxon pair-matched signed-rank test. *p<0.05, **p<0.01, ***p<0.001, ****p<0.0001.\n \n7 \nd \n. \nt \ne \n) \ny \n-\n. \nd \n, \n \ny \ne \nd \n, \nd \ne \nf \nt \nd \n(which was not certified by peer review) is the author/funder. All rights reserved. No reuse allowed without permission. \nThe copyright holder for this preprint\nthis version posted June 12, 2025. \n; \nhttps://doi.org/10.1101/2025.06.11.659165\ndoi: \nbioRxiv preprint", "metadata": {"producer": "Acrobat Distiller 8.1.0 (Windows)", "creator": "PScript5.dll Version 5.2.2", "creationdate": "2025-06-11T13:55:55-07:00", "source": "/home/donbr/open-biosciences/biosciences-research/data/raw/2025.06.11.659165v1.full.pdf", "file_path": "/home/donbr/open-biosciences/biosciences-research/data/raw/2025.06.11.659165v1.full.pdf", "total_pages": 24, "format": "PDF 1.4", "title": "Evaluating Large Language Models for Gene-to-Phenotype Mapping: The Critical Role of Full-Text Database Access", "author": "Administrator", "subject": "", "keywords": "", "moddate": "2026-01-10T17:13:47-08:00", "trapped": "", "modDate": "D:20260110171347-08'00'", "creationDate": "D:20250611135555-07'00'", "page": 16}}
{"page_content": "18\n \nFigure 4. Accuracy and impact factor of references provided by ScholarGPT. (a) Accuracy\nof gene-to-phenotype mappings by ChatGPT4o and ScholarGPT. Each dot represents a single\ngene-to-phenotype query, with a value of 1 indicating agreement between the tested LLM and\nhuman answers, and 0 indicating a discrepancy. Data are presented as mean ± s.e.m.\nanalyzed using the Wilcoxon pair-matched signed-rank test. (b) Proportion of hallucinations (red\nbars) and irrelevant references (blue bars) provided by ChatGPT4o and ScholarGPT. Data are\npresented as mean ± s.e.m., analyzed using the Chi-square test.(c-d) Impact factor of\nreferences provided by human, ChatGPT4o, and ScholarGPT. Each dot represents the impact\nfactor of a single gene-to-phenotype query. Data are presented as mean ± s.e.m., analyzed\nusing the Wilcoxon pair-matched signed-rank test. *p<0.05, **p<0.01, ***p<0.001, ****p<0.0001.\n \nFigure 5. Accuracy and impact factor of references provided by SciSpace and ScholarAI.\n(a) Accuracy of gene-to-phenotype mappings by ChatGPT4o, SciSpace, and ScholarAI. Each\ndot represents a single gene-to-phenotype query, with a value of 1 indicating agreement\nbetween the tested LLM and human answers, and 0 indicating a discrepancy. Data are\npresented as mean ± s.e.m., analyzed using the Wilcoxon pair-matched signed-rank test. (b)\nPercentage of correct gene-to-phenotype mappings by PubmedGPT, ScholarGPT, SciSpace\nand ScholarAI. Each dot represents a single gene-to-phenotype query, with a value of 1\nindicating agreement between the tested LLM and human answers, and 0 indicating a\ndiscrepancy. Data are presented as mean ± s.e.m., analyzed using the Wilcoxon pair-matched\nsigned-rank test. (c) Proportion of hallucination (red bars) and irrelevant references (blue bars)\n8 \n \ny \ne \nd \n, \nd \ne \nf \nt \nd \n \n. \nh \nt \ne \n) \n, \n1 \na \nd \n) \n(which was not certified by peer review) is the author/funder. All rights reserved. No reuse allowed without permission. \nThe copyright holder for this preprint\nthis version posted June 12, 2025. \n; \nhttps://doi.org/10.1101/2025.06.11.659165\ndoi: \nbioRxiv preprint", "metadata": {"producer": "Acrobat Distiller 8.1.0 (Windows)", "creator": "PScript5.dll Version 5.2.2", "creationdate": "2025-06-11T13:55:55-07:00", "source": "/home/donbr/open-biosciences/biosciences-research/data/raw/2025.06.11.659165v1.full.pdf", "file_path": "/home/donbr/open-biosciences/biosciences-research/data/raw/2025.06.11.659165v1.full.pdf", "total_pages": 24, "format": "PDF 1.4", "title": "Evaluating Large Language Models for Gene-to-Phenotype Mapping: The Critical Role of Full-Text Database Access", "author": "Administrator", "subject": "", "keywords": "", "moddate": "2026-01-10T17:13:47-08:00", "trapped": "", "modDate": "D:20260110171347-08'00'", "creationDate": "D:20250611135555-07'00'", "page": 17}}
{"page_content": "19 \n \nprovided by ChatGPT4o, SciSpace, and ScholarAI. Data are presented as mean ± s.e.m., \nanalyzed using the Chi-square test. (d) Impact factor of references provided by human, \nChatGPT4o, SciSpace, and ScholarAI. Each dot represents the impact factor of a single gene-\nto-phenotype query. Data are presented as mean ± s.e.m., analyzed using the Wilcoxon pair-\nmatched signed-rank test. *p<0.05, **p<0.01, ***p<0.001, ****p<0.0001. \n \n \n \n(which was not certified by peer review) is the author/funder. All rights reserved. No reuse allowed without permission. \nThe copyright holder for this preprint\nthis version posted June 12, 2025. \n; \nhttps://doi.org/10.1101/2025.06.11.659165\ndoi: \nbioRxiv preprint", "metadata": {"producer": "Acrobat Distiller 8.1.0 (Windows)", "creator": "PScript5.dll Version 5.2.2", "creationdate": "2025-06-11T13:55:55-07:00", "source": "/home/donbr/open-biosciences/biosciences-research/data/raw/2025.06.11.659165v1.full.pdf", "file_path": "/home/donbr/open-biosciences/biosciences-research/data/raw/2025.06.11.659165v1.full.pdf", "total_pages": 24, "format": "PDF 1.4", "title": "Evaluating Large Language Models for Gene-to-Phenotype Mapping: The Critical Role of Full-Text Database Access", "author": "Administrator", "subject": "", "keywords": "", "moddate": "2026-01-10T17:13:47-08:00", "trapped": "", "modDate": "D:20260110171347-08'00'", "creationDate": "D:20250611135555-07'00'", "page": 18}}
{"page_content": "20\n \nSupplementary Figure  1: Phenotype-specific performance heatmap showing agreement \nrates (%) for each model across six different phenotypes. The heatmap reveals consistent \npatterns: cartilage formation (lighter yellow regions) poses the greatest challenge across all \nmodels, while fibrosis and cell proliferation (darker red regions) generally show higher \nagreement rates. Database-augmented models (ScholarAI and SciSpace) maintain more \nconsistent performance across phenotypes compared to general-purpose LLMs, which show \ngreater variability. \n \n \n0 \n \n(which was not certified by peer review) is the author/funder. All rights reserved. No reuse allowed without permission. \nThe copyright holder for this preprint\nthis version posted June 12, 2025. \n; \nhttps://doi.org/10.1101/2025.06.11.659165\ndoi: \nbioRxiv preprint", "metadata": {"producer": "Acrobat Distiller 8.1.0 (Windows)", "creator": "PScript5.dll Version 5.2.2", "creationdate": "2025-06-11T13:55:55-07:00", "source": "/home/donbr/open-biosciences/biosciences-research/data/raw/2025.06.11.659165v1.full.pdf", "file_path": "/home/donbr/open-biosciences/biosciences-research/data/raw/2025.06.11.659165v1.full.pdf", "total_pages": 24, "format": "PDF 1.4", "title": "Evaluating Large Language Models for Gene-to-Phenotype Mapping: The Critical Role of Full-Text Database Access", "author": "Administrator", "subject": "", "keywords": "", "moddate": "2026-01-10T17:13:47-08:00", "trapped": "", "modDate": "D:20260110171347-08'00'", "creationDate": "D:20250611135555-07'00'", "page": 19}}
{"page_content": "21 \n \nAuthor Contributions \nNicolas Suhardi and Vincentius Suhardi selected the list of genes and phenotypes, \ntested the relevance and accuracy of the references provided by the LLMs, conducted \ndata analysis, and formulated questions. Anastasia Oktarina, Damanpreet Dhillon, and \nDona Ninan recorded the responses generated by all LLMs and organized the raw text \nresponses into Excel spreadsheets. Anastasia Oktarina verified the relevance and \naccuracy of the references provided by LLMs. Vincentius Suhardi performed the \nmanual curation of references and manual gene-to-phenotype mapping that are used \nas ground truth for this study. Mathias Bostrom and Xu Yang assisted in editing the \nmanuscript. \nFunding \nThis project was funded by the OREF under awards 994088 and 892405, a Hospital for \nSpecial Surgery Surgeon in-Chief Grant, and a Complex Joint Reconstruction Center \ngrant given to VJS. XY is supported by grant UL1 TR000457 from the Clinical and \nTranslational Science Center at Weill Cornell Medicine, the Feldstein Medical \nFoundation, and grant W81XWH-21-1-0900 from the Department of Defense. \nData Availability \nAll data generated in this study are available from the corresponding author upon \nreasonable request. \nDeclaration of Competing Interest \nThe authors declare that they have no known competing financial interests or personal \nrelationships that could have appeared to influence the work reported in this paper. \nAcknowledgments \nThe authors thank the research staff at Hospital for Special Surgery for their support in \nthis study. We acknowledge the contributions of the large language model developers \nwho made their systems publicly accessible for research purposes. \n \n \n(which was not certified by peer review) is the author/funder. All rights reserved. No reuse allowed without permission. \nThe copyright holder for this preprint\nthis version posted June 12, 2025. \n; \nhttps://doi.org/10.1101/2025.06.11.659165\ndoi: \nbioRxiv preprint", "metadata": {"producer": "Acrobat Distiller 8.1.0 (Windows)", "creator": "PScript5.dll Version 5.2.2", "creationdate": "2025-06-11T13:55:55-07:00", "source": "/home/donbr/open-biosciences/biosciences-research/data/raw/2025.06.11.659165v1.full.pdf", "file_path": "/home/donbr/open-biosciences/biosciences-research/data/raw/2025.06.11.659165v1.full.pdf", "total_pages": 24, "format": "PDF 1.4", "title": "Evaluating Large Language Models for Gene-to-Phenotype Mapping: The Critical Role of Full-Text Database Access", "author": "Administrator", "subject": "", "keywords": "", "moddate": "2026-01-10T17:13:47-08:00", "trapped": "", "modDate": "D:20260110171347-08'00'", "creationDate": "D:20250611135555-07'00'", "page": 20}}
{"page_content": "22 \n \nReferences \n[1] A. Dixit, O. Parnas, B. Li, et al., Perturb-seq: Dissecting molecular circuits with \nscalable single-cell rna profiling of pooled genetic screens, Cell 167 (7) (2016) \n1853–1866.e17. doi:10.1016/j.cell.2016.11.038. \n[2] H. Cui, C. Wang, H. Maan, et al., scgpt: toward building a foundation model for \nsingle-cell multi-omics using generative ai, Nature Methods 21 (8) (2024) 1470–\n1480. doi:10.1038/s41592-024-02201-0. \n[3] M. A. Hanson, P. Gómez Barreiro, P. Crosetto, D. Brockington, The strain on \nscientific publishing, arXivArXiv:2309.15884 (2024). \n[4] S. Gururangan, A. Marasovic, S. Swayamdipta, K. Lo, I. Beltagy, D. Downey, N. A. \nSmith, Don’t stop´ pretraining: Adapt language models to domains and tasks, \narXivArXiv:2004.10964 (2020). \n[5] X. Qiu, T. Sun, Y. Xu, Y. Shao, N. Dai, X. Huang, Pre-trained models for natural \nlanguage processing: A survey, Science China Technological Sciences 63 (2020) \n1872–1897. \n[6] J. Liu, M. Yang, Y. Yu, H. Xu, K. Li, X. Zhou, Large language models in \nbioinformatics: applications and perspectives, ArXiv (2024). \n[7] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, Attention is all you \nneed, arXivArXiv:1706.03762 (2017). \n[8] S. Reza, M. C. Ferreira, J. J. M. Machado, J. M. R. S. Tavares, A multi-head \nattention-based transformer model for traffic flow forecasting with a comparative \nanalysis to recurrent neural networks, Expert Systems with Applications 202 (2022) \n117275. \n[9] A. V. Gougherty, H. L. Clipp, Testing the reliability of an ai-based large language \nmodel to extract ecological information from the scientific literature, NPJ \nBiodiversity 3 (1) (2024) 13. doi:10.1038/ s44185-024-00043-9. \n[10] S. Banerjee, A. Agarwal, S. Singla, Llms will always hallucinate, and we need to live \nwith this, arXivArXiv:2409.05746 (2024). \n[11] M. C. O’Neill, M., Amplifying limitations, harms and risks of large language models, \nArXivArXiv:2307.04821 (2023). \n[12] M. Hu, S. Alkhairy, I. Lee, et al., Evaluation of large language models for discovery \nof gene set function, Nature Methods (2024). doi:10.1038/s41592-024-02525-x. \n[13] S. Nikolic, C. Sandison, R. Haque, et al., Chatgpt, copilot, gemini, scispace and \nwolfram versus higher education assessments: an updated multiinstitutional study \nof the academic integrity impacts of generative artificial intelligence (genai) on \nassessment, teaching and learning in engineering, Australasian Journal of \nEngineering Education 29 (2) (2024) 126–153. \n(which was not certified by peer review) is the author/funder. All rights reserved. No reuse allowed without permission. \nThe copyright holder for this preprint\nthis version posted June 12, 2025. \n; \nhttps://doi.org/10.1101/2025.06.11.659165\ndoi: \nbioRxiv preprint", "metadata": {"producer": "Acrobat Distiller 8.1.0 (Windows)", "creator": "PScript5.dll Version 5.2.2", "creationdate": "2025-06-11T13:55:55-07:00", "source": "/home/donbr/open-biosciences/biosciences-research/data/raw/2025.06.11.659165v1.full.pdf", "file_path": "/home/donbr/open-biosciences/biosciences-research/data/raw/2025.06.11.659165v1.full.pdf", "total_pages": 24, "format": "PDF 1.4", "title": "Evaluating Large Language Models for Gene-to-Phenotype Mapping: The Critical Role of Full-Text Database Access", "author": "Administrator", "subject": "", "keywords": "", "moddate": "2026-01-10T17:13:47-08:00", "trapped": "", "modDate": "D:20260110171347-08'00'", "creationDate": "D:20250611135555-07'00'", "page": 21}}
{"page_content": "23 \n \n[14] Y. Mao, Y.-Y. Lin, N.-K. Y. Wong, et al., Phenotype prediction from single-cell rna-\nseq data using attentionbased neural networks, Bioinformatics 40 (2) (2024). \ndoi:10.1093/bioinformatics/btae067. \n[15] M. Elgart, M. O. Goodman, C. Isasi, et al., Correlations between complex human \nphenotypes vary by genetic background, gender, and environment, Cell Reports \nMedicine 3 (12) (2022) 100844. doi:10.1016/j. xcrm.2022.100844. \n[16] V. J. Suhardi, A. Oktarina, M. Hammad, et al., Prevention and treatment of peri-\nimplant fibrosis by functionally inhibiting skeletal cells expressing the leptin \nreceptor, \nNature \nBiomedical \nEngineering \n8 \n(10) \n(2024) \n1285–1307. \ndoi:10.1038/s41551-024-01238-y. \n[17] B. T. Bicknell, D. Butler, S. Whalen, et al., Chatgpt-4 omni performance in usmle \ndisciplines and clinical skills: Comparative analysis, JMIR Medical Education 10 \n(2024) e63430. doi:10.2196/63430. \n[18] P. C. Lee, S. K. Sharma, S. Motaganahalli, A. Huang, Evaluating the clinical \ndecision-making ability of large language models using mksap-19 cardiology \nquestions, JACC Advances 2 (9) (2023) 100658. doi: \n10.1016/j.jacadv.2023.100658. \n[19] J. E. Kung, C. Marshall, C. Gauthier, T. A. Gonzalez, J. B. Jackson III, Evaluating \nchatgpt performance on the orthopaedic in-training examination, JB JS Open \nAccess 8 (3) (2023). doi:10.2106/JBJS.OA.23.00056. \n[20] F. Antaki, D. Milad, M. A. Chia, et al., Capabilities of gpt-4 in ophthalmology: an \nanalysis of model entropy and progress towards human-level medical question \nanswering, British Journal of Ophthalmology 108 (10) (2024) 1371–1378. \ndoi:10.1136/bjo-2023-324438. \n[21] M. Bhattacharyya, V. M. Miller, D. Bhattacharyya, L. E. Miller, High rates of \nfabricated and inaccurate references in chatgpt-generated medical content, Cureus \n15 (5) (2023) e39238. doi:10.7759/cureus. \n39238. \n[22] W. H. Walters, E. I. Wilder, Fabrication and errors in the bibliographic citations \ngenerated by chatgpt, Scientific Reports 13 (1) (2023) 14045. doi:10.1038/s41598-\n023-41032-5. \n[23] M. Chelli, J. Descamps, V. Lavoué, et al., Hallucination rates and reference \naccuracy of chatgpt and bard for systematic reviews: Comparative analysis, Journal \nof Medical Internet Research 26 (2024) e53164. doi: 10.2196/53164. \n[24] O. Abuyaman, Strengths and weaknesses of chatgpt models for scientific writing \nabout medical vitamin b12: Mixed methods study, JMIR Formative Research 7 \n(2023) e49459. doi:10.2196/49459. \n(which was not certified by peer review) is the author/funder. All rights reserved. No reuse allowed without permission. \nThe copyright holder for this preprint\nthis version posted June 12, 2025. \n; \nhttps://doi.org/10.1101/2025.06.11.659165\ndoi: \nbioRxiv preprint", "metadata": {"producer": "Acrobat Distiller 8.1.0 (Windows)", "creator": "PScript5.dll Version 5.2.2", "creationdate": "2025-06-11T13:55:55-07:00", "source": "/home/donbr/open-biosciences/biosciences-research/data/raw/2025.06.11.659165v1.full.pdf", "file_path": "/home/donbr/open-biosciences/biosciences-research/data/raw/2025.06.11.659165v1.full.pdf", "total_pages": 24, "format": "PDF 1.4", "title": "Evaluating Large Language Models for Gene-to-Phenotype Mapping: The Critical Role of Full-Text Database Access", "author": "Administrator", "subject": "", "keywords": "", "moddate": "2026-01-10T17:13:47-08:00", "trapped": "", "modDate": "D:20260110171347-08'00'", "creationDate": "D:20250611135555-07'00'", "page": 22}}
{"page_content": "24 \n \n[25] A. Preisler, Correctness and quality of references generated by ai-based research \nassistant tools: The case of scopus ai, elicit, scispace and scite in the field of \nbusiness \nadministration \n(2024).  \nhttps://www.researchgate.net/publication/383988574_Correctness_ \nand_Quality_of_References_generated_by_AI-based_Research_Assistant_ \nTools_The_Case_of_Scopus_AI_Elicit_SciSpace_and_Scite_in_the_Field_of_ \nBusiness_Administration/link/66e3eb172390e50b2c869d36/download?_tp= \neyJjb250ZXh0Ijp7ImZpcnN0UGFnZSI6InB1YmxpY2F0aW9uIiwicGFnZSI6InB1Ym\nxpY2F0aW9uIn19 \n[26] M. Toufiq, D. Rinchai, E. Bettacchioli, et al., Harnessing large language models \n(llms) for candidate gene prioritization and selection, Journal of Translational \nMedicine 21 (1) (2023) 728. doi:10.1186/ s12967-023-04576-8. \n[27] J. Kim, K. Wang, C. Weng, C. Liu, Assessing the utility of large language models for \nphenotype-driven gene prioritization in the diagnosis of rare genetic disease, \nAmerican \nJournal \nof \nHuman \nGenetics \n111 \n(10) \n(2024) \n2205–2219. \ndoi:10.1016/j.ajhg.2024.08.010. \n \n(which was not certified by peer review) is the author/funder. All rights reserved. No reuse allowed without permission. \nThe copyright holder for this preprint\nthis version posted June 12, 2025. \n; \nhttps://doi.org/10.1101/2025.06.11.659165\ndoi: \nbioRxiv preprint", "metadata": {"producer": "Acrobat Distiller 8.1.0 (Windows)", "creator": "PScript5.dll Version 5.2.2", "creationdate": "2025-06-11T13:55:55-07:00", "source": "/home/donbr/open-biosciences/biosciences-research/data/raw/2025.06.11.659165v1.full.pdf", "file_path": "/home/donbr/open-biosciences/biosciences-research/data/raw/2025.06.11.659165v1.full.pdf", "total_pages": 24, "format": "PDF 1.4", "title": "Evaluating Large Language Models for Gene-to-Phenotype Mapping: The Critical Role of Full-Text Database Access", "author": "Administrator", "subject": "", "keywords": "", "moddate": "2026-01-10T17:13:47-08:00", "trapped": "", "modDate": "D:20260110171347-08'00'", "creationDate": "D:20250611135555-07'00'", "page": 23}}
{"page_content": "D380–D384\nNucleic Acids Research, 2016, Vol. 44, Database issue\nPublished online 20 November 2015\ndoi: 10.1093/nar/gkv1277\nSTITCH 5: augmenting protein–chemical interaction\nnetworks with tissue and afﬁnity data\nDamian Szklarczyk1, Alberto Santos2, Christian von Mering1, Lars Juhl Jensen2,\nPeer Bork3,4,* and Michael Kuhn5,*\n1Institute of Molecular Life Sciences, University of Zurich and Swiss Institute of Bioinformatics, Winterthurerstrasse\n190, 8057 Zurich, Switzerland, 2Novo Nordisk Foundation Center for Protein Research, Faculty of Health and Medical\nSciences, University of Copenhagen, 2200 Copenhagen N, Denmark, 3European Molecular Biology Laboratory,\nStructural and Computational Biology Unit, Molecular Medicine Partnership Unit, Meyerhofstrasse 1, 69117\nHeidelberg, Germany, 4Max-Delbr¨uck-Centre for Molecular Medicine, Robert-R¨ossle-Strasse 10, 13092 Berlin,\nGermany and 5Max Planck Institute of Molecular Cell Biology and Genetics, Pfotenhauerstrasse 108, 01307 Dresden\nReceived October 1, 2015; Revised November 3, 2015; Accepted November 3, 2015\nABSTRACT\nInteractions between proteins and small molecules\nare an integral part of biological processes in liv-\ning organisms. Information on these interactions is\ndispersed over many databases, texts and predic-\ntion methods, which makes it difﬁcult to get a com-\nprehensive overview of the available evidence. To\naddress this, we have developed STITCH (‘Search\nTool for Interacting Chemicals’) that integrates these\ndisparate data sources for 430 000 chemicals into\na single, easy-to-use resource. In addition to the in-\ncreased scope of the database, we have implemented\na new network view that gives the user the ability to\nview binding afﬁnities of chemicals in the interac-\ntion network. This enables the user to get a quick\noverview of the potential effects of the chemical on\nits interaction partners. For each organism, STITCH\nprovides a global network; however, not all proteins\nhave the same pattern of spatial expression. There-\nfore, only a certain subset of interactions can occur\nsimultaneously. In the new, ﬁfth release of STITCH,\nwe have implemented functionality to ﬁlter out the\nproteins and chemicals not associated with a given\ntissue. The STITCH database can be downloaded\nin full, accessed programmatically via an extensive\nAPI, or searched via a redesigned web interface at\nhttp://stitch.embl.de.\nINTRODUCTION\nThe role of small molecules in biological systems can be\nunderstood only in the relation to the function of the tar-\ngeted biomolecules, which, in turn, is largely defined by their\ninteraction partners (1–3). The role of the interaction net-\nwork is even more prominent in the area of the drug devel-\nopment, since diseases are often a consequence of multiple\nchanges in the same pathway or protein complex (4,5). Tak-\ning into account the neighborhood of the targeted proteins\nand the topology of the network itself can lead to a bet-\nter understanding of a drug’s cellular impact (6,7). Further-\nmore, as only a subset of all proteins are viable drug targets\n(8), most therapeutics target proteins in the network vicin-\nity from more prospective, but undruggable, proteins (7).\nSeveral databases provide proteome-wide protein–chemical\ninteractions (9–11) and several other (12–14) put protein–\nchemical interactions in the context of protein–protein in-\nteraction networks, which is essential for effective in silico\ndrug discovery.\nA drug’s impact on the organism and its efficacy depend\non its engagement with the targeted proteins and the ex-\ntent to which it disrupts the protein–protein and protein–\nchemical interaction network (7,15). This is related to the\nconcentration of the drug, the strength with which it modu-\nlates the activity of the target, and the distribution of target\nproteins among different tissues (16). To enable the users to\nrationally select possible drug targets, we have added two\nnew features to STITCH: a new mode that allows users to\nshow known binding affinities between proteins and chem-\nicals, and the ability to filter the network to show only pro-\nteins related to a selected tissue.\nSTITCH, in its fifth release, shares protein space with\nSTRING v10 (17) and now encompasses more than\n9 600 000 proteins from 2031 eukaryotic and prokary-\notic genomes. Also, its chemical space grew by a quarter\ncompared to the previous version (18), from 340 000 to\n430 000 compounds (not including different stereoisomers).\nSTITCH is available through new redesigned web interface\nat http://stitch.embl.de and via an extensive API that al-\n*To whom correspondence should be addressed. Tel: +49 351 210 2901; Fax: +49 351 210 1389; Email: mkuhn@mpi-cbg.de\nCorrespondence may also be addressed to Peer Bork. Tel: +49 6221 387 8526; Fax: +49 6221 387 8517; Email: bork@embl.de\nC⃝The Author(s) 2015. Published by Oxford University Press on behalf of Nucleic Acids Research.\nThis is an Open Access article distributed under the terms of the Creative Commons Attribution License (http://creativecommons.org/licenses/by/4.0/), which\npermits unrestricted reuse, distribution, and reproduction in any medium, provided the original work is properly cited.\nDownloaded from https://academic.oup.com/nar/article/44/D1/D380/2503089 by guest on 11 January 2026", "metadata": {"producer": "Acrobat Distiller 9.0.0 (Windows); modified using iTextSharp.LGPLv2.Core 3.7.4.0", "creator": "XMLPublish v.3.0 Copyright 1991-2015 Aptara Inc.", "creationdate": "2015-12-24T22:46:17-05:00", "source": "/home/donbr/open-biosciences/biosciences-research/data/raw/gkv1277.pdf", "file_path": "/home/donbr/open-biosciences/biosciences-research/data/raw/gkv1277.pdf", "total_pages": 5, "format": "PDF 1.4", "title": "", "author": "", "subject": "", "keywords": "", "moddate": "2026-01-11T01:04:03+00:00", "trapped": "", "modDate": "D:20260111010403+00'00'", "creationDate": "D:20151224224617-05'00'", "page": 0}}
{"page_content": "Nucleic Acids Research, 2016, Vol. 44, Database issue D381\nlows programmatic access, including the ability to disam-\nbiguate queries, modify all network parameters and gen-\nerate images. In order to enable large-scale analysis, which\nmay not be feasible through web-interface or API, the pre-\ncomputed network and the supplementary information are\nfreely available for download.\nSOURCES OF INTERACTIONS\nAlthough there is a plethora of data available from which\nprotein–chemical networks could be derived, their dis-\npersed nature, different precision, name-space and focus\nmake it cumbersome to assemble a full picture of all avail-\nable knowledge. The STITCH pipeline aggregates high-\nthroughput experiments data, manually curated datasets\nand the results of several prediction methods into a single\nglobal network of protein–protein and protein–chemical in-\nteractions. This does not expose the user to the heterogene-\nity of the underlying data, yet, at the same time, keeps all\nthe primary evidence of the interaction readily accessible.\nA large part of the known interactions comes from man-\nually curated datasets such as DrugBank (19), GPCR-\nligand database (GLIDA) (20), Matador (21), the Thera-\npeutic Targets Database (TTD) (22) and the Comparative\nToxicogenomics Database (CTD) (23), and several path-\nway databases including the Kyoto Encyclopedia of Genes\nand Genomes (KEGG) (12), NCI/Nature Pathway Inter-\naction Database (24), Reactome (25) and BioCyc (26). As\nthere can be overlap between different manually curated\ndatasets, we do not consider multiple reports of identi-\ncal interactions as being independent from each other. In-\nstead, we count redundant interactions only once and do\nnot increase the confidence level. Other large sources of\nprotein–chemical links are the datasets of experimentally\nvalidated interactions, which include ChEMBL (27), PDSP\nKi Database (28), Protein Data Bank (PDB) (29) and two\nhigh-throughput kinase–ligand interactions studies (30,31).\nAlso in this case, interactions may be reported in different\ndatabases and with different binding affinities. To compute\nthe final confidence score, we only take the strongest re-\nported affinity into account.\nThe sources of verified protein–chemical interactions are\ncomplemented by automated text mining and a structure-\nbased prediction method (18). The text-mining pipeline in-\nclude co-occurrence text-mining and natural language pro-\ncessing of all MEDLINE abstracts as well as available\nPubMed Central open-access full-text articles (32). The\nnewest addition to the text-mining sources are NIH Re-\nPORTER grant abstracts (https://projectreporter.nih.gov/).\nConsidering co-occurring terms, adding the RePORTER\ndata increased the number of high-confidence interactions\nbetween human proteins and chemicals from 2740 to 4740.\nExtensive benchmarking of each data source allows us to\nprovide unified confidence score for every interaction while\ntaking into account the sources’ predicted precision.\nDISPLAY OF BINDING AFFINITIES IN THE NETWORK\nVIEW\nSmall molecules that activate or inhibit proteins such as en-\nzymes or receptors are among the most studied classes of\nexogenous small molecules. In order to assess the effect and\nconfidence of protein–ligand binding, as well as variabil-\nity in the affinity of known ligands, it is essential to know\nthe binding affinity between the compound and its target.\nUsually, this binding affinity is quantified as the inhibition\nconstant Ki. In some cases, Ki values are not available, but\nother values such as the IC50 or EC50 (half of the maxi-\nmal inhibitory concentration) can serve as an approxima-\ntion. Ki values of drugs vary greatly, from nanomolar in-\nhibition constants to relatively high values, such as 52 \u0002M\nbetween aspirin and cyclooxygenase 2 (27). Therefore, for\nany given drug, it is not so much the absolute value of the\nKi, but rather the relative binding affinities that determine\nthe impact on the interaction network.\nIn previous versions of STITCH, Ki values from primary\nsources (27,28) were accessible to the user through the web-\ninterface. In the new release of STITCH, the user can now\nchoose to switch the network view to show the binding\naffinities of all protein–chemical interactions for which this\nvalue is known (Figure 1). This new network view is similar\nto the STITCH’s confidence view: the thickness of the edge\nbetween nodes scales with the Ki value. If a Ki is not avail-\nable, EC50 or IC50 will be used to determine the depicted\nstrength of the interaction. If there are multiple measure-\nments available, the lowest value (i.e. highest reported affin-\nity) will be used to determine the thickness of the edge.\nDATA AND FILTERING FOR TISSUE SPECIFICITY\nThe protein–chemical network in STITCH is global and\nas such considers interactions anywhere in an organism.\nHowever, in multicellular organisms such as humans, not\nall proteins are present in every tissue. STITCH 5 addresses\nthis through a new feature that allows users to filter a\nhuman interaction network so that only the proteins be-\nlieved to be present in a specified tissue are shown (Fig-\nure 2). To provide this feature, STITCH now integrates\ntissue-specific protein expression patterns from two data\nsources. First, the TISSUES resource (33), which combines\nevidence from UniProt annotations, systematic large-scale\ntranscriptomics and proteomics studies, and co-occurrence\ntext mining. For use in STITCH, the text-mining evidence\nwas recomputed based on the same texts used elsewhere in\nSTITCH. Second, STITCH incorporates baseline expres-\nsion patterns from tissues deposited in the Expression At-\nlas (34). Before augmenting the network with tissues data,\nusers have to choose if they want to use data from TISSUES\nor Expression Atlas. The TISSUES resource contains con-\nfidence levels ranging from one (lowest confidence) to five\n(highest confidence). Accordingly, on the STITCH website\nusers can select a tissue and a minimum confidence level.\nIn contrast, datasets from the Expression Atlas are trans-\nformed into percentiles. The confidence score for a protein–\nprotein interaction in the given tissue is then multiplied with\nthe geometric mean of the two proteins’ expression per-\ncentiles. For protein–chemical interactions, the confidence\nscore is multiplied with the protein’s expression percentile.\nTo access the tissue expression patterns, users can search for\ntissues either by typing parts of the tissue names or by select-\ning a tissue from a list. Then, users can submit the changed\nsettings to STITCH. In return, an updated network will be\nDownloaded from https://academic.oup.com/nar/article/44/D1/D380/2503089 by guest on 11 January 2026", "metadata": {"producer": "Acrobat Distiller 9.0.0 (Windows); modified using iTextSharp.LGPLv2.Core 3.7.4.0", "creator": "XMLPublish v.3.0 Copyright 1991-2015 Aptara Inc.", "creationdate": "2015-12-24T22:46:17-05:00", "source": "/home/donbr/open-biosciences/biosciences-research/data/raw/gkv1277.pdf", "file_path": "/home/donbr/open-biosciences/biosciences-research/data/raw/gkv1277.pdf", "total_pages": 5, "format": "PDF 1.4", "title": "", "author": "", "subject": "", "keywords": "", "moddate": "2026-01-11T01:04:03+00:00", "trapped": "", "modDate": "D:20260111010403+00'00'", "creationDate": "D:20151224224617-05'00'", "page": 1}}
{"page_content": "D382 Nucleic Acids Research, 2016, Vol. 44, Database issue\nFigure 1. Display of binding affinities. The user interface of STITCH has been updated and the option to scale edge width of protein–chemical interactions\naccording to binding affinity has been added. The shown network of multiple NSAIDs makes their different binding affinities clear: for example, aspirin\nhas relatively low binding affinities, whereas rofecoxib is specifically binding PTGS2.\nshown. As non-expressed nodes are removed (using TIS-\nSUES) or confidence values get updated (using Expression\nAtlas), other interaction partners may become part of the\nnetwork.\nUSE CASES\nSTITCH has been widely used for a variety of different\npurposes. These fall into three broad classes: (i) small- to\nmedium-scale analyses performed via the web interface, (ii)\nlarge-scale analyses that make use of the bulk download files\nand (iii) reuse of data from STITCH for development of new\nweb-based resources.\nWork by O’Reilly et al. on identifying potential drug\ntargets for \u00031-antitrypsin deficiency exemplifies the web-\nbased usage (35). Through a genome-wide RNAi screen in\na Caenorhabditis elegans disease model, the authors identi-\nfied 104 C. elegans genes of interest (having 85 human or-\nthologs). To validate these as potential drug targets, the au-\nthors queried STITCH and MetaCore for each of the hu-\nman proteins and thereby identified a compounds for use\nin follow-up experiments. Conversely, STITCH can also be\nqueried for a set of chemicals to identify possible targets,\nas exemplified by the screen by Kumar et al. of compounds\ncapable of altering intracellular manganese levels (36). The\nability to see binding affinities in the new web interface\nmakes STITCH 5 even better suited for such use cases than\nprevious versions.\nSTITCH is also commonly used for large-scale analy-\nses, which we facilitate by making the data available for\nbulk download. Ligeti et al. used these files to construct a\nnetwork neighborhood of proteins around each drug and\nshowed that the neighborhood overlap of two drugs can\nDownloaded from https://academic.oup.com/nar/article/44/D1/D380/2503089 by guest on 11 January 2026", "metadata": {"producer": "Acrobat Distiller 9.0.0 (Windows); modified using iTextSharp.LGPLv2.Core 3.7.4.0", "creator": "XMLPublish v.3.0 Copyright 1991-2015 Aptara Inc.", "creationdate": "2015-12-24T22:46:17-05:00", "source": "/home/donbr/open-biosciences/biosciences-research/data/raw/gkv1277.pdf", "file_path": "/home/donbr/open-biosciences/biosciences-research/data/raw/gkv1277.pdf", "total_pages": 5, "format": "PDF 1.4", "title": "", "author": "", "subject": "", "keywords": "", "moddate": "2026-01-11T01:04:03+00:00", "trapped": "", "modDate": "D:20260111010403+00'00'", "creationDate": "D:20151224224617-05'00'", "page": 2}}
{"page_content": "Nucleic Acids Research, 2016, Vol. 44, Database issue D383\nFigure 2. Filtering interaction networks according to tissue expression pat-\nterns. (A) The interaction network around diclofenac and PTGS1/2 is\nshown without filtering for tissue expression patterns. In this and the fol-\nlowing panels, the top five interaction partners with the highest scores are\nshown. (B) Using the TISSUES resource, only proteins believed to be ex-\npressed in blood platelets (with medium confidence, i.e. three stars in TIS-\nSUES) become part of the interaction network. For these settings, PTGS2\nis not expressed and is therefore shown in a lighter color. (C) Expression\npatterns according to RNA-seq data from the Human Protein Atlas are\nused to focus on genes expressed in smooth muscle. Confidence scores of\ninteractions are scaled by the geometric mean of the binding partners’ ex-\npression percentiles. Due to the recomputed confidence scores, four inter-\naction partners have been replaced by other proteins.\npredict synergy of drug combinations (37). On a related\nnote, Vogt et al. made use of both the drug thesaurus and\nthe protein–chemical interaction from STITCH to predict\ndrug contraindications (38).\nLast, but not least, the integrated data provided by\nSTITCH is useful to researchers who develop their own web\nresources and prediction methods. An example of this is the\nChemDIS resource, which combines the protein–chemical\ninteractions from STITCH with tools for gene enrichment\nanalysis to link chemicals via proteins to GO terms, path-\nways and diseases (39). The experimental protein–chemical\ninteractions from STITCH are also sometimes used as a\nbenchmark set when developing prediction methods as ex-\nemplified by Zhou et al. (40).\nACKNOWLEDGEMENT\nThe authors wish to thank Yan P. Yuan (EMBL) for his\noutstanding support with the STITCH servers and Rebeca\nQui˜nones (NNF Center for Protein Research) for help with\nmigrating the text-mining pipeline to a high-performance\ncompute centre.\nFUNDING\nNovo Nordisk Foundation [NNF14CC0001, in part]; Euro-\npean Molecular Biology Laboratory (EMBL, Heidelberg).\nFunding for open access charge: EMBL (Heidelberg).\nConflict of interest statement. None declared.\nREFERENCES\n1. Schwikowski,B., Uetz,P. and Fields,S. (2000) A network of\nprotein–protein interactions in yeast. Nat. Biotechnol., 18,\n1257–1261.\n2. Hishigaki,H., Nakai,K., Ono,T., Tanigami,A. and Takagi,T. (2001)\nAssessment of prediction accuracy of protein function from\nprotein-protein interaction data. Yeast, 18, 523–531.\n3. Sharan,R., Ulitsky,I. and Shamir,R. (2007) Network-based\nprediction of protein function. Mol. Syst. Biol., 3, 88.\n4. Barab´asi,A.-L., Gulbahce,N. and Loscalzo,J. (2011) Network\nmedicine: a network-based approach to human disease. Nat. Rev.\nGenet., 12, 56–68.\n5. Oti,M., Snel,B., Huynen,M.A. and Brunner,H.G. (2006) Predicting\ndisease genes using protein-protein interactions. J. Med. Genet., 43,\n691–698.\n6. Yang,K., Bai,H., Ouyang,Q., Lai,L. and Tang,C. (2008) Finding\nmultiple target optimal intervention in disease-related molecular\nnetwork. Mol. Syst. Biol., 4, 228.\n7. Hopkins,A.L. (2008) Network pharmacology: the next paradigm in\ndrug discovery. Nat. Chem. Biol., 4, 682–690.\n8. Hopkins,A.L. and Groom,C.R. (2002) The druggable genome. Nat.\nRev. Drug Discov., 1, 727–730.\n9. Kalinina,O.V., Wichmann,O., Apic,G. and Russell,R.B. (2012)\nProtChemSI: a network of protein–chemical structural interactions.\nNucleic Acids Res., 40, D549–D553.\n10. Reddy,A.S., Amarnath,H.S.D., Bapi,R.S., Sastry,G.M. and\nSastry,G.N. (2008) Protein ligand interaction database (PLID).\nComput. Biol. Chem., 32, 387–390.\n11. Wassermann,A.M. and Bajorath,J. (2011) BindingDB and\nChEMBL: online compound databases for drug discovery. Expert\nOpin. Drug Discov., 6, 683–687.\n12. Kanehisa,M., Goto,S., Sato,Y., Kawashima,M., Furumichi,M. and\nTanabe,M. (2014) Data, information, knowledge and principle: back\nto metabolism in KEGG. Nucleic Acids Res., 42, D199–D205.\n13. Chatr-Aryamontri,A., Breitkreutz,B.J., Heinicke,S., Boucher,L.,\nWinter,A., Stark,C., Nixon,J., Ramage,L., Kolas,N., O’Donnell,L.\nDownloaded from https://academic.oup.com/nar/article/44/D1/D380/2503089 by guest on 11 January 2026", "metadata": {"producer": "Acrobat Distiller 9.0.0 (Windows); modified using iTextSharp.LGPLv2.Core 3.7.4.0", "creator": "XMLPublish v.3.0 Copyright 1991-2015 Aptara Inc.", "creationdate": "2015-12-24T22:46:17-05:00", "source": "/home/donbr/open-biosciences/biosciences-research/data/raw/gkv1277.pdf", "file_path": "/home/donbr/open-biosciences/biosciences-research/data/raw/gkv1277.pdf", "total_pages": 5, "format": "PDF 1.4", "title": "", "author": "", "subject": "", "keywords": "", "moddate": "2026-01-11T01:04:03+00:00", "trapped": "", "modDate": "D:20260111010403+00'00'", "creationDate": "D:20151224224617-05'00'", "page": 3}}
{"page_content": "D384 Nucleic Acids Research, 2016, Vol. 44, Database issue\net al. (2013) The BioGRID interaction database: 2013 update.\nNucleic Acids Res., 41, D816–D823.\n14. Kjærulff,S.K., Wich,L., Kringelum,J., Jacobsen,U.P.,\nKouskoumvekaki,I., Audouze,K., Lund,O., Brunak,S., Oprea,T.I.\nand Taboureau,O. (2013) ChemProt-2.0: visual navigation in a\ndisease chemical biology database. Nucleic Acids Res., 41,\nD464–D469.\n15. Hopkins,A.L., Groom,C.R. and Alex,A. (2004) Ligand efficiency: a\nuseful metric for lead selection. Drug Discov. Today, 9, 430–431.\n16. Geeleher,P., Cox,N.J. and Huang,R.S. (2014) Clinical drug response\ncan be predicted using baseline gene expression levels and in vitro\ndrug sensitivity in cell lines. Genome Biol., 15, R47.\n17. Szklarczyk,D., Franceschini,A., Wyder,S., Forslund,K., Heller,D.,\nHuerta-Cepas,J., Simonovic,M., Roth,A., Santos,A., Tsafou,K.P.\net al. (2015) STRING v10: protein–protein interaction networks,\nintegrated over the tree of life. Nucleic Acids Res., 43, D447–D452.\n18. Kuhn,M., Szklarczyk,D., Pletscher-Frankild,S., Blicher,T.H., von\nMering,C., Jensen,L.J. and Bork,P. (2014) STITCH 4: integration of\nprotein–chemical interactions with user data. Nucleic Acids Res., 42,\nD401–D407.\n19. Law,V., Knox,C., Djoumbou,Y., Jewison,T., Guo,A.C., Liu,Y.,\nMaciejewski,A., Arndt,D., Wilson,M., Neveu,V. et al. (2014)\nDrugBank 4.0: shedding new light on drug metabolism. Nucleic\nAcids Res., 42, D1091–D1097.\n20. Okuno,Y., Tamon,A., Yabuuchi,H., Niijima,S., Minowa,Y.,\nTonomura,K., Kunimoto,R. and Feng,C. (2008) GLIDA:\nGPCR-ligand database for chemical genomics drug\ndiscovery––database and tools update. Nucleic Acids Res., 36,\nD907–D912.\n21. G¨unther,S., Kuhn,M., Dunkel,M., Campillos,M., Senger,C.,\nPetsalaki,E., Ahmed,J., Urdiales,E.G., Gewiess,A., Jensen,L.J. et al.\n(2008) SuperTarget and Matador: resources for exploring drug-target\nrelationships. Nucleic Acids Res., 36, D919–D922.\n22. Zhu,F., Shi,Z., Qin,C., Tao,L., Liu,X., Xu,F., Zhang,L., Song,Y.,\nLiu,X., Zhang,J. et al. (2012) Therapeutic target database update\n2012: a resource for facilitating target-oriented drug discovery.\nNucleic Acids Res., 40, D1128–D1136.\n23. Davis,A.P., Grondin,C.J., Lennon-Hopkins,K.,\nSaraceni-Richards,C., Sciaky,D., King,B.L., Wiegers,T.C. and\nMattingly,C.J. (2015) The Comparative Toxicogenomics Database’s\n10th year anniversary: update 2015. Nucleic Acids Res., 43,\nD914–D920.\n24. Schaefer,C.F., Anthony,K., Krupa,S., Buchoff,J., Day,M.,\nHannay,T. and Buetow,K.H. (2009) PID: the Pathway Interaction\nDatabase. Nucleic Acids Res., 37, D674–D679.\n25. Croft,D., Mundo,A.F., Haw,R., Milacic,M., Weiser,J., Wu,G.,\nCaudy,M., Garapati,P., Gillespie,M., Kamdar,M.R. et al. (2014) The\nReactome pathway knowledgebase. Nucleic Acids Res., 42,\nD472–D477.\n26. Caspi,R., Altman,T., Billington,R., Dreher,K., Foerster,H.,\nFulcher,C.A., Holland,T.A., Keseler,I.M., Kothari,A., Kubo,A.\net al. (2014) The MetaCyc database of metabolic pathways and\nenzymes and the BioCyc collection of Pathway/Genome Databases.\nNucleic Acids Res., 42, D459–D471.\n27. Bento,A.P., Gaulton,A., Hersey,A., Bellis,L.J., Chambers,J.,\nDavies,M., Kr¨uger,F.A., Light,Y., Mak,L., McGlinchey,S. et al.\n(2014) The ChEMBL bioactivity database: an update. Nucleic Acids\nRes., 42, D1083–D1090.\n28. Roth,B.L., Lopez,E., Patel,S. and Kroeze,W.K. (2000) The\nmultiplicity of serotonin receptors: uselessly diverse molecules or an\nembarrassment of riches? Neuroscientist, 6, 252–262.\n29. Rose,P.W., Prli´c,A., Bi,C., Bluhm,W.F., Christie,C.H., Dutta,S.,\nGreen,R.K., Goodsell,D.S., Westbrook,J.D., Woo,J. et al. (2015) The\nRCSB Protein Data Bank: views of structural biology for basic and\napplied research and education. Nucleic Acids Res., 43, D345–D356.\n30. Anastassiadis,T., Deacon,S.W., Devarajan,K., Ma,H. and\nPeterson,J.R. (2011) Comprehensive assay of kinase catalytic activity\nreveals features of kinase inhibitor selectivity. Nat. Biotechnol., 29,\n1039–1045.\n31. Davis,M.I., Hunt,J.P., Herrgard,S., Ciceri,P., Wodicka,L.M.,\nPallares,G., Hocker,M., Treiber,D.K. and Zarrinkar,P.P. (2011)\nComprehensive analysis of kinase inhibitor selectivity. Nat.\nBiotechnol., 29, 1046–1051.\n32. Franceschini,A., Szklarczyk,D., Frankild,S., Kuhn,M.,\nSimonovic,M., Roth,A., Lin,J., Minguez,P., Bork,P., von Mering,C.\net al. (2013) STRING v9.1: protein-protein interaction networks,\nwith increased coverage and integration. Nucleic Acids Res., 41,\nD808–D815.\n33. Santos,A., Tsafou,K., Stolte,C., Pletscher-Frankild,S.,\nO’Donoghue,S.I. and Jensen,L.J. (2015) Comprehensive comparison\nof large-scale tissue expression datasets. PeerJ, 3, e1054.\n34. Petryszak,R., Burdett,T., Fiorelli,B., Fonseca,N.A.,\nGonzalez-Porta,M., Hastings,E., Huber,W., Jupp,S., Keays,M.,\nKryvych,N. et al. (2014) Expression Atlas update–a database of gene\nand transcript expression from microarray- and sequencing-based\nfunctional genomics experiments. Nucleic Acids Res., 42,\nD926–D932.\n35. O’Reilly,L.P., Long,O.S., Cobanoglu,M.C., Benson,J.A., Luke,C.J.,\nMiedel,M.T., Hale,P., Perlmutter,D.H., Bahar,I., Silverman,G.A.\net al. (2014) A genome-wide RNAi screen identifies potential drug\ntargets in a C. elegans model of \u00031-antitrypsin deficiency. Hum. Mol.\nGenet., 23, 5123–5132.\n36. Kumar,K.K., Lowe,E.W. Jr, Aboud,A.A., Neely,M.D., Redha,R.,\nBauer,J.A., Odak,M., Weaver,C.D., Meiler,J., Aschner,M. et al.\n(2014) Cellular manganese content is developmentally regulated in\nhuman dopaminergic neurons. Sci. Rep., 4, 6801.\n37. Ligeti,B., P´enzv´alt´o,Z., Vera,R., Gy˝orffy,B. and Pongor,S. (2015) A\nnetwork-based target overlap score for characterizing drug\ncombinations: high correlation with cancer clinical trial results.\nPLoS One, 10, e0129267.\n38. Vogt,I., Prinz,J. and Campillos,M. (2014) Molecularly and clinically\nrelated drugs and diseases are enriched in phenotypically similar\ndrug-disease pairs. Genome Med., 6, 52.\n39. Tung,C.W. (2015) ChemDIS: a chemical-disease inference system\nbased on chemical-protein interactions. J. Cheminform., 7, 25.\n40. Zhou,H., Gao,M. and Skolnick,J. (2015) Comprehensive prediction\nof drug-protein interactions and side effects for the human proteome.\nSci. Rep., 5, 11090.\nDownloaded from https://academic.oup.com/nar/article/44/D1/D380/2503089 by guest on 11 January 2026", "metadata": {"producer": "Acrobat Distiller 9.0.0 (Windows); modified using iTextSharp.LGPLv2.Core 3.7.4.0", "creator": "XMLPublish v.3.0 Copyright 1991-2015 Aptara Inc.", "creationdate": "2015-12-24T22:46:17-05:00", "source": "/home/donbr/open-biosciences/biosciences-research/data/raw/gkv1277.pdf", "file_path": "/home/donbr/open-biosciences/biosciences-research/data/raw/gkv1277.pdf", "total_pages": 5, "format": "PDF 1.4", "title": "", "author": "", "subject": "", "keywords": "", "moddate": "2026-01-11T01:04:03+00:00", "trapped": "", "modDate": "D:20260111010403+00'00'", "creationDate": "D:20151224224617-05'00'", "page": 4}}
